{\rtf1\ansi\ansicpg936\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 => merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:17:14 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:17:14 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:17:14 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 0, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "input/init_models/4splitDomains.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-0.pth", "storage_path": "None", "save_storage_path": "outputs/2023-09-14-06:17:11/4splitDomains/storage-0.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:17:14 4splitDomains](trainer.py 92): INFO => Load model weights: input/init_models/4splitDomains.pth\
[2023-09-14 06:17:15 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:17:16 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:17:16 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:17:16 4splitDomains](iBatchLearn.py 92): INFO ====================== 0 =======================\
[2023-09-14 06:17:16 4splitDomains](regularization.py 45): INFO reg_term: , 0\
[2023-09-14 06:17:16 4splitDomains](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 06:17:16 4splitDomains](trainer.py 223): INFO Epoch:0\
[2023-09-14 06:17:16 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:17:16 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:17:19 4splitDomains](trainer.py 286): INFO [0/46]	3.1532(3.1532)	0.7165(0.7165)	4.096(4.096)	0.00(0.00)\
[2023-09-14 06:17:21 4splitDomains](trainer.py 286): INFO [10/46]	0.1582(0.4318)	0.0004(0.0668)	4.007(4.069)	3.12(2.41)\
[2023-09-14 06:17:23 4splitDomains](trainer.py 286): INFO [20/46]	0.1585(0.3025)	0.0008(0.0358)	3.759(3.978)	14.06(6.40)\
[2023-09-14 06:17:24 4splitDomains](trainer.py 286): INFO [30/46]	0.1585(0.2566)	0.0006(0.0244)	3.474(3.875)	35.94(12.85)\
[2023-09-14 06:17:26 4splitDomains](trainer.py 286): INFO [40/46]	0.1578(0.2329)	0.0002(0.0188)	3.297(3.763)	43.75(19.47)\
[2023-09-14 06:17:27 4splitDomains](trainer.py 286): INFO [45/46]	0.1735(0.2251)	0.0001(0.0168)	3.472(3.711)	15.79(21.94)\
[2023-09-14 06:17:27 4splitDomains](trainer.py 288): INFO  * Train Acc 21.939\
[2023-09-14 06:17:28 4splitDomains](trainer.py 147): INFO  * Val Acc 48.806, Total time 1.22\
[2023-09-14 06:17:28 4splitDomains](trainer.py 223): INFO Epoch:1\
[2023-09-14 06:17:28 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:17:28 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:17:29 4splitDomains](trainer.py 286): INFO [0/46]	0.8546(0.8546)	0.6968(0.6968)	2.914(2.914)	57.81(57.81)\
[2023-09-14 06:17:30 4splitDomains](trainer.py 286): INFO [10/46]	0.1592(0.2255)	0.0008(0.0672)	2.698(2.940)	54.69(49.86)\
[2023-09-14 06:17:32 4splitDomains](trainer.py 286): INFO [20/46]	0.1603(0.1939)	0.0006(0.0355)	2.462(2.772)	59.38(54.76)\
[2023-09-14 06:17:34 4splitDomains](trainer.py 286): INFO [30/46]	0.1591(0.1829)	0.0007(0.0242)	2.308(2.641)	60.94(56.35)\
[2023-09-14 06:17:35 4splitDomains](trainer.py 286): INFO [40/46]	0.1582(0.1772)	0.0003(0.0185)	1.823(2.520)	73.44(58.00)\
[2023-09-14 06:17:36 4splitDomains](trainer.py 286): INFO [45/46]	0.0517(0.1728)	0.0001(0.0165)	1.972(2.472)	68.42(58.74)\
[2023-09-14 06:17:36 4splitDomains](trainer.py 288): INFO  * Train Acc 58.744\
[2023-09-14 06:17:37 4splitDomains](trainer.py 147): INFO  * Val Acc 64.987, Total time 1.24\
[2023-09-14 06:17:37 4splitDomains](trainer.py 223): INFO Epoch:2\
[2023-09-14 06:17:37 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:17:37 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:17:38 4splitDomains](trainer.py 286): INFO [0/46]	0.7885(0.7885)	0.6259(0.6259)	1.914(1.914)	65.62(65.62)\
[2023-09-14 06:17:40 4splitDomains](trainer.py 286): INFO [10/46]	0.1587(0.2171)	0.0005(0.0574)	1.731(1.828)	75.00(69.46)\
[2023-09-14 06:17:41 4splitDomains](trainer.py 286): INFO [20/46]	0.1647(0.1901)	0.0005(0.0306)	1.583(1.709)	78.12(71.06)\
[2023-09-14 06:17:43 4splitDomains](trainer.py 286): INFO [30/46]	0.1631(0.1801)	0.0053(0.0210)	1.398(1.636)	79.69(72.73)\
[2023-09-14 06:17:44 4splitDomains](trainer.py 286): INFO [40/46]	0.1574(0.1750)	0.0003(0.0161)	1.238(1.567)	78.12(74.09)\
[2023-09-14 06:17:45 4splitDomains](trainer.py 286): INFO [45/46]	0.0510(0.1708)	0.0001(0.0144)	1.642(1.550)	78.95(74.27)\
[2023-09-14 06:17:45 4splitDomains](trainer.py 288): INFO  * Train Acc 74.267\
[2023-09-14 06:17:46 4splitDomains](trainer.py 147): INFO  * Val Acc 73.210, Total time 1.26\
[2023-09-14 06:17:46 4splitDomains](trainer.py 223): INFO Epoch:3\
[2023-09-14 06:17:46 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:17:46 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:17:47 4splitDomains](trainer.py 286): INFO [0/46]	0.9382(0.9382)	0.7799(0.7799)	1.420(1.420)	71.88(71.88)\
[2023-09-14 06:17:49 4splitDomains](trainer.py 286): INFO [10/46]	0.1578(0.2306)	0.0003(0.0724)	1.171(1.124)	82.81(83.10)\
[2023-09-14 06:17:50 4splitDomains](trainer.py 286): INFO [20/46]	0.1580(0.1961)	0.0005(0.0382)	0.958(1.127)	82.81(81.92)\
[2023-09-14 06:17:52 4splitDomains](trainer.py 286): INFO [30/46]	0.1579(0.1839)	0.0003(0.0261)	1.067(1.094)	79.69(82.21)\
[2023-09-14 06:17:54 4splitDomains](trainer.py 286): INFO [40/46]	0.1571(0.1776)	0.0001(0.0199)	0.938(1.058)	82.81(82.66)\
[2023-09-14 06:17:54 4splitDomains](trainer.py 286): INFO [45/46]	0.0517(0.1731)	0.0002(0.0178)	1.176(1.038)	73.68(82.93)\
[2023-09-14 06:17:54 4splitDomains](trainer.py 288): INFO  * Train Acc 82.925\
[2023-09-14 06:17:56 4splitDomains](trainer.py 147): INFO  * Val Acc 81.167, Total time 1.24\
[2023-09-14 06:17:56 4splitDomains](trainer.py 223): INFO Epoch:4\
[2023-09-14 06:17:56 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:17:56 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:17:56 4splitDomains](trainer.py 286): INFO [0/46]	0.8113(0.8113)	0.6535(0.6535)	0.896(0.896)	87.50(87.50)\
[2023-09-14 06:17:58 4splitDomains](trainer.py 286): INFO [10/46]	0.1578(0.2215)	0.0003(0.0638)	0.715(0.745)	85.94(88.35)\
[2023-09-14 06:18:00 4splitDomains](trainer.py 286): INFO [20/46]	0.1585(0.1925)	0.0004(0.0339)	0.832(0.764)	89.06(87.57)\
[2023-09-14 06:18:01 4splitDomains](trainer.py 286): INFO [30/46]	0.1585(0.1817)	0.0008(0.0233)	0.695(0.769)	85.94(87.35)\
[2023-09-14 06:18:03 4splitDomains](trainer.py 286): INFO [40/46]	0.1579(0.1764)	0.0002(0.0178)	0.776(0.750)	87.50(88.00)\
[2023-09-14 06:18:04 4splitDomains](trainer.py 286): INFO [45/46]	0.0515(0.1721)	0.0002(0.0159)	1.070(0.746)	84.21(88.13)\
[2023-09-14 06:18:04 4splitDomains](trainer.py 288): INFO  * Train Acc 88.134\
[2023-09-14 06:18:05 4splitDomains](trainer.py 147): INFO  * Val Acc 85.146, Total time 1.42\
[2023-09-14 06:18:05 4splitDomains](trainer.py 223): INFO Epoch:5\
[2023-09-14 06:18:05 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:18:05 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:18:06 4splitDomains](trainer.py 286): INFO [0/46]	0.8292(0.8292)	0.6704(0.6704)	0.577(0.577)	92.19(92.19)\
[2023-09-14 06:18:08 4splitDomains](trainer.py 286): INFO [10/46]	0.1587(0.2237)	0.0005(0.0655)	0.392(0.578)	96.88(91.05)\
[2023-09-14 06:18:09 4splitDomains](trainer.py 286): INFO [20/46]	0.1589(0.1932)	0.0004(0.0347)	0.642(0.568)	90.62(91.52)\
[2023-09-14 06:18:11 4splitDomains](trainer.py 286): INFO [30/46]	0.1595(0.1824)	0.0009(0.0239)	0.561(0.560)	90.62(91.43)\
[2023-09-14 06:18:12 4splitDomains](trainer.py 286): INFO [40/46]	0.1582(0.1767)	0.0002(0.0183)	0.466(0.563)	89.06(90.89)\
[2023-09-14 06:18:13 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1724)	0.0002(0.0164)	0.569(0.564)	94.74(90.96)\
[2023-09-14 06:18:13 4splitDomains](trainer.py 288): INFO  * Train Acc 90.962\
[2023-09-14 06:18:14 4splitDomains](trainer.py 147): INFO  * Val Acc 87.268, Total time 1.27\
[2023-09-14 06:18:14 4splitDomains](trainer.py 223): INFO Epoch:6\
[2023-09-14 06:18:14 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:18:14 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:18:15 4splitDomains](trainer.py 286): INFO [0/46]	0.8121(0.8121)	0.6521(0.6521)	0.484(0.484)	95.31(95.31)\
[2023-09-14 06:18:17 4splitDomains](trainer.py 286): INFO [10/46]	0.1603(0.2258)	0.0005(0.0652)	0.598(0.478)	87.50(93.04)\
[2023-09-14 06:18:18 4splitDomains](trainer.py 286): INFO [20/46]	0.1583(0.1940)	0.0004(0.0347)	0.439(0.451)	92.19(93.53)\
[2023-09-14 06:18:20 4splitDomains](trainer.py 286): INFO [30/46]	0.1579(0.1828)	0.0005(0.0237)	0.276(0.453)	95.31(92.74)\
[2023-09-14 06:18:22 4splitDomains](trainer.py 286): INFO [40/46]	0.1578(0.1772)	0.0003(0.0183)	0.543(0.454)	89.06(92.64)\
[2023-09-14 06:18:22 4splitDomains](trainer.py 286): INFO [45/46]	0.0511(0.1727)	0.0001(0.0163)	0.594(0.453)	89.47(92.51)\
[2023-09-14 06:18:22 4splitDomains](trainer.py 288): INFO  * Train Acc 92.515\
[2023-09-14 06:18:24 4splitDomains](trainer.py 147): INFO  * Val Acc 88.859, Total time 1.26\
[2023-09-14 06:18:24 4splitDomains](trainer.py 223): INFO Epoch:7\
[2023-09-14 06:18:24 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:18:24 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:18:24 4splitDomains](trainer.py 286): INFO [0/46]	0.8517(0.8517)	0.6934(0.6934)	0.417(0.417)	93.75(93.75)\
[2023-09-14 06:18:26 4splitDomains](trainer.py 286): INFO [10/46]	0.1581(0.2245)	0.0003(0.0666)	0.604(0.384)	85.94(94.32)\
[2023-09-14 06:18:28 4splitDomains](trainer.py 286): INFO [20/46]	0.1583(0.1935)	0.0004(0.0351)	0.428(0.383)	93.75(94.12)\
[2023-09-14 06:18:29 4splitDomains](trainer.py 286): INFO [30/46]	0.1582(0.1824)	0.0004(0.0240)	0.267(0.368)	93.75(94.25)\
[2023-09-14 06:18:31 4splitDomains](trainer.py 286): INFO [40/46]	0.1577(0.1766)	0.0002(0.0183)	0.259(0.360)	96.88(94.32)\
[2023-09-14 06:18:32 4splitDomains](trainer.py 286): INFO [45/46]	0.0510(0.1722)	0.0001(0.0163)	0.657(0.360)	89.47(94.27)\
[2023-09-14 06:18:32 4splitDomains](trainer.py 288): INFO  * Train Acc 94.274\
[2023-09-14 06:18:33 4splitDomains](trainer.py 147): INFO  * Val Acc 89.920, Total time 1.26\
[2023-09-14 06:18:33 4splitDomains](trainer.py 223): INFO Epoch:8\
[2023-09-14 06:18:33 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:18:33 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:18:34 4splitDomains](trainer.py 286): INFO [0/46]	0.7882(0.7882)	0.6301(0.6301)	0.370(0.370)	98.44(98.44)\
[2023-09-14 06:18:35 4splitDomains](trainer.py 286): INFO [10/46]	0.1586(0.2210)	0.0005(0.0627)	0.382(0.299)	93.75(96.31)\
[2023-09-14 06:18:37 4splitDomains](trainer.py 286): INFO [20/46]	0.1583(0.1929)	0.0004(0.0346)	0.303(0.306)	96.88(95.68)\
[2023-09-14 06:18:38 4splitDomains](trainer.py 286): INFO [30/46]	0.1610(0.1820)	0.0006(0.0238)	0.299(0.305)	95.31(95.72)\
[2023-09-14 06:18:40 4splitDomains](trainer.py 286): INFO [40/46]	0.1586(0.1765)	0.0004(0.0182)	0.293(0.308)	98.44(95.50)\
[2023-09-14 06:18:41 4splitDomains](trainer.py 286): INFO [45/46]	0.0511(0.1722)	0.0001(0.0163)	0.498(0.308)	84.21(95.45)\
[2023-09-14 06:18:41 4splitDomains](trainer.py 288): INFO  * Train Acc 95.447\
[2023-09-14 06:18:42 4splitDomains](trainer.py 147): INFO  * Val Acc 90.186, Total time 1.27\
[2023-09-14 06:18:42 4splitDomains](trainer.py 223): INFO Epoch:9\
[2023-09-14 06:18:42 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:18:42 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:18:43 4splitDomains](trainer.py 286): INFO [0/46]	0.8364(0.8364)	0.6784(0.6784)	0.243(0.243)	98.44(98.44)\
[2023-09-14 06:18:45 4splitDomains](trainer.py 286): INFO [10/46]	0.1583(0.2223)	0.0004(0.0639)	0.204(0.251)	98.44(96.16)\
[2023-09-14 06:18:46 4splitDomains](trainer.py 286): INFO [20/46]	0.1589(0.1920)	0.0006(0.0338)	0.239(0.263)	98.44(96.28)\
[2023-09-14 06:18:48 4splitDomains](trainer.py 286): INFO [30/46]	0.1586(0.1813)	0.0006(0.0231)	0.238(0.255)	96.88(96.62)\
[2023-09-14 06:18:49 4splitDomains](trainer.py 286): INFO [40/46]	0.1585(0.1761)	0.0003(0.0177)	0.240(0.258)	98.44(96.42)\
[2023-09-14 06:18:50 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1718)	0.0001(0.0158)	0.270(0.254)	94.74(96.48)\
[2023-09-14 06:18:50 4splitDomains](trainer.py 288): INFO  * Train Acc 96.482\
[2023-09-14 06:18:51 4splitDomains](trainer.py 147): INFO  * Val Acc 89.920, Total time 1.24\
[2023-09-14 06:18:51 4splitDomains](trainer.py 223): INFO Epoch:10\
[2023-09-14 06:18:51 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:18:51 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:18:52 4splitDomains](trainer.py 286): INFO [0/46]	0.8064(0.8064)	0.6466(0.6466)	0.256(0.256)	93.75(93.75)\
[2023-09-14 06:18:54 4splitDomains](trainer.py 286): INFO [10/46]	0.1639(0.2231)	0.0007(0.0645)	0.249(0.221)	95.31(97.30)\
[2023-09-14 06:18:55 4splitDomains](trainer.py 286): INFO [20/46]	0.1588(0.1934)	0.0004(0.0343)	0.262(0.220)	96.88(97.10)\
[2023-09-14 06:18:57 4splitDomains](trainer.py 286): INFO [30/46]	0.1588(0.1827)	0.0004(0.0235)	0.168(0.210)	96.88(97.33)\
[2023-09-14 06:18:59 4splitDomains](trainer.py 286): INFO [40/46]	0.1582(0.1770)	0.0003(0.0179)	0.243(0.218)	92.19(97.14)\
[2023-09-14 06:18:59 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1727)	0.0001(0.0160)	0.271(0.215)	94.74(97.17)\
[2023-09-14 06:18:59 4splitDomains](trainer.py 288): INFO  * Train Acc 97.171\
[2023-09-14 06:19:01 4splitDomains](trainer.py 147): INFO  * Val Acc 90.716, Total time 1.22\
[2023-09-14 06:19:01 4splitDomains](trainer.py 223): INFO Epoch:11\
[2023-09-14 06:19:01 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:19:01 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:19:01 4splitDomains](trainer.py 286): INFO [0/46]	0.8560(0.8560)	0.6983(0.6983)	0.142(0.142)	98.44(98.44)\
[2023-09-14 06:19:03 4splitDomains](trainer.py 286): INFO [10/46]	0.1591(0.2234)	0.0004(0.0644)	0.127(0.169)	100.00(98.15)\
[2023-09-14 06:19:05 4splitDomains](trainer.py 286): INFO [20/46]	0.1591(0.1928)	0.0004(0.0340)	0.161(0.177)	98.44(98.07)\
[2023-09-14 06:19:06 4splitDomains](trainer.py 286): INFO [30/46]	0.1589(0.1820)	0.0005(0.0232)	0.137(0.181)	100.00(97.98)\
[2023-09-14 06:19:08 4splitDomains](trainer.py 286): INFO [40/46]	0.1582(0.1763)	0.0003(0.0177)	0.132(0.184)	98.44(97.90)\
[2023-09-14 06:19:08 4splitDomains](trainer.py 286): INFO [45/46]	0.0517(0.1720)	0.0002(0.0158)	0.186(0.180)	100.00(97.93)\
[2023-09-14 06:19:09 4splitDomains](trainer.py 288): INFO  * Train Acc 97.930\
[2023-09-14 06:19:10 4splitDomains](trainer.py 147): INFO  * Val Acc 90.451, Total time 1.35\
[2023-09-14 06:19:10 4splitDomains](trainer.py 223): INFO Epoch:12\
[2023-09-14 06:19:10 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:19:10 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:19:11 4splitDomains](trainer.py 286): INFO [0/46]	0.8451(0.8451)	0.6871(0.6871)	0.126(0.126)	98.44(98.44)\
[2023-09-14 06:19:12 4splitDomains](trainer.py 286): INFO [10/46]	0.1611(0.2231)	0.0007(0.0646)	0.207(0.163)	96.88(98.30)\
[2023-09-14 06:19:14 4splitDomains](trainer.py 286): INFO [20/46]	0.1587(0.1925)	0.0004(0.0341)	0.126(0.157)	98.44(98.51)\
[2023-09-14 06:19:16 4splitDomains](trainer.py 286): INFO [30/46]	0.1595(0.1820)	0.0009(0.0234)	0.114(0.161)	100.00(98.14)\
[2023-09-14 06:19:17 4splitDomains](trainer.py 286): INFO [40/46]	0.1585(0.1767)	0.0002(0.0179)	0.175(0.159)	96.88(98.29)\
[2023-09-14 06:19:18 4splitDomains](trainer.py 286): INFO [45/46]	0.0514(0.1724)	0.0002(0.0160)	0.336(0.163)	89.47(98.07)\
[2023-09-14 06:19:18 4splitDomains](trainer.py 288): INFO  * Train Acc 98.068\
[2023-09-14 06:19:19 4splitDomains](trainer.py 147): INFO  * Val Acc 90.186, Total time 1.25\
[2023-09-14 06:19:19 4splitDomains](trainer.py 223): INFO Epoch:13\
[2023-09-14 06:19:19 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:19:19 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:19:20 4splitDomains](trainer.py 286): INFO [0/46]	0.8200(0.8200)	0.6618(0.6618)	0.178(0.178)	96.88(96.88)\
[2023-09-14 06:19:22 4splitDomains](trainer.py 286): INFO [10/46]	0.1593(0.2197)	0.0008(0.0611)	0.146(0.148)	98.44(98.30)\
[2023-09-14 06:19:23 4splitDomains](trainer.py 286): INFO [20/46]	0.1586(0.1909)	0.0004(0.0323)	0.074(0.144)	100.00(98.51)\
[2023-09-14 06:19:25 4splitDomains](trainer.py 286): INFO [30/46]	0.1589(0.1813)	0.0004(0.0221)	0.115(0.140)	98.44(98.44)\
[2023-09-14 06:19:26 4splitDomains](trainer.py 286): INFO [40/46]	0.1584(0.1764)	0.0003(0.0169)	0.123(0.143)	96.88(98.29)\
[2023-09-14 06:19:27 4splitDomains](trainer.py 286): INFO [45/46]	0.0514(0.1721)	0.0002(0.0151)	0.476(0.145)	100.00(98.38)\
[2023-09-14 06:19:27 4splitDomains](trainer.py 288): INFO  * Train Acc 98.379\
[2023-09-14 06:19:28 4splitDomains](trainer.py 147): INFO  * Val Acc 91.247, Total time 1.23\
[2023-09-14 06:19:28 4splitDomains](trainer.py 223): INFO Epoch:14\
[2023-09-14 06:19:28 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:19:28 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:19:29 4splitDomains](trainer.py 286): INFO [0/46]	0.7884(0.7884)	0.6304(0.6304)	0.091(0.091)	100.00(100.00)\
[2023-09-14 06:19:31 4splitDomains](trainer.py 286): INFO [10/46]	0.1589(0.2184)	0.0005(0.0595)	0.155(0.133)	98.44(98.86)\
[2023-09-14 06:19:32 4splitDomains](trainer.py 286): INFO [20/46]	0.1589(0.1926)	0.0004(0.0337)	0.185(0.130)	98.44(98.74)\
[2023-09-14 06:19:34 4splitDomains](trainer.py 286): INFO [30/46]	0.1591(0.1818)	0.0005(0.0231)	0.081(0.127)	100.00(98.69)\
[2023-09-14 06:19:36 4splitDomains](trainer.py 286): INFO [40/46]	0.1586(0.1765)	0.0003(0.0176)	0.112(0.123)	98.44(98.74)\
[2023-09-14 06:19:36 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1722)	0.0001(0.0157)	0.388(0.125)	94.74(98.79)\
[2023-09-14 06:19:36 4splitDomains](trainer.py 288): INFO  * Train Acc 98.793\
[2023-09-14 06:19:38 4splitDomains](trainer.py 147): INFO  * Val Acc 90.186, Total time 1.42\
[2023-09-14 06:19:38 4splitDomains](trainer.py 223): INFO Epoch:15\
[2023-09-14 06:19:38 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:19:38 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:19:39 4splitDomains](trainer.py 286): INFO [0/46]	0.9049(0.9049)	0.7469(0.7469)	0.106(0.106)	98.44(98.44)\
[2023-09-14 06:19:40 4splitDomains](trainer.py 286): INFO [10/46]	0.1656(0.2310)	0.0069(0.0724)	0.108(0.112)	98.44(99.15)\
[2023-09-14 06:19:42 4splitDomains](trainer.py 286): INFO [20/46]	0.1591(0.1970)	0.0006(0.0382)	0.084(0.113)	100.00(98.88)\
[2023-09-14 06:19:44 4splitDomains](trainer.py 286): INFO [30/46]	0.1606(0.1852)	0.0023(0.0262)	0.074(0.110)	100.00(98.84)\
[2023-09-14 06:19:45 4splitDomains](trainer.py 286): INFO [40/46]	0.1582(0.1789)	0.0002(0.0200)	0.123(0.109)	98.44(98.89)\
[2023-09-14 06:19:46 4splitDomains](trainer.py 286): INFO [45/46]	0.0514(0.1743)	0.0002(0.0178)	0.301(0.109)	94.74(98.93)\
[2023-09-14 06:19:46 4splitDomains](trainer.py 288): INFO  * Train Acc 98.931\
[2023-09-14 06:19:47 4splitDomains](trainer.py 147): INFO  * Val Acc 90.716, Total time 1.24\
[2023-09-14 06:19:47 4splitDomains](trainer.py 223): INFO Epoch:16\
[2023-09-14 06:19:47 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:19:47 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:19:48 4splitDomains](trainer.py 286): INFO [0/46]	0.8611(0.8611)	0.7029(0.7029)	0.117(0.117)	98.44(98.44)\
[2023-09-14 06:19:50 4splitDomains](trainer.py 286): INFO [10/46]	0.1594(0.2270)	0.0003(0.0676)	0.059(0.095)	100.00(99.29)\
[2023-09-14 06:19:51 4splitDomains](trainer.py 286): INFO [20/46]	0.1588(0.1953)	0.0004(0.0357)	0.134(0.091)	98.44(99.40)\
[2023-09-14 06:19:53 4splitDomains](trainer.py 286): INFO [30/46]	0.1587(0.1842)	0.0004(0.0244)	0.129(0.101)	98.44(99.19)\
[2023-09-14 06:19:54 4splitDomains](trainer.py 286): INFO [40/46]	0.1580(0.1781)	0.0001(0.0186)	0.070(0.095)	100.00(99.35)\
[2023-09-14 06:19:55 4splitDomains](trainer.py 286): INFO [45/46]	0.0513(0.1736)	0.0002(0.0166)	0.209(0.095)	100.00(99.31)\
[2023-09-14 06:19:55 4splitDomains](trainer.py 288): INFO  * Train Acc 99.310\
[2023-09-14 06:19:56 4splitDomains](trainer.py 147): INFO  * Val Acc 91.247, Total time 1.23\
[2023-09-14 06:19:56 4splitDomains](trainer.py 223): INFO Epoch:17\
[2023-09-14 06:19:56 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:19:56 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:19:57 4splitDomains](trainer.py 286): INFO [0/46]	0.8105(0.8105)	0.6527(0.6527)	0.061(0.061)	100.00(100.00)\
[2023-09-14 06:19:59 4splitDomains](trainer.py 286): INFO [10/46]	0.1594(0.2187)	0.0006(0.0603)	0.057(0.085)	100.00(99.29)\
[2023-09-14 06:20:00 4splitDomains](trainer.py 286): INFO [20/46]	0.1594(0.1904)	0.0008(0.0319)	0.065(0.081)	100.00(99.40)\
[2023-09-14 06:20:02 4splitDomains](trainer.py 286): INFO [30/46]	0.1597(0.1806)	0.0004(0.0218)	0.072(0.083)	100.00(99.34)\
[2023-09-14 06:20:04 4splitDomains](trainer.py 286): INFO [40/46]	0.1589(0.1757)	0.0002(0.0168)	0.075(0.085)	100.00(99.24)\
[2023-09-14 06:20:04 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1715)	0.0001(0.0150)	0.106(0.084)	100.00(99.28)\
[2023-09-14 06:20:04 4splitDomains](trainer.py 288): INFO  * Train Acc 99.276\
[2023-09-14 06:20:06 4splitDomains](trainer.py 147): INFO  * Val Acc 90.981, Total time 1.27\
[2023-09-14 06:20:06 4splitDomains](trainer.py 223): INFO Epoch:18\
[2023-09-14 06:20:06 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:20:06 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:20:07 4splitDomains](trainer.py 286): INFO [0/46]	0.8307(0.8307)	0.6694(0.6694)	0.059(0.059)	100.00(100.00)\
[2023-09-14 06:20:08 4splitDomains](trainer.py 286): INFO [10/46]	0.1596(0.2207)	0.0007(0.0622)	0.042(0.067)	100.00(99.72)\
[2023-09-14 06:20:10 4splitDomains](trainer.py 286): INFO [20/46]	0.1588(0.1914)	0.0006(0.0329)	0.045(0.072)	100.00(99.63)\
[2023-09-14 06:20:11 4splitDomains](trainer.py 286): INFO [30/46]	0.1618(0.1811)	0.0004(0.0225)	0.086(0.071)	98.44(99.55)\
[2023-09-14 06:20:13 4splitDomains](trainer.py 286): INFO [40/46]	0.1588(0.1760)	0.0005(0.0172)	0.079(0.072)	100.00(99.47)\
[2023-09-14 06:20:14 4splitDomains](trainer.py 286): INFO [45/46]	0.0522(0.1718)	0.0001(0.0153)	0.389(0.076)	89.47(99.34)\
[2023-09-14 06:20:14 4splitDomains](trainer.py 288): INFO  * Train Acc 99.345\
[2023-09-14 06:20:15 4splitDomains](trainer.py 147): INFO  * Val Acc 92.042, Total time 1.22\
[2023-09-14 06:20:15 4splitDomains](trainer.py 223): INFO Epoch:19\
[2023-09-14 06:20:15 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:20:15 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:20:16 4splitDomains](trainer.py 286): INFO [0/46]	0.8086(0.8086)	0.6439(0.6439)	0.065(0.065)	100.00(100.00)\
[2023-09-14 06:20:17 4splitDomains](trainer.py 286): INFO [10/46]	0.1718(0.2212)	0.0006(0.0606)	0.086(0.069)	100.00(99.57)\
[2023-09-14 06:20:19 4splitDomains](trainer.py 286): INFO [20/46]	0.1592(0.1919)	0.0003(0.0321)	0.059(0.065)	100.00(99.55)\
[2023-09-14 06:20:21 4splitDomains](trainer.py 286): INFO [30/46]	0.1597(0.1814)	0.0008(0.0220)	0.047(0.067)	100.00(99.55)\
[2023-09-14 06:20:22 4splitDomains](trainer.py 286): INFO [40/46]	0.1586(0.1763)	0.0004(0.0170)	0.066(0.066)	100.00(99.66)\
[2023-09-14 06:20:23 4splitDomains](trainer.py 286): INFO [45/46]	0.0519(0.1720)	0.0002(0.0152)	0.284(0.067)	94.74(99.55)\
[2023-09-14 06:20:23 4splitDomains](trainer.py 288): INFO  * Train Acc 99.552\
[2023-09-14 06:20:24 4splitDomains](trainer.py 147): INFO  * Val Acc 92.308, Total time 1.25\
[2023-09-14 06:20:24 4splitDomains](trainer.py 223): INFO Epoch:20\
[2023-09-14 06:20:24 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:20:24 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:20:25 4splitDomains](trainer.py 286): INFO [0/46]	0.7944(0.7944)	0.6358(0.6358)	0.084(0.084)	98.44(98.44)\
[2023-09-14 06:20:27 4splitDomains](trainer.py 286): INFO [10/46]	0.1597(0.2202)	0.0012(0.0616)	0.045(0.064)	100.00(99.43)\
[2023-09-14 06:20:28 4splitDomains](trainer.py 286): INFO [20/46]	0.1587(0.1919)	0.0004(0.0326)	0.086(0.067)	98.44(99.33)\
[2023-09-14 06:20:30 4splitDomains](trainer.py 286): INFO [30/46]	0.1596(0.1816)	0.0005(0.0223)	0.048(0.063)	100.00(99.50)\
[2023-09-14 06:20:31 4splitDomains](trainer.py 286): INFO [40/46]	0.1590(0.1763)	0.0002(0.0170)	0.076(0.065)	98.44(99.43)\
[2023-09-14 06:20:32 4splitDomains](trainer.py 286): INFO [45/46]	0.0514(0.1720)	0.0002(0.0152)	0.143(0.065)	100.00(99.48)\
[2023-09-14 06:20:32 4splitDomains](trainer.py 288): INFO  * Train Acc 99.483\
[2023-09-14 06:20:33 4splitDomains](trainer.py 147): INFO  * Val Acc 92.308, Total time 1.25\
[2023-09-14 06:20:33 4splitDomains](trainer.py 223): INFO Epoch:21\
[2023-09-14 06:20:33 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:20:33 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:20:34 4splitDomains](trainer.py 286): INFO [0/46]	0.8686(0.8686)	0.7106(0.7106)	0.061(0.061)	100.00(100.00)\
[2023-09-14 06:20:36 4splitDomains](trainer.py 286): INFO [10/46]	0.1592(0.2276)	0.0004(0.0686)	0.070(0.055)	98.44(99.43)\
[2023-09-14 06:20:37 4splitDomains](trainer.py 286): INFO [20/46]	0.1588(0.1952)	0.0006(0.0362)	0.049(0.054)	100.00(99.63)\
[2023-09-14 06:20:39 4splitDomains](trainer.py 286): INFO [30/46]	0.1590(0.1836)	0.0005(0.0248)	0.050(0.051)	100.00(99.75)\
[2023-09-14 06:20:41 4splitDomains](trainer.py 286): INFO [40/46]	0.1586(0.1777)	0.0003(0.0189)	0.053(0.052)	100.00(99.77)\
[2023-09-14 06:20:41 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1733)	0.0001(0.0169)	0.108(0.053)	100.00(99.69)\
[2023-09-14 06:20:41 4splitDomains](trainer.py 288): INFO  * Train Acc 99.690\
[2023-09-14 06:20:43 4splitDomains](trainer.py 147): INFO  * Val Acc 92.573, Total time 1.29\
[2023-09-14 06:20:43 4splitDomains](trainer.py 223): INFO Epoch:22\
[2023-09-14 06:20:43 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:20:43 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:20:44 4splitDomains](trainer.py 286): INFO [0/46]	0.8171(0.8171)	0.6586(0.6586)	0.052(0.052)	100.00(100.00)\
[2023-09-14 06:20:45 4splitDomains](trainer.py 286): INFO [10/46]	0.1589(0.2210)	0.0002(0.0623)	0.038(0.046)	100.00(100.00)\
[2023-09-14 06:20:47 4splitDomains](trainer.py 286): INFO [20/46]	0.1590(0.1915)	0.0005(0.0329)	0.043(0.046)	100.00(100.00)\
[2023-09-14 06:20:48 4splitDomains](trainer.py 286): INFO [30/46]	0.1687(0.1815)	0.0008(0.0225)	0.046(0.045)	100.00(99.95)\
[2023-09-14 06:20:50 4splitDomains](trainer.py 286): INFO [40/46]	0.1606(0.1761)	0.0026(0.0173)	0.053(0.047)	100.00(99.92)\
[2023-09-14 06:20:51 4splitDomains](trainer.py 286): INFO [45/46]	0.0514(0.1718)	0.0002(0.0154)	0.042(0.047)	100.00(99.86)\
[2023-09-14 06:20:51 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862\
[2023-09-14 06:20:52 4splitDomains](trainer.py 147): INFO  * Val Acc 92.308, Total time 1.22\
[2023-09-14 06:20:52 4splitDomains](trainer.py 223): INFO Epoch:23\
[2023-09-14 06:20:52 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:20:52 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:20:53 4splitDomains](trainer.py 286): INFO [0/46]	0.9272(0.9272)	0.7691(0.7691)	0.041(0.041)	100.00(100.00)\
[2023-09-14 06:20:54 4splitDomains](trainer.py 286): INFO [10/46]	0.1591(0.2305)	0.0009(0.0722)	0.036(0.051)	100.00(99.86)\
[2023-09-14 06:20:56 4splitDomains](trainer.py 286): INFO [20/46]	0.1590(0.1967)	0.0005(0.0382)	0.050(0.048)	100.00(99.85)\
[2023-09-14 06:20:58 4splitDomains](trainer.py 286): INFO [30/46]	0.1596(0.1849)	0.0007(0.0261)	0.042(0.047)	100.00(99.85)\
[2023-09-14 06:20:59 4splitDomains](trainer.py 286): INFO [40/46]	0.1583(0.1789)	0.0002(0.0199)	0.048(0.047)	100.00(99.89)\
[2023-09-14 06:21:00 4splitDomains](trainer.py 286): INFO [45/46]	0.0514(0.1743)	0.0001(0.0178)	0.230(0.048)	100.00(99.90)\
[2023-09-14 06:21:00 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897\
[2023-09-14 06:21:01 4splitDomains](trainer.py 147): INFO  * Val Acc 92.573, Total time 1.24\
[2023-09-14 06:21:01 4splitDomains](trainer.py 223): INFO Epoch:24\
[2023-09-14 06:21:01 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:21:01 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:21:02 4splitDomains](trainer.py 286): INFO [0/46]	0.8372(0.8372)	0.6790(0.6790)	0.087(0.087)	98.44(98.44)\
[2023-09-14 06:21:04 4splitDomains](trainer.py 286): INFO [10/46]	0.1589(0.2215)	0.0004(0.0621)	0.033(0.046)	100.00(99.72)\
[2023-09-14 06:21:05 4splitDomains](trainer.py 286): INFO [20/46]	0.1588(0.1922)	0.0005(0.0329)	0.047(0.047)	100.00(99.63)\
[2023-09-14 06:21:07 4splitDomains](trainer.py 286): INFO [30/46]	0.1591(0.1818)	0.0006(0.0225)	0.045(0.045)	100.00(99.65)\
[2023-09-14 06:21:08 4splitDomains](trainer.py 286): INFO [40/46]	0.1587(0.1766)	0.0002(0.0172)	0.050(0.045)	100.00(99.73)\
[2023-09-14 06:21:09 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1723)	0.0001(0.0153)	0.118(0.044)	100.00(99.76)\
[2023-09-14 06:21:09 4splitDomains](trainer.py 288): INFO  * Train Acc 99.759\
[2023-09-14 06:21:10 4splitDomains](trainer.py 147): INFO  * Val Acc 92.308, Total time 1.26\
[2023-09-14 06:21:10 4splitDomains](trainer.py 223): INFO Epoch:25\
[2023-09-14 06:21:10 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:21:10 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:21:11 4splitDomains](trainer.py 286): INFO [0/46]	0.8971(0.8971)	0.7391(0.7391)	0.064(0.064)	100.00(100.00)\
[2023-09-14 06:21:13 4splitDomains](trainer.py 286): INFO [10/46]	0.1602(0.2289)	0.0019(0.0698)	0.031(0.041)	100.00(100.00)\
[2023-09-14 06:21:15 4splitDomains](trainer.py 286): INFO [20/46]	0.1587(0.1964)	0.0004(0.0371)	0.052(0.040)	100.00(100.00)\
[2023-09-14 06:21:16 4splitDomains](trainer.py 286): INFO [30/46]	0.1589(0.1846)	0.0004(0.0253)	0.032(0.040)	100.00(99.95)\
[2023-09-14 06:21:18 4splitDomains](trainer.py 286): INFO [40/46]	0.1601(0.1784)	0.0018(0.0193)	0.043(0.041)	100.00(99.89)\
[2023-09-14 06:21:18 4splitDomains](trainer.py 286): INFO [45/46]	0.0512(0.1739)	0.0001(0.0173)	0.035(0.040)	100.00(99.90)\
[2023-09-14 06:21:19 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897\
[2023-09-14 06:21:20 4splitDomains](trainer.py 147): INFO  * Val Acc 92.308, Total time 1.34\
[2023-09-14 06:21:20 4splitDomains](trainer.py 223): INFO Epoch:26\
[2023-09-14 06:21:20 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:21:20 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:21:21 4splitDomains](trainer.py 286): INFO [0/46]	0.8412(0.8412)	0.6827(0.6827)	0.032(0.032)	100.00(100.00)\
[2023-09-14 06:21:22 4splitDomains](trainer.py 286): INFO [10/46]	0.1587(0.2260)	0.0003(0.0663)	0.044(0.033)	100.00(99.86)\
[2023-09-14 06:21:24 4splitDomains](trainer.py 286): INFO [20/46]	0.1589(0.1946)	0.0005(0.0350)	0.040(0.040)	100.00(99.78)\
[2023-09-14 06:21:26 4splitDomains](trainer.py 286): INFO [30/46]	0.1603(0.1837)	0.0009(0.0240)	0.037(0.038)	100.00(99.85)\
[2023-09-14 06:21:27 4splitDomains](trainer.py 286): INFO [40/46]	0.1586(0.1778)	0.0004(0.0183)	0.026(0.039)	100.00(99.81)\
[2023-09-14 06:21:28 4splitDomains](trainer.py 286): INFO [45/46]	0.0517(0.1734)	0.0001(0.0163)	0.087(0.039)	100.00(99.83)\
[2023-09-14 06:21:28 4splitDomains](trainer.py 288): INFO  * Train Acc 99.828\
[2023-09-14 06:21:29 4splitDomains](trainer.py 147): INFO  * Val Acc 91.777, Total time 1.31\
[2023-09-14 06:21:29 4splitDomains](trainer.py 223): INFO Epoch:27\
[2023-09-14 06:21:29 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:21:29 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:21:30 4splitDomains](trainer.py 286): INFO [0/46]	0.8906(0.8906)	0.7322(0.7322)	0.040(0.040)	100.00(100.00)\
[2023-09-14 06:21:32 4splitDomains](trainer.py 286): INFO [10/46]	0.1657(0.2288)	0.0004(0.0693)	0.025(0.036)	100.00(100.00)\
[2023-09-14 06:21:33 4splitDomains](trainer.py 286): INFO [20/46]	0.1592(0.1957)	0.0006(0.0366)	0.039(0.037)	100.00(100.00)\
[2023-09-14 06:21:35 4splitDomains](trainer.py 286): INFO [30/46]	0.1593(0.1841)	0.0006(0.0251)	0.026(0.037)	100.00(99.95)\
[2023-09-14 06:21:37 4splitDomains](trainer.py 286): INFO [40/46]	0.1583(0.1781)	0.0001(0.0191)	0.023(0.036)	100.00(99.92)\
[2023-09-14 06:21:37 4splitDomains](trainer.py 286): INFO [45/46]	0.0513(0.1736)	0.0001(0.0170)	0.158(0.036)	94.74(99.86)\
[2023-09-14 06:21:37 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862\
[2023-09-14 06:21:39 4splitDomains](trainer.py 147): INFO  * Val Acc 92.838, Total time 1.22\
[2023-09-14 06:21:39 4splitDomains](trainer.py 223): INFO Epoch:28\
[2023-09-14 06:21:39 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:21:39 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:21:39 4splitDomains](trainer.py 286): INFO [0/46]	0.8912(0.8912)	0.7331(0.7331)	0.032(0.032)	100.00(100.00)\
[2023-09-14 06:21:41 4splitDomains](trainer.py 286): INFO [10/46]	0.1587(0.2268)	0.0002(0.0682)	0.031(0.034)	100.00(99.57)\
[2023-09-14 06:21:43 4splitDomains](trainer.py 286): INFO [20/46]	0.1587(0.1945)	0.0003(0.0359)	0.045(0.033)	100.00(99.70)\
[2023-09-14 06:21:44 4splitDomains](trainer.py 286): INFO [30/46]	0.1594(0.1833)	0.0008(0.0246)	0.040(0.033)	100.00(99.80)\
[2023-09-14 06:21:46 4splitDomains](trainer.py 286): INFO [40/46]	0.1582(0.1778)	0.0002(0.0188)	0.028(0.033)	100.00(99.85)\
[2023-09-14 06:21:47 4splitDomains](trainer.py 286): INFO [45/46]	0.0515(0.1733)	0.0002(0.0168)	0.051(0.033)	100.00(99.86)\
[2023-09-14 06:21:47 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862\
[2023-09-14 06:21:48 4splitDomains](trainer.py 147): INFO  * Val Acc 92.042, Total time 1.25\
[2023-09-14 06:21:48 4splitDomains](trainer.py 223): INFO Epoch:29\
[2023-09-14 06:21:48 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:21:48 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:21:49 4splitDomains](trainer.py 286): INFO [0/46]	0.9018(0.9018)	0.7434(0.7434)	0.019(0.019)	100.00(100.00)\
[2023-09-14 06:21:50 4splitDomains](trainer.py 286): INFO [10/46]	0.1589(0.2273)	0.0004(0.0685)	0.031(0.032)	100.00(99.86)\
[2023-09-14 06:21:52 4splitDomains](trainer.py 286): INFO [20/46]	0.1587(0.1958)	0.0004(0.0361)	0.038(0.033)	100.00(99.85)\
[2023-09-14 06:21:54 4splitDomains](trainer.py 286): INFO [30/46]	0.1607(0.1843)	0.0021(0.0247)	0.020(0.031)	100.00(99.90)\
[2023-09-14 06:21:55 4splitDomains](trainer.py 286): INFO [40/46]	0.1589(0.1782)	0.0004(0.0188)	0.022(0.030)	100.00(99.92)\
[2023-09-14 06:21:56 4splitDomains](trainer.py 286): INFO [45/46]	0.0515(0.1737)	0.0002(0.0168)	0.163(0.031)	94.74(99.90)\
[2023-09-14 06:21:56 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897\
[2023-09-14 06:21:57 4splitDomains](trainer.py 147): INFO  * Val Acc 91.777, Total time 1.31\
=> Saving model to: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-0.pth\
=> Save Done\
[2023-09-14 06:21:57 4splitDomains](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 06:21:59 4splitDomains](trainer.py 147): INFO  * Val Acc 91.777, Total time 1.27\
[2023-09-14 06:21:59 4splitDomains](trainer.py 335): INFO saving storage...\
[2023-09-14 06:21:59 4splitDomains](trainer.py 341): INFO done\
[2023-09-14 06:21:59 4splitDomains](iBatchLearn.py 155): INFO Acc:91.77718790393293; BWT:0;\
=> merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:22:03 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:22:03 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:22:03 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 0, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-0.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_0.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:22:04 4splitDomains](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-0.pth\
[2023-09-14 06:22:04 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:22:06 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:22:06 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:22:06 4splitDomains](iBatchLearn.py 167): INFO test split name:0\
--------------------------------Official Evaluation--------------------------------\
0 93.63867684478372\
=> merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:22:13 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:22:13 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:22:13 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 1, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-0.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-1.pth", "storage_path": "outputs/2023-09-14-06:17:11/4splitDomains/storage-0.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/4splitDomains/storage-1.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:22:14 4splitDomains](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-0.pth\
[2023-09-14 06:22:14 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:22:16 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:22:16 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:22:16 4splitDomains](trainer.py 327): INFO load storage...\
[2023-09-14 06:22:16 4splitDomains](trainer.py 331): INFO done\
[2023-09-14 06:22:16 4splitDomains](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 06:22:16 4splitDomains](iBatchLearn.py 92): INFO ====================== 1 =======================\
[2023-09-14 06:22:16 4splitDomains](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 06:22:16 4splitDomains](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 06:22:16 4splitDomains](trainer.py 223): INFO Epoch:0\
[2023-09-14 06:22:16 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:22:16 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:22:19 4splitDomains](trainer.py 286): INFO [0/45]	3.2139(3.2139)	0.8062(0.8062)	1.921(1.921)	48.44(48.44)\
[2023-09-14 06:22:21 4splitDomains](trainer.py 286): INFO [10/45]	0.1746(0.4514)	0.0039(0.0752)	2.188(2.122)	39.06(46.02)\
[2023-09-14 06:22:23 4splitDomains](trainer.py 286): INFO [20/45]	0.1715(0.3182)	0.0007(0.0397)	1.809(2.044)	53.12(48.29)\
[2023-09-14 06:22:24 4splitDomains](trainer.py 286): INFO [30/45]	0.1702(0.2708)	0.0003(0.0270)	1.800(1.974)	54.69(50.05)\
[2023-09-14 06:22:26 4splitDomains](trainer.py 286): INFO [40/45]	0.1696(0.2468)	0.0001(0.0209)	1.636(1.893)	51.56(52.52)\
[2023-09-14 06:22:27 4splitDomains](trainer.py 286): INFO [44/45]	0.2226(0.2411)	0.0002(0.0191)	1.523(1.862)	60.66(53.01)\
[2023-09-14 06:22:27 4splitDomains](trainer.py 288): INFO  * Train Acc 53.007\
[2023-09-14 06:22:28 4splitDomains](trainer.py 147): INFO  * Val Acc 63.757, Total time 1.29\
[2023-09-14 06:22:28 4splitDomains](trainer.py 223): INFO Epoch:1\
[2023-09-14 06:22:28 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:22:28 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:22:29 4splitDomains](trainer.py 286): INFO [0/45]	0.8648(0.8648)	0.6938(0.6938)	0.971(0.971)	84.38(84.38)\
[2023-09-14 06:22:31 4splitDomains](trainer.py 286): INFO [10/45]	0.1725(0.2352)	0.0006(0.0636)	0.756(1.147)	82.81(71.88)\
[2023-09-14 06:22:32 4splitDomains](trainer.py 286): INFO [20/45]	0.1771(0.2058)	0.0062(0.0340)	1.192(1.122)	76.56(72.84)\
[2023-09-14 06:22:34 4splitDomains](trainer.py 286): INFO [30/45]	0.1714(0.1952)	0.0003(0.0233)	1.214(1.105)	70.31(73.84)\
[2023-09-14 06:22:36 4splitDomains](trainer.py 286): INFO [40/45]	0.1704(0.1895)	0.0002(0.0177)	1.167(1.112)	73.44(73.67)\
[2023-09-14 06:22:37 4splitDomains](trainer.py 286): INFO [44/45]	0.1677(0.1878)	0.0001(0.0162)	1.135(1.116)	73.77(73.55)\
[2023-09-14 06:22:37 4splitDomains](trainer.py 288): INFO  * Train Acc 73.549\
[2023-09-14 06:22:38 4splitDomains](trainer.py 147): INFO  * Val Acc 79.101, Total time 1.29\
[2023-09-14 06:22:38 4splitDomains](trainer.py 223): INFO Epoch:2\
[2023-09-14 06:22:38 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:22:38 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:22:39 4splitDomains](trainer.py 286): INFO [0/45]	0.8653(0.8653)	0.6858(0.6858)	0.743(0.743)	84.38(84.38)\
[2023-09-14 06:22:40 4splitDomains](trainer.py 286): INFO [10/45]	0.1704(0.2348)	0.0004(0.0628)	0.869(0.901)	85.94(80.11)\
[2023-09-14 06:22:42 4splitDomains](trainer.py 286): INFO [20/45]	0.1718(0.2051)	0.0006(0.0332)	0.916(0.913)	79.69(79.91)\
[2023-09-14 06:22:44 4splitDomains](trainer.py 286): INFO [30/45]	0.1710(0.1941)	0.0005(0.0227)	0.824(0.903)	82.81(80.54)\
[2023-09-14 06:22:46 4splitDomains](trainer.py 286): INFO [40/45]	0.1698(0.1889)	0.0001(0.0173)	1.026(0.918)	75.00(80.18)\
[2023-09-14 06:22:46 4splitDomains](trainer.py 286): INFO [44/45]	0.1672(0.1871)	0.0001(0.0158)	0.757(0.910)	86.89(80.67)\
[2023-09-14 06:22:46 4splitDomains](trainer.py 288): INFO  * Train Acc 80.674\
[2023-09-14 06:22:48 4splitDomains](trainer.py 147): INFO  * Val Acc 78.836, Total time 1.24\
[2023-09-14 06:22:48 4splitDomains](trainer.py 223): INFO Epoch:3\
[2023-09-14 06:22:48 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:22:48 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:22:49 4splitDomains](trainer.py 286): INFO [0/45]	0.8575(0.8575)	0.6793(0.6793)	0.713(0.713)	87.50(87.50)\
[2023-09-14 06:22:50 4splitDomains](trainer.py 286): INFO [10/45]	0.1704(0.2352)	0.0004(0.0622)	0.683(0.735)	85.94(88.07)\
[2023-09-14 06:22:52 4splitDomains](trainer.py 286): INFO [20/45]	0.1705(0.2052)	0.0004(0.0328)	0.677(0.760)	93.75(86.98)\
[2023-09-14 06:22:54 4splitDomains](trainer.py 286): INFO [30/45]	0.1715(0.1943)	0.0009(0.0225)	0.710(0.786)	87.50(85.99)\
[2023-09-14 06:22:55 4splitDomains](trainer.py 286): INFO [40/45]	0.1701(0.1888)	0.0001(0.0171)	0.708(0.785)	85.94(85.71)\
[2023-09-14 06:22:56 4splitDomains](trainer.py 286): INFO [44/45]	0.1674(0.1871)	0.0001(0.0156)	0.839(0.788)	83.61(85.71)\
[2023-09-14 06:22:56 4splitDomains](trainer.py 288): INFO  * Train Acc 85.714\
[2023-09-14 06:22:57 4splitDomains](trainer.py 147): INFO  * Val Acc 79.365, Total time 1.31\
[2023-09-14 06:22:57 4splitDomains](trainer.py 223): INFO Epoch:4\
[2023-09-14 06:22:57 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:22:57 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:22:58 4splitDomains](trainer.py 286): INFO [0/45]	0.8789(0.8789)	0.7077(0.7077)	0.664(0.664)	92.19(92.19)\
[2023-09-14 06:23:00 4splitDomains](trainer.py 286): INFO [10/45]	0.1771(0.2369)	0.0008(0.0649)	0.669(0.673)	89.06(89.35)\
[2023-09-14 06:23:02 4splitDomains](trainer.py 286): INFO [20/45]	0.1730(0.2072)	0.0004(0.0343)	0.707(0.697)	89.06(89.36)\
[2023-09-14 06:23:04 4splitDomains](trainer.py 286): INFO [30/45]	0.1762(0.1966)	0.0007(0.0235)	0.612(0.696)	93.75(89.26)\
[2023-09-14 06:23:05 4splitDomains](trainer.py 286): INFO [40/45]	0.1705(0.1904)	0.0002(0.0179)	0.785(0.698)	87.50(89.44)\
[2023-09-14 06:23:06 4splitDomains](trainer.py 286): INFO [44/45]	0.1681(0.1886)	0.0005(0.0163)	0.618(0.697)	96.72(89.43)\
[2023-09-14 06:23:06 4splitDomains](trainer.py 288): INFO  * Train Acc 89.433\
[2023-09-14 06:23:07 4splitDomains](trainer.py 147): INFO  * Val Acc 81.746, Total time 1.28\
[2023-09-14 06:23:07 4splitDomains](trainer.py 223): INFO Epoch:5\
[2023-09-14 06:23:07 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:23:07 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:23:08 4splitDomains](trainer.py 286): INFO [0/45]	0.8747(0.8747)	0.7035(0.7035)	0.674(0.674)	87.50(87.50)\
[2023-09-14 06:23:10 4splitDomains](trainer.py 286): INFO [10/45]	0.1798(0.2382)	0.0008(0.0645)	0.696(0.621)	89.06(91.90)\
[2023-09-14 06:23:12 4splitDomains](trainer.py 286): INFO [20/45]	0.1715(0.2077)	0.0005(0.0341)	0.679(0.648)	92.19(90.85)\
[2023-09-14 06:23:13 4splitDomains](trainer.py 286): INFO [30/45]	0.1962(0.1970)	0.0010(0.0233)	0.562(0.641)	96.88(91.23)\
[2023-09-14 06:23:15 4splitDomains](trainer.py 286): INFO [40/45]	0.1709(0.1908)	0.0003(0.0178)	0.663(0.648)	93.75(91.08)\
[2023-09-14 06:23:16 4splitDomains](trainer.py 286): INFO [44/45]	0.1677(0.1890)	0.0002(0.0163)	0.835(0.660)	86.89(90.58)\
[2023-09-14 06:23:16 4splitDomains](trainer.py 288): INFO  * Train Acc 90.580\
[2023-09-14 06:23:17 4splitDomains](trainer.py 147): INFO  * Val Acc 80.688, Total time 1.25\
[2023-09-14 06:23:17 4splitDomains](trainer.py 223): INFO Epoch:6\
[2023-09-14 06:23:17 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:23:17 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:23:18 4splitDomains](trainer.py 286): INFO [0/45]	0.8291(0.8291)	0.6555(0.6555)	0.586(0.586)	92.19(92.19)\
[2023-09-14 06:23:20 4splitDomains](trainer.py 286): INFO [10/45]	0.1714(0.2316)	0.0004(0.0603)	0.699(0.608)	93.75(93.04)\
[2023-09-14 06:23:21 4splitDomains](trainer.py 286): INFO [20/45]	0.1721(0.2035)	0.0008(0.0319)	0.596(0.608)	93.75(92.93)\
[2023-09-14 06:23:23 4splitDomains](trainer.py 286): INFO [30/45]	0.1715(0.1935)	0.0004(0.0219)	0.613(0.610)	95.31(93.04)\
[2023-09-14 06:23:25 4splitDomains](trainer.py 286): INFO [40/45]	0.1703(0.1885)	0.0002(0.0167)	0.662(0.623)	92.19(92.64)\
[2023-09-14 06:23:26 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1868)	0.0001(0.0152)	0.606(0.627)	91.80(92.56)\
[2023-09-14 06:23:26 4splitDomains](trainer.py 288): INFO  * Train Acc 92.562\
[2023-09-14 06:23:27 4splitDomains](trainer.py 147): INFO  * Val Acc 80.688, Total time 1.26\
[2023-09-14 06:23:27 4splitDomains](trainer.py 223): INFO Epoch:7\
[2023-09-14 06:23:27 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:23:27 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:23:28 4splitDomains](trainer.py 286): INFO [0/45]	0.8494(0.8494)	0.6784(0.6784)	0.540(0.540)	95.31(95.31)\
[2023-09-14 06:23:29 4splitDomains](trainer.py 286): INFO [10/45]	0.1711(0.2361)	0.0002(0.0622)	0.518(0.567)	98.44(94.18)\
[2023-09-14 06:23:31 4splitDomains](trainer.py 286): INFO [20/45]	0.1710(0.2058)	0.0004(0.0334)	0.613(0.576)	93.75(94.27)\
[2023-09-14 06:23:33 4splitDomains](trainer.py 286): INFO [30/45]	0.1721(0.1949)	0.0003(0.0228)	0.824(0.587)	84.38(93.90)\
[2023-09-14 06:23:35 4splitDomains](trainer.py 286): INFO [40/45]	0.1702(0.1894)	0.0002(0.0176)	0.636(0.595)	95.31(93.98)\
[2023-09-14 06:23:35 4splitDomains](trainer.py 286): INFO [44/45]	0.1675(0.1876)	0.0001(0.0160)	0.633(0.605)	91.80(93.71)\
[2023-09-14 06:23:35 4splitDomains](trainer.py 288): INFO  * Train Acc 93.709\
[2023-09-14 06:23:37 4splitDomains](trainer.py 147): INFO  * Val Acc 80.952, Total time 1.23\
[2023-09-14 06:23:37 4splitDomains](trainer.py 223): INFO Epoch:8\
[2023-09-14 06:23:37 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:23:37 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:23:37 4splitDomains](trainer.py 286): INFO [0/45]	0.8803(0.8803)	0.7092(0.7092)	0.535(0.535)	96.88(96.88)\
[2023-09-14 06:23:39 4splitDomains](trainer.py 286): INFO [10/45]	0.1710(0.2366)	0.0003(0.0649)	0.676(0.583)	90.62(93.75)\
[2023-09-14 06:23:41 4splitDomains](trainer.py 286): INFO [20/45]	0.1706(0.2063)	0.0003(0.0342)	0.555(0.583)	96.88(94.27)\
[2023-09-14 06:23:43 4splitDomains](trainer.py 286): INFO [30/45]	0.1796(0.1957)	0.0008(0.0234)	0.602(0.595)	92.19(94.15)\
[2023-09-14 06:23:44 4splitDomains](trainer.py 286): INFO [40/45]	0.1701(0.1898)	0.0001(0.0178)	0.558(0.600)	95.31(93.83)\
[2023-09-14 06:23:45 4splitDomains](trainer.py 286): INFO [44/45]	0.1675(0.1880)	0.0002(0.0163)	0.493(0.597)	100.00(93.92)\
[2023-09-14 06:23:45 4splitDomains](trainer.py 288): INFO  * Train Acc 93.917\
[2023-09-14 06:23:46 4splitDomains](trainer.py 147): INFO  * Val Acc 81.217, Total time 1.28\
[2023-09-14 06:23:46 4splitDomains](trainer.py 223): INFO Epoch:9\
[2023-09-14 06:23:46 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:23:46 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:23:47 4splitDomains](trainer.py 286): INFO [0/45]	0.7998(0.7998)	0.6209(0.6209)	0.507(0.507)	93.75(93.75)\
[2023-09-14 06:23:49 4splitDomains](trainer.py 286): INFO [10/45]	0.1716(0.2429)	0.0004(0.0713)	0.582(0.544)	95.31(96.02)\
[2023-09-14 06:23:51 4splitDomains](trainer.py 286): INFO [20/45]	0.1717(0.2095)	0.0006(0.0378)	0.641(0.561)	90.62(94.87)\
[2023-09-14 06:23:53 4splitDomains](trainer.py 286): INFO [30/45]	0.1717(0.1977)	0.0007(0.0258)	0.592(0.552)	96.88(95.46)\
[2023-09-14 06:23:54 4splitDomains](trainer.py 286): INFO [40/45]	0.1704(0.1915)	0.0002(0.0197)	0.612(0.561)	93.75(95.12)\
[2023-09-14 06:23:55 4splitDomains](trainer.py 286): INFO [44/45]	0.1680(0.1896)	0.0002(0.0180)	0.573(0.574)	93.44(94.68)\
[2023-09-14 06:23:55 4splitDomains](trainer.py 288): INFO  * Train Acc 94.682\
[2023-09-14 06:23:56 4splitDomains](trainer.py 147): INFO  * Val Acc 80.159, Total time 1.26\
[2023-09-14 06:23:56 4splitDomains](trainer.py 223): INFO Epoch:10\
[2023-09-14 06:23:56 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:23:56 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:23:57 4splitDomains](trainer.py 286): INFO [0/45]	0.9494(0.9494)	0.7780(0.7780)	0.564(0.564)	96.88(96.88)\
[2023-09-14 06:23:59 4splitDomains](trainer.py 286): INFO [10/45]	0.1840(0.2437)	0.0008(0.0713)	0.574(0.557)	96.88(96.16)\
[2023-09-14 06:24:01 4splitDomains](trainer.py 286): INFO [20/45]	0.1715(0.2093)	0.0005(0.0376)	0.560(0.561)	93.75(95.61)\
[2023-09-14 06:24:02 4splitDomains](trainer.py 286): INFO [30/45]	0.1713(0.1976)	0.0006(0.0257)	0.514(0.563)	98.44(95.61)\
[2023-09-14 06:24:04 4splitDomains](trainer.py 286): INFO [40/45]	0.1701(0.1914)	0.0001(0.0196)	0.612(0.570)	90.62(95.20)\
[2023-09-14 06:24:05 4splitDomains](trainer.py 286): INFO [44/45]	0.1675(0.1895)	0.0002(0.0179)	0.593(0.570)	95.08(95.24)\
[2023-09-14 06:24:05 4splitDomains](trainer.py 288): INFO  * Train Acc 95.238\
[2023-09-14 06:24:06 4splitDomains](trainer.py 147): INFO  * Val Acc 80.159, Total time 1.26\
[2023-09-14 06:24:06 4splitDomains](trainer.py 223): INFO Epoch:11\
[2023-09-14 06:24:06 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:24:06 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:24:07 4splitDomains](trainer.py 286): INFO [0/45]	0.8500(0.8500)	0.6790(0.6790)	0.520(0.520)	93.75(93.75)\
[2023-09-14 06:24:09 4splitDomains](trainer.py 286): INFO [10/45]	0.1709(0.2363)	0.0004(0.0633)	0.514(0.522)	93.75(95.88)\
[2023-09-14 06:24:10 4splitDomains](trainer.py 286): INFO [20/45]	0.1712(0.2061)	0.0003(0.0334)	0.535(0.529)	96.88(96.28)\
[2023-09-14 06:24:12 4splitDomains](trainer.py 286): INFO [30/45]	0.1772(0.1954)	0.0006(0.0229)	0.643(0.541)	95.31(96.22)\
[2023-09-14 06:24:14 4splitDomains](trainer.py 286): INFO [40/45]	0.1700(0.1895)	0.0001(0.0174)	0.550(0.554)	96.88(95.58)\
[2023-09-14 06:24:15 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1878)	0.0001(0.0160)	0.546(0.555)	96.72(95.45)\
[2023-09-14 06:24:15 4splitDomains](trainer.py 288): INFO  * Train Acc 95.447\
[2023-09-14 06:24:16 4splitDomains](trainer.py 147): INFO  * Val Acc 80.952, Total time 1.26\
[2023-09-14 06:24:16 4splitDomains](trainer.py 223): INFO Epoch:12\
[2023-09-14 06:24:16 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:24:16 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:24:17 4splitDomains](trainer.py 286): INFO [0/45]	0.8385(0.8385)	0.6618(0.6618)	0.482(0.482)	96.88(96.88)\
[2023-09-14 06:24:18 4splitDomains](trainer.py 286): INFO [10/45]	0.1713(0.2352)	0.0003(0.0607)	0.533(0.534)	98.44(97.30)\
[2023-09-14 06:24:20 4splitDomains](trainer.py 286): INFO [20/45]	0.1715(0.2054)	0.0005(0.0321)	0.530(0.538)	98.44(96.95)\
[2023-09-14 06:24:22 4splitDomains](trainer.py 286): INFO [30/45]	0.1735(0.1945)	0.0005(0.0219)	0.504(0.532)	98.44(96.77)\
[2023-09-14 06:24:24 4splitDomains](trainer.py 286): INFO [40/45]	0.1703(0.1889)	0.0002(0.0168)	0.575(0.536)	95.31(96.57)\
[2023-09-14 06:24:24 4splitDomains](trainer.py 286): INFO [44/45]	0.1678(0.1872)	0.0002(0.0153)	0.577(0.537)	93.44(96.49)\
[2023-09-14 06:24:24 4splitDomains](trainer.py 288): INFO  * Train Acc 96.489\
[2023-09-14 06:24:26 4splitDomains](trainer.py 147): INFO  * Val Acc 81.481, Total time 1.42\
[2023-09-14 06:24:26 4splitDomains](trainer.py 223): INFO Epoch:13\
[2023-09-14 06:24:26 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:24:26 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:24:27 4splitDomains](trainer.py 286): INFO [0/45]	0.8894(0.8894)	0.7107(0.7107)	0.575(0.575)	96.88(96.88)\
[2023-09-14 06:24:28 4splitDomains](trainer.py 286): INFO [10/45]	0.1708(0.2379)	0.0004(0.0651)	0.509(0.527)	96.88(96.31)\
[2023-09-14 06:24:30 4splitDomains](trainer.py 286): INFO [20/45]	0.1716(0.2075)	0.0007(0.0344)	0.516(0.525)	98.44(96.21)\
[2023-09-14 06:24:32 4splitDomains](trainer.py 286): INFO [30/45]	0.1710(0.1962)	0.0004(0.0235)	0.577(0.530)	93.75(96.27)\
[2023-09-14 06:24:34 4splitDomains](trainer.py 286): INFO [40/45]	0.1702(0.1900)	0.0001(0.0179)	0.700(0.541)	90.62(95.73)\
[2023-09-14 06:24:34 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1882)	0.0002(0.0164)	0.546(0.539)	96.72(95.86)\
[2023-09-14 06:24:34 4splitDomains](trainer.py 288): INFO  * Train Acc 95.864\
[2023-09-14 06:24:36 4splitDomains](trainer.py 147): INFO  * Val Acc 79.894, Total time 1.29\
[2023-09-14 06:24:36 4splitDomains](trainer.py 223): INFO Epoch:14\
[2023-09-14 06:24:36 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:24:36 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:24:36 4splitDomains](trainer.py 286): INFO [0/45]	0.8114(0.8114)	0.6330(0.6330)	0.537(0.537)	98.44(98.44)\
[2023-09-14 06:24:38 4splitDomains](trainer.py 286): INFO [10/45]	0.1714(0.2317)	0.0002(0.0595)	0.492(0.508)	95.31(97.30)\
[2023-09-14 06:24:40 4splitDomains](trainer.py 286): INFO [20/45]	0.1711(0.2041)	0.0005(0.0314)	0.570(0.515)	95.31(97.10)\
[2023-09-14 06:24:42 4splitDomains](trainer.py 286): INFO [30/45]	0.1718(0.1939)	0.0008(0.0215)	0.533(0.527)	93.75(96.67)\
[2023-09-14 06:24:43 4splitDomains](trainer.py 286): INFO [40/45]	0.1702(0.1884)	0.0002(0.0164)	0.524(0.530)	96.88(96.46)\
[2023-09-14 06:24:44 4splitDomains](trainer.py 286): INFO [44/45]	0.1683(0.1868)	0.0002(0.0150)	0.552(0.532)	96.72(96.42)\
[2023-09-14 06:24:44 4splitDomains](trainer.py 288): INFO  * Train Acc 96.420\
[2023-09-14 06:24:45 4splitDomains](trainer.py 147): INFO  * Val Acc 80.159, Total time 1.26\
[2023-09-14 06:24:45 4splitDomains](trainer.py 223): INFO Epoch:15\
[2023-09-14 06:24:45 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:24:45 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:24:46 4splitDomains](trainer.py 286): INFO [0/45]	0.8299(0.8299)	0.6586(0.6586)	0.538(0.538)	95.31(95.31)\
[2023-09-14 06:24:48 4splitDomains](trainer.py 286): INFO [10/45]	0.1747(0.2345)	0.0007(0.0604)	0.507(0.494)	100.00(97.44)\
[2023-09-14 06:24:50 4splitDomains](trainer.py 286): INFO [20/45]	0.1712(0.2048)	0.0004(0.0319)	0.538(0.518)	95.31(96.58)\
[2023-09-14 06:24:51 4splitDomains](trainer.py 286): INFO [30/45]	0.1714(0.1940)	0.0004(0.0218)	0.497(0.518)	96.88(96.77)\
[2023-09-14 06:24:53 4splitDomains](trainer.py 286): INFO [40/45]	0.1704(0.1888)	0.0002(0.0166)	0.561(0.526)	96.88(96.68)\
[2023-09-14 06:24:54 4splitDomains](trainer.py 286): INFO [44/45]	0.1675(0.1871)	0.0001(0.0152)	0.496(0.528)	95.08(96.52)\
[2023-09-14 06:24:54 4splitDomains](trainer.py 288): INFO  * Train Acc 96.524\
[2023-09-14 06:24:55 4splitDomains](trainer.py 147): INFO  * Val Acc 81.217, Total time 1.24\
[2023-09-14 06:24:55 4splitDomains](trainer.py 223): INFO Epoch:16\
[2023-09-14 06:24:55 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:24:55 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:24:56 4splitDomains](trainer.py 286): INFO [0/45]	0.8307(0.8307)	0.6557(0.6557)	0.538(0.538)	93.75(93.75)\
[2023-09-14 06:24:58 4splitDomains](trainer.py 286): INFO [10/45]	0.1716(0.2343)	0.0006(0.0606)	0.407(0.504)	100.00(98.15)\
[2023-09-14 06:24:59 4splitDomains](trainer.py 286): INFO [20/45]	0.1716(0.2052)	0.0005(0.0321)	0.477(0.513)	96.88(97.32)\
[2023-09-14 06:25:01 4splitDomains](trainer.py 286): INFO [30/45]	0.1714(0.1948)	0.0003(0.0220)	0.651(0.519)	93.75(97.03)\
[2023-09-14 06:25:03 4splitDomains](trainer.py 286): INFO [40/45]	0.1704(0.1894)	0.0002(0.0168)	0.703(0.523)	92.19(96.95)\
[2023-09-14 06:25:04 4splitDomains](trainer.py 286): INFO [44/45]	0.1674(0.1876)	0.0001(0.0153)	0.626(0.531)	93.44(96.63)\
[2023-09-14 06:25:04 4splitDomains](trainer.py 288): INFO  * Train Acc 96.628\
[2023-09-14 06:25:05 4splitDomains](trainer.py 147): INFO  * Val Acc 83.333, Total time 1.25\
[2023-09-14 06:25:05 4splitDomains](trainer.py 223): INFO Epoch:17\
[2023-09-14 06:25:05 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:25:05 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:25:06 4splitDomains](trainer.py 286): INFO [0/45]	0.8008(0.8008)	0.5994(0.5994)	0.590(0.590)	90.62(90.62)\
[2023-09-14 06:25:07 4splitDomains](trainer.py 286): INFO [10/45]	0.1712(0.2294)	0.0005(0.0552)	0.544(0.522)	96.88(95.31)\
[2023-09-14 06:25:09 4splitDomains](trainer.py 286): INFO [20/45]	0.1721(0.2018)	0.0010(0.0293)	0.681(0.526)	89.06(95.68)\
[2023-09-14 06:25:11 4splitDomains](trainer.py 286): INFO [30/45]	0.1718(0.1923)	0.0006(0.0200)	0.518(0.528)	98.44(95.87)\
[2023-09-14 06:25:13 4splitDomains](trainer.py 286): INFO [40/45]	0.1701(0.1872)	0.0001(0.0153)	0.504(0.531)	98.44(95.96)\
[2023-09-14 06:25:13 4splitDomains](trainer.py 286): INFO [44/45]	0.1675(0.1857)	0.0001(0.0140)	0.537(0.534)	98.36(95.97)\
[2023-09-14 06:25:13 4splitDomains](trainer.py 288): INFO  * Train Acc 95.968\
[2023-09-14 06:25:15 4splitDomains](trainer.py 147): INFO  * Val Acc 80.952, Total time 1.34\
[2023-09-14 06:25:15 4splitDomains](trainer.py 223): INFO Epoch:18\
[2023-09-14 06:25:15 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:25:15 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:25:15 4splitDomains](trainer.py 286): INFO [0/45]	0.8224(0.8224)	0.6495(0.6495)	0.627(0.627)	92.19(92.19)\
[2023-09-14 06:25:17 4splitDomains](trainer.py 286): INFO [10/45]	0.1729(0.2328)	0.0004(0.0613)	0.504(0.535)	96.88(96.59)\
[2023-09-14 06:25:19 4splitDomains](trainer.py 286): INFO [20/45]	0.1717(0.2042)	0.0003(0.0324)	0.491(0.515)	96.88(97.10)\
[2023-09-14 06:25:21 4splitDomains](trainer.py 286): INFO [30/45]	0.1722(0.1947)	0.0005(0.0223)	0.486(0.519)	100.00(96.72)\
[2023-09-14 06:25:22 4splitDomains](trainer.py 286): INFO [40/45]	0.1702(0.1894)	0.0002(0.0170)	0.465(0.519)	100.00(96.99)\
[2023-09-14 06:25:23 4splitDomains](trainer.py 286): INFO [44/45]	0.1675(0.1876)	0.0001(0.0155)	0.591(0.523)	93.44(96.84)\
[2023-09-14 06:25:23 4splitDomains](trainer.py 288): INFO  * Train Acc 96.837\
[2023-09-14 06:25:24 4splitDomains](trainer.py 147): INFO  * Val Acc 82.540, Total time 1.27\
[2023-09-14 06:25:24 4splitDomains](trainer.py 223): INFO Epoch:19\
[2023-09-14 06:25:24 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:25:24 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:25:25 4splitDomains](trainer.py 286): INFO [0/45]	0.9224(0.9224)	0.7515(0.7515)	0.593(0.593)	93.75(93.75)\
[2023-09-14 06:25:27 4splitDomains](trainer.py 286): INFO [10/45]	0.1715(0.2415)	0.0005(0.0688)	0.510(0.519)	98.44(96.59)\
[2023-09-14 06:25:29 4splitDomains](trainer.py 286): INFO [20/45]	0.1710(0.2092)	0.0004(0.0363)	0.477(0.509)	98.44(97.17)\
[2023-09-14 06:25:31 4splitDomains](trainer.py 286): INFO [30/45]	0.1712(0.1977)	0.0004(0.0249)	0.545(0.512)	95.31(96.88)\
[2023-09-14 06:25:32 4splitDomains](trainer.py 286): INFO [40/45]	0.1700(0.1913)	0.0001(0.0190)	0.473(0.512)	98.44(96.91)\
[2023-09-14 06:25:33 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1894)	0.0001(0.0173)	0.558(0.518)	95.08(96.73)\
[2023-09-14 06:25:33 4splitDomains](trainer.py 288): INFO  * Train Acc 96.733\
[2023-09-14 06:25:34 4splitDomains](trainer.py 147): INFO  * Val Acc 81.217, Total time 1.24\
[2023-09-14 06:25:34 4splitDomains](trainer.py 223): INFO Epoch:20\
[2023-09-14 06:25:34 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:25:34 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:25:35 4splitDomains](trainer.py 286): INFO [0/45]	0.8417(0.8417)	0.6656(0.6656)	0.493(0.493)	96.88(96.88)\
[2023-09-14 06:25:37 4splitDomains](trainer.py 286): INFO [10/45]	0.1758(0.2371)	0.0046(0.0631)	0.493(0.502)	95.31(96.45)\
[2023-09-14 06:25:39 4splitDomains](trainer.py 286): INFO [20/45]	0.1708(0.2075)	0.0004(0.0333)	0.529(0.505)	92.19(96.80)\
[2023-09-14 06:25:40 4splitDomains](trainer.py 286): INFO [30/45]	0.1709(0.1978)	0.0005(0.0241)	0.559(0.512)	95.31(96.62)\
[2023-09-14 06:25:42 4splitDomains](trainer.py 286): INFO [40/45]	0.1704(0.1914)	0.0002(0.0184)	0.510(0.520)	95.31(96.34)\
[2023-09-14 06:25:43 4splitDomains](trainer.py 286): INFO [44/45]	0.1677(0.1895)	0.0002(0.0168)	0.547(0.524)	95.08(96.25)\
[2023-09-14 06:25:43 4splitDomains](trainer.py 288): INFO  * Train Acc 96.246\
[2023-09-14 06:25:44 4splitDomains](trainer.py 147): INFO  * Val Acc 80.423, Total time 1.26\
[2023-09-14 06:25:44 4splitDomains](trainer.py 223): INFO Epoch:21\
[2023-09-14 06:25:44 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:25:44 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:25:45 4splitDomains](trainer.py 286): INFO [0/45]	0.8643(0.8643)	0.6922(0.6922)	0.520(0.520)	93.75(93.75)\
[2023-09-14 06:25:47 4splitDomains](trainer.py 286): INFO [10/45]	0.1711(0.2352)	0.0004(0.0634)	0.522(0.473)	98.44(98.01)\
[2023-09-14 06:25:48 4splitDomains](trainer.py 286): INFO [20/45]	0.1713(0.2051)	0.0004(0.0334)	0.608(0.499)	93.75(97.40)\
[2023-09-14 06:25:50 4splitDomains](trainer.py 286): INFO [30/45]	0.1716(0.1950)	0.0004(0.0228)	0.547(0.503)	95.31(97.38)\
[2023-09-14 06:25:52 4splitDomains](trainer.py 286): INFO [40/45]	0.1703(0.1896)	0.0002(0.0174)	0.620(0.509)	93.75(97.14)\
[2023-09-14 06:25:53 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1879)	0.0001(0.0159)	0.545(0.514)	96.72(97.01)\
[2023-09-14 06:25:53 4splitDomains](trainer.py 288): INFO  * Train Acc 97.011\
[2023-09-14 06:25:54 4splitDomains](trainer.py 147): INFO  * Val Acc 80.952, Total time 1.25\
[2023-09-14 06:25:54 4splitDomains](trainer.py 223): INFO Epoch:22\
[2023-09-14 06:25:54 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:25:54 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:25:55 4splitDomains](trainer.py 286): INFO [0/45]	0.8569(0.8569)	0.6862(0.6862)	0.564(0.564)	96.88(96.88)\
[2023-09-14 06:25:56 4splitDomains](trainer.py 286): INFO [10/45]	0.1710(0.2358)	0.0006(0.0629)	0.549(0.493)	96.88(96.88)\
[2023-09-14 06:25:58 4splitDomains](trainer.py 286): INFO [20/45]	0.1708(0.2054)	0.0002(0.0333)	0.552(0.496)	95.31(97.25)\
[2023-09-14 06:26:00 4splitDomains](trainer.py 286): INFO [30/45]	0.1710(0.1948)	0.0004(0.0228)	0.691(0.507)	92.19(97.33)\
[2023-09-14 06:26:02 4splitDomains](trainer.py 286): INFO [40/45]	0.1704(0.1902)	0.0002(0.0175)	0.501(0.517)	96.88(96.84)\
[2023-09-14 06:26:02 4splitDomains](trainer.py 286): INFO [44/45]	0.1677(0.1884)	0.0001(0.0159)	0.577(0.518)	93.44(96.77)\
[2023-09-14 06:26:02 4splitDomains](trainer.py 288): INFO  * Train Acc 96.767\
[2023-09-14 06:26:04 4splitDomains](trainer.py 147): INFO  * Val Acc 81.481, Total time 1.25\
[2023-09-14 06:26:04 4splitDomains](trainer.py 223): INFO Epoch:23\
[2023-09-14 06:26:04 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:26:04 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:26:04 4splitDomains](trainer.py 286): INFO [0/45]	0.7761(0.7761)	0.6025(0.6025)	0.508(0.508)	95.31(95.31)\
[2023-09-14 06:26:06 4splitDomains](trainer.py 286): INFO [10/45]	0.1713(0.2296)	0.0004(0.0553)	0.440(0.495)	100.00(97.59)\
[2023-09-14 06:26:08 4splitDomains](trainer.py 286): INFO [20/45]	0.1714(0.2024)	0.0005(0.0293)	0.477(0.502)	100.00(97.69)\
[2023-09-14 06:26:10 4splitDomains](trainer.py 286): INFO [30/45]	0.1719(0.1929)	0.0008(0.0201)	0.449(0.505)	98.44(97.53)\
[2023-09-14 06:26:11 4splitDomains](trainer.py 286): INFO [40/45]	0.1702(0.1877)	0.0002(0.0153)	0.550(0.509)	96.88(97.52)\
[2023-09-14 06:26:12 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1861)	0.0002(0.0140)	0.524(0.511)	100.00(97.46)\
[2023-09-14 06:26:12 4splitDomains](trainer.py 288): INFO  * Train Acc 97.463\
[2023-09-14 06:26:13 4splitDomains](trainer.py 147): INFO  * Val Acc 80.952, Total time 1.27\
[2023-09-14 06:26:13 4splitDomains](trainer.py 223): INFO Epoch:24\
[2023-09-14 06:26:13 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:26:13 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:26:14 4splitDomains](trainer.py 286): INFO [0/45]	0.8378(0.8378)	0.6522(0.6522)	0.457(0.457)	98.44(98.44)\
[2023-09-14 06:26:16 4splitDomains](trainer.py 286): INFO [10/45]	0.1710(0.2327)	0.0006(0.0597)	0.538(0.483)	96.88(97.87)\
[2023-09-14 06:26:18 4splitDomains](trainer.py 286): INFO [20/45]	0.1718(0.2039)	0.0004(0.0315)	0.503(0.498)	98.44(97.62)\
[2023-09-14 06:26:19 4splitDomains](trainer.py 286): INFO [30/45]	0.1712(0.1936)	0.0004(0.0216)	0.544(0.501)	100.00(97.68)\
[2023-09-14 06:26:21 4splitDomains](trainer.py 286): INFO [40/45]	0.1706(0.1882)	0.0002(0.0164)	0.544(0.506)	96.88(97.41)\
[2023-09-14 06:26:22 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1866)	0.0001(0.0150)	0.525(0.509)	96.72(97.32)\
[2023-09-14 06:26:22 4splitDomains](trainer.py 288): INFO  * Train Acc 97.324\
[2023-09-14 06:26:23 4splitDomains](trainer.py 147): INFO  * Val Acc 78.307, Total time 1.28\
[2023-09-14 06:26:23 4splitDomains](trainer.py 223): INFO Epoch:25\
[2023-09-14 06:26:23 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:26:23 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:26:24 4splitDomains](trainer.py 286): INFO [0/45]	0.8302(0.8302)	0.6592(0.6592)	0.471(0.471)	100.00(100.00)\
[2023-09-14 06:26:26 4splitDomains](trainer.py 286): INFO [10/45]	0.1724(0.2318)	0.0021(0.0609)	0.443(0.490)	98.44(98.15)\
[2023-09-14 06:26:27 4splitDomains](trainer.py 286): INFO [20/45]	0.1716(0.2037)	0.0004(0.0322)	0.525(0.507)	90.62(97.17)\
[2023-09-14 06:26:29 4splitDomains](trainer.py 286): INFO [30/45]	0.1714(0.1933)	0.0006(0.0220)	0.473(0.516)	100.00(96.77)\
[2023-09-14 06:26:31 4splitDomains](trainer.py 286): INFO [40/45]	0.1701(0.1880)	0.0001(0.0168)	0.604(0.519)	92.19(96.84)\
[2023-09-14 06:26:31 4splitDomains](trainer.py 286): INFO [44/45]	0.1678(0.1864)	0.0002(0.0153)	0.494(0.518)	96.72(96.84)\
[2023-09-14 06:26:32 4splitDomains](trainer.py 288): INFO  * Train Acc 96.837\
[2023-09-14 06:26:33 4splitDomains](trainer.py 147): INFO  * Val Acc 80.688, Total time 1.26\
[2023-09-14 06:26:33 4splitDomains](trainer.py 223): INFO Epoch:26\
[2023-09-14 06:26:33 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:26:33 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:26:34 4splitDomains](trainer.py 286): INFO [0/45]	0.8328(0.8328)	0.6292(0.6292)	0.525(0.525)	96.88(96.88)\
[2023-09-14 06:26:35 4splitDomains](trainer.py 286): INFO [10/45]	0.1711(0.2336)	0.0006(0.0580)	0.461(0.506)	98.44(97.16)\
[2023-09-14 06:26:37 4splitDomains](trainer.py 286): INFO [20/45]	0.1715(0.2050)	0.0005(0.0307)	0.489(0.507)	100.00(97.47)\
[2023-09-14 06:26:39 4splitDomains](trainer.py 286): INFO [30/45]	0.1713(0.1946)	0.0006(0.0210)	0.517(0.511)	98.44(97.08)\
[2023-09-14 06:26:41 4splitDomains](trainer.py 286): INFO [40/45]	0.1701(0.1893)	0.0001(0.0160)	0.466(0.514)	100.00(97.26)\
[2023-09-14 06:26:41 4splitDomains](trainer.py 286): INFO [44/45]	0.1677(0.1875)	0.0001(0.0146)	0.550(0.515)	96.72(97.25)\
[2023-09-14 06:26:41 4splitDomains](trainer.py 288): INFO  * Train Acc 97.254\
[2023-09-14 06:26:43 4splitDomains](trainer.py 147): INFO  * Val Acc 80.688, Total time 1.26\
[2023-09-14 06:26:43 4splitDomains](trainer.py 223): INFO Epoch:27\
[2023-09-14 06:26:43 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:26:43 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:26:43 4splitDomains](trainer.py 286): INFO [0/45]	0.8300(0.8300)	0.6562(0.6562)	0.503(0.503)	98.44(98.44)\
[2023-09-14 06:26:45 4splitDomains](trainer.py 286): INFO [10/45]	0.1711(0.2361)	0.0004(0.0606)	0.505(0.492)	98.44(97.59)\
[2023-09-14 06:26:47 4splitDomains](trainer.py 286): INFO [20/45]	0.1718(0.2067)	0.0005(0.0320)	0.479(0.493)	98.44(97.69)\
[2023-09-14 06:26:49 4splitDomains](trainer.py 286): INFO [30/45]	0.1727(0.1963)	0.0004(0.0219)	0.531(0.496)	98.44(97.78)\
[2023-09-14 06:26:50 4splitDomains](trainer.py 286): INFO [40/45]	0.1701(0.1902)	0.0001(0.0167)	0.519(0.504)	98.44(97.37)\
[2023-09-14 06:26:51 4splitDomains](trainer.py 286): INFO [44/45]	0.1676(0.1884)	0.0001(0.0153)	0.541(0.511)	95.08(97.29)\
[2023-09-14 06:26:51 4splitDomains](trainer.py 288): INFO  * Train Acc 97.289\
[2023-09-14 06:26:52 4splitDomains](trainer.py 147): INFO  * Val Acc 80.688, Total time 1.28\
[2023-09-14 06:26:52 4splitDomains](trainer.py 223): INFO Epoch:28\
[2023-09-14 06:26:52 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:26:52 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:26:53 4splitDomains](trainer.py 286): INFO [0/45]	0.8756(0.8756)	0.7049(0.7049)	0.478(0.478)	98.44(98.44)\
[2023-09-14 06:26:55 4splitDomains](trainer.py 286): INFO [10/45]	0.1715(0.2385)	0.0005(0.0647)	0.497(0.493)	96.88(96.73)\
[2023-09-14 06:26:57 4splitDomains](trainer.py 286): INFO [20/45]	0.1712(0.2093)	0.0003(0.0350)	0.490(0.509)	96.88(96.43)\
[2023-09-14 06:26:59 4splitDomains](trainer.py 286): INFO [30/45]	0.1803(0.1976)	0.0007(0.0240)	0.563(0.510)	92.19(96.42)\
[2023-09-14 06:27:00 4splitDomains](trainer.py 286): INFO [40/45]	0.1702(0.1911)	0.0002(0.0182)	0.522(0.511)	95.31(96.46)\
[2023-09-14 06:27:01 4splitDomains](trainer.py 286): INFO [44/45]	0.1675(0.1892)	0.0001(0.0167)	0.573(0.511)	95.08(96.56)\
[2023-09-14 06:27:01 4splitDomains](trainer.py 288): INFO  * Train Acc 96.559\
[2023-09-14 06:27:02 4splitDomains](trainer.py 147): INFO  * Val Acc 81.746, Total time 1.38\
[2023-09-14 06:27:02 4splitDomains](trainer.py 223): INFO Epoch:29\
[2023-09-14 06:27:02 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:27:02 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:27:03 4splitDomains](trainer.py 286): INFO [0/45]	0.9912(0.9912)	0.8154(0.8154)	0.556(0.556)	96.88(96.88)\
[2023-09-14 06:27:05 4splitDomains](trainer.py 286): INFO [10/45]	0.1710(0.2462)	0.0005(0.0749)	0.546(0.501)	95.31(97.30)\
[2023-09-14 06:27:07 4splitDomains](trainer.py 286): INFO [20/45]	0.1718(0.2112)	0.0004(0.0396)	0.480(0.502)	98.44(97.54)\
[2023-09-14 06:27:09 4splitDomains](trainer.py 286): INFO [30/45]	0.1719(0.1985)	0.0008(0.0271)	0.497(0.507)	96.88(97.23)\
[2023-09-14 06:27:10 4splitDomains](trainer.py 286): INFO [40/45]	0.1709(0.1919)	0.0002(0.0206)	0.501(0.513)	100.00(97.14)\
[2023-09-14 06:27:11 4splitDomains](trainer.py 286): INFO [44/45]	0.1679(0.1899)	0.0003(0.0188)	0.510(0.514)	98.36(97.05)\
[2023-09-14 06:27:11 4splitDomains](trainer.py 288): INFO  * Train Acc 97.046\
[2023-09-14 06:27:12 4splitDomains](trainer.py 147): INFO  * Val Acc 81.481, Total time 1.37\
=> Saving model to: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-1.pth\
=> Save Done\
[2023-09-14 06:27:13 4splitDomains](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 06:27:14 4splitDomains](trainer.py 147): INFO  * Val Acc 85.676, Total time 1.29\
[2023-09-14 06:27:14 4splitDomains](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 06:27:15 4splitDomains](trainer.py 147): INFO  * Val Acc 81.481, Total time 1.22\
[2023-09-14 06:27:15 4splitDomains](trainer.py 335): INFO saving storage...\
[2023-09-14 06:27:15 4splitDomains](trainer.py 341): INFO done\
[2023-09-14 06:27:15 4splitDomains](iBatchLearn.py 155): INFO Acc:83.57893695622235; BWT:-6.100795594071201;\
=> merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:27:19 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:27:19 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:27:19 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 1, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-1.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_1.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:27:20 4splitDomains](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-1.pth\
[2023-09-14 06:27:20 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:27:22 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:27:22 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:27:22 4splitDomains](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 06:27:25 4splitDomains](iBatchLearn.py 167): INFO test split name:1\
--------------------------------Official Evaluation--------------------------------\
1 75.6583116414651\
=> merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:27:33 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:27:33 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:27:33 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 2, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-1.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-2.pth", "storage_path": "outputs/2023-09-14-06:17:11/4splitDomains/storage-1.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/4splitDomains/storage-2.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:27:33 4splitDomains](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-1.pth\
[2023-09-14 06:27:34 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:27:35 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:27:35 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:27:35 4splitDomains](trainer.py 327): INFO load storage...\
[2023-09-14 06:27:35 4splitDomains](trainer.py 331): INFO done\
[2023-09-14 06:27:35 4splitDomains](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 06:27:35 4splitDomains](iBatchLearn.py 92): INFO ====================== 2 =======================\
[2023-09-14 06:27:35 4splitDomains](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 06:27:35 4splitDomains](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 06:27:35 4splitDomains](trainer.py 223): INFO Epoch:0\
[2023-09-14 06:27:35 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:27:35 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:27:41 4splitDomains](trainer.py 286): INFO [0/46]	5.1204(5.1204)	2.6985(2.6985)	0.776(0.776)	82.81(82.81)\
[2023-09-14 06:27:43 4splitDomains](trainer.py 286): INFO [10/46]	0.6799(0.7287)	0.4911(0.3490)	0.460(0.902)	89.06(76.42)\
[2023-09-14 06:27:49 4splitDomains](trainer.py 286): INFO [20/46]	0.1744(0.6362)	0.0003(0.3536)	0.562(0.806)	82.81(78.20)\
[2023-09-14 06:27:55 4splitDomains](trainer.py 286): INFO [30/46]	0.1727(0.6293)	0.0005(0.3813)	0.501(0.754)	89.06(79.54)\
[2023-09-14 06:28:00 4splitDomains](trainer.py 286): INFO [40/46]	0.1696(0.5986)	0.0001(0.3693)	0.493(0.740)	87.50(79.92)\
[2023-09-14 06:28:02 4splitDomains](trainer.py 286): INFO [45/46]	0.2131(0.5788)	0.0001(0.3550)	0.620(0.736)	80.65(80.04)\
[2023-09-14 06:28:02 4splitDomains](trainer.py 288): INFO  * Train Acc 80.041\
[2023-09-14 06:28:07 4splitDomains](trainer.py 147): INFO  * Val Acc 83.684, Total time 4.55\
[2023-09-14 06:28:07 4splitDomains](trainer.py 223): INFO Epoch:1\
[2023-09-14 06:28:07 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:28:07 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:28:08 4splitDomains](trainer.py 286): INFO [0/46]	1.8037(1.8037)	1.6313(1.6313)	0.581(0.581)	85.94(85.94)\
[2023-09-14 06:28:15 4splitDomains](trainer.py 286): INFO [10/46]	1.3914(0.7599)	1.2134(0.5882)	0.715(0.501)	78.12(86.51)\
[2023-09-14 06:28:20 4splitDomains](trainer.py 286): INFO [20/46]	0.1703(0.6189)	0.0004(0.4443)	0.412(0.493)	89.06(86.90)\
[2023-09-14 06:28:25 4splitDomains](trainer.py 286): INFO [30/46]	1.5866(0.6007)	1.4156(0.4274)	0.345(0.476)	90.62(87.45)\
[2023-09-14 06:28:30 4splitDomains](trainer.py 286): INFO [40/46]	0.1694(0.5643)	0.0001(0.3917)	0.330(0.467)	90.62(87.58)\
[2023-09-14 06:28:31 4splitDomains](trainer.py 286): INFO [45/46]	0.1023(0.5393)	0.0001(0.3684)	0.406(0.463)	93.55(87.84)\
[2023-09-14 06:28:32 4splitDomains](trainer.py 288): INFO  * Train Acc 87.839\
[2023-09-14 06:28:36 4splitDomains](trainer.py 147): INFO  * Val Acc 84.211, Total time 4.56\
[2023-09-14 06:28:36 4splitDomains](trainer.py 223): INFO Epoch:2\
[2023-09-14 06:28:36 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:28:36 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:28:39 4splitDomains](trainer.py 286): INFO [0/46]	2.7675(2.7675)	2.5855(2.5855)	0.358(0.358)	93.75(93.75)\
[2023-09-14 06:28:42 4splitDomains](trainer.py 286): INFO [10/46]	0.1711(0.5837)	0.0004(0.4114)	0.381(0.338)	90.62(93.32)\
[2023-09-14 06:28:49 4splitDomains](trainer.py 286): INFO [20/46]	1.2454(0.6074)	1.0745(0.4339)	0.261(0.359)	93.75(92.19)\
[2023-09-14 06:28:53 4splitDomains](trainer.py 286): INFO [30/46]	0.1706(0.5545)	0.0003(0.3819)	0.241(0.361)	95.31(91.99)\
[2023-09-14 06:28:59 4splitDomains](trainer.py 286): INFO [40/46]	1.2382(0.5558)	1.0674(0.3830)	0.429(0.359)	92.19(92.19)\
[2023-09-14 06:29:02 4splitDomains](trainer.py 286): INFO [45/46]	0.1022(0.5609)	0.0001(0.3897)	0.520(0.357)	87.10(92.27)\
[2023-09-14 06:29:02 4splitDomains](trainer.py 288): INFO  * Train Acc 92.271\
[2023-09-14 06:29:07 4splitDomains](trainer.py 147): INFO  * Val Acc 84.737, Total time 4.82\
[2023-09-14 06:29:07 4splitDomains](trainer.py 223): INFO Epoch:3\
[2023-09-14 06:29:07 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:29:07 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:29:09 4splitDomains](trainer.py 286): INFO [0/46]	2.1051(2.1051)	1.9231(1.9231)	0.268(0.268)	95.31(95.31)\
[2023-09-14 06:29:14 4splitDomains](trainer.py 286): INFO [10/46]	0.1710(0.6529)	0.0004(0.4796)	0.347(0.314)	93.75(94.74)\
[2023-09-14 06:29:19 4splitDomains](trainer.py 286): INFO [20/46]	1.1489(0.5815)	0.9781(0.4094)	0.338(0.318)	90.62(94.20)\
[2023-09-14 06:29:24 4splitDomains](trainer.py 286): INFO [30/46]	0.1734(0.5664)	0.0006(0.3943)	0.298(0.317)	96.88(94.10)\
[2023-09-14 06:29:31 4splitDomains](trainer.py 286): INFO [40/46]	1.1211(0.5846)	0.9505(0.4123)	0.393(0.319)	92.19(94.21)\
[2023-09-14 06:29:33 4splitDomains](trainer.py 286): INFO [45/46]	0.1022(0.5682)	0.0001(0.3976)	0.249(0.321)	100.00(94.19)\
[2023-09-14 06:29:33 4splitDomains](trainer.py 288): INFO  * Train Acc 94.194\
[2023-09-14 06:29:38 4splitDomains](trainer.py 147): INFO  * Val Acc 85.789, Total time 4.79\
[2023-09-14 06:29:38 4splitDomains](trainer.py 223): INFO Epoch:4\
[2023-09-14 06:29:38 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:29:38 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:29:40 4splitDomains](trainer.py 286): INFO [0/46]	2.4121(2.4121)	2.2295(2.2295)	0.222(0.222)	96.88(96.88)\
[2023-09-14 06:29:46 4splitDomains](trainer.py 286): INFO [10/46]	2.3137(0.7634)	2.1260(0.5880)	0.300(0.298)	98.44(95.60)\
[2023-09-14 06:29:51 4splitDomains](trainer.py 286): INFO [20/46]	0.1712(0.6144)	0.0004(0.4397)	0.284(0.290)	93.75(95.83)\
[2023-09-14 06:29:57 4splitDomains](trainer.py 286): INFO [30/46]	1.6756(0.6177)	1.5050(0.4433)	0.297(0.291)	96.88(95.87)\
[2023-09-14 06:30:02 4splitDomains](trainer.py 286): INFO [40/46]	0.1709(0.6020)	0.0001(0.4280)	0.405(0.294)	90.62(95.77)\
[2023-09-14 06:30:04 4splitDomains](trainer.py 286): INFO [45/46]	0.1028(0.5775)	0.0002(0.4054)	0.356(0.295)	90.32(95.64)\
[2023-09-14 06:30:04 4splitDomains](trainer.py 288): INFO  * Train Acc 95.637\
[2023-09-14 06:30:09 4splitDomains](trainer.py 147): INFO  * Val Acc 87.105, Total time 4.84\
[2023-09-14 06:30:09 4splitDomains](trainer.py 223): INFO Epoch:5\
[2023-09-14 06:30:09 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:30:09 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:30:11 4splitDomains](trainer.py 286): INFO [0/46]	2.1337(2.1337)	1.9513(1.9513)	0.253(0.253)	96.88(96.88)\
[2023-09-14 06:30:16 4splitDomains](trainer.py 286): INFO [10/46]	0.1890(0.6452)	0.0185(0.4718)	0.254(0.261)	100.00(97.02)\
[2023-09-14 06:30:22 4splitDomains](trainer.py 286): INFO [20/46]	1.0176(0.6028)	0.8471(0.4306)	0.267(0.264)	98.44(97.17)\
[2023-09-14 06:30:26 4splitDomains](trainer.py 286): INFO [30/46]	0.6487(0.5516)	0.4781(0.3797)	0.235(0.267)	98.44(97.03)\
[2023-09-14 06:30:32 4splitDomains](trainer.py 286): INFO [40/46]	0.9149(0.5562)	0.7443(0.3846)	0.222(0.269)	98.44(97.07)\
[2023-09-14 06:30:34 4splitDomains](trainer.py 286): INFO [45/46]	0.1028(0.5462)	0.0001(0.3762)	0.207(0.268)	100.00(97.11)\
[2023-09-14 06:30:34 4splitDomains](trainer.py 288): INFO  * Train Acc 97.114\
[2023-09-14 06:30:39 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.83\
[2023-09-14 06:30:39 4splitDomains](trainer.py 223): INFO Epoch:6\
[2023-09-14 06:30:39 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:30:39 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:30:42 4splitDomains](trainer.py 286): INFO [0/46]	2.6336(2.6336)	2.4524(2.4524)	0.213(0.213)	98.44(98.44)\
[2023-09-14 06:30:47 4splitDomains](trainer.py 286): INFO [10/46]	0.8803(0.6847)	0.7095(0.5131)	0.270(0.258)	95.31(96.73)\
[2023-09-14 06:30:52 4splitDomains](trainer.py 286): INFO [20/46]	0.1712(0.6135)	0.0004(0.4411)	0.309(0.260)	96.88(96.88)\
[2023-09-14 06:30:58 4splitDomains](trainer.py 286): INFO [30/46]	1.1221(0.5943)	0.9512(0.4225)	0.295(0.268)	96.88(96.77)\
[2023-09-14 06:31:02 4splitDomains](trainer.py 286): INFO [40/46]	0.5343(0.5621)	0.3634(0.3905)	0.257(0.265)	98.44(96.84)\
[2023-09-14 06:31:04 4splitDomains](trainer.py 286): INFO [45/46]	0.1024(0.5490)	0.0001(0.3790)	0.268(0.265)	96.77(96.77)\
[2023-09-14 06:31:05 4splitDomains](trainer.py 288): INFO  * Train Acc 96.771\
[2023-09-14 06:31:09 4splitDomains](trainer.py 147): INFO  * Val Acc 85.789, Total time 4.48\
[2023-09-14 06:31:09 4splitDomains](trainer.py 223): INFO Epoch:7\
[2023-09-14 06:31:09 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:31:09 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:31:11 4splitDomains](trainer.py 286): INFO [0/46]	2.1937(2.1937)	2.0118(2.0118)	0.280(0.280)	93.75(93.75)\
[2023-09-14 06:31:17 4splitDomains](trainer.py 286): INFO [10/46]	0.8506(0.6928)	0.6799(0.5208)	0.274(0.266)	98.44(96.02)\
[2023-09-14 06:31:23 4splitDomains](trainer.py 286): INFO [20/46]	1.3575(0.6441)	1.1748(0.4706)	0.265(0.250)	96.88(96.95)\
[2023-09-14 06:31:27 4splitDomains](trainer.py 286): INFO [30/46]	0.1705(0.5767)	0.0004(0.4038)	0.245(0.258)	96.88(96.98)\
[2023-09-14 06:31:33 4splitDomains](trainer.py 286): INFO [40/46]	0.5075(0.5766)	0.3371(0.4035)	0.327(0.265)	95.31(96.95)\
[2023-09-14 06:31:35 4splitDomains](trainer.py 286): INFO [45/46]	0.1035(0.5626)	0.0001(0.3913)	0.257(0.266)	100.00(96.98)\
[2023-09-14 06:31:35 4splitDomains](trainer.py 288): INFO  * Train Acc 96.977\
[2023-09-14 06:31:40 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.80\
[2023-09-14 06:31:40 4splitDomains](trainer.py 223): INFO Epoch:8\
[2023-09-14 06:31:40 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:31:40 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:31:42 4splitDomains](trainer.py 286): INFO [0/46]	2.1758(2.1758)	1.9941(1.9941)	0.260(0.260)	98.44(98.44)\
[2023-09-14 06:31:47 4splitDomains](trainer.py 286): INFO [10/46]	0.8005(0.6187)	0.6295(0.4468)	0.230(0.242)	100.00(98.01)\
[2023-09-14 06:31:52 4splitDomains](trainer.py 286): INFO [20/46]	0.7417(0.5585)	0.5705(0.3865)	0.253(0.244)	96.88(97.84)\
[2023-09-14 06:31:57 4splitDomains](trainer.py 286): INFO [30/46]	0.1720(0.5477)	0.0005(0.3760)	0.213(0.249)	100.00(97.93)\
[2023-09-14 06:32:02 4splitDomains](trainer.py 286): INFO [40/46]	0.1710(0.5498)	0.0002(0.3783)	0.290(0.252)	95.31(97.79)\
[2023-09-14 06:32:05 4splitDomains](trainer.py 286): INFO [45/46]	0.1020(0.5461)	0.0001(0.3762)	0.287(0.254)	96.77(97.70)\
[2023-09-14 06:32:05 4splitDomains](trainer.py 288): INFO  * Train Acc 97.698\
[2023-09-14 06:32:10 4splitDomains](trainer.py 147): INFO  * Val Acc 85.263, Total time 4.85\
[2023-09-14 06:32:10 4splitDomains](trainer.py 223): INFO Epoch:9\
[2023-09-14 06:32:10 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:32:10 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:32:13 4splitDomains](trainer.py 286): INFO [0/46]	3.1756(3.1756)	2.9928(2.9928)	0.266(0.266)	98.44(98.44)\
[2023-09-14 06:32:17 4splitDomains](trainer.py 286): INFO [10/46]	0.1712(0.6921)	0.0006(0.5192)	0.201(0.232)	100.00(98.86)\
[2023-09-14 06:32:23 4splitDomains](trainer.py 286): INFO [20/46]	1.0599(0.6208)	0.8893(0.4490)	0.214(0.248)	100.00(98.14)\
[2023-09-14 06:32:28 4splitDomains](trainer.py 286): INFO [30/46]	0.8181(0.5960)	0.6467(0.4246)	0.260(0.254)	95.31(97.88)\
[2023-09-14 06:32:35 4splitDomains](trainer.py 286): INFO [40/46]	2.2136(0.6041)	2.0336(0.4326)	0.292(0.253)	96.88(97.94)\
[2023-09-14 06:32:36 4splitDomains](trainer.py 286): INFO [45/46]	0.1025(0.5782)	0.0002(0.4083)	0.356(0.255)	96.77(97.84)\
[2023-09-14 06:32:37 4splitDomains](trainer.py 288): INFO  * Train Acc 97.836\
[2023-09-14 06:32:41 4splitDomains](trainer.py 147): INFO  * Val Acc 86.579, Total time 4.85\
[2023-09-14 06:32:41 4splitDomains](trainer.py 223): INFO Epoch:10\
[2023-09-14 06:32:41 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:32:41 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:32:44 4splitDomains](trainer.py 286): INFO [0/46]	2.4181(2.4181)	2.2209(2.2209)	0.233(0.233)	100.00(100.00)\
[2023-09-14 06:32:49 4splitDomains](trainer.py 286): INFO [10/46]	0.8318(0.6665)	0.6542(0.4926)	0.226(0.241)	98.44(97.87)\
[2023-09-14 06:32:55 4splitDomains](trainer.py 286): INFO [20/46]	1.6092(0.6301)	1.4384(0.4560)	0.255(0.243)	96.88(97.84)\
[2023-09-14 06:33:00 4splitDomains](trainer.py 286): INFO [30/46]	1.1546(0.5892)	0.9839(0.4162)	0.253(0.248)	98.44(97.98)\
[2023-09-14 06:33:04 4splitDomains](trainer.py 286): INFO [40/46]	0.1802(0.5616)	0.0002(0.3873)	0.232(0.246)	100.00(98.13)\
[2023-09-14 06:33:07 4splitDomains](trainer.py 286): INFO [45/46]	0.2206(0.5472)	0.1164(0.3747)	0.288(0.246)	96.77(98.08)\
[2023-09-14 06:33:07 4splitDomains](trainer.py 288): INFO  * Train Acc 98.076\
[2023-09-14 06:33:11 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.84\
[2023-09-14 06:33:11 4splitDomains](trainer.py 223): INFO Epoch:11\
[2023-09-14 06:33:11 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:33:11 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:33:14 4splitDomains](trainer.py 286): INFO [0/46]	2.0638(2.0638)	1.8820(1.8820)	0.238(0.238)	100.00(100.00)\
[2023-09-14 06:33:19 4splitDomains](trainer.py 286): INFO [10/46]	0.1714(0.6726)	0.0005(0.5006)	0.233(0.249)	98.44(98.44)\
[2023-09-14 06:33:25 4splitDomains](trainer.py 286): INFO [20/46]	1.5291(0.6411)	1.3585(0.4695)	0.231(0.244)	98.44(98.21)\
[2023-09-14 06:33:29 4splitDomains](trainer.py 286): INFO [30/46]	0.1714(0.5750)	0.0003(0.4035)	0.271(0.242)	96.88(98.54)\
[2023-09-14 06:33:34 4splitDomains](trainer.py 286): INFO [40/46]	0.4123(0.5491)	0.2410(0.3776)	0.202(0.246)	100.00(98.36)\
[2023-09-14 06:33:36 4splitDomains](trainer.py 286): INFO [45/46]	0.1671(0.5359)	0.0641(0.3661)	0.307(0.246)	96.77(98.32)\
[2023-09-14 06:33:36 4splitDomains](trainer.py 288): INFO  * Train Acc 98.317\
[2023-09-14 06:33:41 4splitDomains](trainer.py 147): INFO  * Val Acc 84.211, Total time 4.63\
[2023-09-14 06:33:41 4splitDomains](trainer.py 223): INFO Epoch:12\
[2023-09-14 06:33:41 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:33:41 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:33:43 4splitDomains](trainer.py 286): INFO [0/46]	2.1611(2.1611)	1.9548(1.9548)	0.223(0.223)	100.00(100.00)\
[2023-09-14 06:33:49 4splitDomains](trainer.py 286): INFO [10/46]	1.5277(0.7620)	1.3573(0.5858)	0.241(0.229)	96.88(98.86)\
[2023-09-14 06:33:54 4splitDomains](trainer.py 286): INFO [20/46]	0.1712(0.6442)	0.0004(0.4704)	0.188(0.230)	100.00(98.96)\
[2023-09-14 06:33:59 4splitDomains](trainer.py 286): INFO [30/46]	0.9785(0.6017)	0.8079(0.4281)	0.229(0.240)	98.44(98.44)\
[2023-09-14 06:34:04 4splitDomains](trainer.py 286): INFO [40/46]	0.1703(0.5725)	0.0001(0.3994)	0.245(0.244)	96.88(98.21)\
[2023-09-14 06:34:06 4splitDomains](trainer.py 286): INFO [45/46]	0.4583(0.5548)	0.3556(0.3835)	0.376(0.246)	93.55(98.21)\
[2023-09-14 06:34:06 4splitDomains](trainer.py 288): INFO  * Train Acc 98.214\
[2023-09-14 06:34:11 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.69\
[2023-09-14 06:34:11 4splitDomains](trainer.py 223): INFO Epoch:13\
[2023-09-14 06:34:11 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:34:11 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:34:13 4splitDomains](trainer.py 286): INFO [0/46]	1.8103(1.8103)	1.6387(1.6387)	0.218(0.218)	98.44(98.44)\
[2023-09-14 06:34:18 4splitDomains](trainer.py 286): INFO [10/46]	0.8612(0.6468)	0.6907(0.4749)	0.299(0.239)	98.44(99.01)\
[2023-09-14 06:34:24 4splitDomains](trainer.py 286): INFO [20/46]	1.3956(0.6121)	1.2252(0.4408)	0.312(0.243)	93.75(98.36)\
[2023-09-14 06:34:29 4splitDomains](trainer.py 286): INFO [30/46]	0.9967(0.5736)	0.8259(0.4024)	0.222(0.244)	98.44(98.34)\
[2023-09-14 06:34:34 4splitDomains](trainer.py 286): INFO [40/46]	0.1782(0.5549)	0.0073(0.3831)	0.222(0.246)	98.44(98.09)\
[2023-09-14 06:34:36 4splitDomains](trainer.py 286): INFO [45/46]	0.1023(0.5457)	0.0001(0.3755)	0.342(0.244)	96.77(98.21)\
[2023-09-14 06:34:36 4splitDomains](trainer.py 288): INFO  * Train Acc 98.214\
[2023-09-14 06:34:41 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.76\
[2023-09-14 06:34:41 4splitDomains](trainer.py 223): INFO Epoch:14\
[2023-09-14 06:34:41 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:34:41 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:34:43 4splitDomains](trainer.py 286): INFO [0/46]	2.2176(2.2176)	2.0351(2.0351)	0.240(0.240)	98.44(98.44)\
[2023-09-14 06:34:48 4splitDomains](trainer.py 286): INFO [10/46]	0.1717(0.5926)	0.0004(0.4205)	0.316(0.243)	93.75(98.44)\
[2023-09-14 06:34:54 4splitDomains](trainer.py 286): INFO [20/46]	1.5187(0.6341)	1.3478(0.4615)	0.208(0.236)	100.00(98.66)\
[2023-09-14 06:34:59 4splitDomains](trainer.py 286): INFO [30/46]	0.1714(0.5654)	0.0004(0.3934)	0.223(0.237)	98.44(98.59)\
[2023-09-14 06:35:04 4splitDomains](trainer.py 286): INFO [40/46]	1.2544(0.5579)	1.0758(0.3857)	0.251(0.237)	98.44(98.51)\
[2023-09-14 06:35:06 4splitDomains](trainer.py 286): INFO [45/46]	0.1025(0.5461)	0.0002(0.3756)	0.360(0.239)	96.77(98.59)\
[2023-09-14 06:35:06 4splitDomains](trainer.py 288): INFO  * Train Acc 98.592\
[2023-09-14 06:35:11 4splitDomains](trainer.py 147): INFO  * Val Acc 83.947, Total time 4.84\
[2023-09-14 06:35:11 4splitDomains](trainer.py 223): INFO Epoch:15\
[2023-09-14 06:35:11 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:35:11 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:35:13 4splitDomains](trainer.py 286): INFO [0/46]	2.3596(2.3596)	2.1611(2.1611)	0.180(0.180)	100.00(100.00)\
[2023-09-14 06:35:18 4splitDomains](trainer.py 286): INFO [10/46]	0.2044(0.6389)	0.0331(0.4629)	0.251(0.220)	96.88(99.15)\
[2023-09-14 06:35:25 4splitDomains](trainer.py 286): INFO [20/46]	1.9855(0.6473)	1.8073(0.4734)	0.231(0.226)	100.00(98.88)\
[2023-09-14 06:35:29 4splitDomains](trainer.py 286): INFO [30/46]	0.1712(0.5730)	0.0005(0.3993)	0.231(0.228)	96.88(98.84)\
[2023-09-14 06:35:34 4splitDomains](trainer.py 286): INFO [40/46]	0.5668(0.5625)	0.3965(0.3896)	0.215(0.232)	100.00(98.78)\
[2023-09-14 06:35:37 4splitDomains](trainer.py 286): INFO [45/46]	0.1019(0.5564)	0.0001(0.3852)	0.237(0.235)	96.77(98.66)\
[2023-09-14 06:35:37 4splitDomains](trainer.py 288): INFO  * Train Acc 98.660\
[2023-09-14 06:35:42 4splitDomains](trainer.py 147): INFO  * Val Acc 85.000, Total time 4.83\
[2023-09-14 06:35:42 4splitDomains](trainer.py 223): INFO Epoch:16\
[2023-09-14 06:35:42 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:35:42 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:35:43 4splitDomains](trainer.py 286): INFO [0/46]	1.2005(1.2005)	1.0190(1.0190)	0.210(0.210)	100.00(100.00)\
[2023-09-14 06:35:48 4splitDomains](trainer.py 286): INFO [10/46]	0.1710(0.6072)	0.0004(0.4346)	0.245(0.229)	98.44(99.01)\
[2023-09-14 06:35:54 4splitDomains](trainer.py 286): INFO [20/46]	0.1710(0.5752)	0.0004(0.4027)	0.240(0.232)	100.00(98.74)\
[2023-09-14 06:36:00 4splitDomains](trainer.py 286): INFO [30/46]	0.1711(0.5833)	0.0002(0.4113)	0.229(0.232)	100.00(98.69)\
[2023-09-14 06:36:05 4splitDomains](trainer.py 286): INFO [40/46]	0.1704(0.5651)	0.0001(0.3934)	0.208(0.230)	100.00(98.78)\
[2023-09-14 06:36:07 4splitDomains](trainer.py 286): INFO [45/46]	0.1023(0.5527)	0.0001(0.3826)	0.305(0.235)	96.77(98.52)\
[2023-09-14 06:36:07 4splitDomains](trainer.py 288): INFO  * Train Acc 98.523\
[2023-09-14 06:36:12 4splitDomains](trainer.py 147): INFO  * Val Acc 84.737, Total time 4.89\
[2023-09-14 06:36:12 4splitDomains](trainer.py 223): INFO Epoch:17\
[2023-09-14 06:36:12 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:36:12 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:36:14 4splitDomains](trainer.py 286): INFO [0/46]	2.2266(2.2266)	2.0380(2.0380)	0.228(0.228)	98.44(98.44)\
[2023-09-14 06:36:19 4splitDomains](trainer.py 286): INFO [10/46]	0.1709(0.6586)	0.0004(0.4861)	0.219(0.226)	98.44(98.86)\
[2023-09-14 06:36:26 4splitDomains](trainer.py 286): INFO [20/46]	1.5551(0.6603)	1.3667(0.4865)	0.279(0.237)	96.88(98.74)\
[2023-09-14 06:36:31 4splitDomains](trainer.py 286): INFO [30/46]	0.1719(0.6053)	0.0004(0.4321)	0.244(0.241)	96.88(98.44)\
[2023-09-14 06:36:39 4splitDomains](trainer.py 286): INFO [40/46]	2.4661(0.6493)	2.2863(0.4763)	0.236(0.241)	100.00(98.36)\
[2023-09-14 06:36:40 4splitDomains](trainer.py 286): INFO [45/46]	0.1024(0.6124)	0.0001(0.4411)	0.216(0.239)	100.00(98.39)\
[2023-09-14 06:36:40 4splitDomains](trainer.py 288): INFO  * Train Acc 98.385\
[2023-09-14 06:36:45 4splitDomains](trainer.py 147): INFO  * Val Acc 84.737, Total time 4.83\
[2023-09-14 06:36:45 4splitDomains](trainer.py 223): INFO Epoch:18\
[2023-09-14 06:36:45 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:36:45 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:36:48 4splitDomains](trainer.py 286): INFO [0/46]	2.8615(2.8615)	2.6802(2.6802)	0.211(0.211)	98.44(98.44)\
[2023-09-14 06:36:53 4splitDomains](trainer.py 286): INFO [10/46]	0.1713(0.6874)	0.0004(0.5149)	0.233(0.228)	98.44(99.01)\
[2023-09-14 06:36:58 4splitDomains](trainer.py 286): INFO [20/46]	1.1964(0.6126)	1.0246(0.4407)	0.225(0.235)	96.88(98.88)\
[2023-09-14 06:37:03 4splitDomains](trainer.py 286): INFO [30/46]	0.1711(0.5638)	0.0002(0.3916)	0.213(0.234)	100.00(99.04)\
[2023-09-14 06:37:08 4splitDomains](trainer.py 286): INFO [40/46]	0.8778(0.5552)	0.7068(0.3830)	0.256(0.232)	96.88(98.97)\
[2023-09-14 06:37:10 4splitDomains](trainer.py 286): INFO [45/46]	0.1038(0.5348)	0.0001(0.3642)	0.342(0.234)	100.00(98.94)\
[2023-09-14 06:37:10 4splitDomains](trainer.py 288): INFO  * Train Acc 98.935\
[2023-09-14 06:37:15 4splitDomains](trainer.py 147): INFO  * Val Acc 85.000, Total time 4.82\
[2023-09-14 06:37:15 4splitDomains](trainer.py 223): INFO Epoch:19\
[2023-09-14 06:37:15 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:37:15 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:37:17 4splitDomains](trainer.py 286): INFO [0/46]	2.3177(2.3177)	2.1098(2.1098)	0.172(0.172)	100.00(100.00)\
[2023-09-14 06:37:21 4splitDomains](trainer.py 286): INFO [10/46]	0.1712(0.6314)	0.0005(0.4550)	0.245(0.214)	98.44(99.57)\
[2023-09-14 06:37:27 4splitDomains](trainer.py 286): INFO [20/46]	0.4009(0.5949)	0.2091(0.4196)	0.214(0.220)	98.44(99.11)\
[2023-09-14 06:37:33 4splitDomains](trainer.py 286): INFO [30/46]	0.1710(0.5796)	0.0004(0.4055)	0.251(0.228)	98.44(98.99)\
[2023-09-14 06:37:39 4splitDomains](trainer.py 286): INFO [40/46]	1.7800(0.5884)	1.6090(0.4149)	0.220(0.229)	100.00(98.86)\
[2023-09-14 06:37:40 4splitDomains](trainer.py 286): INFO [45/46]	0.1028(0.5576)	0.0002(0.3859)	0.250(0.231)	96.77(98.69)\
[2023-09-14 06:37:40 4splitDomains](trainer.py 288): INFO  * Train Acc 98.695\
[2023-09-14 06:37:45 4splitDomains](trainer.py 147): INFO  * Val Acc 83.947, Total time 5.01\
[2023-09-14 06:37:45 4splitDomains](trainer.py 223): INFO Epoch:20\
[2023-09-14 06:37:45 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:37:45 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:37:47 4splitDomains](trainer.py 286): INFO [0/46]	2.1107(2.1107)	1.9292(1.9292)	0.216(0.216)	98.44(98.44)\
[2023-09-14 06:37:52 4splitDomains](trainer.py 286): INFO [10/46]	0.1712(0.6384)	0.0004(0.4666)	0.221(0.220)	98.44(99.15)\
[2023-09-14 06:37:58 4splitDomains](trainer.py 286): INFO [20/46]	1.4176(0.6059)	1.2328(0.4332)	0.262(0.237)	100.00(98.66)\
[2023-09-14 06:38:03 4splitDomains](trainer.py 286): INFO [30/46]	0.1712(0.5813)	0.0004(0.4076)	0.357(0.241)	92.19(98.34)\
[2023-09-14 06:38:08 4splitDomains](trainer.py 286): INFO [40/46]	0.6401(0.5537)	0.4694(0.3805)	0.232(0.241)	98.44(98.36)\
[2023-09-14 06:38:10 4splitDomains](trainer.py 286): INFO [45/46]	0.1024(0.5409)	0.0001(0.3696)	0.369(0.241)	90.32(98.35)\
[2023-09-14 06:38:10 4splitDomains](trainer.py 288): INFO  * Train Acc 98.351\
[2023-09-14 06:38:15 4splitDomains](trainer.py 147): INFO  * Val Acc 85.000, Total time 4.84\
[2023-09-14 06:38:15 4splitDomains](trainer.py 223): INFO Epoch:21\
[2023-09-14 06:38:15 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:38:15 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:38:17 4splitDomains](trainer.py 286): INFO [0/46]	2.1368(2.1368)	1.9559(1.9559)	0.209(0.209)	100.00(100.00)\
[2023-09-14 06:38:21 4splitDomains](trainer.py 286): INFO [10/46]	0.1712(0.5801)	0.0004(0.4080)	0.237(0.234)	98.44(98.72)\
[2023-09-14 06:38:27 4splitDomains](trainer.py 286): INFO [20/46]	0.8316(0.5695)	0.6606(0.3971)	0.200(0.231)	100.00(98.74)\
[2023-09-14 06:38:33 4splitDomains](trainer.py 286): INFO [30/46]	1.1549(0.5786)	0.9840(0.4059)	0.249(0.231)	98.44(98.79)\
[2023-09-14 06:38:39 4splitDomains](trainer.py 286): INFO [40/46]	1.4937(0.5786)	1.3233(0.4061)	0.227(0.235)	100.00(98.70)\
[2023-09-14 06:38:41 4splitDomains](trainer.py 286): INFO [45/46]	0.1027(0.5740)	0.0002(0.4031)	0.330(0.235)	96.77(98.76)\
[2023-09-14 06:38:42 4splitDomains](trainer.py 288): INFO  * Train Acc 98.763\
[2023-09-14 06:38:46 4splitDomains](trainer.py 147): INFO  * Val Acc 84.737, Total time 4.88\
[2023-09-14 06:38:46 4splitDomains](trainer.py 223): INFO Epoch:22\
[2023-09-14 06:38:46 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:38:46 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:38:49 4splitDomains](trainer.py 286): INFO [0/46]	2.8344(2.8344)	2.6529(2.6529)	0.258(0.258)	95.31(95.31)\
[2023-09-14 06:38:53 4splitDomains](trainer.py 286): INFO [10/46]	0.1710(0.6015)	0.0004(0.4289)	0.270(0.225)	96.88(98.86)\
[2023-09-14 06:38:59 4splitDomains](trainer.py 286): INFO [20/46]	1.5184(0.6132)	1.3478(0.4402)	0.214(0.224)	96.88(99.11)\
[2023-09-14 06:39:04 4splitDomains](trainer.py 286): INFO [30/46]	0.1844(0.5738)	0.0004(0.4009)	0.211(0.229)	100.00(99.04)\
[2023-09-14 06:39:10 4splitDomains](trainer.py 286): INFO [40/46]	0.8348(0.5833)	0.6645(0.4098)	0.195(0.229)	98.44(98.89)\
[2023-09-14 06:39:13 4splitDomains](trainer.py 286): INFO [45/46]	0.1029(0.5701)	0.0002(0.3984)	0.266(0.230)	100.00(98.80)\
[2023-09-14 06:39:13 4splitDomains](trainer.py 288): INFO  * Train Acc 98.798\
[2023-09-14 06:39:18 4splitDomains](trainer.py 147): INFO  * Val Acc 86.053, Total time 4.83\
[2023-09-14 06:39:18 4splitDomains](trainer.py 223): INFO Epoch:23\
[2023-09-14 06:39:18 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:39:18 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:39:20 4splitDomains](trainer.py 286): INFO [0/46]	2.2287(2.2287)	2.0385(2.0385)	0.213(0.213)	98.44(98.44)\
[2023-09-14 06:39:24 4splitDomains](trainer.py 286): INFO [10/46]	0.1715(0.5995)	0.0006(0.4212)	0.272(0.220)	95.31(99.29)\
[2023-09-14 06:39:30 4splitDomains](trainer.py 286): INFO [20/46]	0.1710(0.5945)	0.0004(0.4165)	0.231(0.228)	98.44(98.81)\
[2023-09-14 06:39:35 4splitDomains](trainer.py 286): INFO [30/46]	0.1721(0.5602)	0.0005(0.3838)	0.223(0.234)	100.00(98.64)\
[2023-09-14 06:39:42 4splitDomains](trainer.py 286): INFO [40/46]	0.1705(0.5952)	0.0001(0.4200)	0.197(0.234)	100.00(98.70)\
[2023-09-14 06:39:44 4splitDomains](trainer.py 286): INFO [45/46]	0.1026(0.5815)	0.0001(0.4082)	0.221(0.235)	100.00(98.63)\
[2023-09-14 06:39:44 4splitDomains](trainer.py 288): INFO  * Train Acc 98.626\
[2023-09-14 06:39:49 4splitDomains](trainer.py 147): INFO  * Val Acc 84.737, Total time 4.95\
[2023-09-14 06:39:49 4splitDomains](trainer.py 223): INFO Epoch:24\
[2023-09-14 06:39:49 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:39:49 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:39:52 4splitDomains](trainer.py 286): INFO [0/46]	2.6983(2.6983)	2.5156(2.5156)	0.244(0.244)	98.44(98.44)\
[2023-09-14 06:39:57 4splitDomains](trainer.py 286): INFO [10/46]	0.1721(0.6836)	0.0009(0.5079)	0.246(0.224)	98.44(99.15)\
[2023-09-14 06:40:03 4splitDomains](trainer.py 286): INFO [20/46]	0.1739(0.6333)	0.0004(0.4564)	0.222(0.226)	100.00(99.11)\
[2023-09-14 06:40:08 4splitDomains](trainer.py 286): INFO [30/46]	0.7946(0.5908)	0.6229(0.4156)	0.201(0.228)	100.00(99.09)\
[2023-09-14 06:40:13 4splitDomains](trainer.py 286): INFO [40/46]	1.1401(0.5811)	0.9694(0.4070)	0.218(0.234)	100.00(98.82)\
[2023-09-14 06:40:15 4splitDomains](trainer.py 286): INFO [45/46]	0.1023(0.5585)	0.0001(0.3863)	0.273(0.234)	100.00(98.80)\
[2023-09-14 06:40:15 4splitDomains](trainer.py 288): INFO  * Train Acc 98.798\
[2023-09-14 06:40:20 4splitDomains](trainer.py 147): INFO  * Val Acc 84.737, Total time 4.83\
[2023-09-14 06:40:20 4splitDomains](trainer.py 223): INFO Epoch:25\
[2023-09-14 06:40:20 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:40:20 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:40:22 4splitDomains](trainer.py 286): INFO [0/46]	2.3728(2.3728)	2.1844(2.1844)	0.198(0.198)	100.00(100.00)\
[2023-09-14 06:40:27 4splitDomains](trainer.py 286): INFO [10/46]	0.1712(0.6701)	0.0003(0.4976)	0.200(0.218)	100.00(99.43)\
[2023-09-14 06:40:33 4splitDomains](trainer.py 286): INFO [20/46]	0.9953(0.6276)	0.8246(0.4554)	0.213(0.220)	100.00(99.48)\
[2023-09-14 06:40:37 4splitDomains](trainer.py 286): INFO [30/46]	0.1718(0.5672)	0.0003(0.3953)	0.187(0.225)	100.00(99.19)\
[2023-09-14 06:40:43 4splitDomains](trainer.py 286): INFO [40/46]	0.1704(0.5748)	0.0001(0.4027)	0.225(0.224)	100.00(99.28)\
[2023-09-14 06:40:46 4splitDomains](trainer.py 286): INFO [45/46]	0.1025(0.5656)	0.0001(0.3951)	0.304(0.225)	96.77(99.21)\
[2023-09-14 06:40:46 4splitDomains](trainer.py 288): INFO  * Train Acc 99.210\
[2023-09-14 06:40:50 4splitDomains](trainer.py 147): INFO  * Val Acc 85.000, Total time 4.50\
[2023-09-14 06:40:50 4splitDomains](trainer.py 223): INFO Epoch:26\
[2023-09-14 06:40:50 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:40:50 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:40:53 4splitDomains](trainer.py 286): INFO [0/46]	2.2075(2.2075)	2.0270(2.0270)	0.196(0.196)	100.00(100.00)\
[2023-09-14 06:40:57 4splitDomains](trainer.py 286): INFO [10/46]	0.1712(0.6116)	0.0003(0.4396)	0.217(0.219)	98.44(99.15)\
[2023-09-14 06:41:03 4splitDomains](trainer.py 286): INFO [20/46]	0.3108(0.5876)	0.1399(0.4138)	0.189(0.217)	98.44(98.88)\
[2023-09-14 06:41:09 4splitDomains](trainer.py 286): INFO [30/46]	0.1762(0.5996)	0.0006(0.4258)	0.216(0.221)	98.44(98.89)\
[2023-09-14 06:41:14 4splitDomains](trainer.py 286): INFO [40/46]	0.6693(0.5706)	0.4960(0.3974)	0.252(0.225)	98.44(98.82)\
[2023-09-14 06:41:17 4splitDomains](trainer.py 286): INFO [45/46]	0.1025(0.5712)	0.0001(0.3998)	0.331(0.229)	96.77(98.73)\
[2023-09-14 06:41:17 4splitDomains](trainer.py 288): INFO  * Train Acc 98.729\
[2023-09-14 06:41:21 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.48\
[2023-09-14 06:41:21 4splitDomains](trainer.py 223): INFO Epoch:27\
[2023-09-14 06:41:21 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:41:21 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:41:24 4splitDomains](trainer.py 286): INFO [0/46]	2.5616(2.5616)	2.3806(2.3806)	0.206(0.206)	100.00(100.00)\
[2023-09-14 06:41:29 4splitDomains](trainer.py 286): INFO [10/46]	0.4100(0.6749)	0.2393(0.5002)	0.237(0.213)	98.44(99.29)\
[2023-09-14 06:41:35 4splitDomains](trainer.py 286): INFO [20/46]	1.3433(0.6295)	1.1722(0.4550)	0.280(0.211)	93.75(99.26)\
[2023-09-14 06:41:40 4splitDomains](trainer.py 286): INFO [30/46]	1.6615(0.6032)	1.4907(0.4298)	0.439(0.223)	92.19(98.84)\
[2023-09-14 06:41:44 4splitDomains](trainer.py 286): INFO [40/46]	0.1711(0.5612)	0.0002(0.3883)	0.225(0.224)	98.44(98.86)\
[2023-09-14 06:41:46 4splitDomains](trainer.py 286): INFO [45/46]	0.1024(0.5421)	0.0001(0.3709)	0.333(0.227)	96.77(98.73)\
[2023-09-14 06:41:46 4splitDomains](trainer.py 288): INFO  * Train Acc 98.729\
[2023-09-14 06:41:51 4splitDomains](trainer.py 147): INFO  * Val Acc 83.421, Total time 4.45\
[2023-09-14 06:41:51 4splitDomains](trainer.py 223): INFO Epoch:28\
[2023-09-14 06:41:51 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:41:51 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:41:53 4splitDomains](trainer.py 286): INFO [0/46]	2.3411(2.3411)	2.1597(2.1597)	0.193(0.193)	100.00(100.00)\
[2023-09-14 06:41:58 4splitDomains](trainer.py 286): INFO [10/46]	0.5613(0.6107)	0.3901(0.4342)	0.200(0.217)	100.00(99.15)\
[2023-09-14 06:42:03 4splitDomains](trainer.py 286): INFO [20/46]	0.1865(0.5876)	0.0047(0.4126)	0.251(0.229)	100.00(99.11)\
[2023-09-14 06:42:08 4splitDomains](trainer.py 286): INFO [30/46]	0.1712(0.5441)	0.0004(0.3691)	0.215(0.229)	98.44(99.04)\
[2023-09-14 06:42:13 4splitDomains](trainer.py 286): INFO [40/46]	0.1720(0.5517)	0.0002(0.3777)	0.260(0.236)	95.31(98.59)\
[2023-09-14 06:42:16 4splitDomains](trainer.py 286): INFO [45/46]	0.1034(0.5450)	0.0002(0.3728)	0.512(0.238)	87.10(98.49)\
[2023-09-14 06:42:16 4splitDomains](trainer.py 288): INFO  * Train Acc 98.488\
[2023-09-14 06:42:21 4splitDomains](trainer.py 147): INFO  * Val Acc 85.000, Total time 4.85\
[2023-09-14 06:42:21 4splitDomains](trainer.py 223): INFO Epoch:29\
[2023-09-14 06:42:21 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:42:21 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:42:23 4splitDomains](trainer.py 286): INFO [0/46]	2.3971(2.3971)	2.2167(2.2167)	0.207(0.207)	100.00(100.00)\
[2023-09-14 06:42:28 4splitDomains](trainer.py 286): INFO [10/46]	0.1712(0.6807)	0.0004(0.5074)	0.237(0.213)	98.44(99.29)\
[2023-09-14 06:42:34 4splitDomains](trainer.py 286): INFO [20/46]	1.6376(0.6365)	1.4673(0.4644)	0.247(0.223)	96.88(98.66)\
[2023-09-14 06:42:39 4splitDomains](trainer.py 286): INFO [30/46]	0.1826(0.5806)	0.0008(0.4082)	0.244(0.227)	98.44(98.74)\
[2023-09-14 06:42:44 4splitDomains](trainer.py 286): INFO [40/46]	0.1708(0.5660)	0.0002(0.3939)	0.206(0.230)	100.00(98.67)\
[2023-09-14 06:42:47 4splitDomains](trainer.py 286): INFO [45/46]	0.3789(0.5776)	0.2760(0.4069)	0.306(0.232)	96.77(98.63)\
[2023-09-14 06:42:47 4splitDomains](trainer.py 288): INFO  * Train Acc 98.626\
[2023-09-14 06:42:52 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.84\
=> Saving model to: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-2.pth\
=> Save Done\
[2023-09-14 06:42:52 4splitDomains](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 06:42:54 4splitDomains](trainer.py 147): INFO  * Val Acc 89.920, Total time 1.25\
[2023-09-14 06:42:54 4splitDomains](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 06:42:55 4splitDomains](trainer.py 147): INFO  * Val Acc 69.841, Total time 1.41\
[2023-09-14 06:42:55 4splitDomains](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 06:43:00 4splitDomains](trainer.py 147): INFO  * Val Acc 85.526, Total time 4.97\
[2023-09-14 06:43:00 4splitDomains](trainer.py 335): INFO saving storage...\
[2023-09-14 06:43:00 4splitDomains](trainer.py 341): INFO done\
[2023-09-14 06:43:00 4splitDomains](iBatchLearn.py 155): INFO Acc:81.76267040037611; BWT:-6.748487448977741;\
=> merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:43:05 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:43:05 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:43:05 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 2, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-2.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_2.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:43:06 4splitDomains](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-2.pth\
[2023-09-14 06:43:06 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:43:08 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:43:08 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:43:08 4splitDomains](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 06:43:11 4splitDomains](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 06:43:13 4splitDomains](iBatchLearn.py 167): INFO test split name:2\
--------------------------------Official Evaluation--------------------------------\
2 73.99697451246105\
=> merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:43:24 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:43:24 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:43:24 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 3, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-2.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-3.pth", "storage_path": "outputs/2023-09-14-06:17:11/4splitDomains/storage-2.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/4splitDomains/storage-3.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:43:25 4splitDomains](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-2.pth\
[2023-09-14 06:43:25 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:43:26 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:43:26 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:43:26 4splitDomains](trainer.py 327): INFO load storage...\
[2023-09-14 06:43:27 4splitDomains](trainer.py 331): INFO done\
[2023-09-14 06:43:27 4splitDomains](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 06:43:27 4splitDomains](iBatchLearn.py 92): INFO ====================== 3 =======================\
[2023-09-14 06:43:27 4splitDomains](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 06:43:27 4splitDomains](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 06:43:27 4splitDomains](trainer.py 223): INFO Epoch:0\
[2023-09-14 06:43:27 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:43:27 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:43:30 4splitDomains](trainer.py 286): INFO [0/26]	3.6065(3.6065)	1.1132(1.1132)	1.060(1.060)	68.75(68.75)\
[2023-09-14 06:43:32 4splitDomains](trainer.py 286): INFO [10/26]	0.1713(0.4884)	0.0004(0.1043)	1.193(1.150)	71.88(69.74)\
[2023-09-14 06:43:34 4splitDomains](trainer.py 286): INFO [20/26]	0.1711(0.3459)	0.0002(0.0617)	1.373(1.191)	65.62(69.05)\
[2023-09-14 06:43:35 4splitDomains](trainer.py 286): INFO [25/26]	0.2172(0.3209)	0.0001(0.0560)	1.394(1.172)	66.67(69.45)\
[2023-09-14 06:43:35 4splitDomains](trainer.py 288): INFO  * Train Acc 69.448\
[2023-09-14 06:43:37 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.42\
[2023-09-14 06:43:37 4splitDomains](trainer.py 223): INFO Epoch:1\
[2023-09-14 06:43:37 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:43:37 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:43:38 4splitDomains](trainer.py 286): INFO [0/26]	1.3905(1.3905)	1.2191(1.2191)	0.746(0.746)	82.81(82.81)\
[2023-09-14 06:43:40 4splitDomains](trainer.py 286): INFO [10/26]	0.1711(0.3183)	0.0004(0.1468)	0.756(0.649)	84.38(83.81)\
[2023-09-14 06:43:42 4splitDomains](trainer.py 286): INFO [20/26]	0.2675(0.2675)	0.0950(0.0962)	0.889(0.667)	76.56(83.85)\
[2023-09-14 06:43:43 4splitDomains](trainer.py 286): INFO [25/26]	0.1040(0.2540)	0.0001(0.0854)	0.466(0.656)	90.00(84.54)\
[2023-09-14 06:43:43 4splitDomains](trainer.py 288): INFO  * Train Acc 84.540\
[2023-09-14 06:43:45 4splitDomains](trainer.py 147): INFO  * Val Acc 79.024, Total time 1.39\
[2023-09-14 06:43:45 4splitDomains](trainer.py 223): INFO Epoch:2\
[2023-09-14 06:43:45 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:43:45 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:43:46 4splitDomains](trainer.py 286): INFO [0/26]	1.1901(1.1901)	1.0059(1.0059)	0.561(0.561)	90.62(90.62)\
[2023-09-14 06:43:48 4splitDomains](trainer.py 286): INFO [10/26]	0.1710(0.3110)	0.0004(0.1390)	0.388(0.477)	92.19(90.48)\
[2023-09-14 06:43:50 4splitDomains](trainer.py 286): INFO [20/26]	0.4160(0.2758)	0.2451(0.1033)	0.568(0.496)	87.50(89.66)\
[2023-09-14 06:43:51 4splitDomains](trainer.py 286): INFO [25/26]	0.1026(0.2615)	0.0001(0.0921)	0.716(0.497)	80.00(89.57)\
[2023-09-14 06:43:51 4splitDomains](trainer.py 288): INFO  * Train Acc 89.571\
[2023-09-14 06:43:53 4splitDomains](trainer.py 147): INFO  * Val Acc 81.463, Total time 1.42\
[2023-09-14 06:43:53 4splitDomains](trainer.py 223): INFO Epoch:3\
[2023-09-14 06:43:53 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:43:53 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:43:54 4splitDomains](trainer.py 286): INFO [0/26]	1.2344(1.2344)	1.0536(1.0536)	0.362(0.362)	95.31(95.31)\
[2023-09-14 06:43:56 4splitDomains](trainer.py 286): INFO [10/26]	0.1720(0.3213)	0.0005(0.1469)	0.386(0.385)	95.31(94.32)\
[2023-09-14 06:43:59 4splitDomains](trainer.py 286): INFO [20/26]	0.1703(0.2740)	0.0002(0.1009)	0.329(0.380)	96.88(94.64)\
[2023-09-14 06:44:00 4splitDomains](trainer.py 286): INFO [25/26]	0.1033(0.2604)	0.0002(0.0903)	0.427(0.394)	90.00(94.36)\
[2023-09-14 06:44:00 4splitDomains](trainer.py 288): INFO  * Train Acc 94.356\
[2023-09-14 06:44:01 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.43\
[2023-09-14 06:44:01 4splitDomains](trainer.py 223): INFO Epoch:4\
[2023-09-14 06:44:01 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:01 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:02 4splitDomains](trainer.py 286): INFO [0/26]	1.1958(1.1958)	1.0139(1.0139)	0.426(0.426)	93.75(93.75)\
[2023-09-14 06:44:04 4splitDomains](trainer.py 286): INFO [10/26]	0.1712(0.3021)	0.0004(0.1285)	0.287(0.341)	98.44(96.73)\
[2023-09-14 06:44:07 4splitDomains](trainer.py 286): INFO [20/26]	0.3100(0.2638)	0.1396(0.0908)	0.397(0.336)	96.88(96.65)\
[2023-09-14 06:44:08 4splitDomains](trainer.py 286): INFO [25/26]	0.1024(0.2563)	0.0002(0.0865)	0.446(0.342)	86.67(96.32)\
[2023-09-14 06:44:08 4splitDomains](trainer.py 288): INFO  * Train Acc 96.319\
[2023-09-14 06:44:09 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.39\
[2023-09-14 06:44:09 4splitDomains](trainer.py 223): INFO Epoch:5\
[2023-09-14 06:44:09 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:09 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:11 4splitDomains](trainer.py 286): INFO [0/26]	1.3154(1.3154)	1.1435(1.1435)	0.340(0.340)	98.44(98.44)\
[2023-09-14 06:44:13 4splitDomains](trainer.py 286): INFO [10/26]	0.1706(0.3043)	0.0004(0.1335)	0.304(0.337)	96.88(96.73)\
[2023-09-14 06:44:15 4splitDomains](trainer.py 286): INFO [20/26]	0.5683(0.2818)	0.3979(0.1106)	0.312(0.337)	98.44(96.73)\
[2023-09-14 06:44:16 4splitDomains](trainer.py 286): INFO [25/26]	0.1066(0.2660)	0.0001(0.0973)	0.324(0.336)	100.00(96.93)\
[2023-09-14 06:44:16 4splitDomains](trainer.py 288): INFO  * Train Acc 96.933\
[2023-09-14 06:44:18 4splitDomains](trainer.py 147): INFO  * Val Acc 80.976, Total time 1.49\
[2023-09-14 06:44:18 4splitDomains](trainer.py 223): INFO Epoch:6\
[2023-09-14 06:44:18 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:18 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:19 4splitDomains](trainer.py 286): INFO [0/26]	1.2558(1.2558)	1.0822(1.0822)	0.280(0.280)	98.44(98.44)\
[2023-09-14 06:44:21 4splitDomains](trainer.py 286): INFO [10/26]	0.1710(0.3092)	0.0004(0.1377)	0.286(0.303)	98.44(97.59)\
[2023-09-14 06:44:24 4splitDomains](trainer.py 286): INFO [20/26]	0.2135(0.2752)	0.0430(0.1041)	0.284(0.291)	96.88(97.92)\
[2023-09-14 06:44:24 4splitDomains](trainer.py 286): INFO [25/26]	0.1020(0.2584)	0.0002(0.0901)	0.336(0.295)	100.00(97.85)\
[2023-09-14 06:44:25 4splitDomains](trainer.py 288): INFO  * Train Acc 97.853\
[2023-09-14 06:44:26 4splitDomains](trainer.py 147): INFO  * Val Acc 80.976, Total time 1.39\
[2023-09-14 06:44:26 4splitDomains](trainer.py 223): INFO Epoch:7\
[2023-09-14 06:44:26 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:26 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:27 4splitDomains](trainer.py 286): INFO [0/26]	1.3189(1.3189)	1.1473(1.1473)	0.215(0.215)	100.00(100.00)\
[2023-09-14 06:44:29 4splitDomains](trainer.py 286): INFO [10/26]	0.1708(0.3053)	0.0003(0.1346)	0.280(0.289)	98.44(98.15)\
[2023-09-14 06:44:31 4splitDomains](trainer.py 286): INFO [20/26]	0.3784(0.2641)	0.2079(0.0929)	0.270(0.288)	98.44(98.07)\
[2023-09-14 06:44:32 4splitDomains](trainer.py 286): INFO [25/26]	0.1016(0.2501)	0.0001(0.0818)	0.429(0.285)	93.33(98.10)\
[2023-09-14 06:44:33 4splitDomains](trainer.py 288): INFO  * Train Acc 98.098\
[2023-09-14 06:44:34 4splitDomains](trainer.py 147): INFO  * Val Acc 80.976, Total time 1.32\
[2023-09-14 06:44:34 4splitDomains](trainer.py 223): INFO Epoch:8\
[2023-09-14 06:44:34 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:34 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:35 4splitDomains](trainer.py 286): INFO [0/26]	1.1052(1.1052)	0.9310(0.9310)	0.274(0.274)	98.44(98.44)\
[2023-09-14 06:44:37 4splitDomains](trainer.py 286): INFO [10/26]	0.1717(0.2991)	0.0004(0.1281)	0.208(0.256)	100.00(98.72)\
[2023-09-14 06:44:39 4splitDomains](trainer.py 286): INFO [20/26]	0.2975(0.2641)	0.1273(0.0934)	0.260(0.272)	100.00(98.74)\
[2023-09-14 06:44:40 4splitDomains](trainer.py 286): INFO [25/26]	0.1018(0.2517)	0.0001(0.0838)	0.242(0.269)	100.00(98.83)\
[2023-09-14 06:44:40 4splitDomains](trainer.py 288): INFO  * Train Acc 98.834\
[2023-09-14 06:44:42 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.39\
[2023-09-14 06:44:42 4splitDomains](trainer.py 223): INFO Epoch:9\
[2023-09-14 06:44:42 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:42 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:43 4splitDomains](trainer.py 286): INFO [0/26]	1.1857(1.1857)	1.0136(1.0136)	0.246(0.246)	100.00(100.00)\
[2023-09-14 06:44:45 4splitDomains](trainer.py 286): INFO [10/26]	0.1708(0.2955)	0.0003(0.1246)	0.234(0.254)	100.00(98.86)\
[2023-09-14 06:44:47 4splitDomains](trainer.py 286): INFO [20/26]	0.2898(0.2635)	0.1194(0.0925)	0.304(0.257)	98.44(98.96)\
[2023-09-14 06:44:48 4splitDomains](trainer.py 286): INFO [25/26]	0.1018(0.2517)	0.0001(0.0836)	0.326(0.260)	100.00(98.96)\
[2023-09-14 06:44:48 4splitDomains](trainer.py 288): INFO  * Train Acc 98.957\
[2023-09-14 06:44:50 4splitDomains](trainer.py 147): INFO  * Val Acc 81.463, Total time 1.39\
[2023-09-14 06:44:50 4splitDomains](trainer.py 223): INFO Epoch:10\
[2023-09-14 06:44:50 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:50 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:51 4splitDomains](trainer.py 286): INFO [0/26]	1.2050(1.2050)	1.0308(1.0308)	0.269(0.269)	98.44(98.44)\
[2023-09-14 06:44:53 4splitDomains](trainer.py 286): INFO [10/26]	0.1707(0.3033)	0.0003(0.1324)	0.229(0.263)	98.44(98.58)\
[2023-09-14 06:44:55 4splitDomains](trainer.py 286): INFO [20/26]	0.3359(0.2655)	0.1647(0.0940)	0.239(0.255)	100.00(98.88)\
[2023-09-14 06:44:57 4splitDomains](trainer.py 286): INFO [25/26]	0.1033(0.2588)	0.0001(0.0902)	0.286(0.255)	96.67(98.90)\
[2023-09-14 06:44:57 4splitDomains](trainer.py 288): INFO  * Train Acc 98.896\
[2023-09-14 06:44:58 4splitDomains](trainer.py 147): INFO  * Val Acc 81.463, Total time 1.39\
[2023-09-14 06:44:58 4splitDomains](trainer.py 223): INFO Epoch:11\
[2023-09-14 06:44:58 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:44:58 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:44:59 4splitDomains](trainer.py 286): INFO [0/26]	1.3040(1.3040)	1.1325(1.1325)	0.233(0.233)	100.00(100.00)\
[2023-09-14 06:45:02 4splitDomains](trainer.py 286): INFO [10/26]	0.1708(0.3215)	0.0004(0.1504)	0.249(0.250)	98.44(98.72)\
[2023-09-14 06:45:04 4splitDomains](trainer.py 286): INFO [20/26]	0.4066(0.2800)	0.2364(0.1077)	0.198(0.248)	100.00(98.96)\
[2023-09-14 06:45:05 4splitDomains](trainer.py 286): INFO [25/26]	0.1017(0.2590)	0.0001(0.0898)	0.317(0.248)	100.00(99.14)\
[2023-09-14 06:45:05 4splitDomains](trainer.py 288): INFO  * Train Acc 99.141\
[2023-09-14 06:45:06 4splitDomains](trainer.py 147): INFO  * Val Acc 79.512, Total time 1.41\
[2023-09-14 06:45:06 4splitDomains](trainer.py 223): INFO Epoch:12\
[2023-09-14 06:45:06 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:45:06 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:45:08 4splitDomains](trainer.py 286): INFO [0/26]	1.2390(1.2390)	1.0527(1.0527)	0.216(0.216)	100.00(100.00)\
[2023-09-14 06:45:10 4splitDomains](trainer.py 286): INFO [10/26]	0.1706(0.3098)	0.0003(0.1376)	0.224(0.241)	100.00(99.43)\
[2023-09-14 06:45:12 4splitDomains](trainer.py 286): INFO [20/26]	0.3437(0.2668)	0.1734(0.0955)	0.243(0.247)	96.88(99.33)\
[2023-09-14 06:45:13 4splitDomains](trainer.py 286): INFO [25/26]	0.1020(0.2544)	0.0002(0.0859)	0.265(0.251)	100.00(99.14)\
[2023-09-14 06:45:13 4splitDomains](trainer.py 288): INFO  * Train Acc 99.141\
[2023-09-14 06:45:14 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.40\
[2023-09-14 06:45:14 4splitDomains](trainer.py 223): INFO Epoch:13\
[2023-09-14 06:45:14 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:45:14 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:45:16 4splitDomains](trainer.py 286): INFO [0/26]	1.2212(1.2212)	1.0323(1.0323)	0.245(0.245)	100.00(100.00)\
[2023-09-14 06:45:18 4splitDomains](trainer.py 286): INFO [10/26]	0.1707(0.3051)	0.0004(0.1328)	0.260(0.242)	98.44(99.15)\
[2023-09-14 06:45:20 4splitDomains](trainer.py 286): INFO [20/26]	0.2341(0.2607)	0.0634(0.0886)	0.292(0.244)	98.44(99.11)\
[2023-09-14 06:45:21 4splitDomains](trainer.py 286): INFO [25/26]	0.1017(0.2465)	0.0001(0.0775)	0.306(0.247)	100.00(99.02)\
[2023-09-14 06:45:21 4splitDomains](trainer.py 288): INFO  * Train Acc 99.018\
[2023-09-14 06:45:22 4splitDomains](trainer.py 147): INFO  * Val Acc 79.512, Total time 1.39\
[2023-09-14 06:45:22 4splitDomains](trainer.py 223): INFO Epoch:14\
[2023-09-14 06:45:22 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:45:22 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:45:23 4splitDomains](trainer.py 286): INFO [0/26]	1.2628(1.2628)	1.0797(1.0797)	0.260(0.260)	98.44(98.44)\
[2023-09-14 06:45:26 4splitDomains](trainer.py 286): INFO [10/26]	0.1708(0.3059)	0.0004(0.1334)	0.289(0.243)	96.88(99.15)\
[2023-09-14 06:45:28 4splitDomains](trainer.py 286): INFO [20/26]	0.3825(0.2832)	0.2117(0.1115)	0.280(0.240)	98.44(99.33)\
[2023-09-14 06:45:29 4splitDomains](trainer.py 286): INFO [25/26]	0.1052(0.2683)	0.0006(0.0994)	0.277(0.243)	100.00(99.39)\
[2023-09-14 06:45:29 4splitDomains](trainer.py 288): INFO  * Train Acc 99.387\
[2023-09-14 06:45:31 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.42\
[2023-09-14 06:45:31 4splitDomains](trainer.py 223): INFO Epoch:15\
[2023-09-14 06:45:31 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:45:31 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:45:32 4splitDomains](trainer.py 286): INFO [0/26]	1.3059(1.3059)	1.1307(1.1307)	0.236(0.236)	98.44(98.44)\
[2023-09-14 06:45:34 4splitDomains](trainer.py 286): INFO [10/26]	0.1728(0.3158)	0.0017(0.1419)	0.223(0.234)	100.00(99.43)\
[2023-09-14 06:45:37 4splitDomains](trainer.py 286): INFO [20/26]	0.3681(0.2778)	0.1948(0.1050)	0.252(0.238)	100.00(99.18)\
[2023-09-14 06:45:38 4splitDomains](trainer.py 286): INFO [25/26]	0.1037(0.2629)	0.0001(0.0932)	0.373(0.244)	93.33(99.14)\
[2023-09-14 06:45:38 4splitDomains](trainer.py 288): INFO  * Train Acc 99.141\
[2023-09-14 06:45:39 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.41\
[2023-09-14 06:45:39 4splitDomains](trainer.py 223): INFO Epoch:16\
[2023-09-14 06:45:39 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:45:39 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:45:40 4splitDomains](trainer.py 286): INFO [0/26]	1.1690(1.1690)	0.9788(0.9788)	0.220(0.220)	100.00(100.00)\
[2023-09-14 06:45:42 4splitDomains](trainer.py 286): INFO [10/26]	0.1707(0.2927)	0.0004(0.1198)	0.209(0.240)	100.00(99.15)\
[2023-09-14 06:45:45 4splitDomains](trainer.py 286): INFO [20/26]	0.3184(0.2663)	0.1477(0.0943)	0.223(0.244)	100.00(99.03)\
[2023-09-14 06:45:46 4splitDomains](trainer.py 286): INFO [25/26]	0.1019(0.2611)	0.0002(0.0920)	0.503(0.253)	90.00(98.90)\
[2023-09-14 06:45:46 4splitDomains](trainer.py 288): INFO  * Train Acc 98.896\
[2023-09-14 06:45:47 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.45\
[2023-09-14 06:45:47 4splitDomains](trainer.py 223): INFO Epoch:17\
[2023-09-14 06:45:47 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:45:47 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:45:49 4splitDomains](trainer.py 286): INFO [0/26]	1.2404(1.2404)	1.0682(1.0682)	0.287(0.287)	98.44(98.44)\
[2023-09-14 06:45:51 4splitDomains](trainer.py 286): INFO [10/26]	0.1709(0.3003)	0.0004(0.1294)	0.258(0.239)	98.44(99.43)\
[2023-09-14 06:45:53 4splitDomains](trainer.py 286): INFO [20/26]	0.3450(0.2777)	0.1750(0.1070)	0.225(0.233)	100.00(99.48)\
[2023-09-14 06:45:54 4splitDomains](trainer.py 286): INFO [25/26]	0.1040(0.2599)	0.0001(0.0918)	0.251(0.235)	100.00(99.45)\
[2023-09-14 06:45:54 4splitDomains](trainer.py 288): INFO  * Train Acc 99.448\
[2023-09-14 06:45:56 4splitDomains](trainer.py 147): INFO  * Val Acc 78.537, Total time 1.38\
[2023-09-14 06:45:56 4splitDomains](trainer.py 223): INFO Epoch:18\
[2023-09-14 06:45:56 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:45:56 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:45:57 4splitDomains](trainer.py 286): INFO [0/26]	1.2636(1.2636)	1.0928(1.0928)	0.231(0.231)	100.00(100.00)\
[2023-09-14 06:45:59 4splitDomains](trainer.py 286): INFO [10/26]	0.1710(0.3133)	0.0004(0.1424)	0.251(0.233)	98.44(99.29)\
[2023-09-14 06:46:01 4splitDomains](trainer.py 286): INFO [20/26]	0.5929(0.2824)	0.4232(0.1117)	0.205(0.235)	100.00(99.33)\
[2023-09-14 06:46:02 4splitDomains](trainer.py 286): INFO [25/26]	0.1018(0.2634)	0.0001(0.0955)	0.242(0.239)	100.00(99.26)\
[2023-09-14 06:46:02 4splitDomains](trainer.py 288): INFO  * Train Acc 99.264\
[2023-09-14 06:46:04 4splitDomains](trainer.py 147): INFO  * Val Acc 79.512, Total time 1.34\
[2023-09-14 06:46:04 4splitDomains](trainer.py 223): INFO Epoch:19\
[2023-09-14 06:46:04 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:46:04 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:46:05 4splitDomains](trainer.py 286): INFO [0/26]	1.2162(1.2162)	1.0436(1.0436)	0.190(0.190)	100.00(100.00)\
[2023-09-14 06:46:07 4splitDomains](trainer.py 286): INFO [10/26]	0.1713(0.2963)	0.0004(0.1254)	0.213(0.220)	100.00(99.86)\
[2023-09-14 06:46:10 4splitDomains](trainer.py 286): INFO [20/26]	0.2750(0.2743)	0.1046(0.1035)	0.193(0.223)	100.00(99.55)\
[2023-09-14 06:46:10 4splitDomains](trainer.py 286): INFO [25/26]	0.1017(0.2575)	0.0001(0.0895)	0.326(0.227)	93.33(99.51)\
[2023-09-14 06:46:11 4splitDomains](trainer.py 288): INFO  * Train Acc 99.509\
[2023-09-14 06:46:12 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.40\
[2023-09-14 06:46:12 4splitDomains](trainer.py 223): INFO Epoch:20\
[2023-09-14 06:46:12 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:46:12 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:46:13 4splitDomains](trainer.py 286): INFO [0/26]	1.1738(1.1738)	0.9899(0.9899)	0.226(0.226)	100.00(100.00)\
[2023-09-14 06:46:15 4splitDomains](trainer.py 286): INFO [10/26]	0.1709(0.2932)	0.0004(0.1179)	0.295(0.234)	98.44(99.15)\
[2023-09-14 06:46:17 4splitDomains](trainer.py 286): INFO [20/26]	0.3107(0.2610)	0.1399(0.0873)	0.250(0.238)	98.44(99.18)\
[2023-09-14 06:46:19 4splitDomains](trainer.py 286): INFO [25/26]	0.1016(0.2553)	0.0001(0.0848)	0.341(0.239)	96.67(99.02)\
[2023-09-14 06:46:19 4splitDomains](trainer.py 288): INFO  * Train Acc 99.018\
[2023-09-14 06:46:20 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.39\
[2023-09-14 06:46:20 4splitDomains](trainer.py 223): INFO Epoch:21\
[2023-09-14 06:46:20 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:46:20 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:46:21 4splitDomains](trainer.py 286): INFO [0/26]	1.2355(1.2355)	1.0635(1.0635)	0.235(0.235)	100.00(100.00)\
[2023-09-14 06:46:23 4splitDomains](trainer.py 286): INFO [10/26]	0.1710(0.3101)	0.0004(0.1395)	0.223(0.224)	100.00(99.72)\
[2023-09-14 06:46:26 4splitDomains](trainer.py 286): INFO [20/26]	0.4920(0.2736)	0.3209(0.1010)	0.253(0.229)	98.44(99.48)\
[2023-09-14 06:46:27 4splitDomains](trainer.py 286): INFO [25/26]	0.1038(0.2563)	0.0001(0.0867)	0.257(0.234)	100.00(99.39)\
[2023-09-14 06:46:27 4splitDomains](trainer.py 288): INFO  * Train Acc 99.387\
[2023-09-14 06:46:28 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.36\
[2023-09-14 06:46:28 4splitDomains](trainer.py 223): INFO Epoch:22\
[2023-09-14 06:46:28 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:46:28 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:46:29 4splitDomains](trainer.py 286): INFO [0/26]	1.1990(1.1990)	1.0239(1.0239)	0.216(0.216)	100.00(100.00)\
[2023-09-14 06:46:32 4splitDomains](trainer.py 286): INFO [10/26]	0.1707(0.3030)	0.0003(0.1290)	0.230(0.222)	100.00(99.72)\
[2023-09-14 06:46:34 4splitDomains](trainer.py 286): INFO [20/26]	0.3450(0.2659)	0.1743(0.0936)	0.230(0.225)	100.00(99.55)\
[2023-09-14 06:46:35 4splitDomains](trainer.py 286): INFO [25/26]	0.1027(0.2500)	0.0002(0.0805)	0.483(0.229)	96.67(99.57)\
[2023-09-14 06:46:35 4splitDomains](trainer.py 288): INFO  * Train Acc 99.571\
[2023-09-14 06:46:36 4splitDomains](trainer.py 147): INFO  * Val Acc 81.463, Total time 1.39\
[2023-09-14 06:46:36 4splitDomains](trainer.py 223): INFO Epoch:23\
[2023-09-14 06:46:36 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:46:36 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:46:37 4splitDomains](trainer.py 286): INFO [0/26]	1.3193(1.3193)	1.1485(1.1485)	0.202(0.202)	100.00(100.00)\
[2023-09-14 06:46:40 4splitDomains](trainer.py 286): INFO [10/26]	0.1714(0.3056)	0.0004(0.1348)	0.220(0.231)	98.44(99.29)\
[2023-09-14 06:46:42 4splitDomains](trainer.py 286): INFO [20/26]	0.3578(0.2678)	0.1876(0.0971)	0.254(0.227)	100.00(99.40)\
[2023-09-14 06:46:43 4splitDomains](trainer.py 286): INFO [25/26]	0.1038(0.2561)	0.0001(0.0880)	0.323(0.228)	96.67(99.33)\
[2023-09-14 06:46:43 4splitDomains](trainer.py 288): INFO  * Train Acc 99.325\
[2023-09-14 06:46:44 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.39\
[2023-09-14 06:46:44 4splitDomains](trainer.py 223): INFO Epoch:24\
[2023-09-14 06:46:44 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:46:44 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:46:45 4splitDomains](trainer.py 286): INFO [0/26]	1.1858(1.1858)	1.0152(1.0152)	0.215(0.215)	100.00(100.00)\
[2023-09-14 06:46:47 4splitDomains](trainer.py 286): INFO [10/26]	0.1709(0.2900)	0.0004(0.1173)	0.229(0.223)	98.44(99.29)\
[2023-09-14 06:46:50 4splitDomains](trainer.py 286): INFO [20/26]	0.2077(0.2625)	0.0368(0.0889)	0.290(0.225)	96.88(99.33)\
[2023-09-14 06:46:51 4splitDomains](trainer.py 286): INFO [25/26]	0.1022(0.2506)	0.0002(0.0803)	0.252(0.225)	100.00(99.33)\
[2023-09-14 06:46:51 4splitDomains](trainer.py 288): INFO  * Train Acc 99.325\
[2023-09-14 06:46:52 4splitDomains](trainer.py 147): INFO  * Val Acc 79.512, Total time 1.40\
[2023-09-14 06:46:52 4splitDomains](trainer.py 223): INFO Epoch:25\
[2023-09-14 06:46:52 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:46:52 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:46:53 4splitDomains](trainer.py 286): INFO [0/26]	1.2191(1.2191)	1.0327(1.0327)	0.205(0.205)	100.00(100.00)\
[2023-09-14 06:46:55 4splitDomains](trainer.py 286): INFO [10/26]	0.1716(0.2936)	0.0004(0.1177)	0.238(0.230)	100.00(99.01)\
[2023-09-14 06:46:58 4splitDomains](trainer.py 286): INFO [20/26]	0.1751(0.2640)	0.0002(0.0889)	0.240(0.228)	98.44(99.26)\
[2023-09-14 06:46:59 4splitDomains](trainer.py 286): INFO [25/26]	0.1018(0.2535)	0.0001(0.0820)	0.240(0.227)	100.00(99.26)\
[2023-09-14 06:46:59 4splitDomains](trainer.py 288): INFO  * Train Acc 99.264\
[2023-09-14 06:47:00 4splitDomains](trainer.py 147): INFO  * Val Acc 81.463, Total time 1.37\
[2023-09-14 06:47:00 4splitDomains](trainer.py 223): INFO Epoch:26\
[2023-09-14 06:47:00 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:47:00 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:47:02 4splitDomains](trainer.py 286): INFO [0/26]	1.2144(1.2144)	1.0282(1.0282)	0.219(0.219)	100.00(100.00)\
[2023-09-14 06:47:04 4splitDomains](trainer.py 286): INFO [10/26]	0.1708(0.2937)	0.0004(0.1205)	0.208(0.219)	100.00(99.72)\
[2023-09-14 06:47:06 4splitDomains](trainer.py 286): INFO [20/26]	0.4492(0.2673)	0.2784(0.0951)	0.229(0.227)	100.00(99.33)\
[2023-09-14 06:47:07 4splitDomains](trainer.py 286): INFO [25/26]	0.1015(0.2518)	0.0001(0.0825)	0.255(0.231)	100.00(99.26)\
[2023-09-14 06:47:07 4splitDomains](trainer.py 288): INFO  * Train Acc 99.264\
[2023-09-14 06:47:08 4splitDomains](trainer.py 147): INFO  * Val Acc 81.463, Total time 1.38\
[2023-09-14 06:47:08 4splitDomains](trainer.py 223): INFO Epoch:27\
[2023-09-14 06:47:08 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:47:08 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:47:10 4splitDomains](trainer.py 286): INFO [0/26]	1.2012(1.2012)	1.0244(1.0244)	0.247(0.247)	98.44(98.44)\
[2023-09-14 06:47:12 4splitDomains](trainer.py 286): INFO [10/26]	0.1707(0.3059)	0.0003(0.1347)	0.200(0.220)	100.00(99.72)\
[2023-09-14 06:47:14 4splitDomains](trainer.py 286): INFO [20/26]	0.2775(0.2614)	0.1067(0.0905)	0.236(0.226)	98.44(99.26)\
[2023-09-14 06:47:15 4splitDomains](trainer.py 286): INFO [25/26]	0.1015(0.2525)	0.0001(0.0844)	0.261(0.224)	100.00(99.39)\
[2023-09-14 06:47:15 4splitDomains](trainer.py 288): INFO  * Train Acc 99.387\
[2023-09-14 06:47:16 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.37\
[2023-09-14 06:47:16 4splitDomains](trainer.py 223): INFO Epoch:28\
[2023-09-14 06:47:16 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:47:16 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:47:18 4splitDomains](trainer.py 286): INFO [0/26]	1.2068(1.2068)	1.0360(1.0360)	0.219(0.219)	100.00(100.00)\
[2023-09-14 06:47:20 4splitDomains](trainer.py 286): INFO [10/26]	0.1707(0.3100)	0.0003(0.1382)	0.242(0.218)	100.00(99.43)\
[2023-09-14 06:47:22 4splitDomains](trainer.py 286): INFO [20/26]	0.3347(0.2639)	0.1643(0.0926)	0.234(0.225)	100.00(99.40)\
[2023-09-14 06:47:23 4splitDomains](trainer.py 286): INFO [25/26]	0.1032(0.2497)	0.0001(0.0812)	0.281(0.230)	100.00(99.39)\
[2023-09-14 06:47:23 4splitDomains](trainer.py 288): INFO  * Train Acc 99.387\
[2023-09-14 06:47:24 4splitDomains](trainer.py 147): INFO  * Val Acc 80.000, Total time 1.37\
[2023-09-14 06:47:24 4splitDomains](trainer.py 223): INFO Epoch:29\
[2023-09-14 06:47:24 4splitDomains](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:47:24 4splitDomains](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:47:26 4splitDomains](trainer.py 286): INFO [0/26]	1.2834(1.2834)	1.0981(1.0981)	0.187(0.187)	100.00(100.00)\
[2023-09-14 06:47:28 4splitDomains](trainer.py 286): INFO [10/26]	0.1713(0.3203)	0.0004(0.1468)	0.205(0.217)	98.44(99.01)\
[2023-09-14 06:47:30 4splitDomains](trainer.py 286): INFO [20/26]	0.3453(0.2755)	0.1739(0.1031)	0.220(0.217)	100.00(99.18)\
[2023-09-14 06:47:31 4splitDomains](trainer.py 286): INFO [25/26]	0.1042(0.2596)	0.0001(0.0901)	0.221(0.218)	100.00(99.08)\
[2023-09-14 06:47:31 4splitDomains](trainer.py 288): INFO  * Train Acc 99.080\
[2023-09-14 06:47:33 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.45\
=> Saving model to: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-3.pth\
=> Save Done\
[2023-09-14 06:47:33 4splitDomains](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 06:47:34 4splitDomains](trainer.py 147): INFO  * Val Acc 87.003, Total time 1.23\
[2023-09-14 06:47:34 4splitDomains](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 06:47:36 4splitDomains](trainer.py 147): INFO  * Val Acc 68.783, Total time 1.55\
[2023-09-14 06:47:36 4splitDomains](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 06:47:40 4splitDomains](trainer.py 147): INFO  * Val Acc 81.842, Total time 4.76\
[2023-09-14 06:47:40 4splitDomains](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 06:47:42 4splitDomains](trainer.py 147): INFO  * Val Acc 80.488, Total time 1.41\
[2023-09-14 06:47:42 4splitDomains](trainer.py 335): INFO saving storage...\
[2023-09-14 06:47:42 4splitDomains](trainer.py 341): INFO done\
[2023-09-14 06:47:42 4splitDomains](iBatchLearn.py 155): INFO Acc:79.52890816326793; BWT:-7.052386182453394;\
=> merge config from utils/user_4splitDomains.yaml\
=> merge config from ../official_eva/configs/4splitDomains.yaml\
[2023-09-14 06:47:46 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:47:46 4splitDomains](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 4splitDomains\
  NUM_CLASSES: 60\
  NUM_TASKS: 4\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/4splitDomains\
DOMAIN_INCR: true\
GPUID:\
- 0\
LOGGER_PATH: outputs/4splitDomains/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 30\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:47:46 4splitDomains](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 3, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-3.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_3.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:47:47 4splitDomains](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-3.pth\
[2023-09-14 06:47:47 4splitDomains](trainer.py 97): INFO => Load Done\
[2023-09-14 06:47:48 4splitDomains](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (All): Linear(in_features=2048, out_features=60, bias=False)\
  )\
)\
[2023-09-14 06:47:48 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630972\
[2023-09-14 06:47:48 4splitDomains](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 06:47:52 4splitDomains](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 06:47:54 4splitDomains](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 06:48:01 4splitDomains](iBatchLearn.py 167): INFO test split name:3\
--------------------------------Official Evaluation--------------------------------\
3 71.92450705469481\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 06:48:10 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:48:10 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:48:10 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 0, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "input/init_models/10splitTasks.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-0.pth", "storage_path": "None", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-0.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:48:10 10splitTasks](trainer.py 92): INFO => Load model weights: input/init_models/10splitTasks.pth\
[2023-09-14 06:48:10 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 06:48:12 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 06:48:12 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 06:48:12 10splitTasks](iBatchLearn.py 92): INFO ====================== 0 =======================\
[2023-09-14 06:48:12 10splitTasks](regularization.py 45): INFO reg_term: , 0\
[2023-09-14 06:48:12 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 06:48:12 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 06:48:12 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:48:12 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:48:15 10splitTasks](trainer.py 286): INFO [0/79]	3.2440(3.2440)	0.8685(0.8685)	2.541(2.541)	10.94(10.94)\
[2023-09-14 06:48:17 10splitTasks](trainer.py 286): INFO [10/79]	0.1608(0.4426)	0.0003(0.0794)	2.370(2.470)	14.06(10.65)\
[2023-09-14 06:48:19 10splitTasks](trainer.py 286): INFO [20/79]	0.1611(0.3088)	0.0004(0.0418)	2.384(2.412)	10.94(12.05)\
[2023-09-14 06:48:20 10splitTasks](trainer.py 286): INFO [30/79]	0.1601(0.2611)	0.0003(0.0284)	2.303(2.415)	21.88(13.05)\
[2023-09-14 06:48:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1605(0.2366)	0.0003(0.0217)	2.196(2.389)	20.31(13.57)\
[2023-09-14 06:48:23 10splitTasks](trainer.py 286): INFO [50/79]	0.1608(0.2218)	0.0004(0.0175)	2.288(2.372)	20.31(13.69)\
[2023-09-14 06:48:25 10splitTasks](trainer.py 286): INFO [60/79]	0.1608(0.2118)	0.0004(0.0147)	2.244(2.362)	18.75(14.14)\
[2023-09-14 06:48:27 10splitTasks](trainer.py 286): INFO [70/79]	0.1609(0.2047)	0.0008(0.0127)	2.111(2.339)	21.88(14.85)\
[2023-09-14 06:48:28 10splitTasks](trainer.py 286): INFO [78/79]	0.1815(0.2005)	0.0001(0.0114)	2.048(2.320)	50.00(15.42)\
[2023-09-14 06:48:28 10splitTasks](trainer.py 288): INFO  * Train Acc 15.420\
[2023-09-14 06:48:30 10splitTasks](trainer.py 147): INFO  * Val Acc 17.600, Total time 1.53\
[2023-09-14 06:48:30 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 06:48:30 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:48:30 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:48:30 10splitTasks](trainer.py 286): INFO [0/79]	0.9175(0.9175)	0.7417(0.7417)	2.071(2.071)	23.44(23.44)\
[2023-09-14 06:48:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1604(0.2319)	0.0003(0.0683)	2.092(2.177)	25.00(20.45)\
[2023-09-14 06:48:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1607(0.1980)	0.0004(0.0360)	2.078(2.146)	29.69(20.24)\
[2023-09-14 06:48:35 10splitTasks](trainer.py 286): INFO [30/79]	0.1606(0.1860)	0.0004(0.0245)	2.131(2.138)	25.00(21.67)\
[2023-09-14 06:48:37 10splitTasks](trainer.py 286): INFO [40/79]	0.1607(0.1799)	0.0003(0.0186)	2.104(2.144)	26.56(21.95)\
[2023-09-14 06:48:39 10splitTasks](trainer.py 286): INFO [50/79]	0.1614(0.1763)	0.0004(0.0151)	1.978(2.134)	31.25(22.27)\
[2023-09-14 06:48:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1631(0.1739)	0.0003(0.0127)	2.281(2.123)	20.31(22.85)\
[2023-09-14 06:48:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1615(0.1720)	0.0009(0.0109)	1.806(2.117)	34.38(23.26)\
[2023-09-14 06:48:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0349(0.1692)	0.0001(0.0099)	2.352(2.116)	12.50(23.18)\
[2023-09-14 06:48:43 10splitTasks](trainer.py 288): INFO  * Train Acc 23.180\
[2023-09-14 06:48:44 10splitTasks](trainer.py 147): INFO  * Val Acc 22.200, Total time 1.49\
[2023-09-14 06:48:44 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 06:48:44 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:48:44 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:48:45 10splitTasks](trainer.py 286): INFO [0/79]	0.8478(0.8478)	0.6831(0.6831)	2.192(2.192)	20.31(20.31)\
[2023-09-14 06:48:47 10splitTasks](trainer.py 286): INFO [10/79]	0.1611(0.2232)	0.0004(0.0625)	2.180(2.084)	23.44(24.15)\
[2023-09-14 06:48:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1609(0.1935)	0.0004(0.0329)	1.944(2.064)	28.12(23.36)\
[2023-09-14 06:48:50 10splitTasks](trainer.py 286): INFO [30/79]	0.1609(0.1830)	0.0004(0.0224)	1.949(2.056)	25.00(24.70)\
[2023-09-14 06:48:52 10splitTasks](trainer.py 286): INFO [40/79]	0.1623(0.1776)	0.0006(0.0171)	1.974(2.045)	20.31(25.19)\
[2023-09-14 06:48:53 10splitTasks](trainer.py 286): INFO [50/79]	0.1607(0.1743)	0.0004(0.0138)	2.111(2.036)	25.00(25.70)\
[2023-09-14 06:48:55 10splitTasks](trainer.py 286): INFO [60/79]	0.1610(0.1721)	0.0006(0.0116)	1.995(2.023)	26.56(26.31)\
[2023-09-14 06:48:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1612(0.1705)	0.0009(0.0101)	1.908(2.022)	32.81(26.54)\
[2023-09-14 06:48:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0341(0.1679)	0.0001(0.0091)	1.922(2.016)	50.00(26.74)\
[2023-09-14 06:48:58 10splitTasks](trainer.py 288): INFO  * Train Acc 26.740\
[2023-09-14 06:48:59 10splitTasks](trainer.py 147): INFO  * Val Acc 30.000, Total time 1.54\
[2023-09-14 06:48:59 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 06:48:59 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:48:59 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:49:00 10splitTasks](trainer.py 286): INFO [0/79]	0.8670(0.8670)	0.7016(0.7016)	1.955(1.955)	28.12(28.12)\
[2023-09-14 06:49:02 10splitTasks](trainer.py 286): INFO [10/79]	0.1607(0.2264)	0.0004(0.0655)	1.892(1.915)	28.12(28.98)\
[2023-09-14 06:49:03 10splitTasks](trainer.py 286): INFO [20/79]	0.1606(0.1952)	0.0004(0.0345)	1.892(1.932)	25.00(28.57)\
[2023-09-14 06:49:05 10splitTasks](trainer.py 286): INFO [30/79]	0.1608(0.1844)	0.0004(0.0235)	1.755(1.934)	40.62(28.78)\
[2023-09-14 06:49:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1608(0.1786)	0.0004(0.0179)	1.814(1.927)	28.12(29.50)\
[2023-09-14 06:49:08 10splitTasks](trainer.py 286): INFO [50/79]	0.1606(0.1751)	0.0004(0.0144)	1.724(1.931)	45.31(29.23)\
[2023-09-14 06:49:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1609(0.1729)	0.0004(0.0122)	1.866(1.934)	26.56(29.53)\
[2023-09-14 06:49:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1612(0.1712)	0.0009(0.0105)	1.815(1.924)	32.81(29.89)\
[2023-09-14 06:49:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0342(0.1685)	0.0001(0.0095)	1.623(1.921)	37.50(30.12)\
[2023-09-14 06:49:13 10splitTasks](trainer.py 288): INFO  * Train Acc 30.120\
[2023-09-14 06:49:14 10splitTasks](trainer.py 147): INFO  * Val Acc 31.200, Total time 1.57\
[2023-09-14 06:49:14 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 06:49:14 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:49:14 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:49:15 10splitTasks](trainer.py 286): INFO [0/79]	0.9165(0.9165)	0.7541(0.7541)	1.947(1.947)	26.56(26.56)\
[2023-09-14 06:49:17 10splitTasks](trainer.py 286): INFO [10/79]	0.1608(0.2301)	0.0003(0.0689)	2.028(1.911)	31.25(31.68)\
[2023-09-14 06:49:18 10splitTasks](trainer.py 286): INFO [20/79]	0.1607(0.1971)	0.0003(0.0363)	1.917(1.911)	34.38(31.85)\
[2023-09-14 06:49:20 10splitTasks](trainer.py 286): INFO [30/79]	0.1606(0.1856)	0.0003(0.0247)	1.794(1.876)	39.06(33.06)\
[2023-09-14 06:49:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1607(0.1798)	0.0004(0.0188)	1.802(1.863)	42.19(32.89)\
[2023-09-14 06:49:23 10splitTasks](trainer.py 286): INFO [50/79]	0.1618(0.1762)	0.0006(0.0152)	1.802(1.872)	39.06(32.87)\
[2023-09-14 06:49:25 10splitTasks](trainer.py 286): INFO [60/79]	0.1619(0.1738)	0.0004(0.0128)	1.822(1.866)	32.81(32.97)\
[2023-09-14 06:49:27 10splitTasks](trainer.py 286): INFO [70/79]	0.1619(0.1721)	0.0009(0.0110)	1.626(1.852)	51.56(34.04)\
[2023-09-14 06:49:28 10splitTasks](trainer.py 286): INFO [78/79]	0.0344(0.1694)	0.0001(0.0099)	2.190(1.848)	12.50(34.26)\
[2023-09-14 06:49:28 10splitTasks](trainer.py 288): INFO  * Train Acc 34.260\
[2023-09-14 06:49:29 10splitTasks](trainer.py 147): INFO  * Val Acc 28.200, Total time 1.49\
[2023-09-14 06:49:29 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 06:49:29 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:49:29 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:49:30 10splitTasks](trainer.py 286): INFO [0/79]	0.9078(0.9078)	0.7442(0.7442)	1.931(1.931)	28.12(28.12)\
[2023-09-14 06:49:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1608(0.2290)	0.0004(0.0681)	2.555(2.036)	18.75(29.97)\
[2023-09-14 06:49:33 10splitTasks](trainer.py 286): INFO [20/79]	0.1612(0.1973)	0.0003(0.0359)	1.901(1.929)	32.81(31.92)\
[2023-09-14 06:49:35 10splitTasks](trainer.py 286): INFO [30/79]	0.1614(0.1857)	0.0004(0.0245)	1.984(1.892)	31.25(33.37)\
[2023-09-14 06:49:37 10splitTasks](trainer.py 286): INFO [40/79]	0.1612(0.1798)	0.0003(0.0186)	1.704(1.861)	46.88(34.57)\
[2023-09-14 06:49:38 10splitTasks](trainer.py 286): INFO [50/79]	0.1625(0.1762)	0.0004(0.0150)	1.621(1.843)	37.50(34.04)\
[2023-09-14 06:49:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1616(0.1738)	0.0004(0.0126)	1.969(1.837)	25.00(33.97)\
[2023-09-14 06:49:41 10splitTasks](trainer.py 286): INFO [70/79]	0.1622(0.1722)	0.0008(0.0109)	1.642(1.823)	43.75(34.68)\
[2023-09-14 06:49:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0348(0.1695)	0.0001(0.0099)	1.423(1.811)	50.00(35.36)\
[2023-09-14 06:49:43 10splitTasks](trainer.py 288): INFO  * Train Acc 35.360\
[2023-09-14 06:49:44 10splitTasks](trainer.py 147): INFO  * Val Acc 37.200, Total time 1.72\
[2023-09-14 06:49:44 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 06:49:44 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:49:44 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:49:45 10splitTasks](trainer.py 286): INFO [0/79]	0.8906(0.8906)	0.7284(0.7284)	1.627(1.627)	37.50(37.50)\
[2023-09-14 06:49:47 10splitTasks](trainer.py 286): INFO [10/79]	0.1619(0.2287)	0.0003(0.0666)	1.888(1.735)	32.81(38.49)\
[2023-09-14 06:49:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1615(0.1977)	0.0003(0.0351)	1.656(1.779)	34.38(37.05)\
[2023-09-14 06:49:50 10splitTasks](trainer.py 286): INFO [30/79]	0.1616(0.1862)	0.0004(0.0239)	1.720(1.751)	35.94(37.00)\
[2023-09-14 06:49:52 10splitTasks](trainer.py 286): INFO [40/79]	0.1620(0.1804)	0.0003(0.0182)	1.941(1.748)	25.00(36.59)\
[2023-09-14 06:49:53 10splitTasks](trainer.py 286): INFO [50/79]	0.1614(0.1768)	0.0003(0.0147)	1.566(1.746)	45.31(37.44)\
[2023-09-14 06:49:55 10splitTasks](trainer.py 286): INFO [60/79]	0.1611(0.1744)	0.0002(0.0124)	1.711(1.736)	34.38(37.55)\
[2023-09-14 06:49:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1624(0.1726)	0.0010(0.0107)	1.712(1.731)	39.06(37.94)\
[2023-09-14 06:49:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0349(0.1698)	0.0002(0.0097)	1.873(1.724)	50.00(37.98)\
[2023-09-14 06:49:58 10splitTasks](trainer.py 288): INFO  * Train Acc 37.980\
[2023-09-14 06:49:59 10splitTasks](trainer.py 147): INFO  * Val Acc 41.000, Total time 1.52\
[2023-09-14 06:49:59 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 06:49:59 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:49:59 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:50:00 10splitTasks](trainer.py 286): INFO [0/79]	0.8705(0.8705)	0.7071(0.7071)	1.562(1.562)	45.31(45.31)\
[2023-09-14 06:50:02 10splitTasks](trainer.py 286): INFO [10/79]	0.1625(0.2270)	0.0004(0.0654)	1.821(1.688)	32.81(39.63)\
[2023-09-14 06:50:04 10splitTasks](trainer.py 286): INFO [20/79]	0.1616(0.1965)	0.0004(0.0346)	1.480(1.652)	48.44(40.18)\
[2023-09-14 06:50:05 10splitTasks](trainer.py 286): INFO [30/79]	0.1613(0.1855)	0.0003(0.0236)	1.733(1.670)	43.75(40.32)\
[2023-09-14 06:50:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1618(0.1798)	0.0004(0.0180)	1.695(1.666)	46.88(40.59)\
[2023-09-14 06:50:08 10splitTasks](trainer.py 286): INFO [50/79]	0.1608(0.1764)	0.0004(0.0146)	1.617(1.663)	34.38(40.32)\
[2023-09-14 06:50:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1657(0.1740)	0.0005(0.0122)	1.824(1.672)	46.88(40.50)\
[2023-09-14 06:50:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1618(0.1725)	0.0009(0.0106)	1.591(1.663)	46.88(40.67)\
[2023-09-14 06:50:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0348(0.1697)	0.0001(0.0096)	1.266(1.653)	37.50(41.24)\
[2023-09-14 06:50:13 10splitTasks](trainer.py 288): INFO  * Train Acc 41.240\
[2023-09-14 06:50:15 10splitTasks](trainer.py 147): INFO  * Val Acc 42.400, Total time 1.67\
[2023-09-14 06:50:15 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 06:50:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:50:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:50:15 10splitTasks](trainer.py 286): INFO [0/79]	0.8835(0.8835)	0.7125(0.7125)	1.612(1.612)	42.19(42.19)\
[2023-09-14 06:50:17 10splitTasks](trainer.py 286): INFO [10/79]	0.1613(0.2420)	0.0004(0.0799)	1.523(1.604)	46.88(42.61)\
[2023-09-14 06:50:19 10splitTasks](trainer.py 286): INFO [20/79]	0.1615(0.2083)	0.0005(0.0466)	1.470(1.581)	48.44(44.57)\
[2023-09-14 06:50:21 10splitTasks](trainer.py 286): INFO [30/79]	0.1616(0.1933)	0.0004(0.0317)	1.575(1.590)	42.19(44.46)\
[2023-09-14 06:50:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1615(0.1858)	0.0004(0.0242)	1.658(1.587)	39.06(44.02)\
[2023-09-14 06:50:24 10splitTasks](trainer.py 286): INFO [50/79]	0.1617(0.1811)	0.0005(0.0195)	1.581(1.591)	54.69(44.12)\
[2023-09-14 06:50:25 10splitTasks](trainer.py 286): INFO [60/79]	0.1613(0.1779)	0.0003(0.0164)	1.355(1.586)	51.56(44.39)\
[2023-09-14 06:50:27 10splitTasks](trainer.py 286): INFO [70/79]	0.1619(0.1756)	0.0010(0.0142)	1.807(1.589)	39.06(44.45)\
[2023-09-14 06:50:28 10splitTasks](trainer.py 286): INFO [78/79]	0.0346(0.1725)	0.0001(0.0128)	2.304(1.585)	37.50(44.74)\
[2023-09-14 06:50:28 10splitTasks](trainer.py 288): INFO  * Train Acc 44.740\
[2023-09-14 06:50:30 10splitTasks](trainer.py 147): INFO  * Val Acc 43.400, Total time 1.46\
[2023-09-14 06:50:30 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 06:50:30 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:50:30 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:50:31 10splitTasks](trainer.py 286): INFO [0/79]	0.8832(0.8832)	0.7215(0.7215)	1.596(1.596)	42.19(42.19)\
[2023-09-14 06:50:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1613(0.2361)	0.0004(0.0754)	1.451(1.508)	48.44(45.31)\
[2023-09-14 06:50:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1615(0.2009)	0.0004(0.0397)	1.375(1.500)	51.56(46.88)\
[2023-09-14 06:50:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1613(0.1881)	0.0003(0.0270)	1.576(1.505)	45.31(45.97)\
[2023-09-14 06:50:37 10splitTasks](trainer.py 286): INFO [40/79]	0.1613(0.1817)	0.0004(0.0205)	1.620(1.522)	40.62(45.85)\
[2023-09-14 06:50:39 10splitTasks](trainer.py 286): INFO [50/79]	0.1622(0.1779)	0.0004(0.0166)	1.541(1.505)	46.88(46.88)\
[2023-09-14 06:50:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1616(0.1753)	0.0004(0.0140)	1.534(1.509)	48.44(46.88)\
[2023-09-14 06:50:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1662(0.1736)	0.0052(0.0122)	1.422(1.497)	46.88(47.25)\
[2023-09-14 06:50:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0353(0.1708)	0.0002(0.0110)	1.850(1.496)	50.00(47.44)\
[2023-09-14 06:50:43 10splitTasks](trainer.py 288): INFO  * Train Acc 47.440\
[2023-09-14 06:50:45 10splitTasks](trainer.py 147): INFO  * Val Acc 49.000, Total time 1.47\
[2023-09-14 06:50:45 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 06:50:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:50:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:50:46 10splitTasks](trainer.py 286): INFO [0/79]	0.9781(0.9781)	0.8164(0.8164)	1.399(1.399)	56.25(56.25)\
[2023-09-14 06:50:47 10splitTasks](trainer.py 286): INFO [10/79]	0.1611(0.2445)	0.0003(0.0826)	1.363(1.430)	53.12(50.71)\
[2023-09-14 06:50:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1620(0.2057)	0.0005(0.0435)	1.270(1.425)	53.12(51.19)\
[2023-09-14 06:50:51 10splitTasks](trainer.py 286): INFO [30/79]	0.1632(0.1915)	0.0007(0.0296)	1.369(1.444)	46.88(50.25)\
[2023-09-14 06:50:52 10splitTasks](trainer.py 286): INFO [40/79]	0.1645(0.1843)	0.0003(0.0225)	1.504(1.429)	50.00(50.50)\
[2023-09-14 06:50:54 10splitTasks](trainer.py 286): INFO [50/79]	0.1612(0.1801)	0.0004(0.0182)	1.585(1.437)	43.75(49.97)\
[2023-09-14 06:50:56 10splitTasks](trainer.py 286): INFO [60/79]	0.1615(0.1770)	0.0004(0.0153)	1.676(1.438)	35.94(49.54)\
[2023-09-14 06:50:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1621(0.1749)	0.0008(0.0132)	1.580(1.441)	45.31(49.23)\
[2023-09-14 06:50:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0341(0.1718)	0.0001(0.0119)	2.047(1.435)	25.00(49.22)\
[2023-09-14 06:50:58 10splitTasks](trainer.py 288): INFO  * Train Acc 49.220\
[2023-09-14 06:51:00 10splitTasks](trainer.py 147): INFO  * Val Acc 50.200, Total time 1.48\
[2023-09-14 06:51:00 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 06:51:00 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:51:00 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:51:01 10splitTasks](trainer.py 286): INFO [0/79]	0.8774(0.8774)	0.7118(0.7118)	1.530(1.530)	48.44(48.44)\
[2023-09-14 06:51:02 10splitTasks](trainer.py 286): INFO [10/79]	0.1615(0.2264)	0.0003(0.0651)	1.323(1.519)	51.56(47.73)\
[2023-09-14 06:51:04 10splitTasks](trainer.py 286): INFO [20/79]	0.1616(0.1955)	0.0004(0.0343)	1.367(1.460)	53.12(48.66)\
[2023-09-14 06:51:06 10splitTasks](trainer.py 286): INFO [30/79]	0.1616(0.1847)	0.0005(0.0234)	1.306(1.444)	59.38(49.29)\
[2023-09-14 06:51:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1670(0.1792)	0.0005(0.0178)	1.548(1.431)	43.75(49.50)\
[2023-09-14 06:51:09 10splitTasks](trainer.py 286): INFO [50/79]	0.1614(0.1760)	0.0004(0.0144)	1.312(1.419)	53.12(50.15)\
[2023-09-14 06:51:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1615(0.1736)	0.0003(0.0121)	1.279(1.407)	56.25(50.92)\
[2023-09-14 06:51:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1620(0.1719)	0.0008(0.0104)	1.324(1.411)	51.56(50.62)\
[2023-09-14 06:51:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0341(0.1691)	0.0001(0.0094)	1.257(1.402)	62.50(51.02)\
[2023-09-14 06:51:13 10splitTasks](trainer.py 288): INFO  * Train Acc 51.020\
[2023-09-14 06:51:15 10splitTasks](trainer.py 147): INFO  * Val Acc 51.200, Total time 1.48\
[2023-09-14 06:51:15 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 06:51:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:51:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:51:16 10splitTasks](trainer.py 286): INFO [0/79]	0.9426(0.9426)	0.7791(0.7791)	1.227(1.227)	59.38(59.38)\
[2023-09-14 06:51:17 10splitTasks](trainer.py 286): INFO [10/79]	0.1613(0.2325)	0.0003(0.0714)	1.311(1.305)	53.12(54.12)\
[2023-09-14 06:51:19 10splitTasks](trainer.py 286): INFO [20/79]	0.1615(0.1992)	0.0004(0.0381)	1.400(1.352)	39.06(52.53)\
[2023-09-14 06:51:21 10splitTasks](trainer.py 286): INFO [30/79]	0.1616(0.1891)	0.0004(0.0276)	1.316(1.354)	54.69(52.52)\
[2023-09-14 06:51:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1614(0.1825)	0.0003(0.0210)	1.504(1.360)	46.88(52.67)\
[2023-09-14 06:51:24 10splitTasks](trainer.py 286): INFO [50/79]	0.1611(0.1783)	0.0003(0.0170)	1.330(1.363)	56.25(53.25)\
[2023-09-14 06:51:26 10splitTasks](trainer.py 286): INFO [60/79]	0.1615(0.1756)	0.0003(0.0143)	1.098(1.349)	59.38(53.46)\
[2023-09-14 06:51:27 10splitTasks](trainer.py 286): INFO [70/79]	0.1618(0.1736)	0.0009(0.0123)	1.364(1.339)	48.44(53.83)\
[2023-09-14 06:51:28 10splitTasks](trainer.py 286): INFO [78/79]	0.0341(0.1707)	0.0001(0.0111)	1.483(1.346)	50.00(53.70)\
[2023-09-14 06:51:28 10splitTasks](trainer.py 288): INFO  * Train Acc 53.700\
[2023-09-14 06:51:30 10splitTasks](trainer.py 147): INFO  * Val Acc 50.400, Total time 1.48\
[2023-09-14 06:51:30 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 06:51:30 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:51:30 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:51:31 10splitTasks](trainer.py 286): INFO [0/79]	0.9175(0.9175)	0.7556(0.7556)	1.048(1.048)	67.19(67.19)\
[2023-09-14 06:51:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1618(0.2340)	0.0005(0.0725)	1.094(1.280)	54.69(52.41)\
[2023-09-14 06:51:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1626(0.2047)	0.0004(0.0424)	1.177(1.308)	54.69(53.12)\
[2023-09-14 06:51:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1614(0.1907)	0.0004(0.0289)	1.200(1.300)	59.38(54.28)\
[2023-09-14 06:51:37 10splitTasks](trainer.py 286): INFO [40/79]	0.1625(0.1838)	0.0005(0.0219)	1.414(1.307)	50.00(54.46)\
[2023-09-14 06:51:39 10splitTasks](trainer.py 286): INFO [50/79]	0.1614(0.1797)	0.0004(0.0177)	1.247(1.319)	51.56(53.65)\
[2023-09-14 06:51:41 10splitTasks](trainer.py 286): INFO [60/79]	0.1619(0.1768)	0.0005(0.0149)	1.211(1.310)	57.81(53.97)\
[2023-09-14 06:51:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1617(0.1747)	0.0009(0.0129)	1.291(1.311)	48.44(54.03)\
[2023-09-14 06:51:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0341(0.1716)	0.0001(0.0116)	1.494(1.310)	37.50(54.12)\
[2023-09-14 06:51:43 10splitTasks](trainer.py 288): INFO  * Train Acc 54.120\
[2023-09-14 06:51:45 10splitTasks](trainer.py 147): INFO  * Val Acc 51.000, Total time 1.50\
[2023-09-14 06:51:45 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 06:51:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:51:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:51:46 10splitTasks](trainer.py 286): INFO [0/79]	0.9211(0.9211)	0.7593(0.7593)	1.368(1.368)	54.69(54.69)\
[2023-09-14 06:51:48 10splitTasks](trainer.py 286): INFO [10/79]	0.1610(0.2394)	0.0003(0.0777)	1.084(1.350)	65.62(52.84)\
[2023-09-14 06:51:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1607(0.2022)	0.0004(0.0409)	1.172(1.314)	48.44(53.65)\
[2023-09-14 06:51:51 10splitTasks](trainer.py 286): INFO [30/79]	0.1614(0.1895)	0.0003(0.0278)	1.412(1.297)	51.56(54.69)\
[2023-09-14 06:51:52 10splitTasks](trainer.py 286): INFO [40/79]	0.1616(0.1828)	0.0004(0.0212)	1.202(1.288)	54.69(54.88)\
[2023-09-14 06:51:54 10splitTasks](trainer.py 286): INFO [50/79]	0.1626(0.1787)	0.0003(0.0171)	1.119(1.279)	65.62(55.30)\
[2023-09-14 06:51:56 10splitTasks](trainer.py 286): INFO [60/79]	0.1622(0.1759)	0.0003(0.0144)	1.153(1.263)	59.38(55.89)\
[2023-09-14 06:51:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1650(0.1740)	0.0009(0.0124)	1.259(1.262)	60.94(56.12)\
[2023-09-14 06:51:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0357(0.1711)	0.0001(0.0112)	0.927(1.259)	75.00(56.20)\
[2023-09-14 06:51:59 10splitTasks](trainer.py 288): INFO  * Train Acc 56.200\
[2023-09-14 06:52:00 10splitTasks](trainer.py 147): INFO  * Val Acc 55.200, Total time 1.48\
[2023-09-14 06:52:00 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 06:52:00 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:52:00 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:52:01 10splitTasks](trainer.py 286): INFO [0/79]	0.8597(0.8597)	0.6944(0.6944)	1.660(1.660)	45.31(45.31)\
[2023-09-14 06:52:03 10splitTasks](trainer.py 286): INFO [10/79]	0.1613(0.2248)	0.0003(0.0635)	1.212(1.217)	57.81(56.39)\
[2023-09-14 06:52:04 10splitTasks](trainer.py 286): INFO [20/79]	0.1614(0.1952)	0.0004(0.0334)	1.099(1.202)	59.38(56.03)\
[2023-09-14 06:52:06 10splitTasks](trainer.py 286): INFO [30/79]	0.1615(0.1844)	0.0003(0.0228)	1.319(1.205)	50.00(56.35)\
[2023-09-14 06:52:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1615(0.1790)	0.0004(0.0174)	1.249(1.213)	57.81(56.17)\
[2023-09-14 06:52:09 10splitTasks](trainer.py 286): INFO [50/79]	0.1615(0.1758)	0.0003(0.0140)	1.171(1.208)	59.38(56.50)\
[2023-09-14 06:52:11 10splitTasks](trainer.py 286): INFO [60/79]	0.1612(0.1738)	0.0003(0.0118)	0.852(1.201)	67.19(56.81)\
[2023-09-14 06:52:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1618(0.1722)	0.0009(0.0102)	1.060(1.200)	60.94(57.20)\
[2023-09-14 06:52:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0350(0.1694)	0.0002(0.0092)	1.446(1.208)	50.00(57.08)\
[2023-09-14 06:52:13 10splitTasks](trainer.py 288): INFO  * Train Acc 57.080\
[2023-09-14 06:52:15 10splitTasks](trainer.py 147): INFO  * Val Acc 55.200, Total time 1.49\
[2023-09-14 06:52:15 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 06:52:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:52:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:52:16 10splitTasks](trainer.py 286): INFO [0/79]	0.8775(0.8775)	0.7155(0.7155)	1.018(1.018)	59.38(59.38)\
[2023-09-14 06:52:17 10splitTasks](trainer.py 286): INFO [10/79]	0.1616(0.2262)	0.0003(0.0654)	1.003(1.112)	60.94(58.66)\
[2023-09-14 06:52:19 10splitTasks](trainer.py 286): INFO [20/79]	0.1613(0.1953)	0.0004(0.0344)	1.224(1.165)	60.94(58.18)\
[2023-09-14 06:52:21 10splitTasks](trainer.py 286): INFO [30/79]	0.1612(0.1845)	0.0003(0.0235)	1.216(1.124)	62.50(60.03)\
[2023-09-14 06:52:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1616(0.1794)	0.0004(0.0178)	1.106(1.155)	60.94(59.30)\
[2023-09-14 06:52:24 10splitTasks](trainer.py 286): INFO [50/79]	0.1617(0.1760)	0.0003(0.0144)	1.037(1.146)	64.06(59.74)\
[2023-09-14 06:52:26 10splitTasks](trainer.py 286): INFO [60/79]	0.1614(0.1737)	0.0003(0.0121)	0.953(1.160)	65.62(59.14)\
[2023-09-14 06:52:27 10splitTasks](trainer.py 286): INFO [70/79]	0.1621(0.1720)	0.0007(0.0105)	1.261(1.156)	56.25(59.26)\
[2023-09-14 06:52:28 10splitTasks](trainer.py 286): INFO [78/79]	0.0346(0.1693)	0.0001(0.0094)	0.921(1.163)	62.50(59.18)\
[2023-09-14 06:52:28 10splitTasks](trainer.py 288): INFO  * Train Acc 59.180\
[2023-09-14 06:52:30 10splitTasks](trainer.py 147): INFO  * Val Acc 54.600, Total time 1.47\
[2023-09-14 06:52:30 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 06:52:30 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:52:30 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:52:31 10splitTasks](trainer.py 286): INFO [0/79]	0.8825(0.8825)	0.7137(0.7137)	1.030(1.030)	62.50(62.50)\
[2023-09-14 06:52:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1616(0.2268)	0.0003(0.0652)	1.372(1.132)	50.00(61.36)\
[2023-09-14 06:52:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1615(0.1959)	0.0003(0.0345)	1.541(1.136)	46.88(61.90)\
[2023-09-14 06:52:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1614(0.1851)	0.0004(0.0235)	0.803(1.148)	73.44(60.99)\
[2023-09-14 06:52:37 10splitTasks](trainer.py 286): INFO [40/79]	0.1616(0.1796)	0.0003(0.0179)	1.079(1.131)	60.94(61.62)\
[2023-09-14 06:52:39 10splitTasks](trainer.py 286): INFO [50/79]	0.1615(0.1760)	0.0004(0.0145)	1.202(1.140)	59.38(61.27)\
[2023-09-14 06:52:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1615(0.1737)	0.0004(0.0122)	1.059(1.145)	62.50(60.76)\
[2023-09-14 06:52:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1618(0.1720)	0.0008(0.0105)	0.944(1.140)	68.75(60.83)\
[2023-09-14 06:52:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0364(0.1693)	0.0001(0.0095)	1.764(1.138)	50.00(60.72)\
[2023-09-14 06:52:43 10splitTasks](trainer.py 288): INFO  * Train Acc 60.720\
[2023-09-14 06:52:45 10splitTasks](trainer.py 147): INFO  * Val Acc 60.000, Total time 1.50\
[2023-09-14 06:52:45 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 06:52:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:52:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:52:46 10splitTasks](trainer.py 286): INFO [0/79]	0.9061(0.9061)	0.7440(0.7440)	1.236(1.236)	56.25(56.25)\
[2023-09-14 06:52:47 10splitTasks](trainer.py 286): INFO [10/79]	0.1612(0.2288)	0.0003(0.0680)	1.409(1.152)	51.56(57.53)\
[2023-09-14 06:52:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1611(0.1968)	0.0004(0.0359)	0.870(1.139)	71.88(59.30)\
[2023-09-14 06:52:51 10splitTasks](trainer.py 286): INFO [30/79]	0.1613(0.1858)	0.0004(0.0245)	1.069(1.147)	62.50(59.38)\
[2023-09-14 06:52:52 10splitTasks](trainer.py 286): INFO [40/79]	0.1612(0.1799)	0.0003(0.0186)	1.197(1.156)	51.56(59.49)\
[2023-09-14 06:52:54 10splitTasks](trainer.py 286): INFO [50/79]	0.1614(0.1764)	0.0003(0.0151)	1.039(1.132)	64.06(60.39)\
[2023-09-14 06:52:55 10splitTasks](trainer.py 286): INFO [60/79]	0.1612(0.1740)	0.0003(0.0127)	1.136(1.117)	60.94(60.84)\
[2023-09-14 06:52:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1623(0.1724)	0.0009(0.0110)	1.038(1.107)	62.50(61.42)\
[2023-09-14 06:52:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0340(0.1696)	0.0001(0.0099)	0.883(1.104)	62.50(61.78)\
[2023-09-14 06:52:58 10splitTasks](trainer.py 288): INFO  * Train Acc 61.780\
[2023-09-14 06:53:00 10splitTasks](trainer.py 147): INFO  * Val Acc 52.200, Total time 1.48\
[2023-09-14 06:53:00 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 06:53:00 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:53:00 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:53:01 10splitTasks](trainer.py 286): INFO [0/79]	0.9303(0.9303)	0.7675(0.7675)	1.149(1.149)	64.06(64.06)\
[2023-09-14 06:53:02 10splitTasks](trainer.py 286): INFO [10/79]	0.1609(0.2311)	0.0003(0.0702)	1.058(1.130)	64.06(61.36)\
[2023-09-14 06:53:04 10splitTasks](trainer.py 286): INFO [20/79]	0.1614(0.1980)	0.0003(0.0370)	1.018(1.114)	64.06(61.90)\
[2023-09-14 06:53:06 10splitTasks](trainer.py 286): INFO [30/79]	0.1609(0.1862)	0.0003(0.0252)	0.826(1.119)	70.31(62.85)\
[2023-09-14 06:53:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1614(0.1802)	0.0003(0.0191)	0.675(1.088)	76.56(63.30)\
[2023-09-14 06:53:09 10splitTasks](trainer.py 286): INFO [50/79]	0.1616(0.1766)	0.0003(0.0155)	1.006(1.079)	71.88(63.66)\
[2023-09-14 06:53:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1614(0.1742)	0.0003(0.0130)	1.086(1.080)	60.94(63.19)\
[2023-09-14 06:53:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1622(0.1725)	0.0011(0.0113)	1.240(1.071)	57.81(63.36)\
[2023-09-14 06:53:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0340(0.1697)	0.0001(0.0101)	2.239(1.080)	50.00(62.96)\
[2023-09-14 06:53:13 10splitTasks](trainer.py 288): INFO  * Train Acc 62.960\
[2023-09-14 06:53:15 10splitTasks](trainer.py 147): INFO  * Val Acc 60.400, Total time 1.52\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-0.pth\
=> Save Done\
[2023-09-14 06:53:15 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 06:53:16 10splitTasks](trainer.py 147): INFO  * Val Acc 60.400, Total time 1.44\
[2023-09-14 06:53:16 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 06:53:17 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 06:53:17 10splitTasks](iBatchLearn.py 155): INFO Acc:60.4; BWT:0;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 06:53:21 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:53:21 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:53:21 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 0, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-0.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_0.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:53:22 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-0.pth\
[2023-09-14 06:53:22 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 06:53:23 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 06:53:23 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 06:53:23 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
--------------------------------Official Evaluation--------------------------------\
0 62.9\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 06:53:34 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:53:34 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:53:34 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 1, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-0.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-1.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-0.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-1.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:53:34 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-0.pth\
[2023-09-14 06:53:34 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 06:53:36 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 06:53:36 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 06:53:36 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 06:53:36 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 06:53:36 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 06:53:36 10splitTasks](iBatchLearn.py 92): INFO ====================== 1 =======================\
[2023-09-14 06:53:36 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 06:53:36 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 06:53:36 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 06:53:36 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:53:36 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:53:40 10splitTasks](trainer.py 286): INFO [0/79]	3.5882(3.5882)	0.9058(0.9058)	2.467(2.467)	4.69(4.69)\
[2023-09-14 06:53:42 10splitTasks](trainer.py 286): INFO [10/79]	0.1957(0.5097)	0.0004(0.0828)	2.231(2.344)	21.88(13.78)\
[2023-09-14 06:53:44 10splitTasks](trainer.py 286): INFO [20/79]	0.1953(0.3605)	0.0003(0.0436)	2.071(2.250)	23.44(17.63)\
[2023-09-14 06:53:46 10splitTasks](trainer.py 286): INFO [30/79]	0.1953(0.3075)	0.0004(0.0297)	1.932(2.182)	34.38(20.36)\
[2023-09-14 06:53:48 10splitTasks](trainer.py 286): INFO [40/79]	0.1991(0.2805)	0.0004(0.0225)	1.734(2.108)	35.94(23.51)\
[2023-09-14 06:53:50 10splitTasks](trainer.py 286): INFO [50/79]	0.1974(0.2643)	0.0004(0.0182)	1.620(2.056)	34.38(25.70)\
[2023-09-14 06:53:52 10splitTasks](trainer.py 286): INFO [60/79]	0.1959(0.2535)	0.0004(0.0153)	1.883(2.020)	28.12(26.84)\
[2023-09-14 06:53:54 10splitTasks](trainer.py 286): INFO [70/79]	0.1954(0.2454)	0.0010(0.0132)	1.821(1.977)	42.19(28.70)\
[2023-09-14 06:53:55 10splitTasks](trainer.py 286): INFO [78/79]	0.2207(0.2406)	0.0001(0.0119)	1.634(1.950)	50.00(29.94)\
[2023-09-14 06:53:55 10splitTasks](trainer.py 288): INFO  * Train Acc 29.940\
[2023-09-14 06:53:57 10splitTasks](trainer.py 147): INFO  * Val Acc 46.000, Total time 1.66\
[2023-09-14 06:53:57 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 06:53:57 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:53:57 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:53:58 10splitTasks](trainer.py 286): INFO [0/79]	0.9691(0.9691)	0.7612(0.7612)	1.735(1.735)	50.00(50.00)\
[2023-09-14 06:54:00 10splitTasks](trainer.py 286): INFO [10/79]	0.1977(0.2688)	0.0003(0.0697)	1.690(1.677)	43.75(43.89)\
[2023-09-14 06:54:02 10splitTasks](trainer.py 286): INFO [20/79]	0.2013(0.2347)	0.0004(0.0367)	1.686(1.677)	42.19(43.15)\
[2023-09-14 06:54:04 10splitTasks](trainer.py 286): INFO [30/79]	0.1992(0.2234)	0.0006(0.0251)	1.812(1.685)	43.75(42.54)\
[2023-09-14 06:54:06 10splitTasks](trainer.py 286): INFO [40/79]	0.1970(0.2169)	0.0007(0.0191)	1.690(1.682)	45.31(43.18)\
[2023-09-14 06:54:08 10splitTasks](trainer.py 286): INFO [50/79]	0.1997(0.2129)	0.0003(0.0154)	1.632(1.670)	40.62(43.84)\
[2023-09-14 06:54:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1974(0.2106)	0.0004(0.0131)	1.818(1.666)	42.19(44.24)\
[2023-09-14 06:54:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1994(0.2087)	0.0023(0.0113)	1.644(1.660)	51.56(44.50)\
[2023-09-14 06:54:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2058)	0.0001(0.0102)	1.431(1.655)	62.50(44.96)\
[2023-09-14 06:54:13 10splitTasks](trainer.py 288): INFO  * Train Acc 44.960\
[2023-09-14 06:54:15 10splitTasks](trainer.py 147): INFO  * Val Acc 41.600, Total time 1.73\
[2023-09-14 06:54:15 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 06:54:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:54:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:54:16 10splitTasks](trainer.py 286): INFO [0/79]	0.9633(0.9633)	0.7644(0.7644)	1.777(1.777)	40.62(40.62)\
[2023-09-14 06:54:18 10splitTasks](trainer.py 286): INFO [10/79]	0.1955(0.2670)	0.0003(0.0699)	1.444(1.581)	57.81(46.73)\
[2023-09-14 06:54:20 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2337)	0.0004(0.0368)	1.638(1.575)	54.69(48.81)\
[2023-09-14 06:54:22 10splitTasks](trainer.py 286): INFO [30/79]	0.1984(0.2223)	0.0005(0.0251)	1.418(1.590)	51.56(48.24)\
[2023-09-14 06:54:24 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2163)	0.0003(0.0191)	1.552(1.597)	48.44(47.87)\
[2023-09-14 06:54:26 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2126)	0.0004(0.0155)	1.696(1.597)	45.31(47.73)\
[2023-09-14 06:54:28 10splitTasks](trainer.py 286): INFO [60/79]	0.1993(0.2102)	0.0004(0.0130)	1.511(1.601)	48.44(47.72)\
[2023-09-14 06:54:30 10splitTasks](trainer.py 286): INFO [70/79]	0.1989(0.2089)	0.0011(0.0113)	1.718(1.605)	43.75(47.89)\
[2023-09-14 06:54:31 10splitTasks](trainer.py 286): INFO [78/79]	0.0720(0.2059)	0.0001(0.0101)	1.443(1.616)	37.50(47.62)\
[2023-09-14 06:54:31 10splitTasks](trainer.py 288): INFO  * Train Acc 47.620\
[2023-09-14 06:54:33 10splitTasks](trainer.py 147): INFO  * Val Acc 50.000, Total time 1.57\
[2023-09-14 06:54:33 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 06:54:33 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:54:33 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:54:34 10splitTasks](trainer.py 286): INFO [0/79]	0.9593(0.9593)	0.7551(0.7551)	1.593(1.593)	42.19(42.19)\
[2023-09-14 06:54:36 10splitTasks](trainer.py 286): INFO [10/79]	0.1971(0.2668)	0.0004(0.0690)	1.660(1.650)	42.19(45.03)\
[2023-09-14 06:54:38 10splitTasks](trainer.py 286): INFO [20/79]	0.2071(0.2343)	0.0006(0.0364)	1.583(1.605)	56.25(48.21)\
[2023-09-14 06:54:40 10splitTasks](trainer.py 286): INFO [30/79]	0.1951(0.2224)	0.0004(0.0248)	1.523(1.625)	46.88(47.13)\
[2023-09-14 06:54:42 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2161)	0.0003(0.0189)	1.509(1.602)	57.81(48.70)\
[2023-09-14 06:54:44 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2123)	0.0004(0.0153)	1.351(1.602)	59.38(48.81)\
[2023-09-14 06:54:46 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2097)	0.0003(0.0129)	1.619(1.601)	42.19(48.87)\
[2023-09-14 06:54:48 10splitTasks](trainer.py 286): INFO [70/79]	0.1958(0.2080)	0.0011(0.0112)	1.817(1.607)	39.06(48.59)\
[2023-09-14 06:54:49 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2051)	0.0002(0.0101)	1.533(1.611)	50.00(48.64)\
[2023-09-14 06:54:49 10splitTasks](trainer.py 288): INFO  * Train Acc 48.640\
[2023-09-14 06:54:51 10splitTasks](trainer.py 147): INFO  * Val Acc 52.600, Total time 1.57\
[2023-09-14 06:54:51 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 06:54:51 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:54:51 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:54:52 10splitTasks](trainer.py 286): INFO [0/79]	0.9944(0.9944)	0.7950(0.7950)	1.346(1.346)	54.69(54.69)\
[2023-09-14 06:54:54 10splitTasks](trainer.py 286): INFO [10/79]	0.1981(0.2705)	0.0004(0.0727)	1.486(1.633)	59.38(48.72)\
[2023-09-14 06:54:56 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2363)	0.0003(0.0391)	1.620(1.584)	46.88(50.82)\
[2023-09-14 06:54:58 10splitTasks](trainer.py 286): INFO [30/79]	0.1985(0.2239)	0.0006(0.0266)	1.411(1.602)	56.25(50.55)\
[2023-09-14 06:55:00 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2173)	0.0004(0.0202)	1.627(1.598)	46.88(50.88)\
[2023-09-14 06:55:02 10splitTasks](trainer.py 286): INFO [50/79]	0.2113(0.2141)	0.0007(0.0164)	1.573(1.599)	50.00(51.07)\
[2023-09-14 06:55:04 10splitTasks](trainer.py 286): INFO [60/79]	0.1986(0.2118)	0.0004(0.0139)	1.650(1.598)	53.12(50.79)\
[2023-09-14 06:55:06 10splitTasks](trainer.py 286): INFO [70/79]	0.1966(0.2098)	0.0010(0.0120)	1.416(1.593)	59.38(50.95)\
[2023-09-14 06:55:07 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2067)	0.0001(0.0108)	2.136(1.595)	50.00(51.06)\
[2023-09-14 06:55:07 10splitTasks](trainer.py 288): INFO  * Train Acc 51.060\
[2023-09-14 06:55:09 10splitTasks](trainer.py 147): INFO  * Val Acc 53.400, Total time 1.61\
[2023-09-14 06:55:09 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 06:55:09 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:55:09 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:55:10 10splitTasks](trainer.py 286): INFO [0/79]	1.0989(1.0989)	0.8988(0.8988)	1.502(1.502)	56.25(56.25)\
[2023-09-14 06:55:12 10splitTasks](trainer.py 286): INFO [10/79]	0.2005(0.2804)	0.0004(0.0821)	1.796(1.654)	50.00(52.13)\
[2023-09-14 06:55:14 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2417)	0.0004(0.0435)	1.630(1.615)	56.25(51.64)\
[2023-09-14 06:55:16 10splitTasks](trainer.py 286): INFO [30/79]	0.2189(0.2284)	0.0007(0.0297)	1.427(1.596)	62.50(52.77)\
[2023-09-14 06:55:18 10splitTasks](trainer.py 286): INFO [40/79]	0.1972(0.2210)	0.0004(0.0226)	1.543(1.598)	59.38(52.32)\
[2023-09-14 06:55:20 10splitTasks](trainer.py 286): INFO [50/79]	0.1970(0.2163)	0.0007(0.0183)	1.577(1.608)	56.25(52.02)\
[2023-09-14 06:55:22 10splitTasks](trainer.py 286): INFO [60/79]	0.1958(0.2131)	0.0004(0.0154)	1.594(1.591)	62.50(52.69)\
[2023-09-14 06:55:24 10splitTasks](trainer.py 286): INFO [70/79]	0.1983(0.2110)	0.0010(0.0133)	1.329(1.589)	60.94(52.86)\
[2023-09-14 06:55:25 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2078)	0.0001(0.0120)	1.601(1.584)	50.00(53.02)\
[2023-09-14 06:55:25 10splitTasks](trainer.py 288): INFO  * Train Acc 53.020\
[2023-09-14 06:55:27 10splitTasks](trainer.py 147): INFO  * Val Acc 53.400, Total time 1.64\
[2023-09-14 06:55:27 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 06:55:27 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:55:27 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:55:28 10splitTasks](trainer.py 286): INFO [0/79]	1.0242(1.0242)	0.8253(0.8253)	1.469(1.469)	60.94(60.94)\
[2023-09-14 06:55:30 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2748)	0.0004(0.0755)	1.677(1.498)	45.31(58.10)\
[2023-09-14 06:55:32 10splitTasks](trainer.py 286): INFO [20/79]	0.1984(0.2388)	0.0006(0.0398)	1.595(1.524)	56.25(55.80)\
[2023-09-14 06:55:34 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2258)	0.0005(0.0271)	1.860(1.534)	43.75(55.19)\
[2023-09-14 06:55:36 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2187)	0.0003(0.0207)	1.658(1.528)	48.44(55.49)\
[2023-09-14 06:55:38 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2145)	0.0004(0.0167)	1.416(1.524)	57.81(55.64)\
[2023-09-14 06:55:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1954(0.2115)	0.0004(0.0140)	1.754(1.539)	45.31(54.76)\
[2023-09-14 06:55:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2096)	0.0013(0.0121)	1.668(1.545)	57.81(54.78)\
[2023-09-14 06:55:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2066)	0.0002(0.0109)	2.031(1.554)	25.00(54.86)\
[2023-09-14 06:55:43 10splitTasks](trainer.py 288): INFO  * Train Acc 54.860\
[2023-09-14 06:55:45 10splitTasks](trainer.py 147): INFO  * Val Acc 51.400, Total time 1.62\
[2023-09-14 06:55:45 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 06:55:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:55:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:55:46 10splitTasks](trainer.py 286): INFO [0/79]	0.9574(0.9574)	0.7592(0.7592)	1.842(1.842)	53.12(53.12)\
[2023-09-14 06:55:48 10splitTasks](trainer.py 286): INFO [10/79]	0.1956(0.2654)	0.0003(0.0694)	1.551(1.555)	53.12(55.26)\
[2023-09-14 06:55:50 10splitTasks](trainer.py 286): INFO [20/79]	0.1967(0.2333)	0.0003(0.0366)	1.520(1.554)	60.94(56.40)\
[2023-09-14 06:55:52 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2223)	0.0004(0.0254)	1.371(1.536)	57.81(56.65)\
[2023-09-14 06:55:54 10splitTasks](trainer.py 286): INFO [40/79]	0.1967(0.2162)	0.0003(0.0193)	1.550(1.531)	50.00(56.21)\
[2023-09-14 06:55:56 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2129)	0.0004(0.0157)	1.631(1.536)	56.25(55.94)\
[2023-09-14 06:55:58 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2102)	0.0004(0.0132)	1.695(1.549)	56.25(55.48)\
[2023-09-14 06:56:00 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2084)	0.0011(0.0115)	1.430(1.544)	59.38(55.79)\
[2023-09-14 06:56:01 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2055)	0.0002(0.0103)	1.907(1.545)	37.50(55.96)\
[2023-09-14 06:56:01 10splitTasks](trainer.py 288): INFO  * Train Acc 55.960\
[2023-09-14 06:56:03 10splitTasks](trainer.py 147): INFO  * Val Acc 55.000, Total time 1.61\
[2023-09-14 06:56:03 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 06:56:03 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:56:03 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:56:04 10splitTasks](trainer.py 286): INFO [0/79]	0.9874(0.9874)	0.7848(0.7848)	1.566(1.566)	56.25(56.25)\
[2023-09-14 06:56:06 10splitTasks](trainer.py 286): INFO [10/79]	0.2042(0.2731)	0.0002(0.0718)	1.681(1.554)	53.12(56.11)\
[2023-09-14 06:56:08 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2367)	0.0004(0.0378)	1.414(1.563)	54.69(55.21)\
[2023-09-14 06:56:10 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2238)	0.0003(0.0258)	1.420(1.553)	53.12(55.24)\
[2023-09-14 06:56:12 10splitTasks](trainer.py 286): INFO [40/79]	0.2000(0.2176)	0.0004(0.0196)	1.674(1.545)	51.56(55.95)\
[2023-09-14 06:56:14 10splitTasks](trainer.py 286): INFO [50/79]	0.1969(0.2139)	0.0003(0.0159)	1.626(1.552)	51.56(55.88)\
[2023-09-14 06:56:16 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2118)	0.0004(0.0133)	1.577(1.554)	57.81(55.74)\
[2023-09-14 06:56:18 10splitTasks](trainer.py 286): INFO [70/79]	0.1972(0.2098)	0.0010(0.0115)	1.508(1.559)	57.81(55.52)\
[2023-09-14 06:56:19 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2068)	0.0001(0.0104)	1.852(1.559)	37.50(55.96)\
[2023-09-14 06:56:19 10splitTasks](trainer.py 288): INFO  * Train Acc 55.960\
[2023-09-14 06:56:21 10splitTasks](trainer.py 147): INFO  * Val Acc 53.400, Total time 1.57\
[2023-09-14 06:56:21 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 06:56:21 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:56:21 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:56:22 10splitTasks](trainer.py 286): INFO [0/79]	1.1511(1.1511)	0.9524(0.9524)	1.633(1.633)	53.12(53.12)\
[2023-09-14 06:56:24 10splitTasks](trainer.py 286): INFO [10/79]	0.1958(0.2836)	0.0003(0.0871)	1.535(1.598)	59.38(54.12)\
[2023-09-14 06:56:26 10splitTasks](trainer.py 286): INFO [20/79]	0.1960(0.2426)	0.0003(0.0458)	1.519(1.552)	56.25(56.62)\
[2023-09-14 06:56:28 10splitTasks](trainer.py 286): INFO [30/79]	0.1979(0.2283)	0.0004(0.0312)	1.525(1.571)	60.94(55.85)\
[2023-09-14 06:56:30 10splitTasks](trainer.py 286): INFO [40/79]	0.1976(0.2208)	0.0006(0.0237)	1.515(1.566)	60.94(55.87)\
[2023-09-14 06:56:32 10splitTasks](trainer.py 286): INFO [50/79]	0.2001(0.2164)	0.0004(0.0192)	1.735(1.575)	46.88(55.51)\
[2023-09-14 06:56:34 10splitTasks](trainer.py 286): INFO [60/79]	0.1956(0.2136)	0.0003(0.0161)	1.543(1.568)	56.25(56.05)\
[2023-09-14 06:56:36 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2113)	0.0010(0.0139)	1.474(1.572)	54.69(56.05)\
[2023-09-14 06:56:37 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2081)	0.0001(0.0125)	2.137(1.571)	37.50(56.16)\
[2023-09-14 06:56:37 10splitTasks](trainer.py 288): INFO  * Train Acc 56.160\
[2023-09-14 06:56:39 10splitTasks](trainer.py 147): INFO  * Val Acc 58.000, Total time 1.56\
[2023-09-14 06:56:39 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 06:56:39 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:56:39 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:56:40 10splitTasks](trainer.py 286): INFO [0/79]	0.9256(0.9256)	0.6977(0.6977)	1.490(1.490)	59.38(59.38)\
[2023-09-14 06:56:42 10splitTasks](trainer.py 286): INFO [10/79]	0.1978(0.2657)	0.0002(0.0639)	1.525(1.606)	64.06(55.40)\
[2023-09-14 06:56:44 10splitTasks](trainer.py 286): INFO [20/79]	0.2066(0.2348)	0.0106(0.0342)	1.604(1.569)	54.69(57.66)\
[2023-09-14 06:56:46 10splitTasks](trainer.py 286): INFO [30/79]	0.1956(0.2225)	0.0004(0.0233)	1.543(1.566)	59.38(57.71)\
[2023-09-14 06:56:48 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2165)	0.0003(0.0177)	1.422(1.569)	60.94(57.28)\
[2023-09-14 06:56:50 10splitTasks](trainer.py 286): INFO [50/79]	0.2034(0.2129)	0.0004(0.0144)	1.496(1.554)	57.81(57.44)\
[2023-09-14 06:56:52 10splitTasks](trainer.py 286): INFO [60/79]	0.1958(0.2103)	0.0003(0.0121)	1.539(1.546)	62.50(58.20)\
[2023-09-14 06:56:54 10splitTasks](trainer.py 286): INFO [70/79]	0.1974(0.2085)	0.0011(0.0105)	1.278(1.536)	68.75(58.30)\
[2023-09-14 06:56:55 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2056)	0.0001(0.0094)	1.634(1.536)	37.50(58.04)\
[2023-09-14 06:56:55 10splitTasks](trainer.py 288): INFO  * Train Acc 58.040\
[2023-09-14 06:56:57 10splitTasks](trainer.py 147): INFO  * Val Acc 58.000, Total time 1.62\
[2023-09-14 06:56:57 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 06:56:57 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:56:57 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:56:58 10splitTasks](trainer.py 286): INFO [0/79]	1.0013(1.0013)	0.8032(0.8032)	1.436(1.436)	65.62(65.62)\
[2023-09-14 06:57:00 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2698)	0.0003(0.0734)	1.305(1.518)	67.19(58.66)\
[2023-09-14 06:57:02 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2350)	0.0004(0.0386)	1.299(1.499)	67.19(58.85)\
[2023-09-14 06:57:04 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2259)	0.0003(0.0263)	1.660(1.546)	50.00(56.70)\
[2023-09-14 06:57:06 10splitTasks](trainer.py 286): INFO [40/79]	0.1970(0.2188)	0.0006(0.0200)	1.605(1.533)	60.94(57.39)\
[2023-09-14 06:57:08 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2147)	0.0003(0.0164)	1.503(1.544)	64.06(57.26)\
[2023-09-14 06:57:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2117)	0.0003(0.0138)	1.612(1.547)	56.25(57.25)\
[2023-09-14 06:57:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1976(0.2096)	0.0010(0.0119)	1.824(1.547)	45.31(57.61)\
[2023-09-14 06:57:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2065)	0.0001(0.0107)	2.527(1.547)	25.00(57.70)\
[2023-09-14 06:57:13 10splitTasks](trainer.py 288): INFO  * Train Acc 57.700\
[2023-09-14 06:57:15 10splitTasks](trainer.py 147): INFO  * Val Acc 57.800, Total time 1.62\
[2023-09-14 06:57:15 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 06:57:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:57:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:57:16 10splitTasks](trainer.py 286): INFO [0/79]	1.0364(1.0364)	0.8384(0.8384)	1.562(1.562)	53.12(53.12)\
[2023-09-14 06:57:18 10splitTasks](trainer.py 286): INFO [10/79]	0.1981(0.2728)	0.0003(0.0766)	1.758(1.600)	50.00(55.40)\
[2023-09-14 06:57:20 10splitTasks](trainer.py 286): INFO [20/79]	0.1968(0.2369)	0.0003(0.0403)	1.441(1.586)	60.94(56.18)\
[2023-09-14 06:57:22 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2240)	0.0002(0.0274)	1.380(1.577)	70.31(57.21)\
[2023-09-14 06:57:24 10splitTasks](trainer.py 286): INFO [40/79]	0.1981(0.2181)	0.0004(0.0210)	1.723(1.567)	50.00(57.51)\
[2023-09-14 06:57:26 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2138)	0.0004(0.0170)	1.481(1.566)	65.62(57.23)\
[2023-09-14 06:57:28 10splitTasks](trainer.py 286): INFO [60/79]	0.2028(0.2113)	0.0006(0.0143)	1.488(1.569)	62.50(57.25)\
[2023-09-14 06:57:30 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2093)	0.0010(0.0123)	1.574(1.564)	56.25(57.42)\
[2023-09-14 06:57:31 10splitTasks](trainer.py 286): INFO [78/79]	0.0706(0.2063)	0.0002(0.0111)	1.861(1.563)	37.50(57.76)\
[2023-09-14 06:57:31 10splitTasks](trainer.py 288): INFO  * Train Acc 57.760\
[2023-09-14 06:57:33 10splitTasks](trainer.py 147): INFO  * Val Acc 57.200, Total time 1.58\
[2023-09-14 06:57:33 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 06:57:33 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:57:33 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:57:34 10splitTasks](trainer.py 286): INFO [0/79]	0.9660(0.9660)	0.7450(0.7450)	1.596(1.596)	62.50(62.50)\
[2023-09-14 06:57:36 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2705)	0.0004(0.0720)	1.667(1.631)	62.50(57.81)\
[2023-09-14 06:57:38 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2353)	0.0004(0.0380)	1.668(1.618)	46.88(57.22)\
[2023-09-14 06:57:40 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2228)	0.0004(0.0259)	1.558(1.617)	57.81(56.45)\
[2023-09-14 06:57:42 10splitTasks](trainer.py 286): INFO [40/79]	0.1974(0.2163)	0.0004(0.0197)	1.632(1.597)	48.44(56.78)\
[2023-09-14 06:57:44 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2125)	0.0004(0.0159)	1.414(1.587)	60.94(57.35)\
[2023-09-14 06:57:46 10splitTasks](trainer.py 286): INFO [60/79]	0.1954(0.2099)	0.0004(0.0134)	1.374(1.572)	62.50(57.68)\
[2023-09-14 06:57:47 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2080)	0.0010(0.0116)	1.530(1.566)	46.88(57.61)\
[2023-09-14 06:57:49 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2051)	0.0001(0.0104)	2.708(1.571)	12.50(57.54)\
[2023-09-14 06:57:49 10splitTasks](trainer.py 288): INFO  * Train Acc 57.540\
[2023-09-14 06:57:51 10splitTasks](trainer.py 147): INFO  * Val Acc 60.200, Total time 1.65\
[2023-09-14 06:57:51 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 06:57:51 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:57:51 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:57:52 10splitTasks](trainer.py 286): INFO [0/79]	0.9351(0.9351)	0.7248(0.7248)	1.744(1.744)	56.25(56.25)\
[2023-09-14 06:57:54 10splitTasks](trainer.py 286): INFO [10/79]	0.2009(0.2649)	0.0007(0.0663)	1.407(1.598)	65.62(57.39)\
[2023-09-14 06:57:56 10splitTasks](trainer.py 286): INFO [20/79]	0.1984(0.2335)	0.0004(0.0350)	1.863(1.602)	50.00(56.92)\
[2023-09-14 06:57:57 10splitTasks](trainer.py 286): INFO [30/79]	0.1986(0.2217)	0.0004(0.0238)	1.705(1.598)	56.25(57.41)\
[2023-09-14 06:57:59 10splitTasks](trainer.py 286): INFO [40/79]	0.1987(0.2160)	0.0006(0.0182)	1.500(1.589)	64.06(57.74)\
[2023-09-14 06:58:01 10splitTasks](trainer.py 286): INFO [50/79]	0.1979(0.2125)	0.0004(0.0147)	1.174(1.581)	73.44(57.81)\
[2023-09-14 06:58:03 10splitTasks](trainer.py 286): INFO [60/79]	0.2015(0.2102)	0.0003(0.0124)	1.408(1.574)	62.50(58.22)\
[2023-09-14 06:58:05 10splitTasks](trainer.py 286): INFO [70/79]	0.1996(0.2086)	0.0009(0.0107)	1.421(1.565)	62.50(58.52)\
[2023-09-14 06:58:07 10splitTasks](trainer.py 286): INFO [78/79]	0.0697(0.2057)	0.0001(0.0097)	2.296(1.565)	50.00(58.68)\
[2023-09-14 06:58:07 10splitTasks](trainer.py 288): INFO  * Train Acc 58.680\
[2023-09-14 06:58:09 10splitTasks](trainer.py 147): INFO  * Val Acc 55.000, Total time 1.62\
[2023-09-14 06:58:09 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 06:58:09 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:58:09 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:58:10 10splitTasks](trainer.py 286): INFO [0/79]	0.9420(0.9420)	0.7302(0.7302)	1.566(1.566)	62.50(62.50)\
[2023-09-14 06:58:11 10splitTasks](trainer.py 286): INFO [10/79]	0.1982(0.2665)	0.0005(0.0668)	1.619(1.626)	57.81(57.67)\
[2023-09-14 06:58:14 10splitTasks](trainer.py 286): INFO [20/79]	0.2013(0.2357)	0.0004(0.0353)	1.678(1.595)	53.12(59.52)\
[2023-09-14 06:58:16 10splitTasks](trainer.py 286): INFO [30/79]	0.1960(0.2237)	0.0004(0.0241)	1.473(1.585)	67.19(60.13)\
[2023-09-14 06:58:17 10splitTasks](trainer.py 286): INFO [40/79]	0.1965(0.2171)	0.0004(0.0183)	1.630(1.575)	62.50(60.52)\
[2023-09-14 06:58:19 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2130)	0.0005(0.0148)	1.416(1.566)	65.62(60.57)\
[2023-09-14 06:58:21 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2104)	0.0004(0.0125)	1.670(1.557)	51.56(60.66)\
[2023-09-14 06:58:23 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2085)	0.0011(0.0108)	1.382(1.551)	67.19(60.52)\
[2023-09-14 06:58:25 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2055)	0.0001(0.0097)	1.706(1.551)	50.00(60.12)\
[2023-09-14 06:58:25 10splitTasks](trainer.py 288): INFO  * Train Acc 60.120\
[2023-09-14 06:58:26 10splitTasks](trainer.py 147): INFO  * Val Acc 58.000, Total time 1.60\
[2023-09-14 06:58:26 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 06:58:26 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:58:26 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:58:28 10splitTasks](trainer.py 286): INFO [0/79]	1.1548(1.1548)	0.9567(0.9567)	1.386(1.386)	65.62(65.62)\
[2023-09-14 06:58:30 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2844)	0.0004(0.0876)	1.390(1.522)	71.88(60.37)\
[2023-09-14 06:58:32 10splitTasks](trainer.py 286): INFO [20/79]	0.2031(0.2430)	0.0009(0.0462)	1.429(1.550)	60.94(59.15)\
[2023-09-14 06:58:34 10splitTasks](trainer.py 286): INFO [30/79]	0.1980(0.2286)	0.0004(0.0314)	1.413(1.518)	62.50(59.98)\
[2023-09-14 06:58:36 10splitTasks](trainer.py 286): INFO [40/79]	0.1975(0.2209)	0.0004(0.0239)	1.393(1.504)	68.75(61.01)\
[2023-09-14 06:58:37 10splitTasks](trainer.py 286): INFO [50/79]	0.1973(0.2162)	0.0011(0.0193)	1.733(1.510)	59.38(60.51)\
[2023-09-14 06:58:39 10splitTasks](trainer.py 286): INFO [60/79]	0.1957(0.2130)	0.0004(0.0163)	1.343(1.501)	62.50(61.01)\
[2023-09-14 06:58:41 10splitTasks](trainer.py 286): INFO [70/79]	0.1974(0.2108)	0.0009(0.0140)	1.452(1.502)	59.38(61.18)\
[2023-09-14 06:58:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0707(0.2077)	0.0002(0.0126)	1.946(1.512)	50.00(60.94)\
[2023-09-14 06:58:43 10splitTasks](trainer.py 288): INFO  * Train Acc 60.940\
[2023-09-14 06:58:45 10splitTasks](trainer.py 147): INFO  * Val Acc 56.200, Total time 1.59\
[2023-09-14 06:58:45 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 06:58:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:58:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:58:45 10splitTasks](trainer.py 286): INFO [0/79]	0.9543(0.9543)	0.7378(0.7378)	1.320(1.320)	62.50(62.50)\
[2023-09-14 06:58:47 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2676)	0.0003(0.0675)	1.499(1.449)	57.81(62.36)\
[2023-09-14 06:58:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1985(0.2346)	0.0005(0.0356)	1.818(1.505)	51.56(61.38)\
[2023-09-14 06:58:51 10splitTasks](trainer.py 286): INFO [30/79]	0.1974(0.2234)	0.0004(0.0243)	1.624(1.522)	59.38(60.99)\
[2023-09-14 06:58:53 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2169)	0.0004(0.0185)	1.599(1.518)	62.50(60.94)\
[2023-09-14 06:58:55 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2130)	0.0004(0.0150)	1.784(1.526)	57.81(60.91)\
[2023-09-14 06:58:57 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2105)	0.0003(0.0126)	1.452(1.528)	64.06(61.19)\
[2023-09-14 06:58:59 10splitTasks](trainer.py 286): INFO [70/79]	0.1990(0.2086)	0.0013(0.0109)	1.489(1.520)	59.38(61.38)\
[2023-09-14 06:59:01 10splitTasks](trainer.py 286): INFO [78/79]	0.0712(0.2057)	0.0002(0.0098)	1.929(1.526)	50.00(61.36)\
[2023-09-14 06:59:01 10splitTasks](trainer.py 288): INFO  * Train Acc 61.360\
[2023-09-14 06:59:02 10splitTasks](trainer.py 147): INFO  * Val Acc 56.600, Total time 1.58\
[2023-09-14 06:59:02 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 06:59:02 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:59:02 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:59:03 10splitTasks](trainer.py 286): INFO [0/79]	0.9904(0.9904)	0.7925(0.7925)	1.597(1.597)	54.69(54.69)\
[2023-09-14 06:59:05 10splitTasks](trainer.py 286): INFO [10/79]	0.1981(0.2687)	0.0003(0.0725)	1.776(1.588)	51.56(57.24)\
[2023-09-14 06:59:07 10splitTasks](trainer.py 286): INFO [20/79]	0.1977(0.2347)	0.0005(0.0382)	1.749(1.597)	56.25(57.59)\
[2023-09-14 06:59:09 10splitTasks](trainer.py 286): INFO [30/79]	0.2047(0.2228)	0.0006(0.0261)	1.657(1.565)	54.69(59.32)\
[2023-09-14 06:59:11 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2165)	0.0003(0.0198)	1.484(1.554)	65.62(59.91)\
[2023-09-14 06:59:13 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2130)	0.0003(0.0160)	1.450(1.545)	65.62(60.32)\
[2023-09-14 06:59:15 10splitTasks](trainer.py 286): INFO [60/79]	0.1972(0.2106)	0.0004(0.0135)	1.485(1.552)	68.75(59.96)\
[2023-09-14 06:59:17 10splitTasks](trainer.py 286): INFO [70/79]	0.1973(0.2088)	0.0010(0.0117)	1.319(1.540)	71.88(60.52)\
[2023-09-14 06:59:19 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2058)	0.0001(0.0105)	1.692(1.541)	75.00(60.64)\
[2023-09-14 06:59:19 10splitTasks](trainer.py 288): INFO  * Train Acc 60.640\
[2023-09-14 06:59:20 10splitTasks](trainer.py 147): INFO  * Val Acc 59.000, Total time 1.59\
[2023-09-14 06:59:20 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 06:59:20 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 06:59:20 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 06:59:21 10splitTasks](trainer.py 286): INFO [0/79]	1.1066(1.1066)	0.9086(0.9086)	1.750(1.750)	51.56(51.56)\
[2023-09-14 06:59:23 10splitTasks](trainer.py 286): INFO [10/79]	0.1949(0.2791)	0.0003(0.0830)	1.652(1.685)	54.69(55.11)\
[2023-09-14 06:59:25 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2403)	0.0004(0.0440)	1.327(1.599)	67.19(58.26)\
[2023-09-14 06:59:27 10splitTasks](trainer.py 286): INFO [30/79]	0.2160(0.2278)	0.0004(0.0300)	1.615(1.587)	56.25(58.62)\
[2023-09-14 06:59:29 10splitTasks](trainer.py 286): INFO [40/79]	0.1958(0.2206)	0.0003(0.0228)	1.611(1.582)	59.38(59.15)\
[2023-09-14 06:59:31 10splitTasks](trainer.py 286): INFO [50/79]	0.2024(0.2166)	0.0007(0.0184)	1.536(1.567)	56.25(59.31)\
[2023-09-14 06:59:33 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2135)	0.0004(0.0155)	1.671(1.569)	46.88(58.89)\
[2023-09-14 06:59:35 10splitTasks](trainer.py 286): INFO [70/79]	0.2124(0.2116)	0.0029(0.0134)	1.689(1.566)	56.25(59.35)\
[2023-09-14 06:59:37 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2084)	0.0001(0.0121)	1.876(1.556)	50.00(59.86)\
[2023-09-14 06:59:37 10splitTasks](trainer.py 288): INFO  * Train Acc 59.860\
[2023-09-14 06:59:39 10splitTasks](trainer.py 147): INFO  * Val Acc 58.600, Total time 1.66\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-1.pth\
=> Save Done\
[2023-09-14 06:59:39 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 06:59:40 10splitTasks](trainer.py 147): INFO  * Val Acc 44.400, Total time 1.46\
[2023-09-14 06:59:40 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 06:59:42 10splitTasks](trainer.py 147): INFO  * Val Acc 58.600, Total time 1.63\
[2023-09-14 06:59:42 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 06:59:42 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 06:59:42 10splitTasks](iBatchLearn.py 155): INFO Acc:51.50000018310547; BWT:-15.99999981689453;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 06:59:47 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 06:59:47 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 06:59:47 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 1, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-1.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_1.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 06:59:47 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-1.pth\
[2023-09-14 06:59:47 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 06:59:49 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 06:59:49 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 06:59:49 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 06:59:53 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
--------------------------------Official Evaluation--------------------------------\
1 31.0\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:00:02 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:00:02 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:00:02 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 2, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-1.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-2.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-1.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-2.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:00:02 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-1.pth\
[2023-09-14 07:00:02 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:00:04 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:00:04 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:00:04 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:00:04 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:00:04 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:00:04 10splitTasks](iBatchLearn.py 92): INFO ====================== 2 =======================\
[2023-09-14 07:00:04 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:00:04 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:00:04 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:00:04 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:00:04 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:00:08 10splitTasks](trainer.py 286): INFO [0/79]	3.2996(3.2996)	0.8045(0.8045)	2.380(2.380)	9.38(9.38)\
[2023-09-14 07:00:09 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.4813)	0.0003(0.0736)	2.151(2.268)	28.12(15.48)\
[2023-09-14 07:00:11 10splitTasks](trainer.py 286): INFO [20/79]	0.1955(0.3462)	0.0004(0.0390)	2.018(2.129)	21.88(22.25)\
[2023-09-14 07:00:13 10splitTasks](trainer.py 286): INFO [30/79]	0.1978(0.2982)	0.0003(0.0266)	1.796(2.056)	35.94(25.96)\
[2023-09-14 07:00:15 10splitTasks](trainer.py 286): INFO [40/79]	0.1965(0.2738)	0.0004(0.0203)	1.618(1.985)	37.50(28.93)\
[2023-09-14 07:00:17 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2586)	0.0004(0.0164)	2.016(1.949)	34.38(30.55)\
[2023-09-14 07:00:19 10splitTasks](trainer.py 286): INFO [60/79]	0.1966(0.2488)	0.0004(0.0139)	1.492(1.910)	43.75(32.30)\
[2023-09-14 07:00:21 10splitTasks](trainer.py 286): INFO [70/79]	0.2005(0.2414)	0.0012(0.0120)	1.500(1.870)	54.69(34.11)\
[2023-09-14 07:00:23 10splitTasks](trainer.py 286): INFO [78/79]	0.2250(0.2373)	0.0001(0.0108)	1.260(1.842)	50.00(35.30)\
[2023-09-14 07:00:23 10splitTasks](trainer.py 288): INFO  * Train Acc 35.300\
[2023-09-14 07:00:25 10splitTasks](trainer.py 147): INFO  * Val Acc 44.600, Total time 1.75\
[2023-09-14 07:00:25 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:00:25 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:00:25 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:00:26 10splitTasks](trainer.py 286): INFO [0/79]	0.9491(0.9491)	0.7425(0.7425)	1.608(1.608)	45.31(45.31)\
[2023-09-14 07:00:28 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2657)	0.0003(0.0679)	1.459(1.533)	50.00(46.31)\
[2023-09-14 07:00:30 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2344)	0.0003(0.0358)	1.556(1.573)	50.00(46.88)\
[2023-09-14 07:00:32 10splitTasks](trainer.py 286): INFO [30/79]	0.1975(0.2224)	0.0004(0.0244)	1.477(1.586)	48.44(46.82)\
[2023-09-14 07:00:34 10splitTasks](trainer.py 286): INFO [40/79]	0.1972(0.2165)	0.0005(0.0186)	1.699(1.583)	42.19(47.29)\
[2023-09-14 07:00:36 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2126)	0.0004(0.0150)	1.345(1.567)	57.81(47.67)\
[2023-09-14 07:00:38 10splitTasks](trainer.py 286): INFO [60/79]	0.1958(0.2100)	0.0004(0.0127)	1.635(1.571)	43.75(47.08)\
[2023-09-14 07:00:40 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2085)	0.0010(0.0110)	1.357(1.563)	60.94(47.51)\
[2023-09-14 07:00:41 10splitTasks](trainer.py 286): INFO [78/79]	0.0717(0.2056)	0.0001(0.0099)	1.562(1.558)	50.00(47.78)\
[2023-09-14 07:00:41 10splitTasks](trainer.py 288): INFO  * Train Acc 47.780\
[2023-09-14 07:00:43 10splitTasks](trainer.py 147): INFO  * Val Acc 44.200, Total time 1.60\
[2023-09-14 07:00:43 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:00:43 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:00:43 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:00:44 10splitTasks](trainer.py 286): INFO [0/79]	0.9103(0.9103)	0.7047(0.7047)	1.479(1.479)	51.56(51.56)\
[2023-09-14 07:00:46 10splitTasks](trainer.py 286): INFO [10/79]	0.1957(0.2625)	0.0003(0.0645)	1.528(1.520)	50.00(50.14)\
[2023-09-14 07:00:48 10splitTasks](trainer.py 286): INFO [20/79]	0.1973(0.2331)	0.0006(0.0340)	1.401(1.510)	56.25(52.08)\
[2023-09-14 07:00:50 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2217)	0.0004(0.0233)	1.640(1.533)	48.44(51.06)\
[2023-09-14 07:00:51 10splitTasks](trainer.py 286): INFO [40/79]	0.1958(0.2155)	0.0003(0.0177)	1.686(1.544)	46.88(50.61)\
[2023-09-14 07:00:53 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2116)	0.0003(0.0143)	1.634(1.539)	45.31(50.77)\
[2023-09-14 07:00:55 10splitTasks](trainer.py 286): INFO [60/79]	0.1971(0.2093)	0.0003(0.0120)	1.695(1.538)	50.00(50.54)\
[2023-09-14 07:00:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2077)	0.0010(0.0104)	1.585(1.533)	42.19(50.42)\
[2023-09-14 07:00:59 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2048)	0.0001(0.0094)	2.419(1.532)	37.50(50.78)\
[2023-09-14 07:00:59 10splitTasks](trainer.py 288): INFO  * Train Acc 50.780\
[2023-09-14 07:01:01 10splitTasks](trainer.py 147): INFO  * Val Acc 51.000, Total time 1.58\
[2023-09-14 07:01:01 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:01:01 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:01:01 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:01:01 10splitTasks](trainer.py 286): INFO [0/79]	0.9403(0.9403)	0.7377(0.7377)	1.330(1.330)	57.81(57.81)\
[2023-09-14 07:01:03 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2665)	0.0004(0.0701)	1.542(1.588)	50.00(49.86)\
[2023-09-14 07:01:05 10splitTasks](trainer.py 286): INFO [20/79]	0.1969(0.2337)	0.0006(0.0370)	1.392(1.530)	57.81(53.72)\
[2023-09-14 07:01:07 10splitTasks](trainer.py 286): INFO [30/79]	0.1982(0.2219)	0.0003(0.0252)	1.566(1.540)	45.31(52.77)\
[2023-09-14 07:01:09 10splitTasks](trainer.py 286): INFO [40/79]	0.1954(0.2157)	0.0004(0.0192)	1.724(1.542)	46.88(53.01)\
[2023-09-14 07:01:11 10splitTasks](trainer.py 286): INFO [50/79]	0.1969(0.2121)	0.0005(0.0155)	1.402(1.533)	50.00(53.06)\
[2023-09-14 07:01:13 10splitTasks](trainer.py 286): INFO [60/79]	0.1978(0.2100)	0.0004(0.0131)	1.710(1.521)	48.44(53.20)\
[2023-09-14 07:01:15 10splitTasks](trainer.py 286): INFO [70/79]	0.1964(0.2080)	0.0009(0.0113)	1.645(1.517)	42.19(52.97)\
[2023-09-14 07:01:17 10splitTasks](trainer.py 286): INFO [78/79]	0.0708(0.2052)	0.0002(0.0102)	1.252(1.519)	62.50(53.04)\
[2023-09-14 07:01:17 10splitTasks](trainer.py 288): INFO  * Train Acc 53.040\
[2023-09-14 07:01:18 10splitTasks](trainer.py 147): INFO  * Val Acc 52.800, Total time 1.68\
[2023-09-14 07:01:18 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:01:18 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:01:18 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:01:19 10splitTasks](trainer.py 286): INFO [0/79]	0.9324(0.9324)	0.7225(0.7225)	1.336(1.336)	64.06(64.06)\
[2023-09-14 07:01:21 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2656)	0.0003(0.0661)	1.539(1.565)	53.12(51.85)\
[2023-09-14 07:01:23 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2329)	0.0003(0.0349)	1.533(1.516)	51.56(54.09)\
[2023-09-14 07:01:25 10splitTasks](trainer.py 286): INFO [30/79]	0.1992(0.2216)	0.0003(0.0238)	1.422(1.507)	54.69(54.54)\
[2023-09-14 07:01:27 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2155)	0.0003(0.0181)	1.612(1.511)	43.75(53.70)\
[2023-09-14 07:01:29 10splitTasks](trainer.py 286): INFO [50/79]	0.1956(0.2118)	0.0004(0.0147)	1.586(1.500)	56.25(54.20)\
[2023-09-14 07:01:31 10splitTasks](trainer.py 286): INFO [60/79]	0.1957(0.2094)	0.0003(0.0123)	1.442(1.490)	62.50(54.84)\
[2023-09-14 07:01:33 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2078)	0.0010(0.0107)	1.255(1.488)	64.06(54.75)\
[2023-09-14 07:01:35 10splitTasks](trainer.py 286): INFO [78/79]	0.0711(0.2050)	0.0002(0.0096)	2.329(1.492)	50.00(54.66)\
[2023-09-14 07:01:35 10splitTasks](trainer.py 288): INFO  * Train Acc 54.660\
[2023-09-14 07:01:36 10splitTasks](trainer.py 147): INFO  * Val Acc 51.800, Total time 1.56\
[2023-09-14 07:01:36 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:01:36 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:01:36 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:01:37 10splitTasks](trainer.py 286): INFO [0/79]	0.9330(0.9330)	0.7345(0.7345)	1.398(1.398)	60.94(60.94)\
[2023-09-14 07:01:39 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2651)	0.0003(0.0672)	1.620(1.521)	51.56(53.12)\
[2023-09-14 07:01:41 10splitTasks](trainer.py 286): INFO [20/79]	0.1959(0.2332)	0.0003(0.0356)	1.312(1.469)	57.81(56.32)\
[2023-09-14 07:01:43 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2215)	0.0006(0.0243)	1.618(1.479)	51.56(56.20)\
[2023-09-14 07:01:45 10splitTasks](trainer.py 286): INFO [40/79]	0.1958(0.2157)	0.0003(0.0187)	1.496(1.499)	54.69(55.49)\
[2023-09-14 07:01:47 10splitTasks](trainer.py 286): INFO [50/79]	0.1985(0.2121)	0.0006(0.0151)	1.842(1.529)	56.25(54.96)\
[2023-09-14 07:01:49 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2097)	0.0003(0.0127)	1.415(1.533)	64.06(54.56)\
[2023-09-14 07:01:51 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2079)	0.0009(0.0110)	1.453(1.523)	51.56(55.02)\
[2023-09-14 07:01:52 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2051)	0.0001(0.0099)	1.183(1.518)	62.50(55.00)\
[2023-09-14 07:01:53 10splitTasks](trainer.py 288): INFO  * Train Acc 55.000\
[2023-09-14 07:01:54 10splitTasks](trainer.py 147): INFO  * Val Acc 55.200, Total time 1.57\
[2023-09-14 07:01:54 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:01:54 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:01:54 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:01:55 10splitTasks](trainer.py 286): INFO [0/79]	0.9217(0.9217)	0.7065(0.7065)	1.283(1.283)	62.50(62.50)\
[2023-09-14 07:01:57 10splitTasks](trainer.py 286): INFO [10/79]	0.1980(0.2631)	0.0004(0.0646)	1.496(1.428)	57.81(56.39)\
[2023-09-14 07:01:59 10splitTasks](trainer.py 286): INFO [20/79]	0.1956(0.2316)	0.0004(0.0341)	1.505(1.448)	54.69(55.88)\
[2023-09-14 07:02:01 10splitTasks](trainer.py 286): INFO [30/79]	0.1987(0.2207)	0.0004(0.0233)	1.491(1.434)	56.25(56.25)\
[2023-09-14 07:02:03 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2150)	0.0004(0.0177)	1.395(1.448)	65.62(56.44)\
[2023-09-14 07:02:05 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2113)	0.0003(0.0143)	1.330(1.467)	62.50(55.91)\
[2023-09-14 07:02:07 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2091)	0.0003(0.0120)	1.411(1.469)	57.81(56.02)\
[2023-09-14 07:02:09 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2075)	0.0008(0.0104)	1.539(1.474)	51.56(55.63)\
[2023-09-14 07:02:10 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2046)	0.0002(0.0094)	2.175(1.471)	25.00(55.76)\
[2023-09-14 07:02:10 10splitTasks](trainer.py 288): INFO  * Train Acc 55.760\
[2023-09-14 07:02:12 10splitTasks](trainer.py 147): INFO  * Val Acc 53.200, Total time 1.63\
[2023-09-14 07:02:12 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:02:12 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:02:12 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:02:13 10splitTasks](trainer.py 286): INFO [0/79]	0.9592(0.9592)	0.7540(0.7540)	1.273(1.273)	56.25(56.25)\
[2023-09-14 07:02:15 10splitTasks](trainer.py 286): INFO [10/79]	0.1957(0.2655)	0.0003(0.0690)	1.826(1.472)	43.75(56.53)\
[2023-09-14 07:02:17 10splitTasks](trainer.py 286): INFO [20/79]	0.1959(0.2330)	0.0003(0.0364)	1.343(1.482)	64.06(56.70)\
[2023-09-14 07:02:19 10splitTasks](trainer.py 286): INFO [30/79]	0.2002(0.2212)	0.0006(0.0248)	1.461(1.484)	54.69(57.16)\
[2023-09-14 07:02:21 10splitTasks](trainer.py 286): INFO [40/79]	0.1967(0.2152)	0.0003(0.0188)	1.881(1.473)	45.31(57.32)\
[2023-09-14 07:02:23 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2117)	0.0003(0.0153)	1.469(1.478)	57.81(57.08)\
[2023-09-14 07:02:25 10splitTasks](trainer.py 286): INFO [60/79]	0.1968(0.2094)	0.0003(0.0128)	1.498(1.487)	54.69(56.81)\
[2023-09-14 07:02:27 10splitTasks](trainer.py 286): INFO [70/79]	0.1983(0.2077)	0.0008(0.0111)	1.088(1.479)	71.88(57.15)\
[2023-09-14 07:02:28 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2048)	0.0001(0.0100)	1.726(1.472)	75.00(57.52)\
[2023-09-14 07:02:28 10splitTasks](trainer.py 288): INFO  * Train Acc 57.520\
[2023-09-14 07:02:30 10splitTasks](trainer.py 147): INFO  * Val Acc 55.400, Total time 1.61\
[2023-09-14 07:02:30 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:02:30 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:02:30 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:02:31 10splitTasks](trainer.py 286): INFO [0/79]	0.9131(0.9131)	0.7147(0.7147)	1.332(1.332)	68.75(68.75)\
[2023-09-14 07:02:33 10splitTasks](trainer.py 286): INFO [10/79]	0.1974(0.2630)	0.0005(0.0654)	1.755(1.385)	53.12(64.49)\
[2023-09-14 07:02:35 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2316)	0.0003(0.0345)	1.517(1.399)	56.25(62.35)\
[2023-09-14 07:02:37 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2203)	0.0003(0.0235)	1.533(1.444)	51.56(59.93)\
[2023-09-14 07:02:39 10splitTasks](trainer.py 286): INFO [40/79]	0.1956(0.2151)	0.0003(0.0179)	1.278(1.431)	67.19(60.37)\
[2023-09-14 07:02:41 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2115)	0.0003(0.0145)	1.676(1.448)	46.88(59.90)\
[2023-09-14 07:02:43 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2091)	0.0003(0.0122)	1.456(1.454)	59.38(59.43)\
[2023-09-14 07:02:45 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2073)	0.0009(0.0105)	1.466(1.452)	56.25(59.35)\
[2023-09-14 07:02:46 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2045)	0.0001(0.0095)	1.258(1.455)	62.50(59.22)\
[2023-09-14 07:02:46 10splitTasks](trainer.py 288): INFO  * Train Acc 59.220\
[2023-09-14 07:02:48 10splitTasks](trainer.py 147): INFO  * Val Acc 54.400, Total time 1.70\
[2023-09-14 07:02:48 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:02:48 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:02:48 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:02:49 10splitTasks](trainer.py 286): INFO [0/79]	0.9809(0.9809)	0.7810(0.7810)	1.413(1.413)	62.50(62.50)\
[2023-09-14 07:02:51 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2685)	0.0003(0.0714)	1.598(1.401)	48.44(60.80)\
[2023-09-14 07:02:53 10splitTasks](trainer.py 286): INFO [20/79]	0.1977(0.2357)	0.0004(0.0378)	1.457(1.415)	60.94(60.27)\
[2023-09-14 07:02:55 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2233)	0.0003(0.0258)	1.510(1.408)	57.81(60.99)\
[2023-09-14 07:02:57 10splitTasks](trainer.py 286): INFO [40/79]	0.1965(0.2168)	0.0004(0.0196)	1.425(1.419)	57.81(60.37)\
[2023-09-14 07:02:59 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2132)	0.0003(0.0159)	1.431(1.432)	54.69(59.93)\
[2023-09-14 07:03:01 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2105)	0.0003(0.0133)	1.397(1.430)	64.06(59.61)\
[2023-09-14 07:03:03 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2087)	0.0010(0.0115)	1.343(1.439)	62.50(59.42)\
[2023-09-14 07:03:04 10splitTasks](trainer.py 286): INFO [78/79]	0.0715(0.2058)	0.0003(0.0104)	1.731(1.446)	62.50(59.32)\
[2023-09-14 07:03:04 10splitTasks](trainer.py 288): INFO  * Train Acc 59.320\
[2023-09-14 07:03:06 10splitTasks](trainer.py 147): INFO  * Val Acc 56.200, Total time 1.57\
[2023-09-14 07:03:06 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:03:06 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:03:06 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:03:07 10splitTasks](trainer.py 286): INFO [0/79]	1.1267(1.1267)	0.9288(0.9288)	1.442(1.442)	59.38(59.38)\
[2023-09-14 07:03:09 10splitTasks](trainer.py 286): INFO [10/79]	0.1991(0.2835)	0.0004(0.0849)	1.350(1.392)	65.62(63.64)\
[2023-09-14 07:03:11 10splitTasks](trainer.py 286): INFO [20/79]	0.1953(0.2418)	0.0003(0.0447)	1.356(1.375)	60.94(63.39)\
[2023-09-14 07:03:13 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2272)	0.0004(0.0304)	1.381(1.388)	59.38(61.90)\
[2023-09-14 07:03:15 10splitTasks](trainer.py 286): INFO [40/79]	0.1956(0.2202)	0.0004(0.0232)	1.524(1.420)	60.94(60.98)\
[2023-09-14 07:03:17 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2158)	0.0003(0.0187)	1.448(1.430)	59.38(60.85)\
[2023-09-14 07:03:19 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2129)	0.0004(0.0157)	1.412(1.443)	62.50(60.17)\
[2023-09-14 07:03:21 10splitTasks](trainer.py 286): INFO [70/79]	0.2005(0.2108)	0.0009(0.0137)	1.427(1.446)	62.50(60.17)\
[2023-09-14 07:03:22 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2077)	0.0002(0.0123)	2.409(1.448)	37.50(60.20)\
[2023-09-14 07:03:22 10splitTasks](trainer.py 288): INFO  * Train Acc 60.200\
[2023-09-14 07:03:24 10splitTasks](trainer.py 147): INFO  * Val Acc 54.000, Total time 1.58\
[2023-09-14 07:03:24 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:03:24 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:03:24 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:03:25 10splitTasks](trainer.py 286): INFO [0/79]	1.0056(1.0056)	0.8055(0.8055)	1.584(1.584)	50.00(50.00)\
[2023-09-14 07:03:27 10splitTasks](trainer.py 286): INFO [10/79]	0.1950(0.2701)	0.0003(0.0737)	1.612(1.534)	51.56(57.39)\
[2023-09-14 07:03:29 10splitTasks](trainer.py 286): INFO [20/79]	0.1969(0.2350)	0.0003(0.0388)	1.226(1.530)	75.00(56.85)\
[2023-09-14 07:03:31 10splitTasks](trainer.py 286): INFO [30/79]	0.1975(0.2232)	0.0004(0.0264)	1.556(1.502)	53.12(58.01)\
[2023-09-14 07:03:33 10splitTasks](trainer.py 286): INFO [40/79]	0.1984(0.2169)	0.0004(0.0201)	1.457(1.496)	64.06(58.31)\
[2023-09-14 07:03:35 10splitTasks](trainer.py 286): INFO [50/79]	0.1957(0.2130)	0.0004(0.0163)	1.510(1.488)	51.56(58.70)\
[2023-09-14 07:03:37 10splitTasks](trainer.py 286): INFO [60/79]	0.1979(0.2105)	0.0004(0.0137)	1.388(1.484)	65.62(59.07)\
[2023-09-14 07:03:39 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2086)	0.0017(0.0118)	1.398(1.478)	67.19(59.46)\
[2023-09-14 07:03:40 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2057)	0.0002(0.0107)	2.513(1.474)	12.50(59.48)\
[2023-09-14 07:03:40 10splitTasks](trainer.py 288): INFO  * Train Acc 59.480\
[2023-09-14 07:03:42 10splitTasks](trainer.py 147): INFO  * Val Acc 58.600, Total time 1.66\
[2023-09-14 07:03:42 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:03:42 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:03:42 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:03:43 10splitTasks](trainer.py 286): INFO [0/79]	1.0628(1.0628)	0.8623(0.8623)	1.615(1.615)	60.94(60.94)\
[2023-09-14 07:03:45 10splitTasks](trainer.py 286): INFO [10/79]	0.1972(0.2787)	0.0004(0.0788)	1.578(1.530)	60.94(59.66)\
[2023-09-14 07:03:47 10splitTasks](trainer.py 286): INFO [20/79]	0.1981(0.2397)	0.0004(0.0415)	1.324(1.510)	65.62(59.90)\
[2023-09-14 07:03:49 10splitTasks](trainer.py 286): INFO [30/79]	0.1971(0.2259)	0.0003(0.0283)	1.498(1.494)	59.38(60.48)\
[2023-09-14 07:03:51 10splitTasks](trainer.py 286): INFO [40/79]	0.1965(0.2192)	0.0003(0.0215)	1.512(1.474)	53.12(60.90)\
[2023-09-14 07:03:53 10splitTasks](trainer.py 286): INFO [50/79]	0.1975(0.2149)	0.0004(0.0174)	1.373(1.460)	65.62(61.61)\
[2023-09-14 07:03:55 10splitTasks](trainer.py 286): INFO [60/79]	0.1976(0.2119)	0.0004(0.0146)	1.301(1.457)	67.19(61.42)\
[2023-09-14 07:03:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2101)	0.0010(0.0126)	1.427(1.461)	59.38(60.98)\
[2023-09-14 07:03:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2070)	0.0001(0.0114)	2.590(1.461)	25.00(60.92)\
[2023-09-14 07:03:58 10splitTasks](trainer.py 288): INFO  * Train Acc 60.920\
[2023-09-14 07:04:00 10splitTasks](trainer.py 147): INFO  * Val Acc 52.600, Total time 1.64\
[2023-09-14 07:04:00 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:04:00 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:04:00 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:04:01 10splitTasks](trainer.py 286): INFO [0/79]	0.9335(0.9335)	0.7275(0.7275)	1.290(1.290)	68.75(68.75)\
[2023-09-14 07:04:03 10splitTasks](trainer.py 286): INFO [10/79]	0.1967(0.2645)	0.0005(0.0666)	1.444(1.436)	56.25(62.50)\
[2023-09-14 07:04:05 10splitTasks](trainer.py 286): INFO [20/79]	0.1968(0.2325)	0.0004(0.0351)	1.491(1.480)	62.50(61.01)\
[2023-09-14 07:04:07 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2209)	0.0003(0.0239)	1.610(1.493)	48.44(60.08)\
[2023-09-14 07:04:09 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2152)	0.0003(0.0182)	1.807(1.501)	51.56(59.64)\
[2023-09-14 07:04:11 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2115)	0.0003(0.0147)	1.367(1.501)	62.50(59.80)\
[2023-09-14 07:04:13 10splitTasks](trainer.py 286): INFO [60/79]	0.2066(0.2097)	0.0004(0.0123)	1.536(1.494)	54.69(60.04)\
[2023-09-14 07:04:15 10splitTasks](trainer.py 286): INFO [70/79]	0.1980(0.2082)	0.0011(0.0107)	1.192(1.494)	70.31(59.95)\
[2023-09-14 07:04:16 10splitTasks](trainer.py 286): INFO [78/79]	0.0713(0.2053)	0.0002(0.0096)	2.062(1.496)	37.50(60.06)\
[2023-09-14 07:04:16 10splitTasks](trainer.py 288): INFO  * Train Acc 60.060\
[2023-09-14 07:04:18 10splitTasks](trainer.py 147): INFO  * Val Acc 54.400, Total time 1.62\
[2023-09-14 07:04:18 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:04:18 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:04:18 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:04:19 10splitTasks](trainer.py 286): INFO [0/79]	0.9132(0.9132)	0.6966(0.6966)	1.386(1.386)	57.81(57.81)\
[2023-09-14 07:04:21 10splitTasks](trainer.py 286): INFO [10/79]	0.1958(0.2635)	0.0002(0.0638)	1.417(1.403)	65.62(63.07)\
[2023-09-14 07:04:23 10splitTasks](trainer.py 286): INFO [20/79]	0.2030(0.2322)	0.0006(0.0336)	1.380(1.428)	64.06(62.80)\
[2023-09-14 07:04:25 10splitTasks](trainer.py 286): INFO [30/79]	0.1971(0.2212)	0.0003(0.0230)	1.599(1.443)	53.12(61.90)\
[2023-09-14 07:04:27 10splitTasks](trainer.py 286): INFO [40/79]	0.1977(0.2158)	0.0005(0.0176)	1.464(1.453)	60.94(61.39)\
[2023-09-14 07:04:29 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2122)	0.0004(0.0142)	1.559(1.465)	57.81(61.40)\
[2023-09-14 07:04:31 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2099)	0.0003(0.0120)	1.437(1.471)	51.56(61.09)\
[2023-09-14 07:04:33 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2082)	0.0010(0.0104)	1.557(1.471)	54.69(60.87)\
[2023-09-14 07:04:34 10splitTasks](trainer.py 286): INFO [78/79]	0.0706(0.2054)	0.0001(0.0093)	1.302(1.457)	75.00(61.22)\
[2023-09-14 07:04:34 10splitTasks](trainer.py 288): INFO  * Train Acc 61.220\
[2023-09-14 07:04:36 10splitTasks](trainer.py 147): INFO  * Val Acc 47.800, Total time 1.61\
[2023-09-14 07:04:36 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:04:36 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:04:36 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:04:37 10splitTasks](trainer.py 286): INFO [0/79]	1.0164(1.0164)	0.8179(0.8179)	1.341(1.341)	65.62(65.62)\
[2023-09-14 07:04:39 10splitTasks](trainer.py 286): INFO [10/79]	0.1973(0.2714)	0.0003(0.0747)	1.474(1.451)	60.94(61.51)\
[2023-09-14 07:04:41 10splitTasks](trainer.py 286): INFO [20/79]	0.1960(0.2358)	0.0003(0.0393)	1.283(1.464)	70.31(61.46)\
[2023-09-14 07:04:43 10splitTasks](trainer.py 286): INFO [30/79]	0.1984(0.2232)	0.0006(0.0268)	1.645(1.454)	56.25(61.59)\
[2023-09-14 07:04:45 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2170)	0.0003(0.0204)	1.777(1.466)	56.25(61.66)\
[2023-09-14 07:04:47 10splitTasks](trainer.py 286): INFO [50/79]	0.1979(0.2132)	0.0004(0.0165)	1.461(1.458)	60.94(61.34)\
[2023-09-14 07:04:48 10splitTasks](trainer.py 286): INFO [60/79]	0.1981(0.2105)	0.0006(0.0139)	1.556(1.459)	53.12(61.37)\
[2023-09-14 07:04:50 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2085)	0.0011(0.0120)	1.360(1.455)	64.06(61.44)\
[2023-09-14 07:04:52 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2056)	0.0001(0.0108)	1.617(1.443)	62.50(62.00)\
[2023-09-14 07:04:52 10splitTasks](trainer.py 288): INFO  * Train Acc 62.000\
[2023-09-14 07:04:54 10splitTasks](trainer.py 147): INFO  * Val Acc 55.800, Total time 1.61\
[2023-09-14 07:04:54 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:04:54 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:04:54 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:04:55 10splitTasks](trainer.py 286): INFO [0/79]	0.9488(0.9488)	0.7504(0.7504)	1.079(1.079)	81.25(81.25)\
[2023-09-14 07:04:56 10splitTasks](trainer.py 286): INFO [10/79]	0.1966(0.2660)	0.0004(0.0686)	1.371(1.364)	60.94(65.48)\
[2023-09-14 07:04:58 10splitTasks](trainer.py 286): INFO [20/79]	0.1973(0.2327)	0.0006(0.0362)	1.586(1.374)	57.81(64.96)\
[2023-09-14 07:05:00 10splitTasks](trainer.py 286): INFO [30/79]	0.1964(0.2211)	0.0004(0.0247)	1.398(1.374)	65.62(64.87)\
[2023-09-14 07:05:02 10splitTasks](trainer.py 286): INFO [40/79]	0.1954(0.2151)	0.0003(0.0188)	1.268(1.387)	70.31(64.56)\
[2023-09-14 07:05:04 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2115)	0.0004(0.0152)	1.600(1.403)	59.38(64.00)\
[2023-09-14 07:05:06 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2091)	0.0004(0.0128)	1.353(1.406)	75.00(63.96)\
[2023-09-14 07:05:08 10splitTasks](trainer.py 286): INFO [70/79]	0.2013(0.2075)	0.0010(0.0110)	1.509(1.426)	59.38(63.31)\
[2023-09-14 07:05:10 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2046)	0.0001(0.0099)	2.278(1.439)	37.50(62.72)\
[2023-09-14 07:05:10 10splitTasks](trainer.py 288): INFO  * Train Acc 62.720\
[2023-09-14 07:05:11 10splitTasks](trainer.py 147): INFO  * Val Acc 54.200, Total time 1.57\
[2023-09-14 07:05:11 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:05:11 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:05:11 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:05:12 10splitTasks](trainer.py 286): INFO [0/79]	0.9227(0.9227)	0.7206(0.7206)	1.311(1.311)	70.31(70.31)\
[2023-09-14 07:05:14 10splitTasks](trainer.py 286): INFO [10/79]	0.1949(0.2626)	0.0003(0.0665)	1.622(1.461)	62.50(62.22)\
[2023-09-14 07:05:16 10splitTasks](trainer.py 286): INFO [20/79]	0.1948(0.2318)	0.0002(0.0353)	1.505(1.514)	67.19(59.97)\
[2023-09-14 07:05:18 10splitTasks](trainer.py 286): INFO [30/79]	0.1958(0.2213)	0.0003(0.0241)	1.372(1.481)	62.50(62.20)\
[2023-09-14 07:05:20 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2153)	0.0003(0.0183)	1.464(1.459)	64.06(63.19)\
[2023-09-14 07:05:22 10splitTasks](trainer.py 286): INFO [50/79]	0.1957(0.2121)	0.0003(0.0149)	1.543(1.446)	57.81(63.20)\
[2023-09-14 07:05:24 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2102)	0.0004(0.0125)	1.673(1.456)	56.25(62.53)\
[2023-09-14 07:05:26 10splitTasks](trainer.py 286): INFO [70/79]	0.1989(0.2085)	0.0010(0.0109)	1.409(1.457)	60.94(62.63)\
[2023-09-14 07:05:28 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2055)	0.0001(0.0098)	1.161(1.465)	75.00(62.18)\
[2023-09-14 07:05:28 10splitTasks](trainer.py 288): INFO  * Train Acc 62.180\
[2023-09-14 07:05:29 10splitTasks](trainer.py 147): INFO  * Val Acc 51.400, Total time 1.60\
[2023-09-14 07:05:29 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:05:29 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:05:29 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:05:30 10splitTasks](trainer.py 286): INFO [0/79]	1.0596(1.0596)	0.8613(0.8613)	1.744(1.744)	57.81(57.81)\
[2023-09-14 07:05:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1958(0.2756)	0.0003(0.0787)	1.618(1.516)	54.69(60.23)\
[2023-09-14 07:05:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2380)	0.0002(0.0415)	1.292(1.466)	64.06(61.38)\
[2023-09-14 07:05:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2249)	0.0003(0.0283)	1.466(1.455)	60.94(61.90)\
[2023-09-14 07:05:38 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2185)	0.0004(0.0216)	1.309(1.443)	65.62(62.54)\
[2023-09-14 07:05:40 10splitTasks](trainer.py 286): INFO [50/79]	0.2007(0.2149)	0.0004(0.0175)	1.242(1.453)	65.62(62.50)\
[2023-09-14 07:05:42 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2120)	0.0003(0.0147)	1.413(1.445)	65.62(62.86)\
[2023-09-14 07:05:44 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2099)	0.0009(0.0127)	1.509(1.441)	65.62(63.07)\
[2023-09-14 07:05:46 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2068)	0.0001(0.0114)	1.916(1.442)	50.00(63.06)\
[2023-09-14 07:05:46 10splitTasks](trainer.py 288): INFO  * Train Acc 63.060\
[2023-09-14 07:05:47 10splitTasks](trainer.py 147): INFO  * Val Acc 53.200, Total time 1.59\
[2023-09-14 07:05:47 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:05:47 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:05:47 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:05:48 10splitTasks](trainer.py 286): INFO [0/79]	0.9122(0.9122)	0.7117(0.7117)	1.303(1.303)	70.31(70.31)\
[2023-09-14 07:05:50 10splitTasks](trainer.py 286): INFO [10/79]	0.2001(0.2617)	0.0004(0.0651)	1.621(1.516)	57.81(61.22)\
[2023-09-14 07:05:52 10splitTasks](trainer.py 286): INFO [20/79]	0.1954(0.2308)	0.0003(0.0343)	1.323(1.479)	60.94(61.61)\
[2023-09-14 07:05:54 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2198)	0.0004(0.0234)	1.773(1.494)	59.38(62.05)\
[2023-09-14 07:05:56 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2148)	0.0007(0.0178)	1.467(1.500)	59.38(61.97)\
[2023-09-14 07:05:58 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2118)	0.0004(0.0144)	1.315(1.480)	65.62(62.62)\
[2023-09-14 07:06:00 10splitTasks](trainer.py 286): INFO [60/79]	0.1976(0.2095)	0.0003(0.0122)	1.373(1.484)	62.50(62.22)\
[2023-09-14 07:06:02 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2076)	0.0020(0.0106)	1.307(1.476)	67.19(62.48)\
[2023-09-14 07:06:03 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2048)	0.0001(0.0095)	1.914(1.470)	50.00(62.70)\
[2023-09-14 07:06:04 10splitTasks](trainer.py 288): INFO  * Train Acc 62.700\
[2023-09-14 07:06:05 10splitTasks](trainer.py 147): INFO  * Val Acc 51.000, Total time 1.58\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-2.pth\
=> Save Done\
[2023-09-14 07:06:05 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:06:07 10splitTasks](trainer.py 147): INFO  * Val Acc 39.600, Total time 1.47\
[2023-09-14 07:06:07 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:06:08 10splitTasks](trainer.py 147): INFO  * Val Acc 39.800, Total time 1.58\
[2023-09-14 07:06:08 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:06:10 10splitTasks](trainer.py 147): INFO  * Val Acc 51.000, Total time 1.67\
[2023-09-14 07:06:10 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:06:10 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:06:10 10splitTasks](iBatchLearn.py 155): INFO Acc:43.46666682179768; BWT:-19.800000011444094;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:06:15 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:06:15 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:06:15 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 2, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-2.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_2.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:06:15 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-2.pth\
[2023-09-14 07:06:15 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:06:17 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:06:17 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:06:17 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:06:21 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:06:24 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
--------------------------------Official Evaluation--------------------------------\
2 25.53333333333333\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:06:33 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:06:33 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:06:33 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 3, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-2.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-3.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-2.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-3.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:06:33 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-2.pth\
[2023-09-14 07:06:34 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:06:35 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:06:35 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:06:35 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:06:35 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:06:35 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:06:35 10splitTasks](iBatchLearn.py 92): INFO ====================== 3 =======================\
[2023-09-14 07:06:35 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:06:35 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:06:35 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:06:35 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:06:35 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:06:39 10splitTasks](trainer.py 286): INFO [0/79]	3.3108(3.3108)	0.7807(0.7807)	2.320(2.320)	14.06(14.06)\
[2023-09-14 07:06:41 10splitTasks](trainer.py 286): INFO [10/79]	0.1969(0.4817)	0.0004(0.0714)	2.091(2.240)	29.69(18.47)\
[2023-09-14 07:06:43 10splitTasks](trainer.py 286): INFO [20/79]	0.1975(0.3473)	0.0004(0.0377)	1.894(2.087)	29.69(25.22)\
[2023-09-14 07:06:45 10splitTasks](trainer.py 286): INFO [30/79]	0.1980(0.2993)	0.0004(0.0257)	1.693(1.998)	45.31(28.88)\
[2023-09-14 07:06:47 10splitTasks](trainer.py 286): INFO [40/79]	0.1976(0.2745)	0.0003(0.0195)	1.554(1.931)	43.75(31.29)\
[2023-09-14 07:06:49 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2594)	0.0004(0.0158)	1.711(1.890)	40.62(33.30)\
[2023-09-14 07:06:51 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2491)	0.0004(0.0133)	1.563(1.850)	43.75(34.94)\
[2023-09-14 07:06:53 10splitTasks](trainer.py 286): INFO [70/79]	0.1974(0.2417)	0.0010(0.0115)	1.138(1.811)	62.50(36.49)\
[2023-09-14 07:06:54 10splitTasks](trainer.py 286): INFO [78/79]	0.2215(0.2373)	0.0001(0.0103)	1.517(1.785)	37.50(37.46)\
[2023-09-14 07:06:54 10splitTasks](trainer.py 288): INFO  * Train Acc 37.460\
[2023-09-14 07:06:56 10splitTasks](trainer.py 147): INFO  * Val Acc 46.000, Total time 1.91\
[2023-09-14 07:06:56 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:06:56 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:06:56 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:06:57 10splitTasks](trainer.py 286): INFO [0/79]	0.9656(0.9656)	0.7665(0.7665)	1.649(1.649)	48.44(48.44)\
[2023-09-14 07:06:59 10splitTasks](trainer.py 286): INFO [10/79]	0.1958(0.2723)	0.0003(0.0701)	1.325(1.495)	53.12(49.29)\
[2023-09-14 07:07:01 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2387)	0.0003(0.0370)	1.606(1.519)	46.88(49.26)\
[2023-09-14 07:07:03 10splitTasks](trainer.py 286): INFO [30/79]	0.1958(0.2255)	0.0003(0.0252)	1.532(1.525)	46.88(48.74)\
[2023-09-14 07:07:05 10splitTasks](trainer.py 286): INFO [40/79]	0.1995(0.2191)	0.0007(0.0192)	1.413(1.517)	51.56(48.67)\
[2023-09-14 07:07:07 10splitTasks](trainer.py 286): INFO [50/79]	0.1954(0.2145)	0.0003(0.0156)	1.249(1.512)	56.25(48.90)\
[2023-09-14 07:07:09 10splitTasks](trainer.py 286): INFO [60/79]	0.1952(0.2115)	0.0004(0.0131)	1.352(1.509)	54.69(49.49)\
[2023-09-14 07:07:11 10splitTasks](trainer.py 286): INFO [70/79]	0.2056(0.2098)	0.0056(0.0114)	1.682(1.499)	50.00(49.89)\
[2023-09-14 07:07:12 10splitTasks](trainer.py 286): INFO [78/79]	0.0706(0.2067)	0.0002(0.0103)	1.101(1.494)	75.00(49.90)\
[2023-09-14 07:07:13 10splitTasks](trainer.py 288): INFO  * Train Acc 49.900\
[2023-09-14 07:07:14 10splitTasks](trainer.py 147): INFO  * Val Acc 49.600, Total time 1.77\
[2023-09-14 07:07:14 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:07:14 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:07:14 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:07:15 10splitTasks](trainer.py 286): INFO [0/79]	0.9784(0.9784)	0.7799(0.7799)	1.346(1.346)	60.94(60.94)\
[2023-09-14 07:07:18 10splitTasks](trainer.py 286): INFO [10/79]	0.1947(0.2971)	0.0004(0.1011)	1.425(1.518)	46.88(51.99)\
[2023-09-14 07:07:20 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2497)	0.0003(0.0535)	1.397(1.497)	51.56(52.68)\
[2023-09-14 07:07:21 10splitTasks](trainer.py 286): INFO [30/79]	0.1958(0.2324)	0.0004(0.0364)	1.555(1.476)	56.25(53.83)\
[2023-09-14 07:07:23 10splitTasks](trainer.py 286): INFO [40/79]	0.1957(0.2235)	0.0003(0.0276)	1.588(1.497)	45.31(52.97)\
[2023-09-14 07:07:25 10splitTasks](trainer.py 286): INFO [50/79]	0.1958(0.2181)	0.0004(0.0223)	1.483(1.493)	51.56(52.94)\
[2023-09-14 07:07:27 10splitTasks](trainer.py 286): INFO [60/79]	0.2056(0.2148)	0.0006(0.0187)	1.664(1.489)	45.31(52.54)\
[2023-09-14 07:07:29 10splitTasks](trainer.py 286): INFO [70/79]	0.1966(0.2125)	0.0010(0.0162)	1.499(1.477)	48.44(53.12)\
[2023-09-14 07:07:31 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2091)	0.0001(0.0145)	1.335(1.482)	62.50(53.02)\
[2023-09-14 07:07:31 10splitTasks](trainer.py 288): INFO  * Train Acc 53.020\
[2023-09-14 07:07:33 10splitTasks](trainer.py 147): INFO  * Val Acc 52.800, Total time 1.78\
[2023-09-14 07:07:33 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:07:33 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:07:33 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:07:34 10splitTasks](trainer.py 286): INFO [0/79]	1.1258(1.1258)	0.9279(0.9279)	1.354(1.354)	53.12(53.12)\
[2023-09-14 07:07:36 10splitTasks](trainer.py 286): INFO [10/79]	0.1956(0.2815)	0.0003(0.0848)	1.142(1.416)	60.94(55.68)\
[2023-09-14 07:07:38 10splitTasks](trainer.py 286): INFO [20/79]	0.1984(0.2412)	0.0008(0.0447)	1.294(1.388)	59.38(56.55)\
[2023-09-14 07:07:40 10splitTasks](trainer.py 286): INFO [30/79]	0.1985(0.2270)	0.0003(0.0304)	1.439(1.441)	60.94(54.99)\
[2023-09-14 07:07:42 10splitTasks](trainer.py 286): INFO [40/79]	0.1960(0.2203)	0.0003(0.0231)	1.397(1.441)	50.00(54.88)\
[2023-09-14 07:07:44 10splitTasks](trainer.py 286): INFO [50/79]	0.1947(0.2156)	0.0003(0.0187)	1.398(1.443)	51.56(54.41)\
[2023-09-14 07:07:46 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2125)	0.0004(0.0157)	1.427(1.437)	56.25(54.82)\
[2023-09-14 07:07:48 10splitTasks](trainer.py 286): INFO [70/79]	0.1963(0.2102)	0.0010(0.0136)	1.513(1.440)	56.25(55.00)\
[2023-09-14 07:07:49 10splitTasks](trainer.py 286): INFO [78/79]	0.0716(0.2071)	0.0001(0.0122)	1.624(1.447)	50.00(54.68)\
[2023-09-14 07:07:49 10splitTasks](trainer.py 288): INFO  * Train Acc 54.680\
[2023-09-14 07:07:51 10splitTasks](trainer.py 147): INFO  * Val Acc 51.800, Total time 1.78\
[2023-09-14 07:07:51 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:07:51 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:07:51 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:07:52 10splitTasks](trainer.py 286): INFO [0/79]	1.0240(1.0240)	0.8259(0.8259)	1.222(1.222)	67.19(67.19)\
[2023-09-14 07:07:54 10splitTasks](trainer.py 286): INFO [10/79]	0.1956(0.2752)	0.0003(0.0755)	1.501(1.342)	57.81(60.51)\
[2023-09-14 07:07:56 10splitTasks](trainer.py 286): INFO [20/79]	0.1956(0.2374)	0.0003(0.0398)	1.414(1.363)	51.56(58.48)\
[2023-09-14 07:07:58 10splitTasks](trainer.py 286): INFO [30/79]	0.1949(0.2240)	0.0003(0.0271)	1.310(1.404)	60.94(57.21)\
[2023-09-14 07:08:00 10splitTasks](trainer.py 286): INFO [40/79]	0.1956(0.2175)	0.0003(0.0206)	1.380(1.448)	51.56(55.83)\
[2023-09-14 07:08:02 10splitTasks](trainer.py 286): INFO [50/79]	0.1974(0.2134)	0.0003(0.0166)	1.479(1.456)	60.94(55.73)\
[2023-09-14 07:08:04 10splitTasks](trainer.py 286): INFO [60/79]	0.1957(0.2110)	0.0004(0.0140)	1.455(1.451)	54.69(55.69)\
[2023-09-14 07:08:06 10splitTasks](trainer.py 286): INFO [70/79]	0.1959(0.2088)	0.0010(0.0121)	1.360(1.463)	62.50(55.19)\
[2023-09-14 07:08:07 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2058)	0.0001(0.0109)	2.507(1.468)	37.50(55.18)\
[2023-09-14 07:08:07 10splitTasks](trainer.py 288): INFO  * Train Acc 55.180\
[2023-09-14 07:08:09 10splitTasks](trainer.py 147): INFO  * Val Acc 51.000, Total time 1.78\
[2023-09-14 07:08:09 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:08:09 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:08:09 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:08:10 10splitTasks](trainer.py 286): INFO [0/79]	0.9822(0.9822)	0.7757(0.7757)	1.461(1.461)	51.56(51.56)\
[2023-09-14 07:08:12 10splitTasks](trainer.py 286): INFO [10/79]	0.1979(0.2713)	0.0004(0.0714)	1.507(1.460)	54.69(55.68)\
[2023-09-14 07:08:14 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2357)	0.0004(0.0376)	1.650(1.433)	43.75(57.37)\
[2023-09-14 07:08:16 10splitTasks](trainer.py 286): INFO [30/79]	0.1954(0.2233)	0.0004(0.0256)	1.488(1.422)	53.12(57.91)\
[2023-09-14 07:08:18 10splitTasks](trainer.py 286): INFO [40/79]	0.2000(0.2171)	0.0006(0.0195)	1.343(1.433)	62.50(57.89)\
[2023-09-14 07:08:20 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2135)	0.0004(0.0158)	1.417(1.442)	57.81(57.69)\
[2023-09-14 07:08:22 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2109)	0.0003(0.0133)	1.575(1.452)	54.69(57.43)\
[2023-09-14 07:08:24 10splitTasks](trainer.py 286): INFO [70/79]	0.1972(0.2090)	0.0010(0.0115)	1.407(1.444)	60.94(57.66)\
[2023-09-14 07:08:25 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2060)	0.0001(0.0103)	1.550(1.440)	50.00(57.76)\
[2023-09-14 07:08:25 10splitTasks](trainer.py 288): INFO  * Train Acc 57.760\
[2023-09-14 07:08:27 10splitTasks](trainer.py 147): INFO  * Val Acc 46.600, Total time 1.82\
[2023-09-14 07:08:27 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:08:27 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:08:27 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:08:28 10splitTasks](trainer.py 286): INFO [0/79]	0.9601(0.9601)	0.7603(0.7603)	1.240(1.240)	64.06(64.06)\
[2023-09-14 07:08:30 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2660)	0.0003(0.0695)	1.753(1.413)	48.44(61.08)\
[2023-09-14 07:08:32 10splitTasks](trainer.py 286): INFO [20/79]	0.1992(0.2340)	0.0003(0.0367)	1.412(1.409)	54.69(60.27)\
[2023-09-14 07:08:34 10splitTasks](trainer.py 286): INFO [30/79]	0.1978(0.2222)	0.0007(0.0250)	1.251(1.422)	65.62(59.68)\
[2023-09-14 07:08:36 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2158)	0.0004(0.0190)	1.260(1.420)	64.06(59.72)\
[2023-09-14 07:08:38 10splitTasks](trainer.py 286): INFO [50/79]	0.1979(0.2123)	0.0003(0.0154)	1.415(1.411)	59.38(59.96)\
[2023-09-14 07:08:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2098)	0.0003(0.0130)	1.318(1.422)	65.62(59.43)\
[2023-09-14 07:08:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1989(0.2080)	0.0010(0.0112)	1.547(1.427)	59.38(59.07)\
[2023-09-14 07:08:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0726(0.2052)	0.0001(0.0101)	1.592(1.427)	50.00(58.86)\
[2023-09-14 07:08:43 10splitTasks](trainer.py 288): INFO  * Train Acc 58.860\
[2023-09-14 07:08:46 10splitTasks](trainer.py 147): INFO  * Val Acc 44.800, Total time 2.11\
[2023-09-14 07:08:46 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:08:46 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:08:46 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:08:47 10splitTasks](trainer.py 286): INFO [0/79]	1.1216(1.1216)	0.9225(0.9225)	1.435(1.435)	62.50(62.50)\
[2023-09-14 07:08:49 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2808)	0.0003(0.0843)	1.222(1.330)	60.94(64.63)\
[2023-09-14 07:08:51 10splitTasks](trainer.py 286): INFO [20/79]	0.2000(0.2413)	0.0004(0.0444)	1.307(1.387)	65.62(61.31)\
[2023-09-14 07:08:53 10splitTasks](trainer.py 286): INFO [30/79]	0.2020(0.2272)	0.0003(0.0302)	1.230(1.396)	68.75(60.89)\
[2023-09-14 07:08:55 10splitTasks](trainer.py 286): INFO [40/79]	0.1952(0.2201)	0.0003(0.0230)	1.215(1.398)	62.50(60.71)\
[2023-09-14 07:08:57 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2158)	0.0003(0.0186)	1.437(1.391)	62.50(60.81)\
[2023-09-14 07:08:59 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2128)	0.0003(0.0156)	1.288(1.397)	71.88(60.99)\
[2023-09-14 07:09:00 10splitTasks](trainer.py 286): INFO [70/79]	0.1984(0.2107)	0.0009(0.0135)	1.342(1.398)	60.94(61.11)\
[2023-09-14 07:09:02 10splitTasks](trainer.py 286): INFO [78/79]	0.0704(0.2076)	0.0001(0.0122)	1.465(1.400)	37.50(60.98)\
[2023-09-14 07:09:02 10splitTasks](trainer.py 288): INFO  * Train Acc 60.980\
[2023-09-14 07:09:04 10splitTasks](trainer.py 147): INFO  * Val Acc 56.400, Total time 1.82\
[2023-09-14 07:09:04 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:09:04 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:09:04 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:09:05 10splitTasks](trainer.py 286): INFO [0/79]	0.9100(0.9100)	0.6945(0.6945)	1.358(1.358)	57.81(57.81)\
[2023-09-14 07:09:07 10splitTasks](trainer.py 286): INFO [10/79]	0.1972(0.2629)	0.0003(0.0635)	1.265(1.340)	65.62(63.92)\
[2023-09-14 07:09:09 10splitTasks](trainer.py 286): INFO [20/79]	0.1968(0.2313)	0.0006(0.0335)	1.379(1.375)	64.06(63.24)\
[2023-09-14 07:09:11 10splitTasks](trainer.py 286): INFO [30/79]	0.1960(0.2206)	0.0004(0.0229)	1.416(1.363)	64.06(63.31)\
[2023-09-14 07:09:13 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2153)	0.0003(0.0174)	1.125(1.365)	70.31(62.92)\
[2023-09-14 07:09:15 10splitTasks](trainer.py 286): INFO [50/79]	0.1969(0.2126)	0.0003(0.0141)	1.248(1.377)	62.50(62.62)\
[2023-09-14 07:09:17 10splitTasks](trainer.py 286): INFO [60/79]	0.2013(0.2102)	0.0006(0.0119)	1.481(1.388)	65.62(61.76)\
[2023-09-14 07:09:19 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2089)	0.0008(0.0103)	1.330(1.379)	59.38(61.77)\
[2023-09-14 07:09:20 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2059)	0.0001(0.0093)	1.744(1.387)	37.50(61.48)\
[2023-09-14 07:09:20 10splitTasks](trainer.py 288): INFO  * Train Acc 61.480\
[2023-09-14 07:09:22 10splitTasks](trainer.py 147): INFO  * Val Acc 45.600, Total time 1.87\
[2023-09-14 07:09:22 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:09:22 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:09:22 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:09:23 10splitTasks](trainer.py 286): INFO [0/79]	0.9295(0.9295)	0.7312(0.7312)	1.395(1.395)	59.38(59.38)\
[2023-09-14 07:09:25 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2657)	0.0003(0.0669)	1.574(1.369)	56.25(61.22)\
[2023-09-14 07:09:27 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2333)	0.0003(0.0352)	1.309(1.393)	64.06(60.86)\
[2023-09-14 07:09:29 10splitTasks](trainer.py 286): INFO [30/79]	0.1958(0.2221)	0.0003(0.0240)	1.519(1.413)	57.81(60.99)\
[2023-09-14 07:09:31 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2165)	0.0003(0.0183)	1.579(1.413)	62.50(61.43)\
[2023-09-14 07:09:33 10splitTasks](trainer.py 286): INFO [50/79]	0.1958(0.2129)	0.0003(0.0148)	1.464(1.411)	62.50(61.21)\
[2023-09-14 07:09:35 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2106)	0.0003(0.0124)	1.315(1.415)	70.31(61.40)\
[2023-09-14 07:09:37 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2089)	0.0009(0.0108)	1.155(1.401)	73.44(61.86)\
[2023-09-14 07:09:38 10splitTasks](trainer.py 286): INFO [78/79]	0.0754(0.2061)	0.0001(0.0097)	1.716(1.399)	50.00(61.98)\
[2023-09-14 07:09:38 10splitTasks](trainer.py 288): INFO  * Train Acc 61.980\
[2023-09-14 07:09:40 10splitTasks](trainer.py 147): INFO  * Val Acc 49.800, Total time 1.90\
[2023-09-14 07:09:40 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:09:40 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:09:40 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:09:41 10splitTasks](trainer.py 286): INFO [0/79]	0.9014(0.9014)	0.7012(0.7012)	1.423(1.423)	56.25(56.25)\
[2023-09-14 07:09:43 10splitTasks](trainer.py 286): INFO [10/79]	0.2118(0.2650)	0.0006(0.0642)	1.692(1.496)	60.94(59.94)\
[2023-09-14 07:09:45 10splitTasks](trainer.py 286): INFO [20/79]	0.1970(0.2332)	0.0009(0.0339)	1.427(1.422)	65.62(62.05)\
[2023-09-14 07:09:47 10splitTasks](trainer.py 286): INFO [30/79]	0.1957(0.2213)	0.0003(0.0231)	1.276(1.433)	67.19(61.09)\
[2023-09-14 07:09:49 10splitTasks](trainer.py 286): INFO [40/79]	0.1952(0.2153)	0.0003(0.0176)	1.437(1.420)	59.38(61.39)\
[2023-09-14 07:09:51 10splitTasks](trainer.py 286): INFO [50/79]	0.1955(0.2117)	0.0004(0.0142)	1.321(1.415)	64.06(61.67)\
[2023-09-14 07:09:53 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2093)	0.0003(0.0120)	1.183(1.393)	70.31(62.50)\
[2023-09-14 07:09:55 10splitTasks](trainer.py 286): INFO [70/79]	0.2005(0.2078)	0.0011(0.0104)	1.614(1.391)	57.81(62.57)\
[2023-09-14 07:09:56 10splitTasks](trainer.py 286): INFO [78/79]	0.0705(0.2049)	0.0001(0.0093)	2.342(1.402)	62.50(62.12)\
[2023-09-14 07:09:57 10splitTasks](trainer.py 288): INFO  * Train Acc 62.120\
[2023-09-14 07:09:58 10splitTasks](trainer.py 147): INFO  * Val Acc 54.400, Total time 1.77\
[2023-09-14 07:09:58 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:09:58 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:09:58 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:09:59 10splitTasks](trainer.py 286): INFO [0/79]	0.9466(0.9466)	0.7247(0.7247)	1.458(1.458)	56.25(56.25)\
[2023-09-14 07:10:01 10splitTasks](trainer.py 286): INFO [10/79]	0.1972(0.2666)	0.0004(0.0671)	1.562(1.480)	62.50(59.09)\
[2023-09-14 07:10:03 10splitTasks](trainer.py 286): INFO [20/79]	0.1952(0.2331)	0.0003(0.0355)	1.549(1.464)	51.56(59.90)\
[2023-09-14 07:10:05 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2219)	0.0003(0.0242)	1.471(1.470)	62.50(60.13)\
[2023-09-14 07:10:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2157)	0.0003(0.0184)	1.341(1.445)	59.38(60.21)\
[2023-09-14 07:10:09 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2119)	0.0004(0.0149)	1.606(1.431)	51.56(60.32)\
[2023-09-14 07:10:11 10splitTasks](trainer.py 286): INFO [60/79]	0.1981(0.2096)	0.0003(0.0125)	1.494(1.427)	62.50(60.71)\
[2023-09-14 07:10:13 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2078)	0.0010(0.0108)	1.379(1.419)	64.06(60.78)\
[2023-09-14 07:10:15 10splitTasks](trainer.py 286): INFO [78/79]	0.0733(0.2051)	0.0001(0.0098)	1.068(1.415)	62.50(61.10)\
[2023-09-14 07:10:15 10splitTasks](trainer.py 288): INFO  * Train Acc 61.100\
[2023-09-14 07:10:16 10splitTasks](trainer.py 147): INFO  * Val Acc 56.200, Total time 1.77\
[2023-09-14 07:10:16 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:10:16 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:10:16 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:10:17 10splitTasks](trainer.py 286): INFO [0/79]	0.9330(0.9330)	0.7348(0.7348)	1.288(1.288)	65.62(65.62)\
[2023-09-14 07:10:19 10splitTasks](trainer.py 286): INFO [10/79]	0.1957(0.2647)	0.0003(0.0672)	1.254(1.337)	70.31(64.06)\
[2023-09-14 07:10:21 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2324)	0.0003(0.0354)	1.548(1.352)	46.88(63.91)\
[2023-09-14 07:10:23 10splitTasks](trainer.py 286): INFO [30/79]	0.1953(0.2213)	0.0003(0.0242)	1.374(1.385)	70.31(62.90)\
[2023-09-14 07:10:25 10splitTasks](trainer.py 286): INFO [40/79]	0.2069(0.2161)	0.0007(0.0184)	1.556(1.385)	54.69(63.34)\
[2023-09-14 07:10:27 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2126)	0.0004(0.0150)	1.462(1.384)	65.62(63.51)\
[2023-09-14 07:10:29 10splitTasks](trainer.py 286): INFO [60/79]	0.1978(0.2102)	0.0003(0.0127)	1.588(1.390)	56.25(63.14)\
[2023-09-14 07:10:31 10splitTasks](trainer.py 286): INFO [70/79]	0.1982(0.2082)	0.0010(0.0110)	1.387(1.402)	64.06(62.68)\
[2023-09-14 07:10:33 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2053)	0.0001(0.0099)	2.174(1.406)	25.00(62.76)\
[2023-09-14 07:10:33 10splitTasks](trainer.py 288): INFO  * Train Acc 62.760\
[2023-09-14 07:10:34 10splitTasks](trainer.py 147): INFO  * Val Acc 53.600, Total time 1.77\
[2023-09-14 07:10:34 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:10:34 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:10:34 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:10:35 10splitTasks](trainer.py 286): INFO [0/79]	0.9363(0.9363)	0.7333(0.7333)	1.389(1.389)	54.69(54.69)\
[2023-09-14 07:10:37 10splitTasks](trainer.py 286): INFO [10/79]	0.1983(0.2642)	0.0003(0.0671)	1.282(1.380)	64.06(62.93)\
[2023-09-14 07:10:39 10splitTasks](trainer.py 286): INFO [20/79]	0.1967(0.2321)	0.0004(0.0353)	1.371(1.395)	65.62(62.87)\
[2023-09-14 07:10:41 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2208)	0.0003(0.0241)	1.535(1.420)	54.69(62.40)\
[2023-09-14 07:10:43 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2153)	0.0003(0.0183)	1.239(1.435)	62.50(62.42)\
[2023-09-14 07:10:45 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2118)	0.0004(0.0148)	1.649(1.428)	56.25(62.59)\
[2023-09-14 07:10:47 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2096)	0.0004(0.0125)	1.196(1.421)	68.75(62.83)\
[2023-09-14 07:10:49 10splitTasks](trainer.py 286): INFO [70/79]	0.2027(0.2079)	0.0025(0.0108)	1.545(1.414)	59.38(62.98)\
[2023-09-14 07:10:51 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2051)	0.0001(0.0098)	1.818(1.428)	62.50(62.46)\
[2023-09-14 07:10:51 10splitTasks](trainer.py 288): INFO  * Train Acc 62.460\
[2023-09-14 07:10:52 10splitTasks](trainer.py 147): INFO  * Val Acc 51.600, Total time 1.78\
[2023-09-14 07:10:52 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:10:53 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:10:53 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:10:54 10splitTasks](trainer.py 286): INFO [0/79]	1.1274(1.1274)	0.9295(0.9295)	1.416(1.416)	57.81(57.81)\
[2023-09-14 07:10:56 10splitTasks](trainer.py 286): INFO [10/79]	0.1958(0.2851)	0.0003(0.0890)	1.404(1.430)	62.50(62.07)\
[2023-09-14 07:10:58 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2431)	0.0003(0.0470)	1.223(1.409)	70.31(62.87)\
[2023-09-14 07:11:00 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2280)	0.0004(0.0320)	1.253(1.409)	67.19(62.35)\
[2023-09-14 07:11:02 10splitTasks](trainer.py 286): INFO [40/79]	0.1952(0.2203)	0.0003(0.0243)	1.436(1.416)	62.50(62.00)\
[2023-09-14 07:11:04 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2158)	0.0003(0.0196)	1.362(1.417)	64.06(61.86)\
[2023-09-14 07:11:05 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2128)	0.0004(0.0165)	1.568(1.423)	57.81(61.78)\
[2023-09-14 07:11:07 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2108)	0.0011(0.0143)	1.653(1.418)	54.69(61.99)\
[2023-09-14 07:11:09 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2076)	0.0001(0.0129)	1.624(1.409)	50.00(62.30)\
[2023-09-14 07:11:09 10splitTasks](trainer.py 288): INFO  * Train Acc 62.300\
[2023-09-14 07:11:11 10splitTasks](trainer.py 147): INFO  * Val Acc 55.600, Total time 1.80\
[2023-09-14 07:11:11 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:11:11 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:11:11 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:11:12 10splitTasks](trainer.py 286): INFO [0/79]	0.9474(0.9474)	0.7173(0.7173)	1.397(1.397)	64.06(64.06)\
[2023-09-14 07:11:14 10splitTasks](trainer.py 286): INFO [10/79]	0.1997(0.2660)	0.0004(0.0657)	1.383(1.381)	67.19(63.21)\
[2023-09-14 07:11:16 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2336)	0.0004(0.0346)	1.200(1.385)	76.56(63.17)\
[2023-09-14 07:11:18 10splitTasks](trainer.py 286): INFO [30/79]	0.1967(0.2221)	0.0004(0.0236)	1.290(1.409)	68.75(62.25)\
[2023-09-14 07:11:20 10splitTasks](trainer.py 286): INFO [40/79]	0.1971(0.2160)	0.0003(0.0180)	1.384(1.386)	65.62(63.53)\
[2023-09-14 07:11:22 10splitTasks](trainer.py 286): INFO [50/79]	0.1957(0.2123)	0.0004(0.0145)	1.472(1.377)	57.81(63.82)\
[2023-09-14 07:11:24 10splitTasks](trainer.py 286): INFO [60/79]	0.1968(0.2097)	0.0007(0.0122)	1.231(1.388)	76.56(63.55)\
[2023-09-14 07:11:26 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2078)	0.0011(0.0106)	1.543(1.389)	54.69(63.75)\
[2023-09-14 07:11:27 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2050)	0.0001(0.0096)	1.813(1.384)	50.00(63.84)\
[2023-09-14 07:11:27 10splitTasks](trainer.py 288): INFO  * Train Acc 63.840\
[2023-09-14 07:11:29 10splitTasks](trainer.py 147): INFO  * Val Acc 60.000, Total time 1.76\
[2023-09-14 07:11:29 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:11:29 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:11:29 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:11:30 10splitTasks](trainer.py 286): INFO [0/79]	0.9303(0.9303)	0.7276(0.7276)	1.218(1.218)	67.19(67.19)\
[2023-09-14 07:11:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1967(0.2636)	0.0003(0.0666)	1.374(1.384)	64.06(63.64)\
[2023-09-14 07:11:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2317)	0.0003(0.0351)	1.398(1.399)	59.38(62.28)\
[2023-09-14 07:11:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1968(0.2205)	0.0004(0.0239)	1.180(1.386)	75.00(63.56)\
[2023-09-14 07:11:38 10splitTasks](trainer.py 286): INFO [40/79]	0.1985(0.2150)	0.0006(0.0182)	1.339(1.378)	62.50(64.02)\
[2023-09-14 07:11:40 10splitTasks](trainer.py 286): INFO [50/79]	0.1972(0.2115)	0.0003(0.0147)	1.396(1.384)	64.06(64.03)\
[2023-09-14 07:11:42 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2091)	0.0003(0.0124)	1.489(1.389)	62.50(64.09)\
[2023-09-14 07:11:44 10splitTasks](trainer.py 286): INFO [70/79]	0.1989(0.2074)	0.0011(0.0107)	1.298(1.391)	65.62(64.00)\
[2023-09-14 07:11:45 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2047)	0.0001(0.0097)	1.525(1.393)	50.00(64.08)\
[2023-09-14 07:11:45 10splitTasks](trainer.py 288): INFO  * Train Acc 64.080\
[2023-09-14 07:11:47 10splitTasks](trainer.py 147): INFO  * Val Acc 55.800, Total time 1.81\
[2023-09-14 07:11:47 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:11:47 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:11:47 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:11:48 10splitTasks](trainer.py 286): INFO [0/79]	0.9640(0.9640)	0.7508(0.7508)	1.382(1.382)	64.06(64.06)\
[2023-09-14 07:11:50 10splitTasks](trainer.py 286): INFO [10/79]	0.1948(0.2666)	0.0004(0.0688)	1.316(1.401)	64.06(63.21)\
[2023-09-14 07:11:52 10splitTasks](trainer.py 286): INFO [20/79]	0.1986(0.2335)	0.0005(0.0363)	1.278(1.378)	67.19(63.69)\
[2023-09-14 07:11:54 10splitTasks](trainer.py 286): INFO [30/79]	0.1976(0.2218)	0.0005(0.0247)	1.466(1.397)	62.50(63.21)\
[2023-09-14 07:11:56 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2160)	0.0004(0.0188)	1.385(1.395)	73.44(63.64)\
[2023-09-14 07:11:58 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2123)	0.0008(0.0152)	1.583(1.416)	60.94(63.05)\
[2023-09-14 07:12:00 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2101)	0.0003(0.0128)	1.380(1.414)	68.75(63.29)\
[2023-09-14 07:12:02 10splitTasks](trainer.py 286): INFO [70/79]	0.1969(0.2083)	0.0010(0.0111)	1.340(1.408)	62.50(63.49)\
[2023-09-14 07:12:03 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2054)	0.0001(0.0100)	1.214(1.412)	75.00(63.26)\
[2023-09-14 07:12:03 10splitTasks](trainer.py 288): INFO  * Train Acc 63.260\
[2023-09-14 07:12:05 10splitTasks](trainer.py 147): INFO  * Val Acc 56.400, Total time 1.84\
[2023-09-14 07:12:05 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:12:05 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:12:05 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:12:06 10splitTasks](trainer.py 286): INFO [0/79]	0.9493(0.9493)	0.7513(0.7513)	1.494(1.494)	62.50(62.50)\
[2023-09-14 07:12:08 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2659)	0.0003(0.0687)	1.443(1.389)	62.50(64.77)\
[2023-09-14 07:12:10 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2334)	0.0003(0.0362)	1.553(1.403)	54.69(63.47)\
[2023-09-14 07:12:12 10splitTasks](trainer.py 286): INFO [30/79]	0.1987(0.2219)	0.0003(0.0248)	1.489(1.394)	65.62(64.01)\
[2023-09-14 07:12:14 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2157)	0.0004(0.0188)	1.367(1.392)	64.06(64.25)\
[2023-09-14 07:12:16 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2121)	0.0007(0.0152)	1.431(1.385)	62.50(64.19)\
[2023-09-14 07:12:18 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2095)	0.0003(0.0128)	1.532(1.395)	60.94(63.81)\
[2023-09-14 07:12:20 10splitTasks](trainer.py 286): INFO [70/79]	0.1999(0.2078)	0.0013(0.0111)	1.261(1.404)	78.12(63.64)\
[2023-09-14 07:12:21 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2050)	0.0001(0.0100)	1.869(1.411)	37.50(63.44)\
[2023-09-14 07:12:21 10splitTasks](trainer.py 288): INFO  * Train Acc 63.440\
[2023-09-14 07:12:23 10splitTasks](trainer.py 147): INFO  * Val Acc 53.400, Total time 1.81\
[2023-09-14 07:12:23 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:12:23 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:12:23 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:12:24 10splitTasks](trainer.py 286): INFO [0/79]	1.3555(1.3555)	1.1561(1.1561)	1.387(1.387)	65.62(65.62)\
[2023-09-14 07:12:26 10splitTasks](trainer.py 286): INFO [10/79]	0.1996(0.3030)	0.0006(0.1056)	1.397(1.398)	65.62(66.05)\
[2023-09-14 07:12:28 10splitTasks](trainer.py 286): INFO [20/79]	0.1978(0.2527)	0.0004(0.0556)	1.504(1.447)	65.62(63.62)\
[2023-09-14 07:12:30 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2352)	0.0004(0.0378)	1.380(1.415)	65.62(64.26)\
[2023-09-14 07:12:32 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2259)	0.0004(0.0287)	1.465(1.400)	64.06(64.41)\
[2023-09-14 07:12:34 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2203)	0.0003(0.0232)	1.533(1.403)	59.38(64.46)\
[2023-09-14 07:12:36 10splitTasks](trainer.py 286): INFO [60/79]	0.1981(0.2166)	0.0004(0.0195)	1.407(1.409)	68.75(64.47)\
[2023-09-14 07:12:38 10splitTasks](trainer.py 286): INFO [70/79]	0.1990(0.2141)	0.0013(0.0168)	1.255(1.407)	75.00(64.57)\
[2023-09-14 07:12:40 10splitTasks](trainer.py 286): INFO [78/79]	0.0740(0.2109)	0.0003(0.0151)	1.586(1.403)	62.50(64.48)\
[2023-09-14 07:12:40 10splitTasks](trainer.py 288): INFO  * Train Acc 64.480\
[2023-09-14 07:12:42 10splitTasks](trainer.py 147): INFO  * Val Acc 53.600, Total time 1.81\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-3.pth\
=> Save Done\
[2023-09-14 07:12:42 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:12:43 10splitTasks](trainer.py 147): INFO  * Val Acc 25.800, Total time 1.53\
[2023-09-14 07:12:43 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:12:45 10splitTasks](trainer.py 147): INFO  * Val Acc 32.600, Total time 1.60\
[2023-09-14 07:12:45 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:12:47 10splitTasks](trainer.py 147): INFO  * Val Acc 43.000, Total time 1.61\
[2023-09-14 07:12:47 10splitTasks](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 07:12:48 10splitTasks](trainer.py 147): INFO  * Val Acc 53.600, Total time 1.84\
[2023-09-14 07:12:48 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:12:49 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:12:49 10splitTasks](iBatchLearn.py 155): INFO Acc:38.75000008201599; BWT:-22.866666755676267;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:12:53 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:12:53 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:12:53 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 3, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-3.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_3.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:12:54 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-3.pth\
[2023-09-14 07:12:54 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:12:56 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:12:56 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:12:56 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:13:00 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:13:03 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 07:13:06 10splitTasks](iBatchLearn.py 167): INFO test split name:3\
--------------------------------Official Evaluation--------------------------------\
3 17.05\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:13:14 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:13:14 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:13:14 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 4, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-3.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-4.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-3.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-4.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:13:15 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-3.pth\
[2023-09-14 07:13:15 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:13:17 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:13:17 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:13:17 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:13:17 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:13:17 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:13:17 10splitTasks](iBatchLearn.py 92): INFO ====================== 4 =======================\
[2023-09-14 07:13:17 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:13:17 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:13:17 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:13:17 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:13:17 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:13:20 10splitTasks](trainer.py 286): INFO [0/79]	3.4021(3.4021)	0.7786(0.7786)	2.576(2.576)	12.50(12.50)\
[2023-09-14 07:13:22 10splitTasks](trainer.py 286): INFO [10/79]	0.1990(0.4891)	0.0004(0.0712)	2.029(2.240)	25.00(17.61)\
[2023-09-14 07:13:24 10splitTasks](trainer.py 286): INFO [20/79]	0.1962(0.3503)	0.0003(0.0376)	1.807(2.018)	35.94(27.31)\
[2023-09-14 07:13:26 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.3007)	0.0004(0.0256)	1.658(1.932)	45.31(30.85)\
[2023-09-14 07:13:28 10splitTasks](trainer.py 286): INFO [40/79]	0.1981(0.2760)	0.0003(0.0195)	1.470(1.847)	42.19(34.38)\
[2023-09-14 07:13:30 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2608)	0.0004(0.0158)	1.672(1.811)	39.06(35.75)\
[2023-09-14 07:13:32 10splitTasks](trainer.py 286): INFO [60/79]	0.1996(0.2503)	0.0006(0.0133)	1.422(1.764)	45.31(37.65)\
[2023-09-14 07:13:34 10splitTasks](trainer.py 286): INFO [70/79]	0.1982(0.2431)	0.0008(0.0115)	1.271(1.714)	54.69(39.44)\
[2023-09-14 07:13:36 10splitTasks](trainer.py 286): INFO [78/79]	0.2256(0.2387)	0.0002(0.0103)	1.631(1.692)	50.00(40.06)\
[2023-09-14 07:13:36 10splitTasks](trainer.py 288): INFO  * Train Acc 40.060\
[2023-09-14 07:13:37 10splitTasks](trainer.py 147): INFO  * Val Acc 48.800, Total time 1.56\
[2023-09-14 07:13:37 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:13:37 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:13:37 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:13:38 10splitTasks](trainer.py 286): INFO [0/79]	1.0422(1.0422)	0.8436(0.8436)	1.387(1.387)	50.00(50.00)\
[2023-09-14 07:13:40 10splitTasks](trainer.py 286): INFO [10/79]	0.2026(0.2759)	0.0003(0.0771)	1.242(1.449)	57.81(49.86)\
[2023-09-14 07:13:42 10splitTasks](trainer.py 286): INFO [20/79]	0.1973(0.2382)	0.0004(0.0406)	1.326(1.478)	51.56(49.40)\
[2023-09-14 07:13:44 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2252)	0.0003(0.0277)	1.274(1.459)	64.06(50.71)\
[2023-09-14 07:13:46 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2181)	0.0004(0.0210)	1.266(1.437)	51.56(51.18)\
[2023-09-14 07:13:48 10splitTasks](trainer.py 286): INFO [50/79]	0.1958(0.2138)	0.0003(0.0170)	1.413(1.428)	57.81(51.41)\
[2023-09-14 07:13:50 10splitTasks](trainer.py 286): INFO [60/79]	0.1950(0.2109)	0.0003(0.0143)	1.461(1.425)	50.00(52.13)\
[2023-09-14 07:13:52 10splitTasks](trainer.py 286): INFO [70/79]	0.1973(0.2089)	0.0010(0.0124)	1.666(1.426)	45.31(52.05)\
[2023-09-14 07:13:54 10splitTasks](trainer.py 286): INFO [78/79]	0.0707(0.2059)	0.0001(0.0111)	1.247(1.430)	50.00(52.04)\
[2023-09-14 07:13:54 10splitTasks](trainer.py 288): INFO  * Train Acc 52.040\
[2023-09-14 07:13:55 10splitTasks](trainer.py 147): INFO  * Val Acc 53.200, Total time 1.54\
[2023-09-14 07:13:55 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:13:55 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:13:55 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:13:56 10splitTasks](trainer.py 286): INFO [0/79]	0.9493(0.9493)	0.7482(0.7482)	1.309(1.309)	57.81(57.81)\
[2023-09-14 07:13:58 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2662)	0.0004(0.0684)	1.347(1.366)	48.44(55.82)\
[2023-09-14 07:14:00 10splitTasks](trainer.py 286): INFO [20/79]	0.1976(0.2332)	0.0003(0.0361)	1.568(1.391)	46.88(53.79)\
[2023-09-14 07:14:02 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2217)	0.0004(0.0246)	1.428(1.389)	53.12(53.63)\
[2023-09-14 07:14:04 10splitTasks](trainer.py 286): INFO [40/79]	0.2021(0.2158)	0.0006(0.0187)	1.660(1.401)	45.31(53.28)\
[2023-09-14 07:14:06 10splitTasks](trainer.py 286): INFO [50/79]	0.1958(0.2122)	0.0003(0.0152)	1.364(1.419)	56.25(52.63)\
[2023-09-14 07:14:08 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2096)	0.0003(0.0127)	1.416(1.411)	59.38(53.36)\
[2023-09-14 07:14:10 10splitTasks](trainer.py 286): INFO [70/79]	0.1965(0.2079)	0.0010(0.0110)	1.611(1.414)	50.00(53.37)\
[2023-09-14 07:14:11 10splitTasks](trainer.py 286): INFO [78/79]	0.0722(0.2050)	0.0001(0.0099)	2.036(1.415)	25.00(53.38)\
[2023-09-14 07:14:12 10splitTasks](trainer.py 288): INFO  * Train Acc 53.380\
[2023-09-14 07:14:13 10splitTasks](trainer.py 147): INFO  * Val Acc 51.800, Total time 1.60\
[2023-09-14 07:14:13 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:14:13 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:14:13 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:14:14 10splitTasks](trainer.py 286): INFO [0/79]	0.9252(0.9252)	0.7249(0.7249)	1.357(1.357)	64.06(64.06)\
[2023-09-14 07:14:16 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2631)	0.0004(0.0663)	1.606(1.438)	48.44(54.83)\
[2023-09-14 07:14:18 10splitTasks](trainer.py 286): INFO [20/79]	0.1967(0.2315)	0.0013(0.0350)	1.204(1.400)	64.06(55.80)\
[2023-09-14 07:14:20 10splitTasks](trainer.py 286): INFO [30/79]	0.1979(0.2202)	0.0003(0.0239)	1.358(1.411)	54.69(55.29)\
[2023-09-14 07:14:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1975(0.2154)	0.0004(0.0182)	1.570(1.401)	54.69(55.72)\
[2023-09-14 07:14:24 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2118)	0.0004(0.0147)	1.092(1.371)	71.88(56.68)\
[2023-09-14 07:14:26 10splitTasks](trainer.py 286): INFO [60/79]	0.1946(0.2092)	0.0003(0.0124)	1.170(1.371)	62.50(56.79)\
[2023-09-14 07:14:28 10splitTasks](trainer.py 286): INFO [70/79]	0.1977(0.2074)	0.0010(0.0107)	1.142(1.377)	64.06(56.34)\
[2023-09-14 07:14:29 10splitTasks](trainer.py 286): INFO [78/79]	0.0704(0.2045)	0.0001(0.0096)	1.268(1.379)	62.50(56.24)\
[2023-09-14 07:14:29 10splitTasks](trainer.py 288): INFO  * Train Acc 56.240\
[2023-09-14 07:14:31 10splitTasks](trainer.py 147): INFO  * Val Acc 54.400, Total time 1.54\
[2023-09-14 07:14:31 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:14:31 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:14:31 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:14:32 10splitTasks](trainer.py 286): INFO [0/79]	0.9641(0.9641)	0.7539(0.7539)	1.268(1.268)	67.19(67.19)\
[2023-09-14 07:14:34 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2669)	0.0003(0.0689)	1.116(1.333)	67.19(58.10)\
[2023-09-14 07:14:36 10splitTasks](trainer.py 286): INFO [20/79]	0.1957(0.2333)	0.0003(0.0363)	1.378(1.319)	54.69(58.85)\
[2023-09-14 07:14:38 10splitTasks](trainer.py 286): INFO [30/79]	0.1960(0.2219)	0.0004(0.0248)	1.362(1.333)	57.81(57.66)\
[2023-09-14 07:14:40 10splitTasks](trainer.py 286): INFO [40/79]	0.1953(0.2157)	0.0003(0.0188)	1.175(1.341)	70.31(57.58)\
[2023-09-14 07:14:42 10splitTasks](trainer.py 286): INFO [50/79]	0.1971(0.2121)	0.0004(0.0152)	1.664(1.368)	51.56(56.86)\
[2023-09-14 07:14:44 10splitTasks](trainer.py 286): INFO [60/79]	0.1955(0.2097)	0.0003(0.0128)	1.557(1.369)	56.25(56.99)\
[2023-09-14 07:14:46 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2079)	0.0010(0.0111)	1.248(1.369)	59.38(56.84)\
[2023-09-14 07:14:47 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2051)	0.0001(0.0100)	2.231(1.376)	12.50(56.72)\
[2023-09-14 07:14:47 10splitTasks](trainer.py 288): INFO  * Train Acc 56.720\
[2023-09-14 07:14:49 10splitTasks](trainer.py 147): INFO  * Val Acc 55.000, Total time 1.55\
[2023-09-14 07:14:49 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:14:49 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:14:49 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:14:50 10splitTasks](trainer.py 286): INFO [0/79]	0.9601(0.9601)	0.7599(0.7599)	1.354(1.354)	54.69(54.69)\
[2023-09-14 07:14:52 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2668)	0.0004(0.0695)	1.452(1.485)	46.88(54.69)\
[2023-09-14 07:14:54 10splitTasks](trainer.py 286): INFO [20/79]	0.1990(0.2350)	0.0007(0.0366)	1.153(1.409)	65.62(57.66)\
[2023-09-14 07:14:56 10splitTasks](trainer.py 286): INFO [30/79]	0.1964(0.2230)	0.0004(0.0250)	1.391(1.404)	54.69(58.22)\
[2023-09-14 07:14:58 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2166)	0.0003(0.0191)	1.332(1.398)	59.38(58.04)\
[2023-09-14 07:15:00 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2130)	0.0004(0.0154)	1.331(1.389)	56.25(58.21)\
[2023-09-14 07:15:02 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2105)	0.0004(0.0130)	1.629(1.392)	54.69(58.38)\
[2023-09-14 07:15:04 10splitTasks](trainer.py 286): INFO [70/79]	0.1991(0.2085)	0.0010(0.0112)	1.299(1.390)	60.94(58.38)\
[2023-09-14 07:15:05 10splitTasks](trainer.py 286): INFO [78/79]	0.0708(0.2056)	0.0001(0.0101)	1.610(1.391)	50.00(58.36)\
[2023-09-14 07:15:05 10splitTasks](trainer.py 288): INFO  * Train Acc 58.360\
[2023-09-14 07:15:07 10splitTasks](trainer.py 147): INFO  * Val Acc 54.800, Total time 1.51\
[2023-09-14 07:15:07 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:15:07 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:15:07 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:15:08 10splitTasks](trainer.py 286): INFO [0/79]	1.0631(1.0631)	0.8644(0.8644)	1.142(1.142)	64.06(64.06)\
[2023-09-14 07:15:10 10splitTasks](trainer.py 286): INFO [10/79]	0.1966(0.2755)	0.0003(0.0790)	1.442(1.333)	56.25(58.10)\
[2023-09-14 07:15:12 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2383)	0.0003(0.0416)	1.309(1.372)	64.06(57.66)\
[2023-09-14 07:15:14 10splitTasks](trainer.py 286): INFO [30/79]	0.1964(0.2249)	0.0004(0.0283)	1.241(1.362)	62.50(58.22)\
[2023-09-14 07:15:15 10splitTasks](trainer.py 286): INFO [40/79]	0.1983(0.2180)	0.0003(0.0215)	1.172(1.363)	65.62(58.61)\
[2023-09-14 07:15:17 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2138)	0.0003(0.0174)	1.131(1.359)	70.31(58.82)\
[2023-09-14 07:15:19 10splitTasks](trainer.py 286): INFO [60/79]	0.1972(0.2110)	0.0003(0.0147)	1.425(1.379)	56.25(58.07)\
[2023-09-14 07:15:21 10splitTasks](trainer.py 286): INFO [70/79]	0.1976(0.2092)	0.0010(0.0127)	1.426(1.390)	64.06(57.86)\
[2023-09-14 07:15:23 10splitTasks](trainer.py 286): INFO [78/79]	0.0705(0.2062)	0.0001(0.0114)	2.258(1.398)	12.50(57.80)\
[2023-09-14 07:15:23 10splitTasks](trainer.py 288): INFO  * Train Acc 57.800\
[2023-09-14 07:15:24 10splitTasks](trainer.py 147): INFO  * Val Acc 53.200, Total time 1.55\
[2023-09-14 07:15:24 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:15:24 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:15:24 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:15:25 10splitTasks](trainer.py 286): INFO [0/79]	0.9097(0.9097)	0.7007(0.7007)	1.314(1.314)	64.06(64.06)\
[2023-09-14 07:15:27 10splitTasks](trainer.py 286): INFO [10/79]	0.1977(0.2635)	0.0003(0.0642)	1.443(1.392)	46.88(58.38)\
[2023-09-14 07:15:29 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2318)	0.0003(0.0338)	1.514(1.362)	56.25(59.75)\
[2023-09-14 07:15:31 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2203)	0.0003(0.0230)	1.437(1.371)	64.06(59.88)\
[2023-09-14 07:15:33 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2145)	0.0004(0.0175)	1.496(1.366)	54.69(59.76)\
[2023-09-14 07:15:35 10splitTasks](trainer.py 286): INFO [50/79]	0.1995(0.2110)	0.0006(0.0142)	1.401(1.371)	60.94(59.31)\
[2023-09-14 07:15:37 10splitTasks](trainer.py 286): INFO [60/79]	0.2073(0.2090)	0.0006(0.0119)	1.098(1.360)	68.75(59.73)\
[2023-09-14 07:15:39 10splitTasks](trainer.py 286): INFO [70/79]	0.1990(0.2076)	0.0010(0.0103)	1.363(1.361)	60.94(59.42)\
[2023-09-14 07:15:41 10splitTasks](trainer.py 286): INFO [78/79]	0.0704(0.2047)	0.0001(0.0093)	1.395(1.355)	62.50(59.52)\
[2023-09-14 07:15:41 10splitTasks](trainer.py 288): INFO  * Train Acc 59.520\
[2023-09-14 07:15:42 10splitTasks](trainer.py 147): INFO  * Val Acc 57.000, Total time 1.50\
[2023-09-14 07:15:42 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:15:42 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:15:42 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:15:43 10splitTasks](trainer.py 286): INFO [0/79]	0.9107(0.9107)	0.7095(0.7095)	1.266(1.266)	67.19(67.19)\
[2023-09-14 07:15:45 10splitTasks](trainer.py 286): INFO [10/79]	0.1967(0.2651)	0.0003(0.0649)	1.306(1.331)	70.31(61.65)\
[2023-09-14 07:15:47 10splitTasks](trainer.py 286): INFO [20/79]	0.1955(0.2325)	0.0004(0.0342)	1.303(1.380)	50.00(60.04)\
[2023-09-14 07:15:49 10splitTasks](trainer.py 286): INFO [30/79]	0.1967(0.2211)	0.0003(0.0233)	1.389(1.390)	56.25(59.38)\
[2023-09-14 07:15:51 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2151)	0.0004(0.0177)	1.261(1.375)	59.38(59.64)\
[2023-09-14 07:15:53 10splitTasks](trainer.py 286): INFO [50/79]	0.1992(0.2118)	0.0006(0.0144)	1.297(1.384)	62.50(59.38)\
[2023-09-14 07:15:55 10splitTasks](trainer.py 286): INFO [60/79]	0.1967(0.2096)	0.0004(0.0121)	1.316(1.381)	68.75(59.43)\
[2023-09-14 07:15:57 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2078)	0.0010(0.0105)	1.192(1.372)	62.50(60.01)\
[2023-09-14 07:15:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0715(0.2051)	0.0002(0.0095)	1.120(1.375)	62.50(60.04)\
[2023-09-14 07:15:59 10splitTasks](trainer.py 288): INFO  * Train Acc 60.040\
[2023-09-14 07:16:00 10splitTasks](trainer.py 147): INFO  * Val Acc 59.600, Total time 1.53\
[2023-09-14 07:16:00 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:16:00 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:16:00 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:16:01 10splitTasks](trainer.py 286): INFO [0/79]	1.1661(1.1661)	0.9677(0.9677)	1.313(1.313)	62.50(62.50)\
[2023-09-14 07:16:03 10splitTasks](trainer.py 286): INFO [10/79]	0.2088(0.2868)	0.0007(0.0885)	1.377(1.360)	64.06(59.52)\
[2023-09-14 07:16:05 10splitTasks](trainer.py 286): INFO [20/79]	0.1983(0.2449)	0.0004(0.0466)	1.547(1.357)	57.81(60.64)\
[2023-09-14 07:16:07 10splitTasks](trainer.py 286): INFO [30/79]	0.1995(0.2298)	0.0007(0.0317)	1.549(1.372)	51.56(60.64)\
[2023-09-14 07:16:09 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2217)	0.0004(0.0241)	1.289(1.370)	62.50(60.40)\
[2023-09-14 07:16:11 10splitTasks](trainer.py 286): INFO [50/79]	0.1994(0.2169)	0.0002(0.0195)	1.399(1.357)	53.12(60.75)\
[2023-09-14 07:16:13 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2136)	0.0004(0.0164)	1.142(1.352)	70.31(61.17)\
[2023-09-14 07:16:15 10splitTasks](trainer.py 286): INFO [70/79]	0.1973(0.2113)	0.0008(0.0142)	1.611(1.364)	60.94(61.22)\
[2023-09-14 07:16:16 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2082)	0.0001(0.0128)	2.672(1.369)	25.00(60.88)\
[2023-09-14 07:16:17 10splitTasks](trainer.py 288): INFO  * Train Acc 60.880\
[2023-09-14 07:16:18 10splitTasks](trainer.py 147): INFO  * Val Acc 46.600, Total time 1.52\
[2023-09-14 07:16:18 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:16:18 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:16:18 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:16:19 10splitTasks](trainer.py 286): INFO [0/79]	0.9382(0.9382)	0.7384(0.7384)	1.204(1.204)	62.50(62.50)\
[2023-09-14 07:16:21 10splitTasks](trainer.py 286): INFO [10/79]	0.2015(0.2672)	0.0005(0.0676)	1.182(1.424)	54.69(58.10)\
[2023-09-14 07:16:23 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2339)	0.0004(0.0356)	1.453(1.426)	54.69(58.56)\
[2023-09-14 07:16:25 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2222)	0.0004(0.0243)	1.334(1.394)	56.25(59.48)\
[2023-09-14 07:16:27 10splitTasks](trainer.py 286): INFO [40/79]	0.1955(0.2161)	0.0004(0.0185)	1.500(1.380)	46.88(59.83)\
[2023-09-14 07:16:29 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2123)	0.0004(0.0150)	1.416(1.380)	51.56(59.80)\
[2023-09-14 07:16:31 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2098)	0.0003(0.0126)	1.232(1.370)	70.31(60.40)\
[2023-09-14 07:16:33 10splitTasks](trainer.py 286): INFO [70/79]	0.1980(0.2080)	0.0011(0.0109)	1.226(1.356)	64.06(60.98)\
[2023-09-14 07:16:34 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2051)	0.0001(0.0098)	2.053(1.364)	37.50(60.80)\
[2023-09-14 07:16:34 10splitTasks](trainer.py 288): INFO  * Train Acc 60.800\
[2023-09-14 07:16:36 10splitTasks](trainer.py 147): INFO  * Val Acc 55.200, Total time 1.59\
[2023-09-14 07:16:36 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:16:36 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:16:36 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:16:37 10splitTasks](trainer.py 286): INFO [0/79]	1.0502(1.0502)	0.8522(0.8522)	1.500(1.500)	57.81(57.81)\
[2023-09-14 07:16:39 10splitTasks](trainer.py 286): INFO [10/79]	0.1966(0.2750)	0.0004(0.0779)	1.295(1.382)	59.38(60.94)\
[2023-09-14 07:16:41 10splitTasks](trainer.py 286): INFO [20/79]	0.2028(0.2379)	0.0007(0.0410)	1.338(1.354)	67.19(63.24)\
[2023-09-14 07:16:43 10splitTasks](trainer.py 286): INFO [30/79]	0.1996(0.2246)	0.0003(0.0279)	1.582(1.361)	53.12(62.05)\
[2023-09-14 07:16:45 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2180)	0.0003(0.0212)	1.445(1.365)	59.38(61.89)\
[2023-09-14 07:16:47 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2143)	0.0003(0.0172)	1.250(1.356)	62.50(61.86)\
[2023-09-14 07:16:49 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2114)	0.0004(0.0144)	1.207(1.361)	71.88(61.94)\
[2023-09-14 07:16:51 10splitTasks](trainer.py 286): INFO [70/79]	0.1976(0.2094)	0.0010(0.0125)	1.384(1.357)	60.94(62.24)\
[2023-09-14 07:16:52 10splitTasks](trainer.py 286): INFO [78/79]	0.0705(0.2064)	0.0002(0.0112)	2.269(1.352)	37.50(62.44)\
[2023-09-14 07:16:52 10splitTasks](trainer.py 288): INFO  * Train Acc 62.440\
[2023-09-14 07:16:54 10splitTasks](trainer.py 147): INFO  * Val Acc 53.800, Total time 1.56\
[2023-09-14 07:16:54 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:16:54 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:16:54 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:16:55 10splitTasks](trainer.py 286): INFO [0/79]	0.9600(0.9600)	0.7608(0.7608)	1.580(1.580)	56.25(56.25)\
[2023-09-14 07:16:57 10splitTasks](trainer.py 286): INFO [10/79]	0.2031(0.2693)	0.0004(0.0698)	1.345(1.362)	67.19(63.21)\
[2023-09-14 07:16:59 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2361)	0.0003(0.0368)	1.373(1.380)	60.94(62.35)\
[2023-09-14 07:17:01 10splitTasks](trainer.py 286): INFO [30/79]	0.2012(0.2239)	0.0004(0.0251)	1.619(1.377)	53.12(61.95)\
[2023-09-14 07:17:03 10splitTasks](trainer.py 286): INFO [40/79]	0.1986(0.2174)	0.0004(0.0191)	1.405(1.365)	59.38(62.00)\
[2023-09-14 07:17:05 10splitTasks](trainer.py 286): INFO [50/79]	0.2037(0.2138)	0.0007(0.0155)	1.145(1.371)	71.88(61.43)\
[2023-09-14 07:17:07 10splitTasks](trainer.py 286): INFO [60/79]	0.1967(0.2110)	0.0004(0.0130)	1.372(1.364)	60.94(61.65)\
[2023-09-14 07:17:09 10splitTasks](trainer.py 286): INFO [70/79]	0.2075(0.2092)	0.0016(0.0113)	1.385(1.367)	60.94(61.38)\
[2023-09-14 07:17:10 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2062)	0.0001(0.0101)	1.648(1.371)	37.50(61.18)\
[2023-09-14 07:17:10 10splitTasks](trainer.py 288): INFO  * Train Acc 61.180\
[2023-09-14 07:17:12 10splitTasks](trainer.py 147): INFO  * Val Acc 55.800, Total time 1.67\
[2023-09-14 07:17:12 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:17:12 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:17:12 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:17:13 10splitTasks](trainer.py 286): INFO [0/79]	0.9418(0.9418)	0.7362(0.7362)	1.243(1.243)	64.06(64.06)\
[2023-09-14 07:17:15 10splitTasks](trainer.py 286): INFO [10/79]	0.1962(0.2679)	0.0004(0.0674)	1.267(1.360)	60.94(63.64)\
[2023-09-14 07:17:17 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2346)	0.0003(0.0356)	1.425(1.368)	57.81(63.76)\
[2023-09-14 07:17:19 10splitTasks](trainer.py 286): INFO [30/79]	0.1977(0.2230)	0.0006(0.0243)	1.308(1.402)	65.62(61.90)\
[2023-09-14 07:17:21 10splitTasks](trainer.py 286): INFO [40/79]	0.1952(0.2166)	0.0003(0.0185)	1.462(1.406)	64.06(61.59)\
[2023-09-14 07:17:23 10splitTasks](trainer.py 286): INFO [50/79]	0.1974(0.2127)	0.0003(0.0149)	1.351(1.402)	60.94(61.70)\
[2023-09-14 07:17:25 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2103)	0.0004(0.0126)	1.252(1.392)	62.50(61.55)\
[2023-09-14 07:17:27 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2086)	0.0010(0.0109)	1.505(1.385)	60.94(61.69)\
[2023-09-14 07:17:28 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2057)	0.0001(0.0098)	2.497(1.377)	25.00(61.94)\
[2023-09-14 07:17:28 10splitTasks](trainer.py 288): INFO  * Train Acc 61.940\
[2023-09-14 07:17:30 10splitTasks](trainer.py 147): INFO  * Val Acc 56.000, Total time 1.54\
[2023-09-14 07:17:30 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:17:30 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:17:30 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:17:31 10splitTasks](trainer.py 286): INFO [0/79]	0.9779(0.9779)	0.7797(0.7797)	1.339(1.339)	65.62(65.62)\
[2023-09-14 07:17:33 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2683)	0.0004(0.0712)	1.257(1.385)	71.88(61.93)\
[2023-09-14 07:17:35 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2358)	0.0003(0.0376)	1.253(1.378)	75.00(62.80)\
[2023-09-14 07:17:37 10splitTasks](trainer.py 286): INFO [30/79]	0.1968(0.2234)	0.0004(0.0256)	1.355(1.385)	68.75(62.55)\
[2023-09-14 07:17:39 10splitTasks](trainer.py 286): INFO [40/79]	0.2002(0.2171)	0.0007(0.0195)	1.541(1.376)	56.25(62.16)\
[2023-09-14 07:17:41 10splitTasks](trainer.py 286): INFO [50/79]	0.1972(0.2131)	0.0004(0.0158)	1.386(1.389)	62.50(61.92)\
[2023-09-14 07:17:43 10splitTasks](trainer.py 286): INFO [60/79]	0.1969(0.2106)	0.0006(0.0133)	1.463(1.389)	59.38(61.96)\
[2023-09-14 07:17:45 10splitTasks](trainer.py 286): INFO [70/79]	0.2084(0.2089)	0.0011(0.0115)	1.340(1.383)	65.62(62.02)\
[2023-09-14 07:17:46 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2059)	0.0001(0.0103)	1.222(1.378)	62.50(62.20)\
[2023-09-14 07:17:46 10splitTasks](trainer.py 288): INFO  * Train Acc 62.200\
[2023-09-14 07:17:48 10splitTasks](trainer.py 147): INFO  * Val Acc 59.400, Total time 1.53\
[2023-09-14 07:17:48 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:17:48 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:17:48 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:17:49 10splitTasks](trainer.py 286): INFO [0/79]	0.9626(0.9626)	0.7650(0.7650)	1.706(1.706)	50.00(50.00)\
[2023-09-14 07:17:51 10splitTasks](trainer.py 286): INFO [10/79]	0.1967(0.2661)	0.0005(0.0699)	1.334(1.425)	64.06(59.38)\
[2023-09-14 07:17:53 10splitTasks](trainer.py 286): INFO [20/79]	0.1962(0.2329)	0.0003(0.0368)	1.310(1.385)	67.19(61.68)\
[2023-09-14 07:17:55 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2215)	0.0003(0.0251)	1.616(1.404)	51.56(60.84)\
[2023-09-14 07:17:57 10splitTasks](trainer.py 286): INFO [40/79]	0.1954(0.2153)	0.0003(0.0191)	1.131(1.389)	73.44(61.70)\
[2023-09-14 07:17:58 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2119)	0.0004(0.0156)	1.250(1.377)	65.62(61.64)\
[2023-09-14 07:18:00 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2095)	0.0003(0.0131)	1.196(1.366)	62.50(61.99)\
[2023-09-14 07:18:02 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2079)	0.0011(0.0113)	1.292(1.362)	65.62(62.46)\
[2023-09-14 07:18:04 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2050)	0.0001(0.0102)	1.529(1.355)	62.50(62.94)\
[2023-09-14 07:18:04 10splitTasks](trainer.py 288): INFO  * Train Acc 62.940\
[2023-09-14 07:18:05 10splitTasks](trainer.py 147): INFO  * Val Acc 51.800, Total time 1.54\
[2023-09-14 07:18:05 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:18:05 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:18:05 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:18:06 10splitTasks](trainer.py 286): INFO [0/79]	0.9633(0.9633)	0.7355(0.7355)	1.209(1.209)	67.19(67.19)\
[2023-09-14 07:18:09 10splitTasks](trainer.py 286): INFO [10/79]	0.2061(0.2745)	0.0006(0.0741)	1.112(1.394)	71.88(62.22)\
[2023-09-14 07:18:10 10splitTasks](trainer.py 286): INFO [20/79]	0.1951(0.2373)	0.0003(0.0390)	1.401(1.375)	57.81(62.20)\
[2023-09-14 07:18:12 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2242)	0.0003(0.0266)	1.150(1.354)	76.56(63.71)\
[2023-09-14 07:18:14 10splitTasks](trainer.py 286): INFO [40/79]	0.1955(0.2178)	0.0004(0.0202)	1.137(1.353)	73.44(63.61)\
[2023-09-14 07:18:16 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2139)	0.0004(0.0164)	1.156(1.350)	70.31(63.73)\
[2023-09-14 07:18:18 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2114)	0.0004(0.0137)	1.346(1.343)	56.25(63.86)\
[2023-09-14 07:18:20 10splitTasks](trainer.py 286): INFO [70/79]	0.2010(0.2096)	0.0009(0.0119)	1.147(1.348)	68.75(63.64)\
[2023-09-14 07:18:22 10splitTasks](trainer.py 286): INFO [78/79]	0.0726(0.2067)	0.0001(0.0107)	1.852(1.342)	37.50(63.74)\
[2023-09-14 07:18:22 10splitTasks](trainer.py 288): INFO  * Train Acc 63.740\
[2023-09-14 07:18:23 10splitTasks](trainer.py 147): INFO  * Val Acc 54.600, Total time 1.53\
[2023-09-14 07:18:23 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:18:23 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:18:23 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:18:24 10splitTasks](trainer.py 286): INFO [0/79]	0.9941(0.9941)	0.7771(0.7771)	1.268(1.268)	64.06(64.06)\
[2023-09-14 07:18:26 10splitTasks](trainer.py 286): INFO [10/79]	0.1972(0.2696)	0.0004(0.0711)	1.252(1.322)	70.31(64.63)\
[2023-09-14 07:18:28 10splitTasks](trainer.py 286): INFO [20/79]	0.1979(0.2356)	0.0004(0.0375)	1.511(1.355)	59.38(64.21)\
[2023-09-14 07:18:30 10splitTasks](trainer.py 286): INFO [30/79]	0.1979(0.2242)	0.0003(0.0255)	1.136(1.347)	71.88(64.31)\
[2023-09-14 07:18:32 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2174)	0.0003(0.0194)	1.375(1.346)	62.50(64.56)\
[2023-09-14 07:18:34 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2134)	0.0003(0.0157)	1.241(1.339)	60.94(64.25)\
[2023-09-14 07:18:36 10splitTasks](trainer.py 286): INFO [60/79]	0.2063(0.2108)	0.0006(0.0133)	1.370(1.339)	62.50(64.29)\
[2023-09-14 07:18:38 10splitTasks](trainer.py 286): INFO [70/79]	0.2026(0.2091)	0.0012(0.0115)	1.413(1.342)	60.94(64.30)\
[2023-09-14 07:18:40 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2061)	0.0001(0.0103)	1.642(1.339)	62.50(64.32)\
[2023-09-14 07:18:40 10splitTasks](trainer.py 288): INFO  * Train Acc 64.320\
[2023-09-14 07:18:41 10splitTasks](trainer.py 147): INFO  * Val Acc 61.000, Total time 1.52\
[2023-09-14 07:18:41 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:18:41 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:18:41 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:18:42 10splitTasks](trainer.py 286): INFO [0/79]	0.9011(0.9011)	0.6975(0.6975)	1.359(1.359)	59.38(59.38)\
[2023-09-14 07:18:44 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2624)	0.0003(0.0643)	1.444(1.347)	59.38(62.64)\
[2023-09-14 07:18:46 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2311)	0.0003(0.0339)	1.335(1.325)	64.06(65.18)\
[2023-09-14 07:18:48 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2204)	0.0004(0.0231)	1.361(1.325)	64.06(64.92)\
[2023-09-14 07:18:50 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2148)	0.0003(0.0176)	1.404(1.336)	68.75(64.33)\
[2023-09-14 07:18:52 10splitTasks](trainer.py 286): INFO [50/79]	0.2055(0.2118)	0.0006(0.0142)	1.496(1.352)	59.38(63.94)\
[2023-09-14 07:18:54 10splitTasks](trainer.py 286): INFO [60/79]	0.1953(0.2097)	0.0003(0.0120)	1.308(1.348)	57.81(63.96)\
[2023-09-14 07:18:56 10splitTasks](trainer.py 286): INFO [70/79]	0.1968(0.2081)	0.0010(0.0104)	1.357(1.352)	56.25(63.93)\
[2023-09-14 07:18:58 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2053)	0.0001(0.0094)	2.826(1.350)	25.00(64.00)\
[2023-09-14 07:18:58 10splitTasks](trainer.py 288): INFO  * Train Acc 64.000\
[2023-09-14 07:18:59 10splitTasks](trainer.py 147): INFO  * Val Acc 52.600, Total time 1.69\
[2023-09-14 07:18:59 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:18:59 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:18:59 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:19:00 10splitTasks](trainer.py 286): INFO [0/79]	0.9132(0.9132)	0.7071(0.7071)	1.252(1.252)	65.62(65.62)\
[2023-09-14 07:19:02 10splitTasks](trainer.py 286): INFO [10/79]	0.2001(0.2638)	0.0007(0.0648)	1.533(1.378)	56.25(62.36)\
[2023-09-14 07:19:04 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2321)	0.0004(0.0342)	1.342(1.367)	60.94(61.98)\
[2023-09-14 07:19:06 10splitTasks](trainer.py 286): INFO [30/79]	0.1988(0.2209)	0.0008(0.0234)	1.219(1.369)	76.56(63.36)\
[2023-09-14 07:19:08 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2153)	0.0004(0.0178)	1.411(1.374)	60.94(63.22)\
[2023-09-14 07:19:10 10splitTasks](trainer.py 286): INFO [50/79]	0.2084(0.2120)	0.0011(0.0144)	1.309(1.373)	59.38(62.96)\
[2023-09-14 07:19:12 10splitTasks](trainer.py 286): INFO [60/79]	0.1966(0.2095)	0.0004(0.0122)	1.366(1.359)	68.75(63.32)\
[2023-09-14 07:19:14 10splitTasks](trainer.py 286): INFO [70/79]	0.1996(0.2079)	0.0012(0.0105)	1.505(1.369)	53.12(62.87)\
[2023-09-14 07:19:16 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2050)	0.0001(0.0095)	2.104(1.370)	25.00(62.78)\
[2023-09-14 07:19:16 10splitTasks](trainer.py 288): INFO  * Train Acc 62.780\
[2023-09-14 07:19:17 10splitTasks](trainer.py 147): INFO  * Val Acc 55.000, Total time 1.61\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-4.pth\
=> Save Done\
[2023-09-14 07:19:17 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:19:19 10splitTasks](trainer.py 147): INFO  * Val Acc 32.200, Total time 1.63\
[2023-09-14 07:19:19 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:19:21 10splitTasks](trainer.py 147): INFO  * Val Acc 43.400, Total time 1.64\
[2023-09-14 07:19:21 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:19:22 10splitTasks](trainer.py 147): INFO  * Val Acc 44.800, Total time 1.74\
[2023-09-14 07:19:22 10splitTasks](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 07:19:24 10splitTasks](trainer.py 147): INFO  * Val Acc 49.600, Total time 1.82\
[2023-09-14 07:19:24 10splitTasks](iBatchLearn.py 131): INFO validation split name:4\
[2023-09-14 07:19:26 10splitTasks](trainer.py 147): INFO  * Val Acc 55.000, Total time 1.53\
[2023-09-14 07:19:26 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:19:26 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:19:26 10splitTasks](iBatchLearn.py 155): INFO Acc:45.000000072860715; BWT:-13.400000103473664;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:19:31 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:19:31 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:19:31 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 4, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-4.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_4.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:19:31 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-4.pth\
[2023-09-14 07:19:31 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:19:33 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:19:33 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:19:33 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:19:38 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:19:40 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 07:19:43 10splitTasks](iBatchLearn.py 167): INFO test split name:3\
[2023-09-14 07:19:46 10splitTasks](iBatchLearn.py 167): INFO test split name:4\
--------------------------------Official Evaluation--------------------------------\
4 32.085\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:19:55 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:19:55 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:19:55 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 5, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-4.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-5.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-4.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-5.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:19:55 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-4.pth\
[2023-09-14 07:19:56 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:19:57 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:19:57 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:19:57 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:19:57 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:19:57 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:19:57 10splitTasks](iBatchLearn.py 92): INFO ====================== 5 =======================\
[2023-09-14 07:19:57 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:19:57 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:19:57 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:19:57 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:19:57 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:20:01 10splitTasks](trainer.py 286): INFO [0/79]	3.4416(3.4416)	0.9343(0.9343)	2.545(2.545)	7.81(7.81)\
[2023-09-14 07:20:03 10splitTasks](trainer.py 286): INFO [10/79]	0.2011(0.4936)	0.0007(0.0854)	1.993(2.186)	25.00(19.60)\
[2023-09-14 07:20:05 10splitTasks](trainer.py 286): INFO [20/79]	0.1973(0.3525)	0.0004(0.0449)	1.658(1.999)	29.69(26.79)\
[2023-09-14 07:20:07 10splitTasks](trainer.py 286): INFO [30/79]	0.1989(0.3024)	0.0004(0.0306)	1.565(1.920)	46.88(30.34)\
[2023-09-14 07:20:09 10splitTasks](trainer.py 286): INFO [40/79]	0.1973(0.2766)	0.0004(0.0232)	1.873(1.842)	39.06(33.46)\
[2023-09-14 07:20:11 10splitTasks](trainer.py 286): INFO [50/79]	0.1972(0.2615)	0.0004(0.0188)	1.561(1.803)	48.44(34.96)\
[2023-09-14 07:20:13 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2509)	0.0003(0.0158)	1.702(1.765)	42.19(37.14)\
[2023-09-14 07:20:15 10splitTasks](trainer.py 286): INFO [70/79]	0.1989(0.2436)	0.0013(0.0136)	1.603(1.743)	51.56(38.51)\
[2023-09-14 07:20:16 10splitTasks](trainer.py 286): INFO [78/79]	0.2190(0.2390)	0.0002(0.0123)	1.866(1.727)	25.00(39.14)\
[2023-09-14 07:20:16 10splitTasks](trainer.py 288): INFO  * Train Acc 39.140\
[2023-09-14 07:20:18 10splitTasks](trainer.py 147): INFO  * Val Acc 43.800, Total time 1.68\
[2023-09-14 07:20:18 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:20:18 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:20:18 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:20:19 10splitTasks](trainer.py 286): INFO [0/79]	0.9064(0.9064)	0.7056(0.7056)	1.340(1.340)	57.81(57.81)\
[2023-09-14 07:20:21 10splitTasks](trainer.py 286): INFO [10/79]	0.1975(0.2624)	0.0004(0.0646)	1.613(1.492)	48.44(46.88)\
[2023-09-14 07:20:23 10splitTasks](trainer.py 286): INFO [20/79]	0.1957(0.2315)	0.0004(0.0341)	1.541(1.535)	43.75(46.95)\
[2023-09-14 07:20:25 10splitTasks](trainer.py 286): INFO [30/79]	0.1958(0.2200)	0.0004(0.0232)	1.656(1.528)	45.31(47.33)\
[2023-09-14 07:20:27 10splitTasks](trainer.py 286): INFO [40/79]	0.1956(0.2140)	0.0003(0.0177)	1.466(1.508)	50.00(47.94)\
[2023-09-14 07:20:29 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2106)	0.0006(0.0143)	1.362(1.506)	56.25(48.47)\
[2023-09-14 07:20:31 10splitTasks](trainer.py 286): INFO [60/79]	0.1957(0.2082)	0.0003(0.0121)	1.508(1.495)	46.88(49.03)\
[2023-09-14 07:20:33 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2065)	0.0010(0.0104)	1.803(1.501)	46.88(48.92)\
[2023-09-14 07:20:34 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2037)	0.0001(0.0094)	1.744(1.487)	50.00(49.70)\
[2023-09-14 07:20:34 10splitTasks](trainer.py 288): INFO  * Train Acc 49.700\
[2023-09-14 07:20:36 10splitTasks](trainer.py 147): INFO  * Val Acc 48.200, Total time 1.55\
[2023-09-14 07:20:36 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:20:36 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:20:36 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:20:37 10splitTasks](trainer.py 286): INFO [0/79]	0.8952(0.8952)	0.6919(0.6919)	1.493(1.493)	53.12(53.12)\
[2023-09-14 07:20:39 10splitTasks](trainer.py 286): INFO [10/79]	0.1954(0.2596)	0.0004(0.0633)	1.558(1.462)	43.75(51.28)\
[2023-09-14 07:20:41 10splitTasks](trainer.py 286): INFO [20/79]	0.2022(0.2299)	0.0003(0.0334)	1.304(1.458)	54.69(51.86)\
[2023-09-14 07:20:43 10splitTasks](trainer.py 286): INFO [30/79]	0.1957(0.2200)	0.0004(0.0228)	1.589(1.493)	42.19(50.55)\
[2023-09-14 07:20:45 10splitTasks](trainer.py 286): INFO [40/79]	0.1983(0.2142)	0.0007(0.0173)	1.473(1.487)	45.31(50.88)\
[2023-09-14 07:20:46 10splitTasks](trainer.py 286): INFO [50/79]	0.1975(0.2108)	0.0006(0.0140)	1.629(1.486)	45.31(51.01)\
[2023-09-14 07:20:48 10splitTasks](trainer.py 286): INFO [60/79]	0.1956(0.2084)	0.0003(0.0118)	1.491(1.481)	53.12(50.72)\
[2023-09-14 07:20:50 10splitTasks](trainer.py 286): INFO [70/79]	0.2033(0.2068)	0.0011(0.0102)	1.881(1.481)	35.94(51.25)\
[2023-09-14 07:20:52 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2040)	0.0001(0.0092)	1.613(1.477)	37.50(51.68)\
[2023-09-14 07:20:52 10splitTasks](trainer.py 288): INFO  * Train Acc 51.680\
[2023-09-14 07:20:53 10splitTasks](trainer.py 147): INFO  * Val Acc 49.200, Total time 1.51\
[2023-09-14 07:20:53 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:20:53 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:20:53 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:20:54 10splitTasks](trainer.py 286): INFO [0/79]	0.9404(0.9404)	0.7404(0.7404)	1.569(1.569)	46.88(46.88)\
[2023-09-14 07:20:56 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2649)	0.0003(0.0677)	1.494(1.568)	51.56(47.59)\
[2023-09-14 07:20:58 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2321)	0.0004(0.0357)	1.365(1.505)	67.19(51.34)\
[2023-09-14 07:21:00 10splitTasks](trainer.py 286): INFO [30/79]	0.1969(0.2204)	0.0006(0.0243)	1.304(1.487)	59.38(51.26)\
[2023-09-14 07:21:02 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2145)	0.0004(0.0185)	1.730(1.473)	51.56(52.36)\
[2023-09-14 07:21:04 10splitTasks](trainer.py 286): INFO [50/79]	0.1955(0.2110)	0.0004(0.0150)	1.516(1.463)	54.69(52.57)\
[2023-09-14 07:21:06 10splitTasks](trainer.py 286): INFO [60/79]	0.1975(0.2085)	0.0004(0.0126)	1.499(1.461)	40.62(52.51)\
[2023-09-14 07:21:08 10splitTasks](trainer.py 286): INFO [70/79]	0.1968(0.2068)	0.0010(0.0109)	1.442(1.458)	51.56(52.64)\
[2023-09-14 07:21:10 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2040)	0.0001(0.0098)	2.222(1.451)	25.00(52.92)\
[2023-09-14 07:21:10 10splitTasks](trainer.py 288): INFO  * Train Acc 52.920\
[2023-09-14 07:21:11 10splitTasks](trainer.py 147): INFO  * Val Acc 54.000, Total time 1.60\
[2023-09-14 07:21:11 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:21:11 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:21:11 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:21:12 10splitTasks](trainer.py 286): INFO [0/79]	0.9357(0.9357)	0.7304(0.7304)	1.224(1.224)	59.38(59.38)\
[2023-09-14 07:21:14 10splitTasks](trainer.py 286): INFO [10/79]	0.1955(0.2664)	0.0004(0.0668)	1.266(1.487)	60.94(53.12)\
[2023-09-14 07:21:16 10splitTasks](trainer.py 286): INFO [20/79]	0.1980(0.2333)	0.0006(0.0353)	1.425(1.461)	53.12(55.06)\
[2023-09-14 07:21:18 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2214)	0.0004(0.0240)	1.367(1.458)	59.38(55.04)\
[2023-09-14 07:21:20 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2153)	0.0003(0.0183)	1.301(1.453)	54.69(54.50)\
[2023-09-14 07:21:22 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2116)	0.0003(0.0148)	1.259(1.450)	62.50(54.78)\
[2023-09-14 07:21:24 10splitTasks](trainer.py 286): INFO [60/79]	0.1959(0.2093)	0.0003(0.0125)	1.709(1.456)	40.62(54.53)\
[2023-09-14 07:21:26 10splitTasks](trainer.py 286): INFO [70/79]	0.1969(0.2075)	0.0009(0.0108)	1.447(1.457)	53.12(54.73)\
[2023-09-14 07:21:27 10splitTasks](trainer.py 286): INFO [78/79]	0.0712(0.2047)	0.0001(0.0097)	3.035(1.464)	12.50(54.26)\
[2023-09-14 07:21:27 10splitTasks](trainer.py 288): INFO  * Train Acc 54.260\
[2023-09-14 07:21:29 10splitTasks](trainer.py 147): INFO  * Val Acc 51.800, Total time 1.54\
[2023-09-14 07:21:29 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:21:29 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:21:29 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:21:30 10splitTasks](trainer.py 286): INFO [0/79]	0.9982(0.9982)	0.7996(0.7996)	1.339(1.339)	59.38(59.38)\
[2023-09-14 07:21:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2704)	0.0003(0.0732)	1.899(1.563)	39.06(50.99)\
[2023-09-14 07:21:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2352)	0.0004(0.0385)	1.301(1.464)	65.62(55.06)\
[2023-09-14 07:21:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1982(0.2231)	0.0006(0.0263)	1.153(1.429)	65.62(55.75)\
[2023-09-14 07:21:38 10splitTasks](trainer.py 286): INFO [40/79]	0.1952(0.2167)	0.0003(0.0200)	1.361(1.438)	54.69(55.87)\
[2023-09-14 07:21:40 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2128)	0.0004(0.0162)	1.562(1.449)	51.56(55.67)\
[2023-09-14 07:21:42 10splitTasks](trainer.py 286): INFO [60/79]	0.1992(0.2102)	0.0003(0.0136)	1.513(1.450)	48.44(55.51)\
[2023-09-14 07:21:44 10splitTasks](trainer.py 286): INFO [70/79]	0.1973(0.2084)	0.0010(0.0118)	1.362(1.448)	59.38(55.46)\
[2023-09-14 07:21:45 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2055)	0.0001(0.0106)	2.127(1.441)	37.50(55.76)\
[2023-09-14 07:21:45 10splitTasks](trainer.py 288): INFO  * Train Acc 55.760\
[2023-09-14 07:21:47 10splitTasks](trainer.py 147): INFO  * Val Acc 55.800, Total time 1.55\
[2023-09-14 07:21:47 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:21:47 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:21:47 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:21:48 10splitTasks](trainer.py 286): INFO [0/79]	0.9351(0.9351)	0.7361(0.7361)	1.320(1.320)	57.81(57.81)\
[2023-09-14 07:21:50 10splitTasks](trainer.py 286): INFO [10/79]	0.2020(0.2641)	0.0006(0.0673)	1.350(1.467)	57.81(53.55)\
[2023-09-14 07:21:52 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2326)	0.0003(0.0355)	1.592(1.479)	48.44(54.24)\
[2023-09-14 07:21:54 10splitTasks](trainer.py 286): INFO [30/79]	0.2093(0.2216)	0.0006(0.0242)	1.447(1.460)	54.69(54.79)\
[2023-09-14 07:21:56 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2155)	0.0004(0.0184)	1.678(1.431)	56.25(55.95)\
[2023-09-14 07:21:58 10splitTasks](trainer.py 286): INFO [50/79]	0.1957(0.2118)	0.0003(0.0149)	1.096(1.424)	68.75(56.34)\
[2023-09-14 07:22:00 10splitTasks](trainer.py 286): INFO [60/79]	0.1953(0.2095)	0.0003(0.0125)	1.561(1.430)	53.12(56.28)\
[2023-09-14 07:22:02 10splitTasks](trainer.py 286): INFO [70/79]	0.1973(0.2079)	0.0010(0.0108)	1.579(1.430)	57.81(56.56)\
[2023-09-14 07:22:03 10splitTasks](trainer.py 286): INFO [78/79]	0.0717(0.2050)	0.0001(0.0098)	2.078(1.433)	50.00(56.24)\
[2023-09-14 07:22:03 10splitTasks](trainer.py 288): INFO  * Train Acc 56.240\
[2023-09-14 07:22:05 10splitTasks](trainer.py 147): INFO  * Val Acc 53.000, Total time 1.51\
[2023-09-14 07:22:05 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:22:05 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:22:05 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:22:06 10splitTasks](trainer.py 286): INFO [0/79]	0.9511(0.9511)	0.7524(0.7524)	1.374(1.374)	59.38(59.38)\
[2023-09-14 07:22:08 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2664)	0.0003(0.0688)	1.312(1.395)	59.38(58.95)\
[2023-09-14 07:22:10 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2330)	0.0003(0.0363)	1.392(1.367)	59.38(59.30)\
[2023-09-14 07:22:11 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2212)	0.0004(0.0247)	1.403(1.371)	53.12(57.91)\
[2023-09-14 07:22:13 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2153)	0.0004(0.0188)	1.556(1.387)	54.69(57.62)\
[2023-09-14 07:22:15 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2120)	0.0004(0.0152)	1.263(1.393)	56.25(56.99)\
[2023-09-14 07:22:17 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2098)	0.0004(0.0128)	1.448(1.407)	54.69(56.15)\
[2023-09-14 07:22:19 10splitTasks](trainer.py 286): INFO [70/79]	0.2035(0.2080)	0.0011(0.0111)	1.237(1.411)	67.19(56.16)\
[2023-09-14 07:22:21 10splitTasks](trainer.py 286): INFO [78/79]	0.0715(0.2051)	0.0002(0.0100)	2.455(1.405)	25.00(56.28)\
[2023-09-14 07:22:21 10splitTasks](trainer.py 288): INFO  * Train Acc 56.280\
[2023-09-14 07:22:22 10splitTasks](trainer.py 147): INFO  * Val Acc 50.400, Total time 1.51\
[2023-09-14 07:22:22 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:22:22 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:22:22 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:22:23 10splitTasks](trainer.py 286): INFO [0/79]	0.9345(0.9345)	0.7133(0.7133)	1.106(1.106)	68.75(68.75)\
[2023-09-14 07:22:25 10splitTasks](trainer.py 286): INFO [10/79]	0.1972(0.2636)	0.0003(0.0653)	1.360(1.338)	51.56(60.23)\
[2023-09-14 07:22:27 10splitTasks](trainer.py 286): INFO [20/79]	0.1962(0.2325)	0.0004(0.0344)	1.557(1.374)	53.12(58.93)\
[2023-09-14 07:22:29 10splitTasks](trainer.py 286): INFO [30/79]	0.1975(0.2209)	0.0004(0.0235)	1.412(1.393)	64.06(58.62)\
[2023-09-14 07:22:31 10splitTasks](trainer.py 286): INFO [40/79]	0.1949(0.2148)	0.0004(0.0179)	1.501(1.407)	54.69(58.19)\
[2023-09-14 07:22:33 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2111)	0.0003(0.0144)	1.263(1.405)	60.94(58.03)\
[2023-09-14 07:22:35 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2087)	0.0003(0.0121)	1.143(1.413)	68.75(57.58)\
[2023-09-14 07:22:37 10splitTasks](trainer.py 286): INFO [70/79]	0.1984(0.2071)	0.0010(0.0105)	1.458(1.414)	59.38(57.61)\
[2023-09-14 07:22:39 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2043)	0.0001(0.0095)	3.014(1.422)	25.00(57.32)\
[2023-09-14 07:22:39 10splitTasks](trainer.py 288): INFO  * Train Acc 57.320\
[2023-09-14 07:22:40 10splitTasks](trainer.py 147): INFO  * Val Acc 52.200, Total time 1.81\
[2023-09-14 07:22:40 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:22:40 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:22:40 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:22:41 10splitTasks](trainer.py 286): INFO [0/79]	0.9376(0.9376)	0.7377(0.7377)	1.466(1.466)	62.50(62.50)\
[2023-09-14 07:22:43 10splitTasks](trainer.py 286): INFO [10/79]	0.1962(0.2685)	0.0004(0.0676)	1.289(1.403)	60.94(58.24)\
[2023-09-14 07:22:45 10splitTasks](trainer.py 286): INFO [20/79]	0.1977(0.2346)	0.0004(0.0356)	1.518(1.400)	54.69(58.41)\
[2023-09-14 07:22:47 10splitTasks](trainer.py 286): INFO [30/79]	0.1969(0.2224)	0.0003(0.0243)	1.180(1.381)	64.06(58.32)\
[2023-09-14 07:22:49 10splitTasks](trainer.py 286): INFO [40/79]	0.1972(0.2162)	0.0003(0.0185)	1.392(1.387)	59.38(58.57)\
[2023-09-14 07:22:51 10splitTasks](trainer.py 286): INFO [50/79]	0.1971(0.2123)	0.0004(0.0149)	1.449(1.389)	60.94(58.79)\
[2023-09-14 07:22:53 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2097)	0.0004(0.0126)	1.493(1.394)	60.94(58.58)\
[2023-09-14 07:22:55 10splitTasks](trainer.py 286): INFO [70/79]	0.2053(0.2080)	0.0010(0.0109)	1.630(1.407)	53.12(58.32)\
[2023-09-14 07:22:57 10splitTasks](trainer.py 286): INFO [78/79]	0.0709(0.2051)	0.0002(0.0098)	2.216(1.416)	25.00(57.76)\
[2023-09-14 07:22:57 10splitTasks](trainer.py 288): INFO  * Train Acc 57.760\
[2023-09-14 07:22:58 10splitTasks](trainer.py 147): INFO  * Val Acc 51.200, Total time 1.59\
[2023-09-14 07:22:58 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:22:58 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:22:58 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:22:59 10splitTasks](trainer.py 286): INFO [0/79]	0.9478(0.9478)	0.7425(0.7425)	1.529(1.529)	50.00(50.00)\
[2023-09-14 07:23:01 10splitTasks](trainer.py 286): INFO [10/79]	0.1974(0.2664)	0.0006(0.0680)	1.375(1.539)	59.38(53.69)\
[2023-09-14 07:23:03 10splitTasks](trainer.py 286): INFO [20/79]	0.1977(0.2333)	0.0003(0.0359)	1.363(1.463)	57.81(56.18)\
[2023-09-14 07:23:05 10splitTasks](trainer.py 286): INFO [30/79]	0.2009(0.2218)	0.0006(0.0245)	1.332(1.450)	60.94(56.50)\
[2023-09-14 07:23:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1970(0.2159)	0.0004(0.0186)	1.582(1.449)	56.25(56.75)\
[2023-09-14 07:23:09 10splitTasks](trainer.py 286): INFO [50/79]	0.2057(0.2123)	0.0007(0.0151)	1.499(1.431)	56.25(57.35)\
[2023-09-14 07:23:11 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2097)	0.0003(0.0127)	1.501(1.424)	51.56(57.56)\
[2023-09-14 07:23:13 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2078)	0.0010(0.0110)	1.535(1.420)	53.12(57.66)\
[2023-09-14 07:23:14 10splitTasks](trainer.py 286): INFO [78/79]	0.0711(0.2049)	0.0002(0.0099)	1.666(1.417)	37.50(57.78)\
[2023-09-14 07:23:15 10splitTasks](trainer.py 288): INFO  * Train Acc 57.780\
[2023-09-14 07:23:16 10splitTasks](trainer.py 147): INFO  * Val Acc 52.800, Total time 1.54\
[2023-09-14 07:23:16 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:23:16 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:23:16 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:23:17 10splitTasks](trainer.py 286): INFO [0/79]	0.9910(0.9910)	0.7897(0.7897)	1.406(1.406)	56.25(56.25)\
[2023-09-14 07:23:19 10splitTasks](trainer.py 286): INFO [10/79]	0.1950(0.2692)	0.0003(0.0723)	1.523(1.464)	57.81(57.39)\
[2023-09-14 07:23:21 10splitTasks](trainer.py 286): INFO [20/79]	0.1989(0.2352)	0.0004(0.0382)	1.351(1.450)	56.25(57.44)\
[2023-09-14 07:23:23 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2227)	0.0008(0.0260)	1.660(1.420)	48.44(58.72)\
[2023-09-14 07:23:25 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2163)	0.0004(0.0198)	1.407(1.388)	46.88(59.83)\
[2023-09-14 07:23:27 10splitTasks](trainer.py 286): INFO [50/79]	0.1954(0.2129)	0.0004(0.0160)	1.653(1.407)	46.88(58.98)\
[2023-09-14 07:23:29 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2102)	0.0004(0.0135)	1.454(1.406)	64.06(58.76)\
[2023-09-14 07:23:31 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2084)	0.0010(0.0117)	1.449(1.411)	57.81(58.78)\
[2023-09-14 07:23:32 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2055)	0.0001(0.0105)	1.741(1.415)	37.50(58.48)\
[2023-09-14 07:23:32 10splitTasks](trainer.py 288): INFO  * Train Acc 58.480\
[2023-09-14 07:23:34 10splitTasks](trainer.py 147): INFO  * Val Acc 52.800, Total time 1.54\
[2023-09-14 07:23:34 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:23:34 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:23:34 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:23:35 10splitTasks](trainer.py 286): INFO [0/79]	0.9485(0.9485)	0.7295(0.7295)	1.467(1.467)	59.38(59.38)\
[2023-09-14 07:23:37 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2677)	0.0003(0.0668)	1.387(1.506)	56.25(58.24)\
[2023-09-14 07:23:39 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2345)	0.0004(0.0353)	1.422(1.492)	64.06(58.18)\
[2023-09-14 07:23:41 10splitTasks](trainer.py 286): INFO [30/79]	0.1955(0.2223)	0.0003(0.0241)	1.587(1.461)	45.31(57.76)\
[2023-09-14 07:23:43 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2159)	0.0003(0.0183)	1.745(1.457)	39.06(57.09)\
[2023-09-14 07:23:45 10splitTasks](trainer.py 286): INFO [50/79]	0.1952(0.2120)	0.0004(0.0148)	1.191(1.444)	64.06(57.78)\
[2023-09-14 07:23:47 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2095)	0.0004(0.0124)	1.175(1.427)	67.19(58.12)\
[2023-09-14 07:23:49 10splitTasks](trainer.py 286): INFO [70/79]	0.1977(0.2078)	0.0009(0.0108)	1.482(1.430)	56.25(57.90)\
[2023-09-14 07:23:50 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2049)	0.0001(0.0098)	2.677(1.435)	12.50(57.90)\
[2023-09-14 07:23:50 10splitTasks](trainer.py 288): INFO  * Train Acc 57.900\
[2023-09-14 07:23:52 10splitTasks](trainer.py 147): INFO  * Val Acc 51.000, Total time 1.51\
[2023-09-14 07:23:52 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:23:52 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:23:52 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:23:53 10splitTasks](trainer.py 286): INFO [0/79]	0.8837(0.8837)	0.6746(0.6746)	1.328(1.328)	65.62(65.62)\
[2023-09-14 07:23:55 10splitTasks](trainer.py 286): INFO [10/79]	0.1977(0.2615)	0.0003(0.0620)	1.477(1.506)	51.56(56.68)\
[2023-09-14 07:23:57 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2307)	0.0003(0.0327)	1.596(1.498)	56.25(56.92)\
[2023-09-14 07:23:59 10splitTasks](trainer.py 286): INFO [30/79]	0.1951(0.2201)	0.0003(0.0223)	1.438(1.489)	60.94(56.35)\
[2023-09-14 07:24:01 10splitTasks](trainer.py 286): INFO [40/79]	0.1953(0.2145)	0.0003(0.0170)	1.440(1.487)	56.25(56.52)\
[2023-09-14 07:24:02 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2109)	0.0003(0.0137)	1.213(1.468)	65.62(57.14)\
[2023-09-14 07:24:04 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2088)	0.0003(0.0115)	1.342(1.458)	64.06(57.56)\
[2023-09-14 07:24:06 10splitTasks](trainer.py 286): INFO [70/79]	0.1967(0.2070)	0.0009(0.0100)	1.335(1.452)	62.50(57.42)\
[2023-09-14 07:24:08 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2042)	0.0001(0.0090)	1.776(1.444)	50.00(57.80)\
[2023-09-14 07:24:08 10splitTasks](trainer.py 288): INFO  * Train Acc 57.800\
[2023-09-14 07:24:09 10splitTasks](trainer.py 147): INFO  * Val Acc 54.400, Total time 1.55\
[2023-09-14 07:24:09 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:24:09 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:24:09 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:24:10 10splitTasks](trainer.py 286): INFO [0/79]	0.8961(0.8961)	0.6958(0.6958)	1.330(1.330)	62.50(62.50)\
[2023-09-14 07:24:12 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2601)	0.0003(0.0637)	1.341(1.339)	56.25(61.08)\
[2023-09-14 07:24:14 10splitTasks](trainer.py 286): INFO [20/79]	0.1952(0.2300)	0.0003(0.0335)	1.221(1.374)	65.62(59.08)\
[2023-09-14 07:24:16 10splitTasks](trainer.py 286): INFO [30/79]	0.1960(0.2194)	0.0003(0.0229)	1.618(1.380)	59.38(60.03)\
[2023-09-14 07:24:18 10splitTasks](trainer.py 286): INFO [40/79]	0.1972(0.2140)	0.0004(0.0174)	1.505(1.397)	48.44(59.26)\
[2023-09-14 07:24:20 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2106)	0.0003(0.0141)	1.609(1.403)	54.69(59.10)\
[2023-09-14 07:24:22 10splitTasks](trainer.py 286): INFO [60/79]	0.1958(0.2083)	0.0003(0.0118)	1.579(1.412)	43.75(58.40)\
[2023-09-14 07:24:24 10splitTasks](trainer.py 286): INFO [70/79]	0.1988(0.2068)	0.0012(0.0103)	1.456(1.413)	60.94(58.60)\
[2023-09-14 07:24:26 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2040)	0.0001(0.0093)	2.383(1.412)	37.50(58.60)\
[2023-09-14 07:24:26 10splitTasks](trainer.py 288): INFO  * Train Acc 58.600\
[2023-09-14 07:24:27 10splitTasks](trainer.py 147): INFO  * Val Acc 52.600, Total time 1.57\
[2023-09-14 07:24:27 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:24:27 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:24:27 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:24:28 10splitTasks](trainer.py 286): INFO [0/79]	0.9450(0.9450)	0.7457(0.7457)	1.404(1.404)	62.50(62.50)\
[2023-09-14 07:24:30 10splitTasks](trainer.py 286): INFO [10/79]	0.1972(0.2672)	0.0004(0.0682)	1.392(1.456)	57.81(57.24)\
[2023-09-14 07:24:32 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2342)	0.0004(0.0360)	1.119(1.407)	75.00(58.93)\
[2023-09-14 07:24:34 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2225)	0.0003(0.0245)	1.133(1.385)	68.75(59.83)\
[2023-09-14 07:24:36 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2165)	0.0003(0.0187)	1.431(1.381)	59.38(60.56)\
[2023-09-14 07:24:38 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2128)	0.0003(0.0151)	1.613(1.385)	45.31(60.54)\
[2023-09-14 07:24:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2101)	0.0004(0.0127)	1.167(1.387)	71.88(60.27)\
[2023-09-14 07:24:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1980(0.2084)	0.0009(0.0110)	1.536(1.391)	46.88(59.90)\
[2023-09-14 07:24:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2054)	0.0001(0.0099)	1.856(1.387)	37.50(59.96)\
[2023-09-14 07:24:44 10splitTasks](trainer.py 288): INFO  * Train Acc 59.960\
[2023-09-14 07:24:45 10splitTasks](trainer.py 147): INFO  * Val Acc 50.400, Total time 1.56\
[2023-09-14 07:24:45 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:24:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:24:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:24:46 10splitTasks](trainer.py 286): INFO [0/79]	0.9273(0.9273)	0.7070(0.7070)	1.325(1.325)	59.38(59.38)\
[2023-09-14 07:24:48 10splitTasks](trainer.py 286): INFO [10/79]	0.1962(0.2634)	0.0004(0.0647)	1.400(1.383)	62.50(61.65)\
[2023-09-14 07:24:50 10splitTasks](trainer.py 286): INFO [20/79]	0.1960(0.2316)	0.0003(0.0341)	1.251(1.412)	59.38(60.42)\
[2023-09-14 07:24:52 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2203)	0.0003(0.0232)	1.468(1.406)	54.69(59.93)\
[2023-09-14 07:24:54 10splitTasks](trainer.py 286): INFO [40/79]	0.2099(0.2148)	0.0005(0.0177)	1.433(1.405)	59.38(59.87)\
[2023-09-14 07:24:56 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2112)	0.0004(0.0143)	1.549(1.402)	64.06(60.08)\
[2023-09-14 07:24:58 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2087)	0.0003(0.0120)	1.421(1.395)	50.00(59.89)\
[2023-09-14 07:25:00 10splitTasks](trainer.py 286): INFO [70/79]	0.1993(0.2072)	0.0010(0.0104)	1.448(1.403)	62.50(59.95)\
[2023-09-14 07:25:01 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2044)	0.0001(0.0094)	1.573(1.409)	50.00(59.82)\
[2023-09-14 07:25:01 10splitTasks](trainer.py 288): INFO  * Train Acc 59.820\
[2023-09-14 07:25:03 10splitTasks](trainer.py 147): INFO  * Val Acc 54.800, Total time 1.75\
[2023-09-14 07:25:03 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:25:03 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:25:03 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:25:04 10splitTasks](trainer.py 286): INFO [0/79]	0.9350(0.9350)	0.7355(0.7355)	1.220(1.220)	71.88(71.88)\
[2023-09-14 07:25:06 10splitTasks](trainer.py 286): INFO [10/79]	0.1951(0.2648)	0.0003(0.0674)	1.394(1.404)	64.06(62.78)\
[2023-09-14 07:25:08 10splitTasks](trainer.py 286): INFO [20/79]	0.1983(0.2325)	0.0004(0.0355)	1.547(1.369)	54.69(62.50)\
[2023-09-14 07:25:10 10splitTasks](trainer.py 286): INFO [30/79]	0.1971(0.2210)	0.0004(0.0242)	1.488(1.409)	62.50(60.58)\
[2023-09-14 07:25:12 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2151)	0.0004(0.0184)	1.224(1.391)	65.62(60.75)\
[2023-09-14 07:25:14 10splitTasks](trainer.py 286): INFO [50/79]	0.2030(0.2118)	0.0010(0.0149)	1.494(1.396)	53.12(60.36)\
[2023-09-14 07:25:16 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2094)	0.0004(0.0125)	1.600(1.394)	45.31(60.30)\
[2023-09-14 07:25:18 10splitTasks](trainer.py 286): INFO [70/79]	0.1963(0.2076)	0.0010(0.0109)	1.509(1.397)	53.12(60.50)\
[2023-09-14 07:25:19 10splitTasks](trainer.py 286): INFO [78/79]	0.0704(0.2048)	0.0001(0.0098)	1.620(1.399)	62.50(60.56)\
[2023-09-14 07:25:19 10splitTasks](trainer.py 288): INFO  * Train Acc 60.560\
[2023-09-14 07:25:21 10splitTasks](trainer.py 147): INFO  * Val Acc 52.400, Total time 1.62\
[2023-09-14 07:25:21 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:25:21 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:25:21 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:25:22 10splitTasks](trainer.py 286): INFO [0/79]	0.9569(0.9569)	0.7576(0.7576)	1.663(1.663)	56.25(56.25)\
[2023-09-14 07:25:24 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2680)	0.0003(0.0693)	1.699(1.530)	51.56(56.39)\
[2023-09-14 07:25:26 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2358)	0.0003(0.0366)	1.414(1.457)	59.38(59.30)\
[2023-09-14 07:25:28 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2231)	0.0003(0.0249)	1.366(1.435)	67.19(60.28)\
[2023-09-14 07:25:30 10splitTasks](trainer.py 286): INFO [40/79]	0.1981(0.2178)	0.0003(0.0191)	1.476(1.422)	59.38(60.67)\
[2023-09-14 07:25:32 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2139)	0.0004(0.0155)	1.379(1.410)	53.12(61.03)\
[2023-09-14 07:25:34 10splitTasks](trainer.py 286): INFO [60/79]	0.1968(0.2111)	0.0003(0.0130)	1.545(1.403)	53.12(61.04)\
[2023-09-14 07:25:36 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2094)	0.0011(0.0113)	1.404(1.414)	62.50(60.48)\
[2023-09-14 07:25:37 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2063)	0.0001(0.0102)	1.138(1.415)	75.00(60.40)\
[2023-09-14 07:25:37 10splitTasks](trainer.py 288): INFO  * Train Acc 60.400\
[2023-09-14 07:25:39 10splitTasks](trainer.py 147): INFO  * Val Acc 56.400, Total time 1.56\
[2023-09-14 07:25:39 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:25:39 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:25:39 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:25:40 10splitTasks](trainer.py 286): INFO [0/79]	0.9760(0.9760)	0.7529(0.7529)	1.291(1.291)	65.62(65.62)\
[2023-09-14 07:25:42 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2673)	0.0003(0.0688)	1.368(1.370)	64.06(61.51)\
[2023-09-14 07:25:44 10splitTasks](trainer.py 286): INFO [20/79]	0.1986(0.2346)	0.0005(0.0363)	1.188(1.388)	67.19(60.94)\
[2023-09-14 07:25:46 10splitTasks](trainer.py 286): INFO [30/79]	0.1964(0.2225)	0.0003(0.0248)	1.519(1.398)	56.25(60.69)\
[2023-09-14 07:25:48 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2163)	0.0004(0.0188)	1.659(1.398)	45.31(60.59)\
[2023-09-14 07:25:50 10splitTasks](trainer.py 286): INFO [50/79]	0.1987(0.2127)	0.0019(0.0153)	1.334(1.408)	56.25(59.99)\
[2023-09-14 07:25:52 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2102)	0.0003(0.0128)	1.411(1.414)	54.69(60.17)\
[2023-09-14 07:25:54 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2085)	0.0011(0.0111)	1.453(1.416)	56.25(60.08)\
[2023-09-14 07:25:55 10splitTasks](trainer.py 286): INFO [78/79]	0.0706(0.2056)	0.0002(0.0100)	2.689(1.415)	25.00(60.06)\
[2023-09-14 07:25:55 10splitTasks](trainer.py 288): INFO  * Train Acc 60.060\
[2023-09-14 07:25:57 10splitTasks](trainer.py 147): INFO  * Val Acc 50.800, Total time 1.53\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-5.pth\
=> Save Done\
[2023-09-14 07:25:57 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:25:58 10splitTasks](trainer.py 147): INFO  * Val Acc 31.800, Total time 1.53\
[2023-09-14 07:25:58 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:26:00 10splitTasks](trainer.py 147): INFO  * Val Acc 34.200, Total time 1.62\
[2023-09-14 07:26:00 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:26:02 10splitTasks](trainer.py 147): INFO  * Val Acc 37.600, Total time 1.57\
[2023-09-14 07:26:02 10splitTasks](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 07:26:03 10splitTasks](trainer.py 147): INFO  * Val Acc 42.600, Total time 1.83\
[2023-09-14 07:26:03 10splitTasks](iBatchLearn.py 131): INFO validation split name:4\
[2023-09-14 07:26:05 10splitTasks](trainer.py 147): INFO  * Val Acc 41.200, Total time 1.50\
[2023-09-14 07:26:05 10splitTasks](iBatchLearn.py 131): INFO validation split name:5\
[2023-09-14 07:26:07 10splitTasks](trainer.py 147): INFO  * Val Acc 50.800, Total time 1.56\
[2023-09-14 07:26:07 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:26:07 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:26:07 10splitTasks](iBatchLearn.py 155): INFO Acc:39.70000014750163; BWT:-18.240000033569334;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:26:11 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:26:11 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:26:11 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 5, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-5.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_5.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:26:12 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-5.pth\
[2023-09-14 07:26:12 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:26:14 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:26:14 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:26:14 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:26:18 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:26:21 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 07:26:23 10splitTasks](iBatchLearn.py 167): INFO test split name:3\
[2023-09-14 07:26:26 10splitTasks](iBatchLearn.py 167): INFO test split name:4\
[2023-09-14 07:26:29 10splitTasks](iBatchLearn.py 167): INFO test split name:5\
--------------------------------Official Evaluation--------------------------------\
5 23.336666666666666\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:26:38 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:26:38 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:26:38 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 6, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-5.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-6.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-5.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-6.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:26:39 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-5.pth\
[2023-09-14 07:26:39 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:26:41 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:26:41 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:26:41 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:26:41 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:26:41 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:26:41 10splitTasks](iBatchLearn.py 92): INFO ====================== 6 =======================\
[2023-09-14 07:26:41 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:26:41 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:26:41 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:26:41 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:26:41 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:26:44 10splitTasks](trainer.py 286): INFO [0/79]	3.5106(3.5106)	0.8495(0.8495)	2.474(2.474)	4.69(4.69)\
[2023-09-14 07:26:46 10splitTasks](trainer.py 286): INFO [10/79]	0.1977(0.4996)	0.0003(0.0776)	1.937(2.268)	45.31(16.76)\
[2023-09-14 07:26:48 10splitTasks](trainer.py 286): INFO [20/79]	0.1977(0.3565)	0.0004(0.0409)	1.653(2.075)	35.94(25.67)\
[2023-09-14 07:26:50 10splitTasks](trainer.py 286): INFO [30/79]	0.2062(0.3054)	0.0006(0.0279)	1.728(1.966)	45.31(31.25)\
[2023-09-14 07:26:52 10splitTasks](trainer.py 286): INFO [40/79]	0.1967(0.2789)	0.0004(0.0212)	1.566(1.859)	51.56(35.25)\
[2023-09-14 07:26:54 10splitTasks](trainer.py 286): INFO [50/79]	0.1992(0.2630)	0.0006(0.0171)	1.294(1.789)	60.94(38.17)\
[2023-09-14 07:26:56 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2523)	0.0004(0.0144)	1.440(1.722)	59.38(40.70)\
[2023-09-14 07:26:58 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2446)	0.0010(0.0125)	1.437(1.673)	46.88(42.67)\
[2023-09-14 07:27:00 10splitTasks](trainer.py 286): INFO [78/79]	0.2192(0.2399)	0.0001(0.0112)	1.199(1.644)	50.00(43.88)\
[2023-09-14 07:27:00 10splitTasks](trainer.py 288): INFO  * Train Acc 43.880\
[2023-09-14 07:27:02 10splitTasks](trainer.py 147): INFO  * Val Acc 60.800, Total time 1.88\
[2023-09-14 07:27:02 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:27:02 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:27:02 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:27:03 10splitTasks](trainer.py 286): INFO [0/79]	1.0614(1.0614)	0.8604(0.8604)	1.356(1.356)	59.38(59.38)\
[2023-09-14 07:27:05 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2766)	0.0004(0.0787)	1.333(1.265)	53.12(58.66)\
[2023-09-14 07:27:07 10splitTasks](trainer.py 286): INFO [20/79]	0.1979(0.2391)	0.0004(0.0414)	1.337(1.288)	46.88(56.92)\
[2023-09-14 07:27:09 10splitTasks](trainer.py 286): INFO [30/79]	0.1954(0.2259)	0.0003(0.0282)	1.253(1.310)	57.81(56.55)\
[2023-09-14 07:27:11 10splitTasks](trainer.py 286): INFO [40/79]	0.1968(0.2188)	0.0005(0.0215)	1.174(1.311)	65.62(57.05)\
[2023-09-14 07:27:13 10splitTasks](trainer.py 286): INFO [50/79]	0.1972(0.2145)	0.0015(0.0174)	1.126(1.309)	65.62(57.60)\
[2023-09-14 07:27:15 10splitTasks](trainer.py 286): INFO [60/79]	0.2004(0.2116)	0.0008(0.0146)	1.490(1.291)	59.38(58.30)\
[2023-09-14 07:27:17 10splitTasks](trainer.py 286): INFO [70/79]	0.2132(0.2101)	0.0009(0.0126)	1.300(1.286)	56.25(58.65)\
[2023-09-14 07:27:18 10splitTasks](trainer.py 286): INFO [78/79]	0.0721(0.2070)	0.0001(0.0114)	1.444(1.282)	50.00(58.98)\
[2023-09-14 07:27:18 10splitTasks](trainer.py 288): INFO  * Train Acc 58.980\
[2023-09-14 07:27:20 10splitTasks](trainer.py 147): INFO  * Val Acc 63.000, Total time 1.72\
[2023-09-14 07:27:20 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:27:20 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:27:20 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:27:21 10splitTasks](trainer.py 286): INFO [0/79]	1.0310(1.0310)	0.8096(0.8096)	1.069(1.069)	67.19(67.19)\
[2023-09-14 07:27:23 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2736)	0.0004(0.0742)	1.116(1.219)	60.94(60.65)\
[2023-09-14 07:27:25 10splitTasks](trainer.py 286): INFO [20/79]	0.1970(0.2378)	0.0005(0.0391)	1.064(1.208)	65.62(61.68)\
[2023-09-14 07:27:27 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2247)	0.0004(0.0266)	1.135(1.202)	68.75(62.90)\
[2023-09-14 07:27:29 10splitTasks](trainer.py 286): INFO [40/79]	0.1989(0.2181)	0.0004(0.0203)	1.544(1.224)	48.44(62.20)\
[2023-09-14 07:27:31 10splitTasks](trainer.py 286): INFO [50/79]	0.1953(0.2138)	0.0004(0.0164)	1.153(1.219)	62.50(62.62)\
[2023-09-14 07:27:33 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2109)	0.0003(0.0138)	1.220(1.226)	62.50(62.73)\
[2023-09-14 07:27:35 10splitTasks](trainer.py 286): INFO [70/79]	0.1956(0.2089)	0.0010(0.0119)	1.747(1.241)	46.88(62.08)\
[2023-09-14 07:27:36 10splitTasks](trainer.py 286): INFO [78/79]	0.0712(0.2059)	0.0001(0.0108)	0.737(1.239)	100.00(62.18)\
[2023-09-14 07:27:36 10splitTasks](trainer.py 288): INFO  * Train Acc 62.180\
[2023-09-14 07:27:38 10splitTasks](trainer.py 147): INFO  * Val Acc 63.200, Total time 1.77\
[2023-09-14 07:27:38 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:27:38 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:27:38 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:27:39 10splitTasks](trainer.py 286): INFO [0/79]	0.8959(0.8959)	0.6664(0.6664)	1.007(1.007)	65.62(65.62)\
[2023-09-14 07:27:41 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2611)	0.0003(0.0610)	1.235(1.217)	68.75(62.93)\
[2023-09-14 07:27:43 10splitTasks](trainer.py 286): INFO [20/79]	0.1956(0.2305)	0.0004(0.0322)	1.234(1.250)	56.25(61.98)\
[2023-09-14 07:27:45 10splitTasks](trainer.py 286): INFO [30/79]	0.1993(0.2197)	0.0006(0.0219)	1.085(1.251)	67.19(61.34)\
[2023-09-14 07:27:47 10splitTasks](trainer.py 286): INFO [40/79]	0.1995(0.2142)	0.0004(0.0167)	1.156(1.222)	65.62(62.65)\
[2023-09-14 07:27:49 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2114)	0.0003(0.0135)	1.034(1.222)	73.44(62.68)\
[2023-09-14 07:27:51 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2090)	0.0003(0.0114)	1.298(1.228)	56.25(62.70)\
[2023-09-14 07:27:53 10splitTasks](trainer.py 286): INFO [70/79]	0.1974(0.2073)	0.0008(0.0098)	1.377(1.232)	59.38(62.90)\
[2023-09-14 07:27:54 10splitTasks](trainer.py 286): INFO [78/79]	0.0710(0.2045)	0.0001(0.0089)	1.617(1.225)	37.50(63.18)\
[2023-09-14 07:27:54 10splitTasks](trainer.py 288): INFO  * Train Acc 63.180\
[2023-09-14 07:27:56 10splitTasks](trainer.py 147): INFO  * Val Acc 62.800, Total time 1.78\
[2023-09-14 07:27:56 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:27:56 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:27:56 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:27:57 10splitTasks](trainer.py 286): INFO [0/79]	1.0061(1.0061)	0.8071(0.8071)	1.066(1.066)	64.06(64.06)\
[2023-09-14 07:27:59 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2714)	0.0003(0.0738)	1.428(1.201)	56.25(65.62)\
[2023-09-14 07:28:01 10splitTasks](trainer.py 286): INFO [20/79]	0.1962(0.2360)	0.0003(0.0389)	1.305(1.216)	65.62(67.04)\
[2023-09-14 07:28:03 10splitTasks](trainer.py 286): INFO [30/79]	0.2188(0.2239)	0.0007(0.0265)	1.356(1.229)	62.50(65.73)\
[2023-09-14 07:28:05 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2173)	0.0004(0.0201)	1.163(1.225)	67.19(65.51)\
[2023-09-14 07:28:07 10splitTasks](trainer.py 286): INFO [50/79]	0.2028(0.2134)	0.0006(0.0163)	1.086(1.209)	68.75(65.50)\
[2023-09-14 07:28:09 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2110)	0.0002(0.0137)	1.162(1.212)	64.06(65.09)\
[2023-09-14 07:28:11 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2090)	0.0009(0.0118)	0.959(1.210)	70.31(65.34)\
[2023-09-14 07:28:12 10splitTasks](trainer.py 286): INFO [78/79]	0.0709(0.2061)	0.0001(0.0107)	2.051(1.222)	50.00(65.06)\
[2023-09-14 07:28:12 10splitTasks](trainer.py 288): INFO  * Train Acc 65.060\
[2023-09-14 07:28:14 10splitTasks](trainer.py 147): INFO  * Val Acc 66.400, Total time 1.78\
[2023-09-14 07:28:14 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:28:14 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:28:14 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:28:15 10splitTasks](trainer.py 286): INFO [0/79]	0.9469(0.9469)	0.7463(0.7463)	1.078(1.078)	71.88(71.88)\
[2023-09-14 07:28:17 10splitTasks](trainer.py 286): INFO [10/79]	0.1968(0.2670)	0.0004(0.0683)	1.432(1.255)	56.25(62.22)\
[2023-09-14 07:28:19 10splitTasks](trainer.py 286): INFO [20/79]	0.1967(0.2344)	0.0004(0.0360)	1.253(1.219)	65.62(64.96)\
[2023-09-14 07:28:21 10splitTasks](trainer.py 286): INFO [30/79]	0.1955(0.2225)	0.0003(0.0245)	1.370(1.253)	54.69(63.86)\
[2023-09-14 07:28:23 10splitTasks](trainer.py 286): INFO [40/79]	0.1974(0.2162)	0.0004(0.0186)	1.129(1.268)	62.50(63.26)\
[2023-09-14 07:28:25 10splitTasks](trainer.py 286): INFO [50/79]	0.1984(0.2126)	0.0008(0.0151)	1.160(1.257)	67.19(63.82)\
[2023-09-14 07:28:27 10splitTasks](trainer.py 286): INFO [60/79]	0.1983(0.2103)	0.0006(0.0127)	1.382(1.248)	59.38(64.11)\
[2023-09-14 07:28:29 10splitTasks](trainer.py 286): INFO [70/79]	0.1984(0.2085)	0.0009(0.0110)	1.124(1.235)	67.19(64.74)\
[2023-09-14 07:28:30 10splitTasks](trainer.py 286): INFO [78/79]	0.0709(0.2056)	0.0001(0.0099)	1.867(1.232)	50.00(65.06)\
[2023-09-14 07:28:30 10splitTasks](trainer.py 288): INFO  * Train Acc 65.060\
[2023-09-14 07:28:32 10splitTasks](trainer.py 147): INFO  * Val Acc 68.800, Total time 1.79\
[2023-09-14 07:28:32 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:28:32 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:28:32 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:28:33 10splitTasks](trainer.py 286): INFO [0/79]	0.9549(0.9549)	0.7502(0.7502)	1.104(1.104)	71.88(71.88)\
[2023-09-14 07:28:35 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2721)	0.0003(0.0738)	1.281(1.192)	62.50(66.34)\
[2023-09-14 07:28:37 10splitTasks](trainer.py 286): INFO [20/79]	0.1972(0.2368)	0.0003(0.0389)	1.092(1.227)	68.75(65.18)\
[2023-09-14 07:28:39 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2244)	0.0004(0.0265)	1.004(1.226)	71.88(65.07)\
[2023-09-14 07:28:41 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2176)	0.0003(0.0201)	0.983(1.212)	75.00(65.93)\
[2023-09-14 07:28:43 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2135)	0.0003(0.0163)	1.107(1.195)	73.44(66.51)\
[2023-09-14 07:28:45 10splitTasks](trainer.py 286): INFO [60/79]	0.1966(0.2109)	0.0003(0.0137)	1.230(1.193)	70.31(66.83)\
[2023-09-14 07:28:47 10splitTasks](trainer.py 286): INFO [70/79]	0.1977(0.2089)	0.0009(0.0118)	0.998(1.202)	76.56(66.64)\
[2023-09-14 07:28:48 10splitTasks](trainer.py 286): INFO [78/79]	0.0710(0.2060)	0.0002(0.0106)	0.960(1.204)	62.50(66.58)\
[2023-09-14 07:28:48 10splitTasks](trainer.py 288): INFO  * Train Acc 66.580\
[2023-09-14 07:28:50 10splitTasks](trainer.py 147): INFO  * Val Acc 66.600, Total time 1.75\
[2023-09-14 07:28:50 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:28:50 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:28:50 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:28:51 10splitTasks](trainer.py 286): INFO [0/79]	0.9357(0.9357)	0.7353(0.7353)	1.034(1.034)	71.88(71.88)\
[2023-09-14 07:28:53 10splitTasks](trainer.py 286): INFO [10/79]	0.1968(0.2646)	0.0003(0.0673)	1.269(1.189)	57.81(68.32)\
[2023-09-14 07:28:55 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2331)	0.0004(0.0355)	0.850(1.155)	82.81(69.79)\
[2023-09-14 07:28:57 10splitTasks](trainer.py 286): INFO [30/79]	0.1981(0.2217)	0.0003(0.0242)	1.216(1.152)	64.06(69.91)\
[2023-09-14 07:28:59 10splitTasks](trainer.py 286): INFO [40/79]	0.1973(0.2163)	0.0003(0.0184)	1.229(1.163)	65.62(69.70)\
[2023-09-14 07:29:01 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2126)	0.0003(0.0149)	1.048(1.155)	73.44(69.70)\
[2023-09-14 07:29:03 10splitTasks](trainer.py 286): INFO [60/79]	0.1969(0.2103)	0.0004(0.0125)	1.093(1.155)	70.31(69.26)\
[2023-09-14 07:29:05 10splitTasks](trainer.py 286): INFO [70/79]	0.2044(0.2087)	0.0013(0.0108)	1.184(1.158)	62.50(68.99)\
[2023-09-14 07:29:06 10splitTasks](trainer.py 286): INFO [78/79]	0.0721(0.2059)	0.0002(0.0098)	1.355(1.158)	75.00(69.02)\
[2023-09-14 07:29:07 10splitTasks](trainer.py 288): INFO  * Train Acc 69.020\
[2023-09-14 07:29:08 10splitTasks](trainer.py 147): INFO  * Val Acc 72.000, Total time 1.72\
[2023-09-14 07:29:08 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:29:08 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:29:08 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:29:09 10splitTasks](trainer.py 286): INFO [0/79]	1.0300(1.0300)	0.8314(0.8314)	0.976(0.976)	76.56(76.56)\
[2023-09-14 07:29:11 10splitTasks](trainer.py 286): INFO [10/79]	0.1968(0.2741)	0.0004(0.0760)	1.250(1.128)	60.94(70.45)\
[2023-09-14 07:29:13 10splitTasks](trainer.py 286): INFO [20/79]	0.1967(0.2377)	0.0003(0.0400)	1.182(1.144)	68.75(69.35)\
[2023-09-14 07:29:15 10splitTasks](trainer.py 286): INFO [30/79]	0.2003(0.2246)	0.0006(0.0273)	1.135(1.157)	65.62(68.60)\
[2023-09-14 07:29:17 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2181)	0.0003(0.0207)	1.095(1.154)	64.06(68.48)\
[2023-09-14 07:29:19 10splitTasks](trainer.py 286): INFO [50/79]	0.1968(0.2143)	0.0003(0.0168)	1.115(1.146)	68.75(68.84)\
[2023-09-14 07:29:21 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2117)	0.0004(0.0141)	0.888(1.161)	82.81(68.67)\
[2023-09-14 07:29:23 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2096)	0.0010(0.0122)	1.673(1.169)	57.81(68.60)\
[2023-09-14 07:29:25 10splitTasks](trainer.py 286): INFO [78/79]	0.0710(0.2066)	0.0001(0.0110)	1.284(1.175)	50.00(68.36)\
[2023-09-14 07:29:25 10splitTasks](trainer.py 288): INFO  * Train Acc 68.360\
[2023-09-14 07:29:26 10splitTasks](trainer.py 147): INFO  * Val Acc 59.600, Total time 1.74\
[2023-09-14 07:29:26 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:29:26 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:29:26 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:29:27 10splitTasks](trainer.py 286): INFO [0/79]	0.9685(0.9685)	0.7681(0.7681)	1.140(1.140)	67.19(67.19)\
[2023-09-14 07:29:29 10splitTasks](trainer.py 286): INFO [10/79]	0.1968(0.2686)	0.0003(0.0703)	1.189(1.203)	62.50(66.19)\
[2023-09-14 07:29:31 10splitTasks](trainer.py 286): INFO [20/79]	0.2017(0.2359)	0.0004(0.0371)	1.229(1.157)	70.31(68.82)\
[2023-09-14 07:29:33 10splitTasks](trainer.py 286): INFO [30/79]	0.1978(0.2237)	0.0005(0.0253)	1.177(1.137)	67.19(69.91)\
[2023-09-14 07:29:35 10splitTasks](trainer.py 286): INFO [40/79]	0.1960(0.2171)	0.0004(0.0192)	1.476(1.133)	59.38(70.24)\
[2023-09-14 07:29:37 10splitTasks](trainer.py 286): INFO [50/79]	0.1968(0.2132)	0.0004(0.0155)	1.251(1.149)	67.19(70.07)\
[2023-09-14 07:29:39 10splitTasks](trainer.py 286): INFO [60/79]	0.1989(0.2106)	0.0005(0.0131)	1.312(1.155)	65.62(69.85)\
[2023-09-14 07:29:41 10splitTasks](trainer.py 286): INFO [70/79]	0.2024(0.2088)	0.0011(0.0113)	1.219(1.159)	71.88(69.74)\
[2023-09-14 07:29:43 10splitTasks](trainer.py 286): INFO [78/79]	0.0726(0.2059)	0.0002(0.0102)	2.458(1.166)	50.00(69.46)\
[2023-09-14 07:29:43 10splitTasks](trainer.py 288): INFO  * Train Acc 69.460\
[2023-09-14 07:29:45 10splitTasks](trainer.py 147): INFO  * Val Acc 68.600, Total time 1.79\
[2023-09-14 07:29:45 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:29:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:29:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:29:45 10splitTasks](trainer.py 286): INFO [0/79]	0.9112(0.9112)	0.7123(0.7123)	1.049(1.049)	67.19(67.19)\
[2023-09-14 07:29:47 10splitTasks](trainer.py 286): INFO [10/79]	0.1968(0.2648)	0.0003(0.0656)	1.108(1.192)	70.31(66.76)\
[2023-09-14 07:29:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2325)	0.0003(0.0346)	1.070(1.175)	71.88(68.01)\
[2023-09-14 07:29:51 10splitTasks](trainer.py 286): INFO [30/79]	0.1974(0.2211)	0.0003(0.0236)	1.225(1.165)	73.44(69.25)\
[2023-09-14 07:29:53 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2157)	0.0004(0.0181)	1.121(1.174)	78.12(69.32)\
[2023-09-14 07:29:55 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2124)	0.0004(0.0146)	1.049(1.162)	71.88(69.85)\
[2023-09-14 07:29:57 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2098)	0.0003(0.0123)	1.009(1.149)	78.12(70.36)\
[2023-09-14 07:29:59 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2079)	0.0010(0.0106)	1.224(1.145)	64.06(70.47)\
[2023-09-14 07:30:01 10splitTasks](trainer.py 286): INFO [78/79]	0.0712(0.2051)	0.0001(0.0096)	1.647(1.149)	50.00(70.16)\
[2023-09-14 07:30:01 10splitTasks](trainer.py 288): INFO  * Train Acc 70.160\
[2023-09-14 07:30:03 10splitTasks](trainer.py 147): INFO  * Val Acc 66.600, Total time 1.77\
[2023-09-14 07:30:03 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:30:03 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:30:03 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:30:04 10splitTasks](trainer.py 286): INFO [0/79]	0.9220(0.9220)	0.7158(0.7158)	1.062(1.062)	70.31(70.31)\
[2023-09-14 07:30:05 10splitTasks](trainer.py 286): INFO [10/79]	0.1967(0.2640)	0.0004(0.0656)	1.082(1.191)	71.88(69.18)\
[2023-09-14 07:30:07 10splitTasks](trainer.py 286): INFO [20/79]	0.1957(0.2323)	0.0005(0.0346)	1.145(1.135)	73.44(70.31)\
[2023-09-14 07:30:09 10splitTasks](trainer.py 286): INFO [30/79]	0.2089(0.2212)	0.0006(0.0236)	1.233(1.143)	70.31(69.96)\
[2023-09-14 07:30:11 10splitTasks](trainer.py 286): INFO [40/79]	0.2007(0.2155)	0.0004(0.0179)	1.097(1.150)	73.44(69.70)\
[2023-09-14 07:30:13 10splitTasks](trainer.py 286): INFO [50/79]	0.2066(0.2122)	0.0006(0.0145)	1.302(1.152)	70.31(69.82)\
[2023-09-14 07:30:15 10splitTasks](trainer.py 286): INFO [60/79]	0.2015(0.2099)	0.0004(0.0122)	1.164(1.159)	68.75(69.67)\
[2023-09-14 07:30:17 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2081)	0.0011(0.0106)	1.058(1.157)	75.00(69.63)\
[2023-09-14 07:30:19 10splitTasks](trainer.py 286): INFO [78/79]	0.0711(0.2053)	0.0001(0.0095)	1.521(1.157)	75.00(69.66)\
[2023-09-14 07:30:19 10splitTasks](trainer.py 288): INFO  * Train Acc 69.660\
[2023-09-14 07:30:21 10splitTasks](trainer.py 147): INFO  * Val Acc 70.400, Total time 1.77\
[2023-09-14 07:30:21 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:30:21 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:30:21 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:30:22 10splitTasks](trainer.py 286): INFO [0/79]	0.9318(0.9318)	0.7325(0.7325)	1.261(1.261)	65.62(65.62)\
[2023-09-14 07:30:24 10splitTasks](trainer.py 286): INFO [10/79]	0.1970(0.2647)	0.0004(0.0671)	1.043(1.158)	70.31(71.16)\
[2023-09-14 07:30:26 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2325)	0.0004(0.0353)	1.197(1.166)	70.31(70.31)\
[2023-09-14 07:30:27 10splitTasks](trainer.py 286): INFO [30/79]	0.1968(0.2214)	0.0003(0.0243)	1.009(1.152)	79.69(70.82)\
[2023-09-14 07:30:29 10splitTasks](trainer.py 286): INFO [40/79]	0.1967(0.2157)	0.0003(0.0185)	1.447(1.143)	56.25(71.38)\
[2023-09-14 07:30:31 10splitTasks](trainer.py 286): INFO [50/79]	0.2002(0.2120)	0.0003(0.0150)	1.195(1.144)	71.88(71.57)\
[2023-09-14 07:30:33 10splitTasks](trainer.py 286): INFO [60/79]	0.1972(0.2096)	0.0003(0.0126)	1.053(1.153)	78.12(71.18)\
[2023-09-14 07:30:35 10splitTasks](trainer.py 286): INFO [70/79]	0.1987(0.2079)	0.0010(0.0109)	1.439(1.162)	60.94(70.73)\
[2023-09-14 07:30:37 10splitTasks](trainer.py 286): INFO [78/79]	0.0709(0.2051)	0.0001(0.0098)	1.674(1.170)	62.50(70.54)\
[2023-09-14 07:30:37 10splitTasks](trainer.py 288): INFO  * Train Acc 70.540\
[2023-09-14 07:30:39 10splitTasks](trainer.py 147): INFO  * Val Acc 62.200, Total time 1.75\
[2023-09-14 07:30:39 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:30:39 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:30:39 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:30:40 10splitTasks](trainer.py 286): INFO [0/79]	0.9620(0.9620)	0.7533(0.7533)	1.242(1.242)	70.31(70.31)\
[2023-09-14 07:30:42 10splitTasks](trainer.py 286): INFO [10/79]	0.1969(0.2669)	0.0004(0.0689)	1.083(1.219)	73.44(69.32)\
[2023-09-14 07:30:44 10splitTasks](trainer.py 286): INFO [20/79]	0.1970(0.2358)	0.0004(0.0363)	1.060(1.191)	76.56(70.76)\
[2023-09-14 07:30:46 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2233)	0.0003(0.0247)	1.104(1.170)	67.19(70.92)\
[2023-09-14 07:30:48 10splitTasks](trainer.py 286): INFO [40/79]	0.2040(0.2171)	0.0004(0.0188)	1.275(1.208)	70.31(70.08)\
[2023-09-14 07:30:50 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2132)	0.0003(0.0152)	1.286(1.200)	67.19(70.44)\
[2023-09-14 07:30:52 10splitTasks](trainer.py 286): INFO [60/79]	0.2052(0.2112)	0.0075(0.0129)	1.168(1.207)	70.31(70.21)\
[2023-09-14 07:30:54 10splitTasks](trainer.py 286): INFO [70/79]	0.1980(0.2097)	0.0010(0.0112)	1.270(1.208)	68.75(69.94)\
[2023-09-14 07:30:55 10splitTasks](trainer.py 286): INFO [78/79]	0.0726(0.2067)	0.0001(0.0101)	1.699(1.203)	50.00(70.14)\
[2023-09-14 07:30:55 10splitTasks](trainer.py 288): INFO  * Train Acc 70.140\
[2023-09-14 07:30:57 10splitTasks](trainer.py 147): INFO  * Val Acc 63.200, Total time 1.76\
[2023-09-14 07:30:57 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:30:57 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:30:57 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:30:58 10splitTasks](trainer.py 286): INFO [0/79]	0.9209(0.9209)	0.7220(0.7220)	1.170(1.170)	68.75(68.75)\
[2023-09-14 07:31:00 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2634)	0.0003(0.0660)	1.678(1.213)	59.38(69.18)\
[2023-09-14 07:31:02 10splitTasks](trainer.py 286): INFO [20/79]	0.1973(0.2330)	0.0003(0.0348)	1.162(1.194)	64.06(69.35)\
[2023-09-14 07:31:04 10splitTasks](trainer.py 286): INFO [30/79]	0.1967(0.2222)	0.0004(0.0237)	1.158(1.176)	68.75(70.46)\
[2023-09-14 07:31:06 10splitTasks](trainer.py 286): INFO [40/79]	0.1956(0.2161)	0.0004(0.0181)	1.161(1.182)	76.56(70.24)\
[2023-09-14 07:31:08 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2122)	0.0004(0.0146)	1.169(1.171)	65.62(70.68)\
[2023-09-14 07:31:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1966(0.2099)	0.0003(0.0123)	1.102(1.171)	76.56(70.82)\
[2023-09-14 07:31:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1986(0.2083)	0.0011(0.0106)	0.931(1.160)	84.38(71.35)\
[2023-09-14 07:31:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0708(0.2054)	0.0001(0.0096)	1.723(1.165)	37.50(71.18)\
[2023-09-14 07:31:13 10splitTasks](trainer.py 288): INFO  * Train Acc 71.180\
[2023-09-14 07:31:15 10splitTasks](trainer.py 147): INFO  * Val Acc 64.600, Total time 1.78\
[2023-09-14 07:31:15 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:31:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:31:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:31:16 10splitTasks](trainer.py 286): INFO [0/79]	0.9119(0.9119)	0.7076(0.7076)	1.096(1.096)	73.44(73.44)\
[2023-09-14 07:31:18 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2619)	0.0004(0.0648)	0.884(1.225)	81.25(68.75)\
[2023-09-14 07:31:20 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2310)	0.0004(0.0341)	1.315(1.210)	67.19(68.68)\
[2023-09-14 07:31:22 10splitTasks](trainer.py 286): INFO [30/79]	0.1972(0.2200)	0.0007(0.0233)	1.150(1.223)	67.19(68.75)\
[2023-09-14 07:31:24 10splitTasks](trainer.py 286): INFO [40/79]	0.1985(0.2144)	0.0003(0.0177)	1.242(1.207)	67.19(69.05)\
[2023-09-14 07:31:26 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2111)	0.0003(0.0143)	1.148(1.201)	76.56(69.67)\
[2023-09-14 07:31:28 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2092)	0.0003(0.0121)	1.296(1.196)	67.19(69.85)\
[2023-09-14 07:31:30 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2074)	0.0011(0.0104)	1.074(1.189)	73.44(70.55)\
[2023-09-14 07:31:31 10splitTasks](trainer.py 286): INFO [78/79]	0.0709(0.2046)	0.0001(0.0094)	1.182(1.190)	62.50(70.28)\
[2023-09-14 07:31:31 10splitTasks](trainer.py 288): INFO  * Train Acc 70.280\
[2023-09-14 07:31:33 10splitTasks](trainer.py 147): INFO  * Val Acc 68.800, Total time 1.78\
[2023-09-14 07:31:33 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:31:33 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:31:33 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:31:34 10splitTasks](trainer.py 286): INFO [0/79]	0.9451(0.9451)	0.7452(0.7452)	1.066(1.066)	78.12(78.12)\
[2023-09-14 07:31:36 10splitTasks](trainer.py 286): INFO [10/79]	0.1977(0.2658)	0.0003(0.0688)	1.187(1.212)	70.31(70.74)\
[2023-09-14 07:31:38 10splitTasks](trainer.py 286): INFO [20/79]	0.1967(0.2332)	0.0004(0.0363)	1.320(1.225)	73.44(70.39)\
[2023-09-14 07:31:40 10splitTasks](trainer.py 286): INFO [30/79]	0.2076(0.2219)	0.0005(0.0247)	1.116(1.187)	75.00(71.32)\
[2023-09-14 07:31:42 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2161)	0.0003(0.0188)	1.093(1.185)	76.56(71.57)\
[2023-09-14 07:31:44 10splitTasks](trainer.py 286): INFO [50/79]	0.2017(0.2126)	0.0003(0.0152)	1.183(1.174)	70.31(71.84)\
[2023-09-14 07:31:46 10splitTasks](trainer.py 286): INFO [60/79]	0.1968(0.2101)	0.0003(0.0128)	1.121(1.162)	70.31(72.16)\
[2023-09-14 07:31:48 10splitTasks](trainer.py 286): INFO [70/79]	0.1980(0.2082)	0.0010(0.0111)	0.816(1.155)	84.38(72.32)\
[2023-09-14 07:31:49 10splitTasks](trainer.py 286): INFO [78/79]	0.0708(0.2053)	0.0001(0.0100)	1.868(1.165)	50.00(72.30)\
[2023-09-14 07:31:49 10splitTasks](trainer.py 288): INFO  * Train Acc 72.300\
[2023-09-14 07:31:51 10splitTasks](trainer.py 147): INFO  * Val Acc 64.800, Total time 1.77\
[2023-09-14 07:31:51 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:31:51 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:31:51 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:31:52 10splitTasks](trainer.py 286): INFO [0/79]	0.8946(0.8946)	0.6874(0.6874)	1.211(1.211)	65.62(65.62)\
[2023-09-14 07:31:54 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2603)	0.0003(0.0629)	1.205(1.125)	68.75(72.30)\
[2023-09-14 07:31:56 10splitTasks](trainer.py 286): INFO [20/79]	0.1955(0.2309)	0.0004(0.0332)	1.272(1.137)	71.88(71.95)\
[2023-09-14 07:31:58 10splitTasks](trainer.py 286): INFO [30/79]	0.2094(0.2207)	0.0017(0.0227)	1.054(1.155)	78.12(71.67)\
[2023-09-14 07:32:00 10splitTasks](trainer.py 286): INFO [40/79]	0.1968(0.2151)	0.0004(0.0173)	0.990(1.159)	81.25(71.68)\
[2023-09-14 07:32:02 10splitTasks](trainer.py 286): INFO [50/79]	0.1967(0.2116)	0.0003(0.0140)	1.308(1.161)	65.62(71.63)\
[2023-09-14 07:32:04 10splitTasks](trainer.py 286): INFO [60/79]	0.1956(0.2093)	0.0004(0.0118)	1.188(1.166)	75.00(71.67)\
[2023-09-14 07:32:06 10splitTasks](trainer.py 286): INFO [70/79]	0.2006(0.2081)	0.0012(0.0102)	1.289(1.165)	57.81(71.70)\
[2023-09-14 07:32:07 10splitTasks](trainer.py 286): INFO [78/79]	0.0737(0.2054)	0.0001(0.0092)	1.653(1.168)	75.00(71.68)\
[2023-09-14 07:32:07 10splitTasks](trainer.py 288): INFO  * Train Acc 71.680\
[2023-09-14 07:32:09 10splitTasks](trainer.py 147): INFO  * Val Acc 66.000, Total time 1.81\
[2023-09-14 07:32:09 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:32:09 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:32:09 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:32:10 10splitTasks](trainer.py 286): INFO [0/79]	0.9546(0.9546)	0.7516(0.7516)	1.205(1.205)	73.44(73.44)\
[2023-09-14 07:32:12 10splitTasks](trainer.py 286): INFO [10/79]	0.1972(0.2665)	0.0003(0.0688)	1.013(1.206)	82.81(72.59)\
[2023-09-14 07:32:14 10splitTasks](trainer.py 286): INFO [20/79]	0.1982(0.2338)	0.0004(0.0362)	1.178(1.195)	73.44(72.25)\
[2023-09-14 07:32:16 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2224)	0.0003(0.0247)	0.918(1.187)	79.69(71.77)\
[2023-09-14 07:32:18 10splitTasks](trainer.py 286): INFO [40/79]	0.2105(0.2168)	0.0008(0.0188)	1.016(1.177)	75.00(71.91)\
[2023-09-14 07:32:20 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2129)	0.0004(0.0152)	0.978(1.158)	78.12(72.40)\
[2023-09-14 07:32:22 10splitTasks](trainer.py 286): INFO [60/79]	0.1955(0.2104)	0.0003(0.0128)	1.209(1.156)	71.88(72.23)\
[2023-09-14 07:32:24 10splitTasks](trainer.py 286): INFO [70/79]	0.1982(0.2086)	0.0010(0.0111)	1.165(1.157)	76.56(72.32)\
[2023-09-14 07:32:25 10splitTasks](trainer.py 286): INFO [78/79]	0.0711(0.2058)	0.0002(0.0100)	1.085(1.161)	87.50(71.92)\
[2023-09-14 07:32:25 10splitTasks](trainer.py 288): INFO  * Train Acc 71.920\
[2023-09-14 07:32:27 10splitTasks](trainer.py 147): INFO  * Val Acc 64.400, Total time 1.75\
[2023-09-14 07:32:27 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:32:27 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:32:27 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:32:28 10splitTasks](trainer.py 286): INFO [0/79]	1.0433(1.0433)	0.8418(0.8418)	1.250(1.250)	70.31(70.31)\
[2023-09-14 07:32:30 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2743)	0.0003(0.0770)	0.960(1.153)	78.12(69.89)\
[2023-09-14 07:32:32 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2375)	0.0003(0.0405)	1.607(1.177)	60.94(70.09)\
[2023-09-14 07:32:34 10splitTasks](trainer.py 286): INFO [30/79]	0.2019(0.2262)	0.0006(0.0276)	0.991(1.170)	76.56(70.11)\
[2023-09-14 07:32:36 10splitTasks](trainer.py 286): INFO [40/79]	0.1958(0.2190)	0.0003(0.0210)	1.095(1.149)	71.88(70.77)\
[2023-09-14 07:32:38 10splitTasks](trainer.py 286): INFO [50/79]	0.1982(0.2148)	0.0004(0.0170)	1.356(1.149)	64.06(71.32)\
[2023-09-14 07:32:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1988(0.2119)	0.0004(0.0143)	1.393(1.160)	62.50(71.21)\
[2023-09-14 07:32:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2098)	0.0010(0.0123)	1.034(1.157)	76.56(71.59)\
[2023-09-14 07:32:44 10splitTasks](trainer.py 286): INFO [78/79]	0.0704(0.2067)	0.0001(0.0111)	2.265(1.162)	37.50(71.60)\
[2023-09-14 07:32:44 10splitTasks](trainer.py 288): INFO  * Train Acc 71.600\
[2023-09-14 07:32:45 10splitTasks](trainer.py 147): INFO  * Val Acc 55.600, Total time 1.74\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-6.pth\
=> Save Done\
[2023-09-14 07:32:46 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:32:47 10splitTasks](trainer.py 147): INFO  * Val Acc 25.600, Total time 1.50\
[2023-09-14 07:32:47 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:32:49 10splitTasks](trainer.py 147): INFO  * Val Acc 26.400, Total time 1.60\
[2023-09-14 07:32:49 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:32:50 10splitTasks](trainer.py 147): INFO  * Val Acc 27.600, Total time 1.58\
[2023-09-14 07:32:50 10splitTasks](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 07:32:52 10splitTasks](trainer.py 147): INFO  * Val Acc 29.000, Total time 1.81\
[2023-09-14 07:32:52 10splitTasks](iBatchLearn.py 131): INFO validation split name:4\
[2023-09-14 07:32:54 10splitTasks](trainer.py 147): INFO  * Val Acc 29.200, Total time 1.55\
[2023-09-14 07:32:54 10splitTasks](iBatchLearn.py 131): INFO validation split name:5\
[2023-09-14 07:32:55 10splitTasks](trainer.py 147): INFO  * Val Acc 36.800, Total time 1.62\
[2023-09-14 07:32:55 10splitTasks](iBatchLearn.py 131): INFO validation split name:6\
[2023-09-14 07:32:57 10splitTasks](trainer.py 147): INFO  * Val Acc 55.600, Total time 1.84\
[2023-09-14 07:32:57 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:32:57 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:32:57 10splitTasks](iBatchLearn.py 155): INFO Acc:32.88571438707624; BWT:-25.800000082651774;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:33:02 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:33:02 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:33:02 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 6, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-6.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_6.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:33:02 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-6.pth\
[2023-09-14 07:33:02 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:33:04 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:33:04 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:33:04 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:33:08 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:33:11 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 07:33:14 10splitTasks](iBatchLearn.py 167): INFO test split name:3\
[2023-09-14 07:33:17 10splitTasks](iBatchLearn.py 167): INFO test split name:4\
[2023-09-14 07:33:20 10splitTasks](iBatchLearn.py 167): INFO test split name:5\
[2023-09-14 07:33:22 10splitTasks](iBatchLearn.py 167): INFO test split name:6\
--------------------------------Official Evaluation--------------------------------\
6 7.385714285714283\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:33:31 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:33:31 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:33:31 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 7, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-6.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-7.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-6.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-7.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:33:32 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-6.pth\
[2023-09-14 07:33:32 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:33:33 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:33:34 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:33:34 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:33:34 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:33:34 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:33:34 10splitTasks](iBatchLearn.py 92): INFO ====================== 7 =======================\
[2023-09-14 07:33:34 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:33:34 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:33:34 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:33:34 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:33:34 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:33:37 10splitTasks](trainer.py 286): INFO [0/79]	3.3141(3.3141)	0.7844(0.7844)	2.712(2.712)	4.69(4.69)\
[2023-09-14 07:33:39 10splitTasks](trainer.py 286): INFO [10/79]	0.2007(0.4820)	0.0007(0.0717)	1.840(2.233)	35.94(19.89)\
[2023-09-14 07:33:41 10splitTasks](trainer.py 286): INFO [20/79]	0.1973(0.3461)	0.0004(0.0378)	1.559(1.986)	53.12(30.88)\
[2023-09-14 07:33:43 10splitTasks](trainer.py 286): INFO [30/79]	0.1977(0.2978)	0.0003(0.0257)	1.484(1.822)	48.44(37.00)\
[2023-09-14 07:33:45 10splitTasks](trainer.py 286): INFO [40/79]	0.1978(0.2731)	0.0005(0.0196)	1.314(1.703)	53.12(41.46)\
[2023-09-14 07:33:47 10splitTasks](trainer.py 286): INFO [50/79]	0.2250(0.2589)	0.0002(0.0158)	1.253(1.638)	62.50(43.84)\
[2023-09-14 07:33:49 10splitTasks](trainer.py 286): INFO [60/79]	0.1976(0.2492)	0.0004(0.0133)	1.473(1.585)	48.44(45.67)\
[2023-09-14 07:33:51 10splitTasks](trainer.py 286): INFO [70/79]	0.1972(0.2421)	0.0010(0.0115)	1.236(1.534)	57.81(47.43)\
[2023-09-14 07:33:53 10splitTasks](trainer.py 286): INFO [78/79]	0.2221(0.2377)	0.0001(0.0104)	1.563(1.498)	37.50(48.78)\
[2023-09-14 07:33:53 10splitTasks](trainer.py 288): INFO  * Train Acc 48.780\
[2023-09-14 07:33:54 10splitTasks](trainer.py 147): INFO  * Val Acc 60.800, Total time 1.75\
[2023-09-14 07:33:54 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:33:54 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:33:54 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:33:55 10splitTasks](trainer.py 286): INFO [0/79]	1.0142(1.0142)	0.8061(0.8061)	1.244(1.244)	64.06(64.06)\
[2023-09-14 07:33:57 10splitTasks](trainer.py 286): INFO [10/79]	0.1974(0.2760)	0.0004(0.0737)	1.216(1.170)	60.94(60.94)\
[2023-09-14 07:33:59 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2382)	0.0008(0.0389)	1.094(1.171)	59.38(61.61)\
[2023-09-14 07:34:01 10splitTasks](trainer.py 286): INFO [30/79]	0.1957(0.2253)	0.0004(0.0265)	0.796(1.167)	81.25(62.30)\
[2023-09-14 07:34:03 10splitTasks](trainer.py 286): INFO [40/79]	0.1970(0.2191)	0.0005(0.0202)	1.005(1.159)	62.50(63.22)\
[2023-09-14 07:34:05 10splitTasks](trainer.py 286): INFO [50/79]	0.1977(0.2151)	0.0005(0.0163)	1.073(1.160)	62.50(63.02)\
[2023-09-14 07:34:07 10splitTasks](trainer.py 286): INFO [60/79]	0.2064(0.2125)	0.0004(0.0137)	1.019(1.153)	67.19(63.35)\
[2023-09-14 07:34:09 10splitTasks](trainer.py 286): INFO [70/79]	0.1995(0.2106)	0.0010(0.0119)	1.144(1.152)	65.62(63.86)\
[2023-09-14 07:34:11 10splitTasks](trainer.py 286): INFO [78/79]	0.0728(0.2077)	0.0001(0.0107)	1.252(1.152)	75.00(63.76)\
[2023-09-14 07:34:11 10splitTasks](trainer.py 288): INFO  * Train Acc 63.760\
[2023-09-14 07:34:13 10splitTasks](trainer.py 147): INFO  * Val Acc 63.400, Total time 1.73\
[2023-09-14 07:34:13 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:34:13 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:34:13 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:34:14 10splitTasks](trainer.py 286): INFO [0/79]	1.0432(1.0432)	0.8433(0.8433)	1.060(1.060)	60.94(60.94)\
[2023-09-14 07:34:16 10splitTasks](trainer.py 286): INFO [10/79]	0.1995(0.2773)	0.0006(0.0772)	0.907(1.041)	75.00(66.19)\
[2023-09-14 07:34:18 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2396)	0.0004(0.0407)	1.013(1.077)	68.75(65.33)\
[2023-09-14 07:34:20 10splitTasks](trainer.py 286): INFO [30/79]	0.2098(0.2275)	0.0007(0.0277)	0.998(1.103)	70.31(64.52)\
[2023-09-14 07:34:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1958(0.2199)	0.0003(0.0211)	1.198(1.107)	59.38(64.79)\
[2023-09-14 07:34:24 10splitTasks](trainer.py 286): INFO [50/79]	0.1955(0.2157)	0.0003(0.0170)	1.286(1.110)	56.25(64.83)\
[2023-09-14 07:34:25 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2126)	0.0003(0.0143)	1.374(1.113)	57.81(65.34)\
[2023-09-14 07:34:27 10splitTasks](trainer.py 286): INFO [70/79]	0.2024(0.2108)	0.0011(0.0124)	1.247(1.113)	64.06(65.51)\
[2023-09-14 07:34:29 10splitTasks](trainer.py 286): INFO [78/79]	0.0709(0.2076)	0.0001(0.0112)	1.931(1.112)	50.00(65.62)\
[2023-09-14 07:34:29 10splitTasks](trainer.py 288): INFO  * Train Acc 65.620\
[2023-09-14 07:34:31 10splitTasks](trainer.py 147): INFO  * Val Acc 66.600, Total time 1.72\
[2023-09-14 07:34:31 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:34:31 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:34:31 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:34:32 10splitTasks](trainer.py 286): INFO [0/79]	0.9510(0.9510)	0.7368(0.7368)	1.124(1.124)	60.94(60.94)\
[2023-09-14 07:34:34 10splitTasks](trainer.py 286): INFO [10/79]	0.1986(0.2670)	0.0003(0.0674)	0.907(1.109)	78.12(66.19)\
[2023-09-14 07:34:36 10splitTasks](trainer.py 286): INFO [20/79]	0.1949(0.2336)	0.0004(0.0355)	1.056(1.098)	65.62(66.74)\
[2023-09-14 07:34:38 10splitTasks](trainer.py 286): INFO [30/79]	0.1982(0.2216)	0.0002(0.0242)	1.189(1.109)	62.50(66.28)\
[2023-09-14 07:34:40 10splitTasks](trainer.py 286): INFO [40/79]	0.1958(0.2157)	0.0003(0.0184)	1.260(1.119)	59.38(66.08)\
[2023-09-14 07:34:42 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2118)	0.0004(0.0149)	1.221(1.130)	73.44(66.27)\
[2023-09-14 07:34:44 10splitTasks](trainer.py 286): INFO [60/79]	0.1959(0.2097)	0.0004(0.0126)	0.966(1.120)	71.88(66.83)\
[2023-09-14 07:34:45 10splitTasks](trainer.py 286): INFO [70/79]	0.1976(0.2078)	0.0010(0.0109)	1.283(1.121)	62.50(66.66)\
[2023-09-14 07:34:47 10splitTasks](trainer.py 286): INFO [78/79]	0.0713(0.2050)	0.0001(0.0098)	1.694(1.119)	62.50(66.78)\
[2023-09-14 07:34:47 10splitTasks](trainer.py 288): INFO  * Train Acc 66.780\
[2023-09-14 07:34:49 10splitTasks](trainer.py 147): INFO  * Val Acc 68.600, Total time 1.68\
[2023-09-14 07:34:49 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:34:49 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:34:49 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:34:50 10splitTasks](trainer.py 286): INFO [0/79]	0.9326(0.9326)	0.7247(0.7247)	0.967(0.967)	65.62(65.62)\
[2023-09-14 07:34:52 10splitTasks](trainer.py 286): INFO [10/79]	0.2147(0.2744)	0.0004(0.0752)	1.058(1.060)	71.88(68.32)\
[2023-09-14 07:34:54 10splitTasks](trainer.py 286): INFO [20/79]	0.1972(0.2382)	0.0004(0.0396)	1.100(1.052)	67.19(68.68)\
[2023-09-14 07:34:56 10splitTasks](trainer.py 286): INFO [30/79]	0.1959(0.2248)	0.0004(0.0270)	0.918(1.073)	81.25(68.15)\
[2023-09-14 07:34:58 10splitTasks](trainer.py 286): INFO [40/79]	0.1986(0.2183)	0.0003(0.0205)	1.226(1.069)	62.50(67.91)\
[2023-09-14 07:35:00 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2142)	0.0003(0.0166)	1.112(1.077)	60.94(67.65)\
[2023-09-14 07:35:02 10splitTasks](trainer.py 286): INFO [60/79]	0.1968(0.2117)	0.0007(0.0140)	1.239(1.086)	65.62(67.55)\
[2023-09-14 07:35:04 10splitTasks](trainer.py 286): INFO [70/79]	0.1969(0.2096)	0.0011(0.0121)	1.217(1.094)	65.62(67.69)\
[2023-09-14 07:35:05 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2066)	0.0001(0.0109)	1.603(1.101)	25.00(67.52)\
[2023-09-14 07:35:05 10splitTasks](trainer.py 288): INFO  * Train Acc 67.520\
[2023-09-14 07:35:07 10splitTasks](trainer.py 147): INFO  * Val Acc 55.400, Total time 1.68\
[2023-09-14 07:35:07 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:35:07 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:35:07 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:35:08 10splitTasks](trainer.py 286): INFO [0/79]	0.9784(0.9784)	0.7647(0.7647)	1.047(1.047)	68.75(68.75)\
[2023-09-14 07:35:10 10splitTasks](trainer.py 286): INFO [10/79]	0.2006(0.2683)	0.0005(0.0699)	1.381(1.256)	56.25(62.78)\
[2023-09-14 07:35:12 10splitTasks](trainer.py 286): INFO [20/79]	0.2013(0.2362)	0.0007(0.0369)	1.142(1.189)	67.19(65.10)\
[2023-09-14 07:35:14 10splitTasks](trainer.py 286): INFO [30/79]	0.1971(0.2247)	0.0003(0.0251)	1.017(1.176)	68.75(66.23)\
[2023-09-14 07:35:16 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2178)	0.0003(0.0191)	1.103(1.163)	60.94(66.46)\
[2023-09-14 07:35:18 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2138)	0.0004(0.0155)	0.980(1.149)	73.44(66.91)\
[2023-09-14 07:35:20 10splitTasks](trainer.py 286): INFO [60/79]	0.2070(0.2114)	0.0002(0.0130)	1.289(1.132)	65.62(67.42)\
[2023-09-14 07:35:22 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2101)	0.0009(0.0113)	1.093(1.126)	76.56(67.94)\
[2023-09-14 07:35:23 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2070)	0.0001(0.0102)	1.589(1.124)	62.50(68.14)\
[2023-09-14 07:35:23 10splitTasks](trainer.py 288): INFO  * Train Acc 68.140\
[2023-09-14 07:35:25 10splitTasks](trainer.py 147): INFO  * Val Acc 60.600, Total time 1.65\
[2023-09-14 07:35:25 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:35:25 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:35:25 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:35:26 10splitTasks](trainer.py 286): INFO [0/79]	0.9632(0.9632)	0.7548(0.7548)	0.815(0.815)	84.38(84.38)\
[2023-09-14 07:35:28 10splitTasks](trainer.py 286): INFO [10/79]	0.1988(0.2677)	0.0003(0.0691)	1.178(1.126)	67.19(67.76)\
[2023-09-14 07:35:30 10splitTasks](trainer.py 286): INFO [20/79]	0.2009(0.2343)	0.0010(0.0364)	1.241(1.142)	68.75(66.82)\
[2023-09-14 07:35:32 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2222)	0.0003(0.0248)	0.879(1.096)	75.00(68.25)\
[2023-09-14 07:35:34 10splitTasks](trainer.py 286): INFO [40/79]	0.1954(0.2159)	0.0003(0.0189)	1.013(1.085)	75.00(68.86)\
[2023-09-14 07:35:36 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2120)	0.0003(0.0152)	0.912(1.085)	70.31(68.96)\
[2023-09-14 07:35:38 10splitTasks](trainer.py 286): INFO [60/79]	0.2016(0.2100)	0.0004(0.0128)	0.928(1.090)	76.56(69.13)\
[2023-09-14 07:35:40 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2081)	0.0010(0.0111)	1.164(1.098)	68.75(69.32)\
[2023-09-14 07:35:41 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2053)	0.0001(0.0100)	1.677(1.103)	50.00(69.18)\
[2023-09-14 07:35:41 10splitTasks](trainer.py 288): INFO  * Train Acc 69.180\
[2023-09-14 07:35:43 10splitTasks](trainer.py 147): INFO  * Val Acc 62.000, Total time 1.70\
[2023-09-14 07:35:43 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:35:43 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:35:43 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:35:44 10splitTasks](trainer.py 286): INFO [0/79]	0.9830(0.9830)	0.7819(0.7819)	0.942(0.942)	76.56(76.56)\
[2023-09-14 07:35:46 10splitTasks](trainer.py 286): INFO [10/79]	0.1974(0.2681)	0.0003(0.0715)	1.069(1.022)	70.31(71.31)\
[2023-09-14 07:35:48 10splitTasks](trainer.py 286): INFO [20/79]	0.1959(0.2341)	0.0007(0.0377)	1.002(1.056)	71.88(70.46)\
[2023-09-14 07:35:50 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2223)	0.0003(0.0257)	1.002(1.058)	68.75(70.97)\
[2023-09-14 07:35:52 10splitTasks](trainer.py 286): INFO [40/79]	0.1960(0.2164)	0.0003(0.0195)	1.143(1.071)	65.62(70.66)\
[2023-09-14 07:35:54 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2126)	0.0003(0.0158)	1.083(1.076)	67.19(70.37)\
[2023-09-14 07:35:56 10splitTasks](trainer.py 286): INFO [60/79]	0.1971(0.2100)	0.0007(0.0133)	1.146(1.097)	67.19(69.60)\
[2023-09-14 07:35:58 10splitTasks](trainer.py 286): INFO [70/79]	0.2053(0.2085)	0.0013(0.0115)	0.871(1.092)	76.56(69.56)\
[2023-09-14 07:35:59 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2056)	0.0001(0.0103)	1.503(1.088)	50.00(69.72)\
[2023-09-14 07:35:59 10splitTasks](trainer.py 288): INFO  * Train Acc 69.720\
[2023-09-14 07:36:01 10splitTasks](trainer.py 147): INFO  * Val Acc 65.600, Total time 1.68\
[2023-09-14 07:36:01 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:36:01 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:36:01 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:36:02 10splitTasks](trainer.py 286): INFO [0/79]	0.9787(0.9787)	0.7793(0.7793)	1.096(1.096)	67.19(67.19)\
[2023-09-14 07:36:04 10splitTasks](trainer.py 286): INFO [10/79]	0.2086(0.2708)	0.0007(0.0713)	1.282(1.066)	60.94(70.17)\
[2023-09-14 07:36:06 10splitTasks](trainer.py 286): INFO [20/79]	0.2104(0.2377)	0.0007(0.0376)	1.057(1.071)	75.00(71.13)\
[2023-09-14 07:36:08 10splitTasks](trainer.py 286): INFO [30/79]	0.2039(0.2248)	0.0004(0.0256)	1.086(1.074)	71.88(71.32)\
[2023-09-14 07:36:10 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2184)	0.0004(0.0195)	1.099(1.082)	70.31(71.19)\
[2023-09-14 07:36:12 10splitTasks](trainer.py 286): INFO [50/79]	0.1977(0.2147)	0.0003(0.0158)	1.107(1.078)	68.75(71.17)\
[2023-09-14 07:36:14 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2123)	0.0003(0.0133)	0.823(1.090)	79.69(71.03)\
[2023-09-14 07:36:16 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2103)	0.0011(0.0115)	1.166(1.083)	59.38(71.06)\
[2023-09-14 07:36:17 10splitTasks](trainer.py 286): INFO [78/79]	0.0706(0.2072)	0.0001(0.0104)	0.998(1.085)	62.50(70.94)\
[2023-09-14 07:36:17 10splitTasks](trainer.py 288): INFO  * Train Acc 70.940\
[2023-09-14 07:36:19 10splitTasks](trainer.py 147): INFO  * Val Acc 72.000, Total time 1.70\
[2023-09-14 07:36:19 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:36:19 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:36:19 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:36:20 10splitTasks](trainer.py 286): INFO [0/79]	1.0671(1.0671)	0.8691(0.8691)	0.862(0.862)	76.56(76.56)\
[2023-09-14 07:36:22 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2795)	0.0003(0.0827)	1.093(1.033)	67.19(70.88)\
[2023-09-14 07:36:24 10splitTasks](trainer.py 286): INFO [20/79]	0.1968(0.2406)	0.0004(0.0435)	1.001(1.025)	67.19(71.21)\
[2023-09-14 07:36:26 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2268)	0.0003(0.0296)	1.123(1.024)	64.06(71.47)\
[2023-09-14 07:36:28 10splitTasks](trainer.py 286): INFO [40/79]	0.1960(0.2199)	0.0003(0.0225)	1.169(1.048)	67.19(70.81)\
[2023-09-14 07:36:30 10splitTasks](trainer.py 286): INFO [50/79]	0.2005(0.2158)	0.0004(0.0182)	1.157(1.052)	64.06(71.02)\
[2023-09-14 07:36:32 10splitTasks](trainer.py 286): INFO [60/79]	0.1979(0.2129)	0.0008(0.0153)	1.249(1.059)	67.19(71.00)\
[2023-09-14 07:36:34 10splitTasks](trainer.py 286): INFO [70/79]	0.1992(0.2108)	0.0014(0.0132)	0.917(1.063)	75.00(71.02)\
[2023-09-14 07:36:35 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2077)	0.0001(0.0119)	2.950(1.064)	37.50(71.24)\
[2023-09-14 07:36:35 10splitTasks](trainer.py 288): INFO  * Train Acc 71.240\
[2023-09-14 07:36:37 10splitTasks](trainer.py 147): INFO  * Val Acc 67.400, Total time 1.67\
[2023-09-14 07:36:37 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:36:37 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:36:37 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:36:38 10splitTasks](trainer.py 286): INFO [0/79]	0.9375(0.9375)	0.7213(0.7213)	0.989(0.989)	75.00(75.00)\
[2023-09-14 07:36:40 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2638)	0.0004(0.0660)	1.088(1.116)	76.56(69.46)\
[2023-09-14 07:36:42 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2317)	0.0003(0.0348)	1.084(1.122)	67.19(69.20)\
[2023-09-14 07:36:44 10splitTasks](trainer.py 286): INFO [30/79]	0.1960(0.2206)	0.0003(0.0238)	1.124(1.122)	70.31(69.00)\
[2023-09-14 07:36:46 10splitTasks](trainer.py 286): INFO [40/79]	0.1953(0.2147)	0.0003(0.0181)	1.136(1.104)	70.31(70.01)\
[2023-09-14 07:36:48 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2115)	0.0003(0.0146)	0.892(1.078)	78.12(71.48)\
[2023-09-14 07:36:50 10splitTasks](trainer.py 286): INFO [60/79]	0.2017(0.2093)	0.0003(0.0123)	1.156(1.072)	67.19(71.67)\
[2023-09-14 07:36:52 10splitTasks](trainer.py 286): INFO [70/79]	0.2048(0.2076)	0.0015(0.0107)	1.585(1.077)	53.12(71.79)\
[2023-09-14 07:36:53 10splitTasks](trainer.py 286): INFO [78/79]	0.0707(0.2047)	0.0002(0.0096)	2.634(1.081)	50.00(71.46)\
[2023-09-14 07:36:53 10splitTasks](trainer.py 288): INFO  * Train Acc 71.460\
[2023-09-14 07:36:55 10splitTasks](trainer.py 147): INFO  * Val Acc 69.000, Total time 1.70\
[2023-09-14 07:36:55 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:36:55 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:36:55 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:36:56 10splitTasks](trainer.py 286): INFO [0/79]	1.0702(1.0702)	0.8719(0.8719)	1.355(1.355)	62.50(62.50)\
[2023-09-14 07:36:58 10splitTasks](trainer.py 286): INFO [10/79]	0.2104(0.2793)	0.0003(0.0797)	1.034(1.086)	71.88(71.88)\
[2023-09-14 07:37:00 10splitTasks](trainer.py 286): INFO [20/79]	0.2016(0.2401)	0.0003(0.0420)	0.988(1.080)	76.56(71.73)\
[2023-09-14 07:37:02 10splitTasks](trainer.py 286): INFO [30/79]	0.1967(0.2260)	0.0003(0.0286)	1.062(1.070)	76.56(72.23)\
[2023-09-14 07:37:04 10splitTasks](trainer.py 286): INFO [40/79]	0.2109(0.2193)	0.0003(0.0217)	0.910(1.067)	76.56(71.84)\
[2023-09-14 07:37:06 10splitTasks](trainer.py 286): INFO [50/79]	0.1951(0.2153)	0.0003(0.0176)	1.288(1.068)	65.62(71.97)\
[2023-09-14 07:37:08 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2122)	0.0004(0.0148)	1.119(1.070)	70.31(71.95)\
[2023-09-14 07:37:10 10splitTasks](trainer.py 286): INFO [70/79]	0.1969(0.2104)	0.0011(0.0128)	0.968(1.074)	73.44(71.99)\
[2023-09-14 07:37:11 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2072)	0.0001(0.0115)	2.356(1.070)	12.50(72.16)\
[2023-09-14 07:37:11 10splitTasks](trainer.py 288): INFO  * Train Acc 72.160\
[2023-09-14 07:37:13 10splitTasks](trainer.py 147): INFO  * Val Acc 68.400, Total time 1.71\
[2023-09-14 07:37:13 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:37:13 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:37:13 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:37:14 10splitTasks](trainer.py 286): INFO [0/79]	0.9475(0.9475)	0.7498(0.7498)	1.166(1.166)	60.94(60.94)\
[2023-09-14 07:37:16 10splitTasks](trainer.py 286): INFO [10/79]	0.1963(0.2651)	0.0003(0.0685)	0.899(1.081)	79.69(71.16)\
[2023-09-14 07:37:18 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2337)	0.0004(0.0361)	0.942(1.067)	75.00(71.73)\
[2023-09-14 07:37:20 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2219)	0.0003(0.0246)	1.284(1.077)	64.06(71.22)\
[2023-09-14 07:37:22 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2160)	0.0004(0.0187)	1.317(1.083)	68.75(71.53)\
[2023-09-14 07:37:24 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2123)	0.0003(0.0151)	1.037(1.087)	76.56(71.42)\
[2023-09-14 07:37:26 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2104)	0.0003(0.0127)	1.164(1.085)	68.75(71.47)\
[2023-09-14 07:37:28 10splitTasks](trainer.py 286): INFO [70/79]	0.2026(0.2086)	0.0011(0.0110)	1.289(1.092)	68.75(71.39)\
[2023-09-14 07:37:29 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2056)	0.0001(0.0099)	1.496(1.087)	50.00(71.48)\
[2023-09-14 07:37:30 10splitTasks](trainer.py 288): INFO  * Train Acc 71.480\
[2023-09-14 07:37:31 10splitTasks](trainer.py 147): INFO  * Val Acc 72.800, Total time 1.69\
[2023-09-14 07:37:31 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:37:31 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:37:31 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:37:32 10splitTasks](trainer.py 286): INFO [0/79]	1.1992(1.1992)	1.0016(1.0016)	1.060(1.060)	73.44(73.44)\
[2023-09-14 07:37:34 10splitTasks](trainer.py 286): INFO [10/79]	0.1981(0.2895)	0.0004(0.0917)	0.960(1.046)	75.00(73.58)\
[2023-09-14 07:37:36 10splitTasks](trainer.py 286): INFO [20/79]	0.2098(0.2466)	0.0003(0.0483)	1.083(1.065)	73.44(72.84)\
[2023-09-14 07:37:38 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2317)	0.0003(0.0328)	1.082(1.067)	75.00(72.33)\
[2023-09-14 07:37:40 10splitTasks](trainer.py 286): INFO [40/79]	0.1977(0.2236)	0.0004(0.0250)	1.235(1.097)	65.62(71.30)\
[2023-09-14 07:37:42 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2186)	0.0003(0.0202)	1.049(1.093)	71.88(71.75)\
[2023-09-14 07:37:44 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2154)	0.0003(0.0170)	1.028(1.087)	73.44(71.80)\
[2023-09-14 07:37:46 10splitTasks](trainer.py 286): INFO [70/79]	0.1990(0.2133)	0.0009(0.0147)	1.018(1.087)	73.44(71.92)\
[2023-09-14 07:37:48 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2099)	0.0001(0.0133)	2.098(1.091)	0.00(71.86)\
[2023-09-14 07:37:48 10splitTasks](trainer.py 288): INFO  * Train Acc 71.860\
[2023-09-14 07:37:50 10splitTasks](trainer.py 147): INFO  * Val Acc 72.600, Total time 1.69\
[2023-09-14 07:37:50 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:37:50 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:37:50 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:37:50 10splitTasks](trainer.py 286): INFO [0/79]	0.9408(0.9408)	0.7140(0.7140)	1.035(1.035)	76.56(76.56)\
[2023-09-14 07:37:52 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2648)	0.0003(0.0653)	1.379(1.034)	64.06(73.15)\
[2023-09-14 07:37:54 10splitTasks](trainer.py 286): INFO [20/79]	0.2010(0.2344)	0.0006(0.0345)	1.257(1.075)	71.88(72.02)\
[2023-09-14 07:37:56 10splitTasks](trainer.py 286): INFO [30/79]	0.2059(0.2224)	0.0006(0.0235)	1.094(1.074)	70.31(71.98)\
[2023-09-14 07:37:58 10splitTasks](trainer.py 286): INFO [40/79]	0.1960(0.2163)	0.0004(0.0179)	0.902(1.078)	82.81(72.03)\
[2023-09-14 07:38:00 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2127)	0.0003(0.0144)	1.149(1.086)	71.88(71.81)\
[2023-09-14 07:38:02 10splitTasks](trainer.py 286): INFO [60/79]	0.1958(0.2101)	0.0003(0.0122)	1.310(1.092)	68.75(71.82)\
[2023-09-14 07:38:04 10splitTasks](trainer.py 286): INFO [70/79]	0.1983(0.2084)	0.0013(0.0105)	1.245(1.092)	67.19(71.76)\
[2023-09-14 07:38:06 10splitTasks](trainer.py 286): INFO [78/79]	0.0697(0.2055)	0.0001(0.0095)	1.788(1.086)	25.00(72.10)\
[2023-09-14 07:38:06 10splitTasks](trainer.py 288): INFO  * Train Acc 72.100\
[2023-09-14 07:38:08 10splitTasks](trainer.py 147): INFO  * Val Acc 70.400, Total time 1.73\
[2023-09-14 07:38:08 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:38:08 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:38:08 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:38:09 10splitTasks](trainer.py 286): INFO [0/79]	0.9575(0.9575)	0.7276(0.7276)	0.947(0.947)	78.12(78.12)\
[2023-09-14 07:38:11 10splitTasks](trainer.py 286): INFO [10/79]	0.1954(0.2707)	0.0004(0.0665)	1.088(1.108)	70.31(70.88)\
[2023-09-14 07:38:13 10splitTasks](trainer.py 286): INFO [20/79]	0.1960(0.2359)	0.0003(0.0351)	1.030(1.088)	75.00(71.58)\
[2023-09-14 07:38:15 10splitTasks](trainer.py 286): INFO [30/79]	0.2127(0.2238)	0.0003(0.0239)	1.153(1.080)	71.88(71.82)\
[2023-09-14 07:38:16 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2172)	0.0003(0.0182)	1.042(1.097)	71.88(71.65)\
[2023-09-14 07:38:18 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2131)	0.0003(0.0147)	1.222(1.087)	73.44(72.27)\
[2023-09-14 07:38:20 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2104)	0.0003(0.0124)	1.039(1.096)	67.19(71.93)\
[2023-09-14 07:38:22 10splitTasks](trainer.py 286): INFO [70/79]	0.1976(0.2084)	0.0009(0.0107)	1.183(1.097)	65.62(71.88)\
[2023-09-14 07:38:24 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2054)	0.0001(0.0096)	1.194(1.095)	50.00(72.14)\
[2023-09-14 07:38:24 10splitTasks](trainer.py 288): INFO  * Train Acc 72.140\
[2023-09-14 07:38:26 10splitTasks](trainer.py 147): INFO  * Val Acc 71.800, Total time 1.73\
[2023-09-14 07:38:26 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:38:26 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:38:26 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:38:27 10splitTasks](trainer.py 286): INFO [0/79]	1.0462(1.0462)	0.8466(0.8466)	0.917(0.917)	78.12(78.12)\
[2023-09-14 07:38:29 10splitTasks](trainer.py 286): INFO [10/79]	0.1979(0.2749)	0.0008(0.0775)	1.317(1.041)	64.06(74.15)\
[2023-09-14 07:38:31 10splitTasks](trainer.py 286): INFO [20/79]	0.1969(0.2377)	0.0004(0.0408)	1.277(1.063)	65.62(72.92)\
[2023-09-14 07:38:33 10splitTasks](trainer.py 286): INFO [30/79]	0.1963(0.2251)	0.0004(0.0278)	1.064(1.051)	71.88(73.54)\
[2023-09-14 07:38:35 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2181)	0.0004(0.0212)	0.891(1.052)	79.69(73.63)\
[2023-09-14 07:38:37 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2142)	0.0004(0.0171)	0.886(1.050)	82.81(73.87)\
[2023-09-14 07:38:39 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2117)	0.0004(0.0144)	1.230(1.045)	70.31(74.28)\
[2023-09-14 07:38:41 10splitTasks](trainer.py 286): INFO [70/79]	0.1979(0.2096)	0.0012(0.0124)	1.170(1.045)	73.44(74.52)\
[2023-09-14 07:38:42 10splitTasks](trainer.py 286): INFO [78/79]	0.0708(0.2066)	0.0001(0.0112)	2.736(1.038)	37.50(74.68)\
[2023-09-14 07:38:42 10splitTasks](trainer.py 288): INFO  * Train Acc 74.680\
[2023-09-14 07:38:44 10splitTasks](trainer.py 147): INFO  * Val Acc 66.200, Total time 1.87\
[2023-09-14 07:38:44 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:38:44 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:38:44 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:38:45 10splitTasks](trainer.py 286): INFO [0/79]	0.9551(0.9551)	0.7293(0.7293)	1.241(1.241)	65.62(65.62)\
[2023-09-14 07:38:47 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2714)	0.0004(0.0667)	1.104(1.137)	71.88(70.31)\
[2023-09-14 07:38:49 10splitTasks](trainer.py 286): INFO [20/79]	0.1962(0.2363)	0.0004(0.0352)	1.157(1.114)	70.31(71.50)\
[2023-09-14 07:38:51 10splitTasks](trainer.py 286): INFO [30/79]	0.1960(0.2236)	0.0009(0.0240)	0.795(1.112)	81.25(71.32)\
[2023-09-14 07:38:53 10splitTasks](trainer.py 286): INFO [40/79]	0.2017(0.2172)	0.0006(0.0183)	1.222(1.107)	65.62(71.46)\
[2023-09-14 07:38:55 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2136)	0.0003(0.0148)	1.016(1.095)	78.12(71.94)\
[2023-09-14 07:38:57 10splitTasks](trainer.py 286): INFO [60/79]	0.1951(0.2109)	0.0004(0.0124)	1.171(1.089)	64.06(72.26)\
[2023-09-14 07:38:59 10splitTasks](trainer.py 286): INFO [70/79]	0.2000(0.2090)	0.0012(0.0108)	1.128(1.079)	71.88(72.62)\
[2023-09-14 07:39:00 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2060)	0.0001(0.0097)	1.556(1.090)	62.50(72.42)\
[2023-09-14 07:39:00 10splitTasks](trainer.py 288): INFO  * Train Acc 72.420\
[2023-09-14 07:39:02 10splitTasks](trainer.py 147): INFO  * Val Acc 73.200, Total time 1.72\
[2023-09-14 07:39:02 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:39:02 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:39:02 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:39:03 10splitTasks](trainer.py 286): INFO [0/79]	1.0110(1.0110)	0.8135(0.8135)	1.260(1.260)	64.06(64.06)\
[2023-09-14 07:39:05 10splitTasks](trainer.py 286): INFO [10/79]	0.1963(0.2704)	0.0004(0.0744)	1.176(1.023)	73.44(75.57)\
[2023-09-14 07:39:07 10splitTasks](trainer.py 286): INFO [20/79]	0.1979(0.2370)	0.0003(0.0392)	1.261(1.071)	67.19(73.88)\
[2023-09-14 07:39:09 10splitTasks](trainer.py 286): INFO [30/79]	0.1960(0.2252)	0.0003(0.0267)	1.092(1.063)	79.69(74.50)\
[2023-09-14 07:39:11 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2185)	0.0004(0.0203)	0.966(1.053)	76.56(74.62)\
[2023-09-14 07:39:13 10splitTasks](trainer.py 286): INFO [50/79]	0.1955(0.2146)	0.0003(0.0164)	1.093(1.051)	79.69(74.85)\
[2023-09-14 07:39:15 10splitTasks](trainer.py 286): INFO [60/79]	0.1965(0.2118)	0.0003(0.0138)	1.041(1.050)	75.00(74.87)\
[2023-09-14 07:39:17 10splitTasks](trainer.py 286): INFO [70/79]	0.1970(0.2103)	0.0010(0.0120)	1.087(1.057)	75.00(74.78)\
[2023-09-14 07:39:18 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2072)	0.0001(0.0108)	0.598(1.057)	100.00(74.62)\
[2023-09-14 07:39:18 10splitTasks](trainer.py 288): INFO  * Train Acc 74.620\
[2023-09-14 07:39:20 10splitTasks](trainer.py 147): INFO  * Val Acc 69.200, Total time 1.67\
[2023-09-14 07:39:20 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:39:20 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:39:20 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:39:21 10splitTasks](trainer.py 286): INFO [0/79]	0.9529(0.9529)	0.7483(0.7483)	1.102(1.102)	65.62(65.62)\
[2023-09-14 07:39:23 10splitTasks](trainer.py 286): INFO [10/79]	0.2002(0.2661)	0.0003(0.0684)	0.851(1.017)	82.81(76.14)\
[2023-09-14 07:39:25 10splitTasks](trainer.py 286): INFO [20/79]	0.1969(0.2329)	0.0003(0.0360)	0.786(1.005)	85.94(76.41)\
[2023-09-14 07:39:27 10splitTasks](trainer.py 286): INFO [30/79]	0.1953(0.2212)	0.0003(0.0246)	1.081(1.014)	73.44(76.21)\
[2023-09-14 07:39:29 10splitTasks](trainer.py 286): INFO [40/79]	0.1999(0.2159)	0.0003(0.0187)	1.043(1.023)	71.88(75.69)\
[2023-09-14 07:39:31 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2122)	0.0004(0.0151)	1.155(1.020)	62.50(75.67)\
[2023-09-14 07:39:33 10splitTasks](trainer.py 286): INFO [60/79]	0.1977(0.2098)	0.0005(0.0127)	1.123(1.036)	71.88(75.00)\
[2023-09-14 07:39:35 10splitTasks](trainer.py 286): INFO [70/79]	0.2004(0.2083)	0.0020(0.0110)	1.144(1.041)	68.75(74.71)\
[2023-09-14 07:39:36 10splitTasks](trainer.py 286): INFO [78/79]	0.0720(0.2054)	0.0001(0.0099)	1.749(1.050)	62.50(74.26)\
[2023-09-14 07:39:36 10splitTasks](trainer.py 288): INFO  * Train Acc 74.260\
[2023-09-14 07:39:38 10splitTasks](trainer.py 147): INFO  * Val Acc 69.800, Total time 1.67\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-7.pth\
=> Save Done\
[2023-09-14 07:39:38 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:39:40 10splitTasks](trainer.py 147): INFO  * Val Acc 26.000, Total time 1.48\
[2023-09-14 07:39:40 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:39:41 10splitTasks](trainer.py 147): INFO  * Val Acc 29.800, Total time 1.58\
[2023-09-14 07:39:41 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:39:43 10splitTasks](trainer.py 147): INFO  * Val Acc 32.400, Total time 1.95\
[2023-09-14 07:39:43 10splitTasks](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 07:39:45 10splitTasks](trainer.py 147): INFO  * Val Acc 38.400, Total time 1.79\
[2023-09-14 07:39:45 10splitTasks](iBatchLearn.py 131): INFO validation split name:4\
[2023-09-14 07:39:47 10splitTasks](trainer.py 147): INFO  * Val Acc 37.400, Total time 1.53\
[2023-09-14 07:39:47 10splitTasks](iBatchLearn.py 131): INFO validation split name:5\
[2023-09-14 07:39:48 10splitTasks](trainer.py 147): INFO  * Val Acc 41.800, Total time 1.62\
[2023-09-14 07:39:48 10splitTasks](iBatchLearn.py 131): INFO validation split name:6\
[2023-09-14 07:39:50 10splitTasks](trainer.py 147): INFO  * Val Acc 43.400, Total time 1.77\
[2023-09-14 07:39:50 10splitTasks](iBatchLearn.py 131): INFO validation split name:7\
[2023-09-14 07:39:52 10splitTasks](trainer.py 147): INFO  * Val Acc 69.800, Total time 1.64\
[2023-09-14 07:39:52 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:39:52 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:39:52 10splitTasks](iBatchLearn.py 155): INFO Acc:39.87500016975403; BWT:-19.40000004795619;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:39:56 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:39:56 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:39:56 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 7, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-7.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_7.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:39:57 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-7.pth\
[2023-09-14 07:39:57 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:39:59 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:39:59 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:39:59 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:40:03 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:40:06 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 07:40:09 10splitTasks](iBatchLearn.py 167): INFO test split name:3\
[2023-09-14 07:40:12 10splitTasks](iBatchLearn.py 167): INFO test split name:4\
[2023-09-14 07:40:15 10splitTasks](iBatchLearn.py 167): INFO test split name:5\
[2023-09-14 07:40:18 10splitTasks](iBatchLearn.py 167): INFO test split name:6\
[2023-09-14 07:40:21 10splitTasks](iBatchLearn.py 167): INFO test split name:7\
--------------------------------Official Evaluation--------------------------------\
7 19.04821428571429\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:40:30 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:40:30 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:40:30 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 8, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-7.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-8.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-7.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-8.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:40:30 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-7.pth\
[2023-09-14 07:40:30 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:40:32 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:40:32 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:40:32 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:40:32 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:40:32 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:40:32 10splitTasks](iBatchLearn.py 92): INFO ====================== 8 =======================\
[2023-09-14 07:40:32 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:40:32 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:40:32 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:40:32 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:40:32 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:40:35 10splitTasks](trainer.py 286): INFO [0/79]	3.3417(3.3417)	0.9032(0.9032)	2.863(2.863)	9.38(9.38)\
[2023-09-14 07:40:37 10splitTasks](trainer.py 286): INFO [10/79]	0.1975(0.4860)	0.0004(0.0826)	1.955(2.278)	26.56(20.17)\
[2023-09-14 07:40:39 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.3491)	0.0003(0.0435)	1.677(1.968)	43.75(31.25)\
[2023-09-14 07:40:41 10splitTasks](trainer.py 286): INFO [30/79]	0.1971(0.3002)	0.0004(0.0296)	1.411(1.829)	51.56(36.29)\
[2023-09-14 07:40:43 10splitTasks](trainer.py 286): INFO [40/79]	0.1986(0.2755)	0.0004(0.0225)	1.125(1.722)	56.25(39.44)\
[2023-09-14 07:40:45 10splitTasks](trainer.py 286): INFO [50/79]	0.1976(0.2604)	0.0004(0.0182)	1.375(1.670)	48.44(41.30)\
[2023-09-14 07:40:47 10splitTasks](trainer.py 286): INFO [60/79]	0.1976(0.2506)	0.0004(0.0153)	1.254(1.629)	56.25(42.75)\
[2023-09-14 07:40:49 10splitTasks](trainer.py 286): INFO [70/79]	0.2004(0.2434)	0.0022(0.0133)	1.093(1.578)	67.19(44.89)\
[2023-09-14 07:40:51 10splitTasks](trainer.py 286): INFO [78/79]	0.2215(0.2389)	0.0001(0.0120)	1.306(1.553)	50.00(46.12)\
[2023-09-14 07:40:51 10splitTasks](trainer.py 288): INFO  * Train Acc 46.120\
[2023-09-14 07:40:53 10splitTasks](trainer.py 147): INFO  * Val Acc 54.400, Total time 1.90\
[2023-09-14 07:40:53 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:40:53 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:40:53 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:40:54 10splitTasks](trainer.py 286): INFO [0/79]	0.9765(0.9765)	0.7708(0.7708)	1.092(1.092)	65.62(65.62)\
[2023-09-14 07:40:56 10splitTasks](trainer.py 286): INFO [10/79]	0.1956(0.2698)	0.0003(0.0714)	1.241(1.234)	62.50(58.81)\
[2023-09-14 07:40:58 10splitTasks](trainer.py 286): INFO [20/79]	0.1974(0.2351)	0.0005(0.0377)	1.257(1.254)	59.38(58.56)\
[2023-09-14 07:41:00 10splitTasks](trainer.py 286): INFO [30/79]	0.2014(0.2239)	0.0014(0.0259)	1.166(1.233)	65.62(58.72)\
[2023-09-14 07:41:02 10splitTasks](trainer.py 286): INFO [40/79]	0.1959(0.2175)	0.0003(0.0197)	1.384(1.235)	56.25(59.15)\
[2023-09-14 07:41:04 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2136)	0.0005(0.0159)	1.275(1.238)	64.06(59.22)\
[2023-09-14 07:41:06 10splitTasks](trainer.py 286): INFO [60/79]	0.1949(0.2109)	0.0004(0.0134)	1.549(1.240)	43.75(59.61)\
[2023-09-14 07:41:08 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2089)	0.0011(0.0116)	1.321(1.226)	54.69(60.12)\
[2023-09-14 07:41:09 10splitTasks](trainer.py 286): INFO [78/79]	0.0735(0.2061)	0.0002(0.0104)	1.595(1.228)	50.00(60.08)\
[2023-09-14 07:41:09 10splitTasks](trainer.py 288): INFO  * Train Acc 60.080\
[2023-09-14 07:41:11 10splitTasks](trainer.py 147): INFO  * Val Acc 57.400, Total time 1.63\
[2023-09-14 07:41:11 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:41:11 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:41:11 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:41:12 10splitTasks](trainer.py 286): INFO [0/79]	0.8928(0.8928)	0.6904(0.6904)	1.358(1.358)	54.69(54.69)\
[2023-09-14 07:41:14 10splitTasks](trainer.py 286): INFO [10/79]	0.2000(0.2614)	0.0004(0.0632)	1.088(1.145)	62.50(64.06)\
[2023-09-14 07:41:16 10splitTasks](trainer.py 286): INFO [20/79]	0.1957(0.2320)	0.0004(0.0334)	0.906(1.119)	73.44(65.10)\
[2023-09-14 07:41:18 10splitTasks](trainer.py 286): INFO [30/79]	0.1964(0.2223)	0.0004(0.0228)	1.094(1.150)	68.75(64.26)\
[2023-09-14 07:41:20 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2161)	0.0003(0.0173)	1.360(1.178)	56.25(63.15)\
[2023-09-14 07:41:22 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2128)	0.0004(0.0140)	1.173(1.180)	60.94(63.20)\
[2023-09-14 07:41:24 10splitTasks](trainer.py 286): INFO [60/79]	0.1974(0.2100)	0.0004(0.0118)	1.210(1.182)	67.19(63.09)\
[2023-09-14 07:41:26 10splitTasks](trainer.py 286): INFO [70/79]	0.1987(0.2082)	0.0008(0.0102)	1.369(1.201)	56.25(62.50)\
[2023-09-14 07:41:27 10splitTasks](trainer.py 286): INFO [78/79]	0.0708(0.2053)	0.0001(0.0092)	1.900(1.200)	37.50(62.58)\
[2023-09-14 07:41:27 10splitTasks](trainer.py 288): INFO  * Train Acc 62.580\
[2023-09-14 07:41:29 10splitTasks](trainer.py 147): INFO  * Val Acc 57.200, Total time 1.67\
[2023-09-14 07:41:29 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:41:29 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:41:29 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:41:30 10splitTasks](trainer.py 286): INFO [0/79]	0.9692(0.9692)	0.7701(0.7701)	1.320(1.320)	57.81(57.81)\
[2023-09-14 07:41:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1971(0.2665)	0.0004(0.0704)	1.271(1.248)	54.69(58.81)\
[2023-09-14 07:41:34 10splitTasks](trainer.py 286): INFO [20/79]	0.1976(0.2332)	0.0003(0.0371)	1.209(1.235)	64.06(60.57)\
[2023-09-14 07:41:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2214)	0.0003(0.0253)	1.394(1.258)	53.12(60.28)\
[2023-09-14 07:41:38 10splitTasks](trainer.py 286): INFO [40/79]	0.1954(0.2152)	0.0003(0.0193)	1.125(1.220)	64.06(61.97)\
[2023-09-14 07:41:40 10splitTasks](trainer.py 286): INFO [50/79]	0.1966(0.2116)	0.0003(0.0156)	1.098(1.201)	64.06(62.87)\
[2023-09-14 07:41:42 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2094)	0.0003(0.0131)	1.409(1.198)	51.56(63.19)\
[2023-09-14 07:41:44 10splitTasks](trainer.py 286): INFO [70/79]	0.1967(0.2080)	0.0008(0.0113)	1.433(1.202)	51.56(63.01)\
[2023-09-14 07:41:45 10splitTasks](trainer.py 286): INFO [78/79]	0.0711(0.2050)	0.0001(0.0102)	1.705(1.198)	50.00(63.26)\
[2023-09-14 07:41:45 10splitTasks](trainer.py 288): INFO  * Train Acc 63.260\
[2023-09-14 07:41:47 10splitTasks](trainer.py 147): INFO  * Val Acc 56.200, Total time 1.65\
[2023-09-14 07:41:47 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:41:47 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:41:47 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:41:48 10splitTasks](trainer.py 286): INFO [0/79]	0.9532(0.9532)	0.7369(0.7369)	1.112(1.112)	67.19(67.19)\
[2023-09-14 07:41:50 10splitTasks](trainer.py 286): INFO [10/79]	0.1963(0.2657)	0.0004(0.0674)	1.063(1.150)	64.06(63.49)\
[2023-09-14 07:41:52 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2333)	0.0003(0.0356)	1.392(1.173)	62.50(64.06)\
[2023-09-14 07:41:54 10splitTasks](trainer.py 286): INFO [30/79]	0.2022(0.2224)	0.0007(0.0242)	1.240(1.171)	54.69(64.11)\
[2023-09-14 07:41:56 10splitTasks](trainer.py 286): INFO [40/79]	0.2060(0.2164)	0.0003(0.0184)	0.933(1.176)	78.12(63.99)\
[2023-09-14 07:41:58 10splitTasks](trainer.py 286): INFO [50/79]	0.1978(0.2132)	0.0006(0.0149)	1.069(1.187)	73.44(63.57)\
[2023-09-14 07:42:00 10splitTasks](trainer.py 286): INFO [60/79]	0.1974(0.2105)	0.0003(0.0125)	1.097(1.194)	65.62(63.65)\
[2023-09-14 07:42:02 10splitTasks](trainer.py 286): INFO [70/79]	0.1966(0.2087)	0.0008(0.0109)	1.261(1.179)	64.06(64.61)\
[2023-09-14 07:42:03 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2058)	0.0001(0.0098)	1.923(1.192)	25.00(64.16)\
[2023-09-14 07:42:03 10splitTasks](trainer.py 288): INFO  * Train Acc 64.160\
[2023-09-14 07:42:05 10splitTasks](trainer.py 147): INFO  * Val Acc 61.800, Total time 1.64\
[2023-09-14 07:42:05 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:42:05 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:42:05 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:42:06 10splitTasks](trainer.py 286): INFO [0/79]	0.9693(0.9693)	0.7709(0.7709)	0.980(0.980)	71.88(71.88)\
[2023-09-14 07:42:08 10splitTasks](trainer.py 286): INFO [10/79]	0.1946(0.2664)	0.0002(0.0705)	1.395(1.257)	56.25(62.50)\
[2023-09-14 07:42:10 10splitTasks](trainer.py 286): INFO [20/79]	0.1951(0.2329)	0.0006(0.0372)	1.091(1.172)	68.75(65.48)\
[2023-09-14 07:42:12 10splitTasks](trainer.py 286): INFO [30/79]	0.1977(0.2217)	0.0005(0.0254)	1.417(1.193)	62.50(65.47)\
[2023-09-14 07:42:14 10splitTasks](trainer.py 286): INFO [40/79]	0.2165(0.2164)	0.0004(0.0193)	1.105(1.199)	65.62(65.32)\
[2023-09-14 07:42:16 10splitTasks](trainer.py 286): INFO [50/79]	0.1972(0.2126)	0.0004(0.0156)	1.254(1.199)	62.50(64.95)\
[2023-09-14 07:42:18 10splitTasks](trainer.py 286): INFO [60/79]	0.1987(0.2100)	0.0005(0.0131)	1.234(1.193)	68.75(64.93)\
[2023-09-14 07:42:20 10splitTasks](trainer.py 286): INFO [70/79]	0.2025(0.2082)	0.0010(0.0114)	1.053(1.181)	64.06(65.71)\
[2023-09-14 07:42:21 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2053)	0.0001(0.0102)	1.528(1.181)	50.00(65.76)\
[2023-09-14 07:42:21 10splitTasks](trainer.py 288): INFO  * Train Acc 65.760\
[2023-09-14 07:42:23 10splitTasks](trainer.py 147): INFO  * Val Acc 62.000, Total time 1.66\
[2023-09-14 07:42:23 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:42:23 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:42:23 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:42:24 10splitTasks](trainer.py 286): INFO [0/79]	0.9052(0.9052)	0.7059(0.7059)	1.061(1.061)	75.00(75.00)\
[2023-09-14 07:42:26 10splitTasks](trainer.py 286): INFO [10/79]	0.1967(0.2612)	0.0005(0.0646)	1.081(1.159)	64.06(66.19)\
[2023-09-14 07:42:28 10splitTasks](trainer.py 286): INFO [20/79]	0.1976(0.2315)	0.0004(0.0341)	1.292(1.158)	62.50(66.00)\
[2023-09-14 07:42:30 10splitTasks](trainer.py 286): INFO [30/79]	0.1972(0.2208)	0.0005(0.0232)	0.934(1.129)	75.00(67.14)\
[2023-09-14 07:42:32 10splitTasks](trainer.py 286): INFO [40/79]	0.1968(0.2161)	0.0005(0.0177)	1.066(1.132)	68.75(67.04)\
[2023-09-14 07:42:34 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2122)	0.0004(0.0144)	1.174(1.143)	71.88(67.10)\
[2023-09-14 07:42:36 10splitTasks](trainer.py 286): INFO [60/79]	0.1981(0.2098)	0.0006(0.0121)	1.172(1.157)	68.75(66.85)\
[2023-09-14 07:42:38 10splitTasks](trainer.py 286): INFO [70/79]	0.1967(0.2082)	0.0010(0.0105)	1.193(1.162)	64.06(66.66)\
[2023-09-14 07:42:39 10splitTasks](trainer.py 286): INFO [78/79]	0.0713(0.2053)	0.0003(0.0094)	1.952(1.168)	50.00(66.60)\
[2023-09-14 07:42:39 10splitTasks](trainer.py 288): INFO  * Train Acc 66.600\
[2023-09-14 07:42:41 10splitTasks](trainer.py 147): INFO  * Val Acc 54.400, Total time 1.61\
[2023-09-14 07:42:41 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:42:41 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:42:41 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:42:42 10splitTasks](trainer.py 286): INFO [0/79]	0.9631(0.9631)	0.7504(0.7504)	1.252(1.252)	59.38(59.38)\
[2023-09-14 07:42:44 10splitTasks](trainer.py 286): INFO [10/79]	0.1966(0.2670)	0.0003(0.0686)	1.309(1.222)	64.06(63.78)\
[2023-09-14 07:42:46 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2344)	0.0004(0.0362)	1.352(1.205)	56.25(65.25)\
[2023-09-14 07:42:48 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2223)	0.0004(0.0247)	0.956(1.179)	71.88(66.08)\
[2023-09-14 07:42:50 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2164)	0.0004(0.0188)	1.282(1.175)	67.19(66.73)\
[2023-09-14 07:42:52 10splitTasks](trainer.py 286): INFO [50/79]	0.2029(0.2126)	0.0010(0.0152)	1.062(1.163)	71.88(66.82)\
[2023-09-14 07:42:53 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2104)	0.0004(0.0128)	1.250(1.161)	59.38(66.80)\
[2023-09-14 07:42:55 10splitTasks](trainer.py 286): INFO [70/79]	0.1969(0.2087)	0.0008(0.0111)	0.942(1.155)	75.00(67.08)\
[2023-09-14 07:42:57 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2057)	0.0001(0.0100)	0.934(1.155)	87.50(66.86)\
[2023-09-14 07:42:57 10splitTasks](trainer.py 288): INFO  * Train Acc 66.860\
[2023-09-14 07:42:59 10splitTasks](trainer.py 147): INFO  * Val Acc 64.000, Total time 1.63\
[2023-09-14 07:42:59 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:42:59 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:42:59 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:43:00 10splitTasks](trainer.py 286): INFO [0/79]	0.9426(0.9426)	0.7296(0.7296)	0.984(0.984)	78.12(78.12)\
[2023-09-14 07:43:02 10splitTasks](trainer.py 286): INFO [10/79]	0.1966(0.2645)	0.0003(0.0667)	0.808(1.099)	81.25(68.89)\
[2023-09-14 07:43:04 10splitTasks](trainer.py 286): INFO [20/79]	0.1967(0.2336)	0.0004(0.0352)	1.006(1.113)	75.00(69.12)\
[2023-09-14 07:43:05 10splitTasks](trainer.py 286): INFO [30/79]	0.2013(0.2223)	0.0004(0.0240)	1.144(1.137)	67.19(68.25)\
[2023-09-14 07:43:07 10splitTasks](trainer.py 286): INFO [40/79]	0.1966(0.2162)	0.0004(0.0183)	1.157(1.139)	65.62(67.99)\
[2023-09-14 07:43:09 10splitTasks](trainer.py 286): INFO [50/79]	0.1979(0.2125)	0.0004(0.0148)	1.116(1.138)	67.19(67.95)\
[2023-09-14 07:43:11 10splitTasks](trainer.py 286): INFO [60/79]	0.2006(0.2102)	0.0004(0.0125)	0.857(1.136)	81.25(68.06)\
[2023-09-14 07:43:13 10splitTasks](trainer.py 286): INFO [70/79]	0.1977(0.2089)	0.0011(0.0109)	1.285(1.140)	60.94(68.09)\
[2023-09-14 07:43:15 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2059)	0.0001(0.0098)	1.590(1.153)	50.00(67.74)\
[2023-09-14 07:43:15 10splitTasks](trainer.py 288): INFO  * Train Acc 67.740\
[2023-09-14 07:43:17 10splitTasks](trainer.py 147): INFO  * Val Acc 62.600, Total time 1.66\
[2023-09-14 07:43:17 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:43:17 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:43:17 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:43:18 10splitTasks](trainer.py 286): INFO [0/79]	0.9451(0.9451)	0.7356(0.7356)	1.101(1.101)	70.31(70.31)\
[2023-09-14 07:43:20 10splitTasks](trainer.py 286): INFO [10/79]	0.1983(0.2652)	0.0003(0.0673)	0.912(1.089)	73.44(69.89)\
[2023-09-14 07:43:21 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2333)	0.0004(0.0355)	1.044(1.097)	68.75(70.09)\
[2023-09-14 07:43:23 10splitTasks](trainer.py 286): INFO [30/79]	0.2023(0.2221)	0.0008(0.0242)	1.236(1.107)	71.88(70.31)\
[2023-09-14 07:43:25 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2162)	0.0003(0.0184)	1.084(1.112)	68.75(69.86)\
[2023-09-14 07:43:27 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2125)	0.0003(0.0149)	1.430(1.132)	64.06(68.90)\
[2023-09-14 07:43:29 10splitTasks](trainer.py 286): INFO [60/79]	0.1966(0.2100)	0.0004(0.0126)	1.254(1.146)	62.50(68.52)\
[2023-09-14 07:43:31 10splitTasks](trainer.py 286): INFO [70/79]	0.2011(0.2083)	0.0012(0.0109)	1.158(1.153)	64.06(68.44)\
[2023-09-14 07:43:33 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2054)	0.0001(0.0098)	1.283(1.158)	50.00(68.06)\
[2023-09-14 07:43:33 10splitTasks](trainer.py 288): INFO  * Train Acc 68.060\
[2023-09-14 07:43:35 10splitTasks](trainer.py 147): INFO  * Val Acc 62.600, Total time 1.67\
[2023-09-14 07:43:35 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:43:35 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:43:35 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:43:36 10splitTasks](trainer.py 286): INFO [0/79]	1.0010(1.0010)	0.7718(0.7718)	1.192(1.192)	65.62(65.62)\
[2023-09-14 07:43:38 10splitTasks](trainer.py 286): INFO [10/79]	0.2049(0.2766)	0.0002(0.0733)	1.076(1.200)	65.62(66.48)\
[2023-09-14 07:43:40 10splitTasks](trainer.py 286): INFO [20/79]	0.1978(0.2391)	0.0004(0.0386)	1.105(1.148)	64.06(68.08)\
[2023-09-14 07:43:42 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2255)	0.0006(0.0263)	1.016(1.137)	76.56(69.00)\
[2023-09-14 07:43:44 10splitTasks](trainer.py 286): INFO [40/79]	0.1970(0.2187)	0.0003(0.0200)	1.123(1.119)	68.75(69.66)\
[2023-09-14 07:43:46 10splitTasks](trainer.py 286): INFO [50/79]	0.1971(0.2149)	0.0005(0.0162)	1.269(1.132)	60.94(69.21)\
[2023-09-14 07:43:47 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2119)	0.0003(0.0136)	0.994(1.131)	70.31(69.42)\
[2023-09-14 07:43:49 10splitTasks](trainer.py 286): INFO [70/79]	0.1977(0.2098)	0.0010(0.0118)	1.220(1.130)	64.06(69.50)\
[2023-09-14 07:43:51 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2067)	0.0001(0.0106)	2.077(1.133)	37.50(69.44)\
[2023-09-14 07:43:51 10splitTasks](trainer.py 288): INFO  * Train Acc 69.440\
[2023-09-14 07:43:53 10splitTasks](trainer.py 147): INFO  * Val Acc 58.200, Total time 1.63\
[2023-09-14 07:43:53 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:43:53 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:43:53 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:43:54 10splitTasks](trainer.py 286): INFO [0/79]	0.9889(0.9889)	0.7908(0.7908)	1.148(1.148)	68.75(68.75)\
[2023-09-14 07:43:56 10splitTasks](trainer.py 286): INFO [10/79]	0.2004(0.2699)	0.0003(0.0723)	1.109(1.172)	70.31(65.62)\
[2023-09-14 07:43:58 10splitTasks](trainer.py 286): INFO [20/79]	0.1975(0.2351)	0.0003(0.0381)	0.866(1.129)	76.56(68.38)\
[2023-09-14 07:44:00 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2227)	0.0003(0.0259)	1.382(1.136)	64.06(68.65)\
[2023-09-14 07:44:01 10splitTasks](trainer.py 286): INFO [40/79]	0.1976(0.2167)	0.0004(0.0197)	1.034(1.130)	75.00(69.13)\
[2023-09-14 07:44:03 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2128)	0.0005(0.0159)	1.124(1.140)	73.44(68.84)\
[2023-09-14 07:44:05 10splitTasks](trainer.py 286): INFO [60/79]	0.1980(0.2104)	0.0021(0.0134)	1.281(1.143)	60.94(68.80)\
[2023-09-14 07:44:07 10splitTasks](trainer.py 286): INFO [70/79]	0.1983(0.2085)	0.0010(0.0116)	1.148(1.149)	71.88(68.66)\
[2023-09-14 07:44:09 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2056)	0.0001(0.0105)	1.719(1.152)	62.50(68.60)\
[2023-09-14 07:44:09 10splitTasks](trainer.py 288): INFO  * Train Acc 68.600\
[2023-09-14 07:44:11 10splitTasks](trainer.py 147): INFO  * Val Acc 61.800, Total time 1.68\
[2023-09-14 07:44:11 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:44:11 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:44:11 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:44:12 10splitTasks](trainer.py 286): INFO [0/79]	0.9683(0.9683)	0.7364(0.7364)	1.197(1.197)	65.62(65.62)\
[2023-09-14 07:44:14 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2678)	0.0004(0.0673)	1.078(1.115)	75.00(70.88)\
[2023-09-14 07:44:16 10splitTasks](trainer.py 286): INFO [20/79]	0.1969(0.2351)	0.0003(0.0355)	1.272(1.111)	67.19(70.91)\
[2023-09-14 07:44:18 10splitTasks](trainer.py 286): INFO [30/79]	0.1975(0.2234)	0.0008(0.0243)	1.100(1.098)	70.31(70.72)\
[2023-09-14 07:44:19 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2170)	0.0004(0.0185)	1.045(1.114)	71.88(70.39)\
[2023-09-14 07:44:21 10splitTasks](trainer.py 286): INFO [50/79]	0.1985(0.2132)	0.0003(0.0149)	0.893(1.121)	81.25(70.10)\
[2023-09-14 07:44:23 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2108)	0.0004(0.0126)	1.344(1.134)	67.19(69.80)\
[2023-09-14 07:44:25 10splitTasks](trainer.py 286): INFO [70/79]	0.2118(0.2092)	0.0014(0.0109)	1.380(1.153)	59.38(69.19)\
[2023-09-14 07:44:27 10splitTasks](trainer.py 286): INFO [78/79]	0.0707(0.2063)	0.0002(0.0098)	1.802(1.149)	37.50(69.30)\
[2023-09-14 07:44:27 10splitTasks](trainer.py 288): INFO  * Train Acc 69.300\
[2023-09-14 07:44:29 10splitTasks](trainer.py 147): INFO  * Val Acc 62.400, Total time 1.67\
[2023-09-14 07:44:29 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:44:29 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:44:29 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:44:30 10splitTasks](trainer.py 286): INFO [0/79]	1.0444(1.0444)	0.8464(0.8464)	0.950(0.950)	76.56(76.56)\
[2023-09-14 07:44:32 10splitTasks](trainer.py 286): INFO [10/79]	0.1961(0.2746)	0.0002(0.0773)	1.153(1.189)	64.06(66.62)\
[2023-09-14 07:44:34 10splitTasks](trainer.py 286): INFO [20/79]	0.2011(0.2384)	0.0007(0.0410)	1.197(1.205)	73.44(66.52)\
[2023-09-14 07:44:36 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2258)	0.0003(0.0279)	1.235(1.209)	62.50(66.28)\
[2023-09-14 07:44:38 10splitTasks](trainer.py 286): INFO [40/79]	0.1965(0.2192)	0.0004(0.0212)	0.912(1.201)	81.25(66.50)\
[2023-09-14 07:44:40 10splitTasks](trainer.py 286): INFO [50/79]	0.1982(0.2153)	0.0006(0.0172)	0.918(1.178)	78.12(67.46)\
[2023-09-14 07:44:42 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2124)	0.0003(0.0144)	1.129(1.185)	62.50(67.37)\
[2023-09-14 07:44:44 10splitTasks](trainer.py 286): INFO [70/79]	0.1985(0.2103)	0.0012(0.0125)	1.228(1.173)	64.06(67.76)\
[2023-09-14 07:44:45 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2072)	0.0001(0.0112)	1.397(1.169)	62.50(68.06)\
[2023-09-14 07:44:45 10splitTasks](trainer.py 288): INFO  * Train Acc 68.060\
[2023-09-14 07:44:47 10splitTasks](trainer.py 147): INFO  * Val Acc 61.600, Total time 1.63\
[2023-09-14 07:44:47 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:44:47 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:44:47 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:44:48 10splitTasks](trainer.py 286): INFO [0/79]	1.0553(1.0553)	0.8573(0.8573)	1.052(1.052)	68.75(68.75)\
[2023-09-14 07:44:50 10splitTasks](trainer.py 286): INFO [10/79]	0.1956(0.2750)	0.0004(0.0783)	1.131(1.070)	76.56(70.88)\
[2023-09-14 07:44:52 10splitTasks](trainer.py 286): INFO [20/79]	0.1965(0.2381)	0.0003(0.0412)	1.159(1.083)	70.31(71.28)\
[2023-09-14 07:44:54 10splitTasks](trainer.py 286): INFO [30/79]	0.1983(0.2250)	0.0003(0.0281)	1.106(1.084)	70.31(71.47)\
[2023-09-14 07:44:56 10splitTasks](trainer.py 286): INFO [40/79]	0.1995(0.2183)	0.0006(0.0213)	1.117(1.091)	67.19(71.23)\
[2023-09-14 07:44:58 10splitTasks](trainer.py 286): INFO [50/79]	0.2040(0.2145)	0.0006(0.0173)	1.405(1.109)	64.06(70.71)\
[2023-09-14 07:45:00 10splitTasks](trainer.py 286): INFO [60/79]	0.1954(0.2116)	0.0004(0.0145)	1.060(1.107)	70.31(71.16)\
[2023-09-14 07:45:02 10splitTasks](trainer.py 286): INFO [70/79]	0.1964(0.2096)	0.0008(0.0125)	1.204(1.116)	64.06(70.86)\
[2023-09-14 07:45:03 10splitTasks](trainer.py 286): INFO [78/79]	0.0704(0.2065)	0.0002(0.0113)	1.614(1.121)	50.00(70.58)\
[2023-09-14 07:45:03 10splitTasks](trainer.py 288): INFO  * Train Acc 70.580\
[2023-09-14 07:45:05 10splitTasks](trainer.py 147): INFO  * Val Acc 65.400, Total time 1.63\
[2023-09-14 07:45:05 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:45:05 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:45:05 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:45:06 10splitTasks](trainer.py 286): INFO [0/79]	1.0030(1.0030)	0.7978(0.7978)	0.925(0.925)	85.94(85.94)\
[2023-09-14 07:45:08 10splitTasks](trainer.py 286): INFO [10/79]	0.1973(0.2762)	0.0007(0.0760)	1.083(1.102)	68.75(71.73)\
[2023-09-14 07:45:10 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2390)	0.0004(0.0401)	1.198(1.114)	70.31(71.28)\
[2023-09-14 07:45:12 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2254)	0.0004(0.0273)	1.189(1.107)	71.88(71.67)\
[2023-09-14 07:45:14 10splitTasks](trainer.py 286): INFO [40/79]	0.1967(0.2184)	0.0004(0.0208)	1.057(1.101)	76.56(72.03)\
[2023-09-14 07:45:16 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2141)	0.0004(0.0168)	1.316(1.124)	64.06(70.71)\
[2023-09-14 07:45:18 10splitTasks](trainer.py 286): INFO [60/79]	0.1953(0.2113)	0.0004(0.0141)	1.081(1.134)	73.44(70.36)\
[2023-09-14 07:45:20 10splitTasks](trainer.py 286): INFO [70/79]	0.1972(0.2092)	0.0010(0.0122)	1.470(1.142)	60.94(70.31)\
[2023-09-14 07:45:21 10splitTasks](trainer.py 286): INFO [78/79]	0.0705(0.2062)	0.0001(0.0110)	1.571(1.146)	50.00(69.98)\
[2023-09-14 07:45:21 10splitTasks](trainer.py 288): INFO  * Train Acc 69.980\
[2023-09-14 07:45:23 10splitTasks](trainer.py 147): INFO  * Val Acc 64.200, Total time 1.68\
[2023-09-14 07:45:23 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:45:23 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:45:23 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:45:24 10splitTasks](trainer.py 286): INFO [0/79]	0.9783(0.9783)	0.7626(0.7626)	1.248(1.248)	65.62(65.62)\
[2023-09-14 07:45:26 10splitTasks](trainer.py 286): INFO [10/79]	0.1964(0.2683)	0.0003(0.0697)	1.131(1.234)	67.19(67.76)\
[2023-09-14 07:45:28 10splitTasks](trainer.py 286): INFO [20/79]	0.2016(0.2348)	0.0003(0.0368)	1.393(1.187)	65.62(69.49)\
[2023-09-14 07:45:30 10splitTasks](trainer.py 286): INFO [30/79]	0.2061(0.2229)	0.0006(0.0251)	1.411(1.189)	54.69(69.05)\
[2023-09-14 07:45:32 10splitTasks](trainer.py 286): INFO [40/79]	0.2085(0.2168)	0.0006(0.0191)	0.922(1.170)	81.25(69.82)\
[2023-09-14 07:45:34 10splitTasks](trainer.py 286): INFO [50/79]	0.1972(0.2129)	0.0006(0.0154)	1.270(1.169)	73.44(70.04)\
[2023-09-14 07:45:36 10splitTasks](trainer.py 286): INFO [60/79]	0.1963(0.2104)	0.0003(0.0130)	1.149(1.164)	68.75(69.70)\
[2023-09-14 07:45:38 10splitTasks](trainer.py 286): INFO [70/79]	0.1984(0.2086)	0.0010(0.0112)	1.117(1.161)	73.44(69.74)\
[2023-09-14 07:45:39 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2056)	0.0001(0.0101)	2.069(1.166)	37.50(69.40)\
[2023-09-14 07:45:39 10splitTasks](trainer.py 288): INFO  * Train Acc 69.400\
[2023-09-14 07:45:41 10splitTasks](trainer.py 147): INFO  * Val Acc 60.600, Total time 1.75\
[2023-09-14 07:45:41 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:45:41 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:45:41 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:45:42 10splitTasks](trainer.py 286): INFO [0/79]	0.9608(0.9608)	0.7619(0.7619)	1.117(1.117)	68.75(68.75)\
[2023-09-14 07:45:44 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2712)	0.0003(0.0728)	1.216(1.230)	67.19(66.34)\
[2023-09-14 07:45:46 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2361)	0.0004(0.0384)	1.613(1.212)	56.25(67.26)\
[2023-09-14 07:45:48 10splitTasks](trainer.py 286): INFO [30/79]	0.1976(0.2245)	0.0005(0.0262)	0.987(1.199)	76.56(67.74)\
[2023-09-14 07:45:50 10splitTasks](trainer.py 286): INFO [40/79]	0.1967(0.2176)	0.0004(0.0199)	1.125(1.188)	68.75(68.41)\
[2023-09-14 07:45:52 10splitTasks](trainer.py 286): INFO [50/79]	0.1963(0.2135)	0.0004(0.0161)	1.005(1.185)	71.88(68.47)\
[2023-09-14 07:45:54 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2107)	0.0003(0.0135)	1.014(1.176)	73.44(68.60)\
[2023-09-14 07:45:56 10splitTasks](trainer.py 286): INFO [70/79]	0.1974(0.2087)	0.0010(0.0117)	1.320(1.175)	59.38(68.84)\
[2023-09-14 07:45:57 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2058)	0.0001(0.0105)	1.823(1.170)	50.00(69.08)\
[2023-09-14 07:45:57 10splitTasks](trainer.py 288): INFO  * Train Acc 69.080\
[2023-09-14 07:45:59 10splitTasks](trainer.py 147): INFO  * Val Acc 57.600, Total time 1.64\
[2023-09-14 07:45:59 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:45:59 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:45:59 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:46:00 10splitTasks](trainer.py 286): INFO [0/79]	1.0079(1.0079)	0.8102(0.8102)	1.273(1.273)	62.50(62.50)\
[2023-09-14 07:46:02 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2728)	0.0003(0.0740)	1.237(1.121)	73.44(72.87)\
[2023-09-14 07:46:04 10splitTasks](trainer.py 286): INFO [20/79]	0.1966(0.2371)	0.0004(0.0390)	1.183(1.146)	68.75(71.06)\
[2023-09-14 07:46:06 10splitTasks](trainer.py 286): INFO [30/79]	0.1967(0.2250)	0.0004(0.0266)	1.047(1.134)	75.00(70.82)\
[2023-09-14 07:46:08 10splitTasks](trainer.py 286): INFO [40/79]	0.1999(0.2183)	0.0005(0.0202)	1.133(1.145)	67.19(70.01)\
[2023-09-14 07:46:10 10splitTasks](trainer.py 286): INFO [50/79]	0.1975(0.2141)	0.0004(0.0164)	1.105(1.148)	68.75(69.98)\
[2023-09-14 07:46:12 10splitTasks](trainer.py 286): INFO [60/79]	0.1964(0.2113)	0.0004(0.0138)	1.055(1.145)	73.44(70.24)\
[2023-09-14 07:46:14 10splitTasks](trainer.py 286): INFO [70/79]	0.1988(0.2095)	0.0010(0.0119)	0.994(1.151)	75.00(70.03)\
[2023-09-14 07:46:15 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2064)	0.0001(0.0107)	1.012(1.151)	75.00(70.10)\
[2023-09-14 07:46:15 10splitTasks](trainer.py 288): INFO  * Train Acc 70.100\
[2023-09-14 07:46:17 10splitTasks](trainer.py 147): INFO  * Val Acc 64.800, Total time 1.70\
[2023-09-14 07:46:17 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:46:17 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:46:17 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:46:18 10splitTasks](trainer.py 286): INFO [0/79]	0.9946(0.9946)	0.7954(0.7954)	1.234(1.234)	70.31(70.31)\
[2023-09-14 07:46:20 10splitTasks](trainer.py 286): INFO [10/79]	0.1987(0.2754)	0.0004(0.0769)	1.054(1.083)	70.31(70.31)\
[2023-09-14 07:46:22 10splitTasks](trainer.py 286): INFO [20/79]	0.1958(0.2381)	0.0004(0.0405)	1.084(1.089)	64.06(69.87)\
[2023-09-14 07:46:24 10splitTasks](trainer.py 286): INFO [30/79]	0.1970(0.2251)	0.0014(0.0276)	0.904(1.102)	75.00(70.21)\
[2023-09-14 07:46:26 10splitTasks](trainer.py 286): INFO [40/79]	0.2033(0.2182)	0.0006(0.0210)	1.155(1.129)	68.75(69.36)\
[2023-09-14 07:46:28 10splitTasks](trainer.py 286): INFO [50/79]	0.1967(0.2141)	0.0004(0.0170)	1.276(1.139)	70.31(69.33)\
[2023-09-14 07:46:30 10splitTasks](trainer.py 286): INFO [60/79]	0.1959(0.2113)	0.0004(0.0143)	1.101(1.142)	73.44(69.75)\
[2023-09-14 07:46:32 10splitTasks](trainer.py 286): INFO [70/79]	0.1966(0.2092)	0.0010(0.0123)	1.310(1.149)	68.75(69.59)\
[2023-09-14 07:46:33 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2062)	0.0001(0.0111)	1.955(1.149)	37.50(69.34)\
[2023-09-14 07:46:33 10splitTasks](trainer.py 288): INFO  * Train Acc 69.340\
[2023-09-14 07:46:35 10splitTasks](trainer.py 147): INFO  * Val Acc 62.200, Total time 1.64\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-8.pth\
=> Save Done\
[2023-09-14 07:46:35 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:46:37 10splitTasks](trainer.py 147): INFO  * Val Acc 26.200, Total time 1.46\
[2023-09-14 07:46:37 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:46:38 10splitTasks](trainer.py 147): INFO  * Val Acc 31.200, Total time 1.57\
[2023-09-14 07:46:38 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:46:40 10splitTasks](trainer.py 147): INFO  * Val Acc 44.600, Total time 1.62\
[2023-09-14 07:46:40 10splitTasks](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 07:46:42 10splitTasks](trainer.py 147): INFO  * Val Acc 42.200, Total time 1.82\
[2023-09-14 07:46:42 10splitTasks](iBatchLearn.py 131): INFO validation split name:4\
[2023-09-14 07:46:43 10splitTasks](trainer.py 147): INFO  * Val Acc 34.800, Total time 1.59\
[2023-09-14 07:46:43 10splitTasks](iBatchLearn.py 131): INFO validation split name:5\
[2023-09-14 07:46:45 10splitTasks](trainer.py 147): INFO  * Val Acc 40.800, Total time 1.55\
[2023-09-14 07:46:45 10splitTasks](iBatchLearn.py 131): INFO validation split name:6\
[2023-09-14 07:46:47 10splitTasks](trainer.py 147): INFO  * Val Acc 42.400, Total time 1.79\
[2023-09-14 07:46:47 10splitTasks](iBatchLearn.py 131): INFO validation split name:7\
[2023-09-14 07:46:48 10splitTasks](trainer.py 147): INFO  * Val Acc 44.200, Total time 1.71\
[2023-09-14 07:46:48 10splitTasks](iBatchLearn.py 131): INFO validation split name:8\
[2023-09-14 07:46:50 10splitTasks](trainer.py 147): INFO  * Val Acc 62.200, Total time 1.71\
[2023-09-14 07:46:50 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:46:50 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:46:50 10splitTasks](iBatchLearn.py 155): INFO Acc:40.95555582767062; BWT:-18.549999958992004;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:46:55 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:46:55 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:46:55 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 8, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-8.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_8.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:46:55 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-8.pth\
[2023-09-14 07:46:55 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:46:57 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:46:57 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:46:57 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:47:01 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:47:04 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 07:47:07 10splitTasks](iBatchLearn.py 167): INFO test split name:3\
[2023-09-14 07:47:10 10splitTasks](iBatchLearn.py 167): INFO test split name:4\
[2023-09-14 07:47:12 10splitTasks](iBatchLearn.py 167): INFO test split name:5\
[2023-09-14 07:47:15 10splitTasks](iBatchLearn.py 167): INFO test split name:6\
[2023-09-14 07:47:18 10splitTasks](iBatchLearn.py 167): INFO test split name:7\
[2023-09-14 07:47:21 10splitTasks](iBatchLearn.py 167): INFO test split name:8\
--------------------------------Official Evaluation--------------------------------\
8 22.525000000000002\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:47:30 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:47:30 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:47:30 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 9, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-8.pth", "save_ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-9.pth", "storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-8.pth", "save_storage_path": "outputs/2023-09-14-06:17:11/10splitTasks/storage-9.pth", "dest_path": null, "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:47:30 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-8.pth\
[2023-09-14 07:47:30 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:47:32 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:47:32 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:47:32 10splitTasks](trainer.py 327): INFO load storage...\
[2023-09-14 07:47:32 10splitTasks](trainer.py 331): INFO done\
[2023-09-14 07:47:32 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0\
[2023-09-14 07:47:32 10splitTasks](iBatchLearn.py 92): INFO ====================== 9 =======================\
[2023-09-14 07:47:32 10splitTasks](regularization.py 45): INFO reg_term: , 1\
[2023-09-14 07:47:32 10splitTasks](trainer.py 207): INFO Optimizer is reset!\
[2023-09-14 07:47:32 10splitTasks](trainer.py 223): INFO Epoch:0\
[2023-09-14 07:47:32 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:47:32 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:47:36 10splitTasks](trainer.py 286): INFO [0/79]	3.3419(3.3419)	0.9237(0.9237)	2.605(2.605)	4.69(4.69)\
[2023-09-14 07:47:38 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.4838)	0.0004(0.0844)	1.781(2.182)	48.44(22.87)\
[2023-09-14 07:47:40 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.3483)	0.0003(0.0445)	1.503(1.939)	51.56(32.96)\
[2023-09-14 07:47:42 10splitTasks](trainer.py 286): INFO [30/79]	0.1988(0.2993)	0.0006(0.0303)	1.465(1.806)	48.44(37.70)\
[2023-09-14 07:47:44 10splitTasks](trainer.py 286): INFO [40/79]	0.1977(0.2748)	0.0005(0.0230)	1.247(1.721)	56.25(40.89)\
[2023-09-14 07:47:46 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2594)	0.0003(0.0186)	1.240(1.664)	59.38(43.01)\
[2023-09-14 07:47:47 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2490)	0.0003(0.0156)	1.499(1.622)	43.75(44.54)\
[2023-09-14 07:47:49 10splitTasks](trainer.py 286): INFO [70/79]	0.1957(0.2415)	0.0008(0.0135)	1.308(1.572)	65.62(46.57)\
[2023-09-14 07:47:51 10splitTasks](trainer.py 286): INFO [78/79]	0.2192(0.2371)	0.0001(0.0122)	1.686(1.547)	25.00(47.46)\
[2023-09-14 07:47:51 10splitTasks](trainer.py 288): INFO  * Train Acc 47.460\
[2023-09-14 07:47:53 10splitTasks](trainer.py 147): INFO  * Val Acc 59.400, Total time 1.61\
[2023-09-14 07:47:53 10splitTasks](trainer.py 223): INFO Epoch:1\
[2023-09-14 07:47:53 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:47:53 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:47:54 10splitTasks](trainer.py 286): INFO [0/79]	0.9330(0.9330)	0.7075(0.7075)	1.235(1.235)	57.81(57.81)\
[2023-09-14 07:47:56 10splitTasks](trainer.py 286): INFO [10/79]	0.1957(0.2662)	0.0003(0.0666)	1.427(1.199)	50.00(58.38)\
[2023-09-14 07:47:58 10splitTasks](trainer.py 286): INFO [20/79]	0.1959(0.2328)	0.0003(0.0351)	1.337(1.231)	65.62(59.15)\
[2023-09-14 07:48:00 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2214)	0.0004(0.0239)	1.342(1.234)	64.06(59.32)\
[2023-09-14 07:48:02 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2153)	0.0003(0.0182)	1.087(1.232)	67.19(60.18)\
[2023-09-14 07:48:03 10splitTasks](trainer.py 286): INFO [50/79]	0.1965(0.2116)	0.0004(0.0147)	1.057(1.229)	62.50(60.66)\
[2023-09-14 07:48:05 10splitTasks](trainer.py 286): INFO [60/79]	0.1982(0.2092)	0.0006(0.0124)	1.279(1.232)	53.12(60.27)\
[2023-09-14 07:48:07 10splitTasks](trainer.py 286): INFO [70/79]	0.2102(0.2079)	0.0010(0.0107)	1.163(1.224)	60.94(60.54)\
[2023-09-14 07:48:09 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2049)	0.0001(0.0097)	1.362(1.217)	50.00(60.80)\
[2023-09-14 07:48:09 10splitTasks](trainer.py 288): INFO  * Train Acc 60.800\
[2023-09-14 07:48:10 10splitTasks](trainer.py 147): INFO  * Val Acc 50.400, Total time 1.55\
[2023-09-14 07:48:10 10splitTasks](trainer.py 223): INFO Epoch:2\
[2023-09-14 07:48:11 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:48:11 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:48:11 10splitTasks](trainer.py 286): INFO [0/79]	0.9643(0.9643)	0.7638(0.7638)	1.266(1.266)	57.81(57.81)\
[2023-09-14 07:48:13 10splitTasks](trainer.py 286): INFO [10/79]	0.1963(0.2675)	0.0004(0.0699)	0.932(1.177)	73.44(65.62)\
[2023-09-14 07:48:15 10splitTasks](trainer.py 286): INFO [20/79]	0.1959(0.2350)	0.0004(0.0370)	0.913(1.133)	68.75(65.10)\
[2023-09-14 07:48:17 10splitTasks](trainer.py 286): INFO [30/79]	0.1950(0.2234)	0.0004(0.0256)	0.973(1.131)	70.31(65.42)\
[2023-09-14 07:48:19 10splitTasks](trainer.py 286): INFO [40/79]	0.1955(0.2169)	0.0003(0.0195)	1.564(1.138)	54.69(65.09)\
[2023-09-14 07:48:21 10splitTasks](trainer.py 286): INFO [50/79]	0.1971(0.2131)	0.0004(0.0158)	1.199(1.139)	65.62(64.74)\
[2023-09-14 07:48:23 10splitTasks](trainer.py 286): INFO [60/79]	0.1981(0.2110)	0.0020(0.0133)	1.110(1.150)	59.38(64.34)\
[2023-09-14 07:48:25 10splitTasks](trainer.py 286): INFO [70/79]	0.1965(0.2095)	0.0012(0.0116)	1.437(1.168)	59.38(63.95)\
[2023-09-14 07:48:27 10splitTasks](trainer.py 286): INFO [78/79]	0.0716(0.2065)	0.0002(0.0104)	1.474(1.168)	62.50(64.08)\
[2023-09-14 07:48:27 10splitTasks](trainer.py 288): INFO  * Train Acc 64.080\
[2023-09-14 07:48:28 10splitTasks](trainer.py 147): INFO  * Val Acc 55.600, Total time 1.52\
[2023-09-14 07:48:28 10splitTasks](trainer.py 223): INFO Epoch:3\
[2023-09-14 07:48:28 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:48:28 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:48:29 10splitTasks](trainer.py 286): INFO [0/79]	0.9425(0.9425)	0.7314(0.7314)	1.369(1.369)	60.94(60.94)\
[2023-09-14 07:48:31 10splitTasks](trainer.py 286): INFO [10/79]	0.1993(0.2643)	0.0002(0.0669)	1.380(1.235)	56.25(61.79)\
[2023-09-14 07:48:33 10splitTasks](trainer.py 286): INFO [20/79]	0.2022(0.2323)	0.0006(0.0353)	1.229(1.224)	67.19(63.10)\
[2023-09-14 07:48:35 10splitTasks](trainer.py 286): INFO [30/79]	0.1961(0.2209)	0.0004(0.0241)	1.007(1.181)	67.19(64.06)\
[2023-09-14 07:48:37 10splitTasks](trainer.py 286): INFO [40/79]	0.1971(0.2148)	0.0006(0.0183)	1.322(1.154)	73.44(65.82)\
[2023-09-14 07:48:39 10splitTasks](trainer.py 286): INFO [50/79]	0.1959(0.2113)	0.0004(0.0148)	0.856(1.131)	76.56(66.61)\
[2023-09-14 07:48:41 10splitTasks](trainer.py 286): INFO [60/79]	0.2011(0.2090)	0.0006(0.0125)	1.097(1.132)	64.06(66.44)\
[2023-09-14 07:48:43 10splitTasks](trainer.py 286): INFO [70/79]	0.1963(0.2072)	0.0011(0.0108)	1.040(1.137)	70.31(66.31)\
[2023-09-14 07:48:45 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2043)	0.0001(0.0097)	1.382(1.140)	62.50(66.36)\
[2023-09-14 07:48:45 10splitTasks](trainer.py 288): INFO  * Train Acc 66.360\
[2023-09-14 07:48:46 10splitTasks](trainer.py 147): INFO  * Val Acc 64.000, Total time 1.55\
[2023-09-14 07:48:46 10splitTasks](trainer.py 223): INFO Epoch:4\
[2023-09-14 07:48:46 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:48:46 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:48:47 10splitTasks](trainer.py 286): INFO [0/79]	1.0093(1.0093)	0.8115(0.8115)	0.885(0.885)	71.88(71.88)\
[2023-09-14 07:48:49 10splitTasks](trainer.py 286): INFO [10/79]	0.1950(0.2704)	0.0003(0.0742)	1.013(1.071)	76.56(69.60)\
[2023-09-14 07:48:51 10splitTasks](trainer.py 286): INFO [20/79]	0.1959(0.2354)	0.0004(0.0391)	0.968(1.098)	68.75(68.60)\
[2023-09-14 07:48:53 10splitTasks](trainer.py 286): INFO [30/79]	0.1954(0.2229)	0.0007(0.0266)	1.005(1.109)	75.00(67.74)\
[2023-09-14 07:48:55 10splitTasks](trainer.py 286): INFO [40/79]	0.1957(0.2169)	0.0004(0.0203)	0.896(1.123)	73.44(66.88)\
[2023-09-14 07:48:57 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2128)	0.0004(0.0164)	0.967(1.121)	71.88(66.73)\
[2023-09-14 07:48:59 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2101)	0.0003(0.0138)	1.236(1.128)	60.94(66.57)\
[2023-09-14 07:49:01 10splitTasks](trainer.py 286): INFO [70/79]	0.1971(0.2083)	0.0010(0.0119)	1.161(1.119)	62.50(67.21)\
[2023-09-14 07:49:02 10splitTasks](trainer.py 286): INFO [78/79]	0.0740(0.2054)	0.0001(0.0107)	1.991(1.124)	37.50(67.06)\
[2023-09-14 07:49:02 10splitTasks](trainer.py 288): INFO  * Train Acc 67.060\
[2023-09-14 07:49:04 10splitTasks](trainer.py 147): INFO  * Val Acc 60.400, Total time 1.60\
[2023-09-14 07:49:04 10splitTasks](trainer.py 223): INFO Epoch:5\
[2023-09-14 07:49:04 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:49:04 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:49:05 10splitTasks](trainer.py 286): INFO [0/79]	0.9607(0.9607)	0.7472(0.7472)	1.042(1.042)	73.44(73.44)\
[2023-09-14 07:49:07 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2671)	0.0003(0.0685)	1.400(1.234)	59.38(66.76)\
[2023-09-14 07:49:09 10splitTasks](trainer.py 286): INFO [20/79]	0.1959(0.2343)	0.0003(0.0362)	1.248(1.180)	59.38(68.08)\
[2023-09-14 07:49:11 10splitTasks](trainer.py 286): INFO [30/79]	0.1988(0.2220)	0.0008(0.0247)	1.232(1.170)	62.50(67.39)\
[2023-09-14 07:49:13 10splitTasks](trainer.py 286): INFO [40/79]	0.1963(0.2159)	0.0003(0.0188)	1.114(1.163)	73.44(67.38)\
[2023-09-14 07:49:15 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2122)	0.0004(0.0152)	1.001(1.158)	73.44(67.77)\
[2023-09-14 07:49:17 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2096)	0.0003(0.0128)	1.019(1.142)	73.44(68.08)\
[2023-09-14 07:49:19 10splitTasks](trainer.py 286): INFO [70/79]	0.1975(0.2078)	0.0010(0.0111)	1.199(1.127)	67.19(68.55)\
[2023-09-14 07:49:20 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2049)	0.0001(0.0100)	0.664(1.120)	75.00(68.88)\
[2023-09-14 07:49:20 10splitTasks](trainer.py 288): INFO  * Train Acc 68.880\
[2023-09-14 07:49:22 10splitTasks](trainer.py 147): INFO  * Val Acc 71.200, Total time 1.52\
[2023-09-14 07:49:22 10splitTasks](trainer.py 223): INFO Epoch:6\
[2023-09-14 07:49:22 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:49:22 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:49:23 10splitTasks](trainer.py 286): INFO [0/79]	1.1917(1.1917)	0.9941(0.9941)	0.801(0.801)	75.00(75.00)\
[2023-09-14 07:49:25 10splitTasks](trainer.py 286): INFO [10/79]	0.1957(0.2868)	0.0003(0.0907)	1.102(1.099)	75.00(68.61)\
[2023-09-14 07:49:27 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2444)	0.0003(0.0478)	1.057(1.103)	68.75(68.60)\
[2023-09-14 07:49:29 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2295)	0.0006(0.0325)	1.116(1.089)	71.88(69.71)\
[2023-09-14 07:49:31 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2218)	0.0006(0.0247)	0.994(1.077)	70.31(70.08)\
[2023-09-14 07:49:33 10splitTasks](trainer.py 286): INFO [50/79]	0.1972(0.2168)	0.0003(0.0200)	1.010(1.093)	81.25(69.76)\
[2023-09-14 07:49:35 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2137)	0.0003(0.0168)	1.192(1.098)	68.75(69.47)\
[2023-09-14 07:49:37 10splitTasks](trainer.py 286): INFO [70/79]	0.1999(0.2113)	0.0011(0.0145)	1.394(1.113)	60.94(68.75)\
[2023-09-14 07:49:38 10splitTasks](trainer.py 286): INFO [78/79]	0.0734(0.2082)	0.0001(0.0130)	1.683(1.115)	75.00(68.76)\
[2023-09-14 07:49:38 10splitTasks](trainer.py 288): INFO  * Train Acc 68.760\
[2023-09-14 07:49:40 10splitTasks](trainer.py 147): INFO  * Val Acc 60.400, Total time 1.52\
[2023-09-14 07:49:40 10splitTasks](trainer.py 223): INFO Epoch:7\
[2023-09-14 07:49:40 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:49:40 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:49:41 10splitTasks](trainer.py 286): INFO [0/79]	0.8982(0.8982)	0.6958(0.6958)	0.928(0.928)	78.12(78.12)\
[2023-09-14 07:49:43 10splitTasks](trainer.py 286): INFO [10/79]	0.1959(0.2601)	0.0004(0.0636)	1.063(1.062)	70.31(72.02)\
[2023-09-14 07:49:45 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2301)	0.0003(0.0335)	1.194(1.097)	62.50(70.31)\
[2023-09-14 07:49:47 10splitTasks](trainer.py 286): INFO [30/79]	0.1993(0.2193)	0.0003(0.0229)	1.136(1.102)	67.19(69.81)\
[2023-09-14 07:49:49 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2138)	0.0003(0.0174)	1.028(1.114)	73.44(69.59)\
[2023-09-14 07:49:51 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2103)	0.0003(0.0141)	1.091(1.102)	70.31(69.61)\
[2023-09-14 07:49:53 10splitTasks](trainer.py 286): INFO [60/79]	0.1962(0.2080)	0.0004(0.0118)	1.139(1.105)	65.62(69.60)\
[2023-09-14 07:49:55 10splitTasks](trainer.py 286): INFO [70/79]	0.1974(0.2067)	0.0012(0.0102)	0.966(1.107)	71.88(69.39)\
[2023-09-14 07:49:56 10splitTasks](trainer.py 286): INFO [78/79]	0.0703(0.2040)	0.0001(0.0092)	1.076(1.103)	75.00(69.62)\
[2023-09-14 07:49:56 10splitTasks](trainer.py 288): INFO  * Train Acc 69.620\
[2023-09-14 07:49:58 10splitTasks](trainer.py 147): INFO  * Val Acc 65.800, Total time 1.51\
[2023-09-14 07:49:58 10splitTasks](trainer.py 223): INFO Epoch:8\
[2023-09-14 07:49:58 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:49:58 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:49:59 10splitTasks](trainer.py 286): INFO [0/79]	0.9693(0.9693)	0.7718(0.7718)	1.034(1.034)	67.19(67.19)\
[2023-09-14 07:50:01 10splitTasks](trainer.py 286): INFO [10/79]	0.1957(0.2677)	0.0003(0.0705)	1.117(1.086)	71.88(71.16)\
[2023-09-14 07:50:03 10splitTasks](trainer.py 286): INFO [20/79]	0.1970(0.2350)	0.0004(0.0372)	1.066(1.123)	70.31(70.68)\
[2023-09-14 07:50:05 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2226)	0.0003(0.0253)	1.226(1.104)	68.75(70.67)\
[2023-09-14 07:50:06 10splitTasks](trainer.py 286): INFO [40/79]	0.1961(0.2165)	0.0004(0.0193)	1.078(1.110)	78.12(70.62)\
[2023-09-14 07:50:08 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2125)	0.0004(0.0156)	0.971(1.096)	81.25(71.14)\
[2023-09-14 07:50:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1986(0.2099)	0.0003(0.0131)	1.424(1.106)	56.25(70.98)\
[2023-09-14 07:50:12 10splitTasks](trainer.py 286): INFO [70/79]	0.1977(0.2080)	0.0010(0.0113)	1.108(1.103)	71.88(71.26)\
[2023-09-14 07:50:14 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2051)	0.0001(0.0102)	0.844(1.109)	87.50(71.08)\
[2023-09-14 07:50:14 10splitTasks](trainer.py 288): INFO  * Train Acc 71.080\
[2023-09-14 07:50:15 10splitTasks](trainer.py 147): INFO  * Val Acc 64.200, Total time 1.50\
[2023-09-14 07:50:15 10splitTasks](trainer.py 223): INFO Epoch:9\
[2023-09-14 07:50:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:50:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:50:16 10splitTasks](trainer.py 286): INFO [0/79]	1.0514(1.0514)	0.8539(0.8539)	0.868(0.868)	75.00(75.00)\
[2023-09-14 07:50:18 10splitTasks](trainer.py 286): INFO [10/79]	0.1971(0.2760)	0.0003(0.0780)	0.997(1.101)	73.44(71.02)\
[2023-09-14 07:50:20 10splitTasks](trainer.py 286): INFO [20/79]	0.1961(0.2380)	0.0004(0.0411)	1.238(1.070)	68.75(72.92)\
[2023-09-14 07:50:22 10splitTasks](trainer.py 286): INFO [30/79]	0.1952(0.2249)	0.0003(0.0280)	0.917(1.054)	76.56(73.54)\
[2023-09-14 07:50:24 10splitTasks](trainer.py 286): INFO [40/79]	0.1967(0.2182)	0.0004(0.0213)	1.131(1.056)	70.31(73.25)\
[2023-09-14 07:50:26 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2139)	0.0003(0.0172)	0.877(1.062)	78.12(72.67)\
[2023-09-14 07:50:28 10splitTasks](trainer.py 286): INFO [60/79]	0.1961(0.2111)	0.0003(0.0144)	1.009(1.067)	71.88(72.41)\
[2023-09-14 07:50:30 10splitTasks](trainer.py 286): INFO [70/79]	0.1973(0.2091)	0.0011(0.0125)	1.008(1.071)	78.12(72.45)\
[2023-09-14 07:50:32 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2061)	0.0001(0.0112)	1.503(1.086)	50.00(71.76)\
[2023-09-14 07:50:32 10splitTasks](trainer.py 288): INFO  * Train Acc 71.760\
[2023-09-14 07:50:33 10splitTasks](trainer.py 147): INFO  * Val Acc 58.800, Total time 1.67\
[2023-09-14 07:50:33 10splitTasks](trainer.py 223): INFO Epoch:10\
[2023-09-14 07:50:33 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:50:33 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:50:35 10splitTasks](trainer.py 286): INFO [0/79]	1.1777(1.1777)	0.9737(0.9737)	1.061(1.061)	73.44(73.44)\
[2023-09-14 07:50:37 10splitTasks](trainer.py 286): INFO [10/79]	0.2027(0.2887)	0.0006(0.0890)	0.796(1.099)	81.25(70.60)\
[2023-09-14 07:50:39 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2460)	0.0004(0.0469)	0.958(1.109)	79.69(70.24)\
[2023-09-14 07:50:41 10splitTasks](trainer.py 286): INFO [30/79]	0.1966(0.2306)	0.0004(0.0319)	0.994(1.074)	76.56(71.77)\
[2023-09-14 07:50:43 10splitTasks](trainer.py 286): INFO [40/79]	0.1977(0.2225)	0.0004(0.0243)	0.951(1.091)	78.12(71.42)\
[2023-09-14 07:50:45 10splitTasks](trainer.py 286): INFO [50/79]	0.1994(0.2182)	0.0004(0.0196)	1.341(1.085)	57.81(71.35)\
[2023-09-14 07:50:47 10splitTasks](trainer.py 286): INFO [60/79]	0.1998(0.2148)	0.0005(0.0165)	1.068(1.077)	67.19(71.59)\
[2023-09-14 07:50:48 10splitTasks](trainer.py 286): INFO [70/79]	0.1976(0.2123)	0.0012(0.0142)	0.958(1.073)	75.00(71.85)\
[2023-09-14 07:50:50 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2090)	0.0001(0.0128)	2.234(1.073)	62.50(72.04)\
[2023-09-14 07:50:50 10splitTasks](trainer.py 288): INFO  * Train Acc 72.040\
[2023-09-14 07:50:52 10splitTasks](trainer.py 147): INFO  * Val Acc 61.600, Total time 1.52\
[2023-09-14 07:50:52 10splitTasks](trainer.py 223): INFO Epoch:11\
[2023-09-14 07:50:52 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:50:52 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:50:53 10splitTasks](trainer.py 286): INFO [0/79]	0.9864(0.9864)	0.7792(0.7792)	0.841(0.841)	79.69(79.69)\
[2023-09-14 07:50:54 10splitTasks](trainer.py 286): INFO [10/79]	0.1963(0.2682)	0.0003(0.0712)	1.239(1.087)	62.50(73.30)\
[2023-09-14 07:50:56 10splitTasks](trainer.py 286): INFO [20/79]	0.1978(0.2352)	0.0003(0.0376)	1.097(1.090)	76.56(72.32)\
[2023-09-14 07:50:58 10splitTasks](trainer.py 286): INFO [30/79]	0.1964(0.2227)	0.0004(0.0256)	1.384(1.103)	68.75(72.13)\
[2023-09-14 07:51:00 10splitTasks](trainer.py 286): INFO [40/79]	0.1972(0.2165)	0.0004(0.0195)	0.975(1.086)	75.00(72.37)\
[2023-09-14 07:51:02 10splitTasks](trainer.py 286): INFO [50/79]	0.1971(0.2128)	0.0004(0.0157)	1.005(1.083)	73.44(72.67)\
[2023-09-14 07:51:04 10splitTasks](trainer.py 286): INFO [60/79]	0.1954(0.2101)	0.0003(0.0132)	1.160(1.078)	70.31(73.05)\
[2023-09-14 07:51:06 10splitTasks](trainer.py 286): INFO [70/79]	0.1981(0.2083)	0.0010(0.0114)	1.261(1.076)	64.06(73.00)\
[2023-09-14 07:51:08 10splitTasks](trainer.py 286): INFO [78/79]	0.0700(0.2054)	0.0001(0.0103)	1.623(1.080)	50.00(72.80)\
[2023-09-14 07:51:08 10splitTasks](trainer.py 288): INFO  * Train Acc 72.800\
[2023-09-14 07:51:09 10splitTasks](trainer.py 147): INFO  * Val Acc 62.200, Total time 1.52\
[2023-09-14 07:51:09 10splitTasks](trainer.py 223): INFO Epoch:12\
[2023-09-14 07:51:09 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:51:09 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:51:10 10splitTasks](trainer.py 286): INFO [0/79]	1.1241(1.1241)	0.9243(0.9243)	1.232(1.232)	64.06(64.06)\
[2023-09-14 07:51:12 10splitTasks](trainer.py 286): INFO [10/79]	0.1965(0.2815)	0.0004(0.0846)	0.932(1.058)	73.44(70.74)\
[2023-09-14 07:51:14 10splitTasks](trainer.py 286): INFO [20/79]	0.1957(0.2410)	0.0004(0.0445)	1.154(1.085)	68.75(71.13)\
[2023-09-14 07:51:16 10splitTasks](trainer.py 286): INFO [30/79]	0.1980(0.2271)	0.0004(0.0303)	1.229(1.094)	75.00(71.37)\
[2023-09-14 07:51:18 10splitTasks](trainer.py 286): INFO [40/79]	0.1955(0.2199)	0.0004(0.0231)	0.996(1.097)	73.44(71.34)\
[2023-09-14 07:51:20 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2156)	0.0004(0.0186)	1.038(1.104)	75.00(71.26)\
[2023-09-14 07:51:22 10splitTasks](trainer.py 286): INFO [60/79]	0.1954(0.2124)	0.0004(0.0156)	1.015(1.095)	81.25(71.67)\
[2023-09-14 07:51:24 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2101)	0.0008(0.0135)	1.322(1.099)	65.62(71.54)\
[2023-09-14 07:51:26 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2070)	0.0001(0.0122)	1.303(1.105)	62.50(71.56)\
[2023-09-14 07:51:26 10splitTasks](trainer.py 288): INFO  * Train Acc 71.560\
[2023-09-14 07:51:27 10splitTasks](trainer.py 147): INFO  * Val Acc 68.200, Total time 1.50\
[2023-09-14 07:51:27 10splitTasks](trainer.py 223): INFO Epoch:13\
[2023-09-14 07:51:27 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:51:27 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:51:28 10splitTasks](trainer.py 286): INFO [0/79]	1.0445(1.0445)	0.8372(0.8372)	1.051(1.051)	68.75(68.75)\
[2023-09-14 07:51:30 10splitTasks](trainer.py 286): INFO [10/79]	0.1975(0.2735)	0.0003(0.0765)	1.068(1.080)	76.56(72.59)\
[2023-09-14 07:51:32 10splitTasks](trainer.py 286): INFO [20/79]	0.1973(0.2368)	0.0014(0.0403)	1.119(1.105)	70.31(71.65)\
[2023-09-14 07:51:34 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2237)	0.0003(0.0274)	1.078(1.107)	70.31(71.72)\
[2023-09-14 07:51:36 10splitTasks](trainer.py 286): INFO [40/79]	0.1984(0.2174)	0.0004(0.0208)	1.032(1.114)	70.31(71.88)\
[2023-09-14 07:51:38 10splitTasks](trainer.py 286): INFO [50/79]	0.1961(0.2134)	0.0003(0.0169)	1.027(1.118)	73.44(71.54)\
[2023-09-14 07:51:40 10splitTasks](trainer.py 286): INFO [60/79]	0.1958(0.2107)	0.0003(0.0143)	0.947(1.107)	73.44(71.82)\
[2023-09-14 07:51:42 10splitTasks](trainer.py 286): INFO [70/79]	0.1976(0.2090)	0.0010(0.0123)	1.105(1.108)	68.75(71.70)\
[2023-09-14 07:51:44 10splitTasks](trainer.py 286): INFO [78/79]	0.0721(0.2061)	0.0001(0.0111)	0.976(1.101)	75.00(72.06)\
[2023-09-14 07:51:44 10splitTasks](trainer.py 288): INFO  * Train Acc 72.060\
[2023-09-14 07:51:45 10splitTasks](trainer.py 147): INFO  * Val Acc 67.800, Total time 1.77\
[2023-09-14 07:51:45 10splitTasks](trainer.py 223): INFO Epoch:14\
[2023-09-14 07:51:45 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:51:45 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:51:47 10splitTasks](trainer.py 286): INFO [0/79]	1.2513(1.2513)	1.0490(1.0490)	1.340(1.340)	54.69(54.69)\
[2023-09-14 07:51:49 10splitTasks](trainer.py 286): INFO [10/79]	0.1954(0.3062)	0.0004(0.1087)	1.136(1.073)	73.44(71.73)\
[2023-09-14 07:51:51 10splitTasks](trainer.py 286): INFO [20/79]	0.1964(0.2539)	0.0004(0.0572)	1.316(1.091)	65.62(71.80)\
[2023-09-14 07:51:53 10splitTasks](trainer.py 286): INFO [30/79]	0.1968(0.2354)	0.0004(0.0389)	1.162(1.101)	68.75(71.88)\
[2023-09-14 07:51:55 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2260)	0.0004(0.0296)	1.067(1.103)	70.31(71.99)\
[2023-09-14 07:51:57 10splitTasks](trainer.py 286): INFO [50/79]	0.1981(0.2206)	0.0004(0.0239)	1.293(1.100)	70.31(72.06)\
[2023-09-14 07:51:59 10splitTasks](trainer.py 286): INFO [60/79]	0.2026(0.2170)	0.0007(0.0200)	1.098(1.102)	71.88(71.72)\
[2023-09-14 07:52:01 10splitTasks](trainer.py 286): INFO [70/79]	0.1965(0.2142)	0.0009(0.0173)	1.176(1.107)	70.31(71.70)\
[2023-09-14 07:52:02 10splitTasks](trainer.py 286): INFO [78/79]	0.0702(0.2107)	0.0001(0.0156)	1.482(1.102)	87.50(71.92)\
[2023-09-14 07:52:02 10splitTasks](trainer.py 288): INFO  * Train Acc 71.920\
[2023-09-14 07:52:04 10splitTasks](trainer.py 147): INFO  * Val Acc 59.200, Total time 1.53\
[2023-09-14 07:52:04 10splitTasks](trainer.py 223): INFO Epoch:15\
[2023-09-14 07:52:04 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:52:04 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:52:05 10splitTasks](trainer.py 286): INFO [0/79]	0.9477(0.9477)	0.7475(0.7475)	1.108(1.108)	70.31(70.31)\
[2023-09-14 07:52:07 10splitTasks](trainer.py 286): INFO [10/79]	0.1960(0.2651)	0.0004(0.0684)	0.934(1.129)	73.44(70.03)\
[2023-09-14 07:52:09 10splitTasks](trainer.py 286): INFO [20/79]	0.1979(0.2330)	0.0003(0.0360)	1.078(1.104)	81.25(72.47)\
[2023-09-14 07:52:11 10splitTasks](trainer.py 286): INFO [30/79]	0.1965(0.2214)	0.0004(0.0246)	1.084(1.114)	73.44(72.73)\
[2023-09-14 07:52:13 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2162)	0.0004(0.0188)	1.098(1.100)	67.19(72.75)\
[2023-09-14 07:52:15 10splitTasks](trainer.py 286): INFO [50/79]	0.1987(0.2124)	0.0006(0.0152)	0.904(1.084)	82.81(73.41)\
[2023-09-14 07:52:16 10splitTasks](trainer.py 286): INFO [60/79]	0.1956(0.2102)	0.0003(0.0128)	1.319(1.096)	67.19(72.98)\
[2023-09-14 07:52:19 10splitTasks](trainer.py 286): INFO [70/79]	0.1986(0.2088)	0.0010(0.0111)	1.182(1.100)	73.44(73.17)\
[2023-09-14 07:52:20 10splitTasks](trainer.py 286): INFO [78/79]	0.0699(0.2058)	0.0001(0.0100)	1.951(1.100)	37.50(73.24)\
[2023-09-14 07:52:20 10splitTasks](trainer.py 288): INFO  * Train Acc 73.240\
[2023-09-14 07:52:22 10splitTasks](trainer.py 147): INFO  * Val Acc 69.200, Total time 1.51\
[2023-09-14 07:52:22 10splitTasks](trainer.py 223): INFO Epoch:16\
[2023-09-14 07:52:22 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:52:22 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:52:22 10splitTasks](trainer.py 286): INFO [0/79]	0.9039(0.9039)	0.7036(0.7036)	1.099(1.099)	73.44(73.44)\
[2023-09-14 07:52:24 10splitTasks](trainer.py 286): INFO [10/79]	0.1958(0.2619)	0.0004(0.0644)	1.243(1.126)	65.62(72.59)\
[2023-09-14 07:52:26 10splitTasks](trainer.py 286): INFO [20/79]	0.2034(0.2313)	0.0007(0.0340)	1.239(1.106)	71.88(73.29)\
[2023-09-14 07:52:28 10splitTasks](trainer.py 286): INFO [30/79]	0.1959(0.2205)	0.0003(0.0231)	1.121(1.072)	70.31(74.24)\
[2023-09-14 07:52:30 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2148)	0.0003(0.0176)	1.032(1.081)	78.12(73.93)\
[2023-09-14 07:52:32 10splitTasks](trainer.py 286): INFO [50/79]	0.1984(0.2112)	0.0003(0.0143)	1.147(1.090)	70.31(73.28)\
[2023-09-14 07:52:34 10splitTasks](trainer.py 286): INFO [60/79]	0.2010(0.2089)	0.0003(0.0120)	0.874(1.078)	79.69(73.72)\
[2023-09-14 07:52:36 10splitTasks](trainer.py 286): INFO [70/79]	0.1978(0.2074)	0.0010(0.0104)	0.901(1.082)	79.69(73.48)\
[2023-09-14 07:52:38 10splitTasks](trainer.py 286): INFO [78/79]	0.0701(0.2046)	0.0001(0.0093)	1.830(1.090)	50.00(73.44)\
[2023-09-14 07:52:38 10splitTasks](trainer.py 288): INFO  * Train Acc 73.440\
[2023-09-14 07:52:39 10splitTasks](trainer.py 147): INFO  * Val Acc 68.600, Total time 1.47\
[2023-09-14 07:52:39 10splitTasks](trainer.py 223): INFO Epoch:17\
[2023-09-14 07:52:39 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:52:39 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:52:40 10splitTasks](trainer.py 286): INFO [0/79]	0.9312(0.9312)	0.7333(0.7333)	1.094(1.094)	71.88(71.88)\
[2023-09-14 07:52:42 10splitTasks](trainer.py 286): INFO [10/79]	0.1974(0.2638)	0.0003(0.0670)	1.208(1.139)	71.88(70.17)\
[2023-09-14 07:52:44 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2353)	0.0004(0.0354)	1.160(1.130)	71.88(70.76)\
[2023-09-14 07:52:46 10splitTasks](trainer.py 286): INFO [30/79]	0.1962(0.2227)	0.0003(0.0241)	1.003(1.114)	78.12(72.53)\
[2023-09-14 07:52:48 10splitTasks](trainer.py 286): INFO [40/79]	0.1964(0.2164)	0.0003(0.0183)	1.240(1.104)	70.31(73.32)\
[2023-09-14 07:52:50 10splitTasks](trainer.py 286): INFO [50/79]	0.1962(0.2127)	0.0003(0.0148)	1.083(1.097)	73.44(73.31)\
[2023-09-14 07:52:52 10splitTasks](trainer.py 286): INFO [60/79]	0.2070(0.2104)	0.0006(0.0125)	1.041(1.097)	71.88(73.23)\
[2023-09-14 07:52:54 10splitTasks](trainer.py 286): INFO [70/79]	0.2051(0.2087)	0.0013(0.0108)	1.075(1.096)	67.19(73.11)\
[2023-09-14 07:52:55 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2057)	0.0001(0.0097)	1.150(1.100)	62.50(72.96)\
[2023-09-14 07:52:56 10splitTasks](trainer.py 288): INFO  * Train Acc 72.960\
[2023-09-14 07:52:57 10splitTasks](trainer.py 147): INFO  * Val Acc 69.600, Total time 1.51\
[2023-09-14 07:52:57 10splitTasks](trainer.py 223): INFO Epoch:18\
[2023-09-14 07:52:57 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:52:57 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:52:58 10splitTasks](trainer.py 286): INFO [0/79]	0.8508(0.8508)	0.6505(0.6505)	1.169(1.169)	68.75(68.75)\
[2023-09-14 07:53:00 10splitTasks](trainer.py 286): INFO [10/79]	0.1970(0.2583)	0.0003(0.0597)	1.113(1.105)	68.75(73.01)\
[2023-09-14 07:53:02 10splitTasks](trainer.py 286): INFO [20/79]	0.2014(0.2295)	0.0008(0.0317)	0.944(1.076)	75.00(74.93)\
[2023-09-14 07:53:04 10splitTasks](trainer.py 286): INFO [30/79]	0.1981(0.2189)	0.0006(0.0218)	1.102(1.101)	70.31(73.39)\
[2023-09-14 07:53:06 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2135)	0.0004(0.0166)	1.356(1.084)	62.50(73.44)\
[2023-09-14 07:53:08 10splitTasks](trainer.py 286): INFO [50/79]	0.1960(0.2101)	0.0003(0.0134)	0.893(1.076)	81.25(74.23)\
[2023-09-14 07:53:10 10splitTasks](trainer.py 286): INFO [60/79]	0.1960(0.2079)	0.0003(0.0114)	1.125(1.075)	71.88(74.23)\
[2023-09-14 07:53:12 10splitTasks](trainer.py 286): INFO [70/79]	0.2005(0.2064)	0.0010(0.0099)	1.273(1.075)	68.75(74.23)\
[2023-09-14 07:53:13 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2037)	0.0001(0.0089)	1.107(1.080)	62.50(73.98)\
[2023-09-14 07:53:13 10splitTasks](trainer.py 288): INFO  * Train Acc 73.980\
[2023-09-14 07:53:15 10splitTasks](trainer.py 147): INFO  * Val Acc 64.600, Total time 1.53\
[2023-09-14 07:53:15 10splitTasks](trainer.py 223): INFO Epoch:19\
[2023-09-14 07:53:15 10splitTasks](trainer.py 230): INFO LR:0.001\
[2023-09-14 07:53:15 10splitTasks](trainer.py 236): INFO  Itr	    Time  	    Data  	  Loss  	  Acc\
[2023-09-14 07:53:16 10splitTasks](trainer.py 286): INFO [0/79]	0.9675(0.9675)	0.7702(0.7702)	1.318(1.318)	60.94(60.94)\
[2023-09-14 07:53:18 10splitTasks](trainer.py 286): INFO [10/79]	0.1958(0.2702)	0.0004(0.0708)	0.859(1.096)	82.81(73.15)\
[2023-09-14 07:53:20 10splitTasks](trainer.py 286): INFO [20/79]	0.1963(0.2355)	0.0004(0.0373)	1.073(1.100)	75.00(72.77)\
[2023-09-14 07:53:22 10splitTasks](trainer.py 286): INFO [30/79]	0.1977(0.2229)	0.0004(0.0254)	0.794(1.095)	85.94(73.34)\
[2023-09-14 07:53:24 10splitTasks](trainer.py 286): INFO [40/79]	0.1962(0.2166)	0.0003(0.0193)	1.193(1.092)	67.19(73.48)\
[2023-09-14 07:53:26 10splitTasks](trainer.py 286): INFO [50/79]	0.1964(0.2126)	0.0004(0.0156)	1.220(1.076)	67.19(73.71)\
[2023-09-14 07:53:28 10splitTasks](trainer.py 286): INFO [60/79]	0.1968(0.2099)	0.0003(0.0131)	1.130(1.083)	73.44(73.87)\
[2023-09-14 07:53:30 10splitTasks](trainer.py 286): INFO [70/79]	0.2027(0.2082)	0.0010(0.0113)	1.236(1.084)	60.94(73.77)\
[2023-09-14 07:53:31 10splitTasks](trainer.py 286): INFO [78/79]	0.0698(0.2053)	0.0001(0.0102)	2.470(1.094)	50.00(73.48)\
[2023-09-14 07:53:31 10splitTasks](trainer.py 288): INFO  * Train Acc 73.480\
[2023-09-14 07:53:33 10splitTasks](trainer.py 147): INFO  * Val Acc 68.600, Total time 1.72\
=> Saving model to: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-9.pth\
=> Save Done\
[2023-09-14 07:53:33 10splitTasks](iBatchLearn.py 131): INFO validation split name:0\
[2023-09-14 07:53:35 10splitTasks](trainer.py 147): INFO  * Val Acc 22.600, Total time 1.75\
[2023-09-14 07:53:35 10splitTasks](iBatchLearn.py 131): INFO validation split name:1\
[2023-09-14 07:53:36 10splitTasks](trainer.py 147): INFO  * Val Acc 23.400, Total time 1.58\
[2023-09-14 07:53:36 10splitTasks](iBatchLearn.py 131): INFO validation split name:2\
[2023-09-14 07:53:38 10splitTasks](trainer.py 147): INFO  * Val Acc 35.400, Total time 1.63\
[2023-09-14 07:53:38 10splitTasks](iBatchLearn.py 131): INFO validation split name:3\
[2023-09-14 07:53:40 10splitTasks](trainer.py 147): INFO  * Val Acc 41.200, Total time 1.87\
[2023-09-14 07:53:40 10splitTasks](iBatchLearn.py 131): INFO validation split name:4\
[2023-09-14 07:53:41 10splitTasks](trainer.py 147): INFO  * Val Acc 31.200, Total time 1.52\
[2023-09-14 07:53:41 10splitTasks](iBatchLearn.py 131): INFO validation split name:5\
[2023-09-14 07:53:43 10splitTasks](trainer.py 147): INFO  * Val Acc 34.800, Total time 1.54\
[2023-09-14 07:53:43 10splitTasks](iBatchLearn.py 131): INFO validation split name:6\
[2023-09-14 07:53:45 10splitTasks](trainer.py 147): INFO  * Val Acc 33.800, Total time 1.85\
[2023-09-14 07:53:45 10splitTasks](iBatchLearn.py 131): INFO validation split name:7\
[2023-09-14 07:53:47 10splitTasks](trainer.py 147): INFO  * Val Acc 38.200, Total time 1.79\
[2023-09-14 07:53:47 10splitTasks](iBatchLearn.py 131): INFO validation split name:8\
[2023-09-14 07:53:48 10splitTasks](trainer.py 147): INFO  * Val Acc 47.200, Total time 1.63\
[2023-09-14 07:53:48 10splitTasks](iBatchLearn.py 131): INFO validation split name:9\
[2023-09-14 07:53:50 10splitTasks](trainer.py 147): INFO  * Val Acc 68.600, Total time 1.53\
[2023-09-14 07:53:50 10splitTasks](trainer.py 335): INFO saving storage...\
[2023-09-14 07:53:50 10splitTasks](trainer.py 341): INFO done\
[2023-09-14 07:53:50 10splitTasks](iBatchLearn.py 155): INFO Acc:37.64000015325547; BWT:-23.24444454373254;\
=> merge config from utils/user_10splitTasks.yaml\
=> merge config from ../official_eva/configs/10splitTasks.yaml\
[2023-09-14 07:53:54 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11/config.json\
[2023-09-14 07:53:54 10splitTasks](iBatchLearn.py 232): INFO AGENT:\
  FIX_BN: false\
  FIX_HEAD: true\
  MODEL_NAME: resnet50\
  MODEL_TYPE: resnet\
  NAME: L2\
  REG_COEF: 0.1\
  TYPE: regularization\
DATASET:\
  BATCHSIZE: 64\
  NAME: 10splitTasks\
  NUM_CLASSES: 100\
  NUM_TASKS: 10\
  NUM_WORKERS: 4\
  ROOT: input/contest_data/10splitTasks\
DOMAIN_INCR: false\
GPUID:\
- 0\
LOGGER_PATH: outputs/10splitTasks/regularization-L2-2023-09-14-06:17:11\
OPT:\
  GAMMA: 0.1\
  LR: 0.001\
  MOMENTUM: 0.9\
  NAME: SGD\
  SCHEDULE:\
  - 20\
  WEIGHT_DECAY: 0.0\
PRINT_FREQ: 10\
SEED: 0\
\
[2023-09-14 07:53:54 10splitTasks](iBatchLearn.py 233): INFO \{"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 9, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-9.pth", "save_ckpt_path": null, "storage_path": null, "save_storage_path": null, "dest_path": "outputs/2023-09-14-06:17:11/prediction_9.pkl", "suffix": "2023-09-14-06:17:11", "distributed": false, "is_main_process": true\}\
[2023-09-14 07:53:55 10splitTasks](trainer.py 92): INFO => Load model weights: outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-9.pth\
[2023-09-14 07:53:55 10splitTasks](trainer.py 97): INFO => Load Done\
[2023-09-14 07:53:57 10splitTasks](iBatchLearn.py 57): INFO IncreResNet(\
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
  (relu): ReLU(inplace=True)\
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\
  (layer1): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer2): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer3): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (3): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (4): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (5): Bottleneck(\
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (layer4): Sequential(\
    (0): Bottleneck(\
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
      (downsample): Sequential(\
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      )\
    )\
    (1): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
    (2): Bottleneck(\
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\
      (relu): ReLU(inplace=True)\
    )\
  )\
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\
  (last): ModuleDict(\
    (0): Linear(in_features=2048, out_features=10, bias=False)\
    (1): Linear(in_features=2048, out_features=10, bias=False)\
    (2): Linear(in_features=2048, out_features=10, bias=False)\
    (3): Linear(in_features=2048, out_features=10, bias=False)\
    (4): Linear(in_features=2048, out_features=10, bias=False)\
    (5): Linear(in_features=2048, out_features=10, bias=False)\
    (6): Linear(in_features=2048, out_features=10, bias=False)\
    (7): Linear(in_features=2048, out_features=10, bias=False)\
    (8): Linear(in_features=2048, out_features=10, bias=False)\
    (9): Linear(in_features=2048, out_features=10, bias=False)\
  )\
)\
[2023-09-14 07:53:57 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832\
[2023-09-14 07:53:57 10splitTasks](iBatchLearn.py 167): INFO test split name:0\
[2023-09-14 07:54:01 10splitTasks](iBatchLearn.py 167): INFO test split name:1\
[2023-09-14 07:54:04 10splitTasks](iBatchLearn.py 167): INFO test split name:2\
[2023-09-14 07:54:07 10splitTasks](iBatchLearn.py 167): INFO test split name:3\
[2023-09-14 07:54:10 10splitTasks](iBatchLearn.py 167): INFO test split name:4\
[2023-09-14 07:54:13 10splitTasks](iBatchLearn.py 167): INFO test split name:5\
[2023-09-14 07:54:16 10splitTasks](iBatchLearn.py 167): INFO test split name:6\
[2023-09-14 07:54:19 10splitTasks](iBatchLearn.py 167): INFO test split name:7\
[2023-09-14 07:54:22 10splitTasks](iBatchLearn.py 167): INFO test split name:8\
[2023-09-14 07:54:25 10splitTasks](iBatchLearn.py 167): INFO test split name:9\
--------------------------------Official Evaluation--------------------------------\
9 12.928888888888885\
4splitDomains_0, 90.44M, 2023-09-14 06:21:57, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-0.pth\
4splitDomains_0, 180.38M, 2023-09-14 06:21:59, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/storage-0.pth\
4splitDomains_1, 90.44M, 2023-09-14 06:27:13, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-1.pth\
4splitDomains_1, 180.38M, 2023-09-14 06:27:15, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/storage-1.pth\
4splitDomains_2, 90.44M, 2023-09-14 06:42:52, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-2.pth\
4splitDomains_2, 180.38M, 2023-09-14 06:43:00, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/storage-2.pth\
4splitDomains_3, 90.44M, 2023-09-14 06:47:33, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/checkpoint-3.pth\
4splitDomains_3, 180.38M, 2023-09-14 06:47:42, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/4splitDomains/storage-3.pth\
10splitTasks_0, 90.76M, 2023-09-14 06:53:15, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-0.pth\
10splitTasks_0, 181.01M, 2023-09-14 06:53:17, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-0.pth\
10splitTasks_1, 90.76M, 2023-09-14 06:59:39, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-1.pth\
10splitTasks_1, 181.01M, 2023-09-14 06:59:42, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-1.pth\
10splitTasks_2, 90.76M, 2023-09-14 07:06:05, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-2.pth\
10splitTasks_2, 181.01M, 2023-09-14 07:06:10, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-2.pth\
10splitTasks_3, 90.76M, 2023-09-14 07:12:42, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-3.pth\
10splitTasks_3, 181.01M, 2023-09-14 07:12:49, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-3.pth\
10splitTasks_4, 90.76M, 2023-09-14 07:19:17, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-4.pth\
10splitTasks_4, 181.01M, 2023-09-14 07:19:26, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-4.pth\
10splitTasks_5, 90.76M, 2023-09-14 07:25:57, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-5.pth\
10splitTasks_5, 181.01M, 2023-09-14 07:26:07, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-5.pth\
10splitTasks_6, 90.76M, 2023-09-14 07:32:46, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-6.pth\
10splitTasks_6, 181.01M, 2023-09-14 07:32:57, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-6.pth\
10splitTasks_7, 90.76M, 2023-09-14 07:39:38, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-7.pth\
10splitTasks_7, 181.01M, 2023-09-14 07:39:52, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-7.pth\
10splitTasks_8, 90.76M, 2023-09-14 07:46:35, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-8.pth\
10splitTasks_8, 181.01M, 2023-09-14 07:46:50, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-8.pth\
10splitTasks_9, 90.76M, 2023-09-14 07:53:33, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/checkpoint-9.pth\
10splitTasks_9, 181.01M, 2023-09-14 07:53:50, /work/home/pzds_a003/contest/week2/code_/outputs/2023-09-14-06:17:11/10splitTasks/storage-9.pth\
--------------------------------Final Official Evaluation--------------------------------\
42.42669797179185\
}