=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:02:02 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:02:02 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:02:02 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 0, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "input/init_models/4splitDomains.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-0.pth", "storage_path": "None", "save_storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-0.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:02:03 4splitDomains](my_trainer.py 108): INFO => Load model weights: input/init_models/4splitDomains.pth
[2023-09-29 12:02:03 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:02:05 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:02:05 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:02:05 4splitDomains](iBatchLearn.py 92): INFO ====================== 0 =======================
[2023-09-29 12:02:05 4splitDomains](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 12:02:05 4splitDomains](my_trainer.py 328): INFO Epoch:0
[2023-09-29 12:02:05 4splitDomains](my_trainer.py 335): INFO LR:0.0020008000000000005
[2023-09-29 12:02:05 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:02:09 4splitDomains](trainer.py 286): INFO [0/91]	3.5080(3.5080)	0.4840(0.4840)	4.111(4.111)	0.00(0.00)
[2023-09-29 12:02:10 4splitDomains](trainer.py 286): INFO [10/91]	0.0971(0.4079)	0.0003(0.0443)	3.949(4.048)	9.38(4.55)
[2023-09-29 12:02:11 4splitDomains](trainer.py 286): INFO [20/91]	0.0969(0.2599)	0.0001(0.0233)	3.632(3.890)	15.62(8.93)
[2023-09-29 12:02:12 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.2074)	0.0003(0.0159)	3.131(3.705)	28.12(17.04)
[2023-09-29 12:02:13 4splitDomains](trainer.py 286): INFO [40/91]	0.0969(0.1806)	0.0002(0.0121)	2.525(3.499)	56.25(23.86)
[2023-09-29 12:02:14 4splitDomains](trainer.py 286): INFO [50/91]	0.1078(0.1644)	0.0006(0.0098)	2.011(3.288)	65.62(30.02)
[2023-09-29 12:02:15 4splitDomains](trainer.py 286): INFO [60/91]	0.0969(0.1534)	0.0002(0.0082)	1.778(3.072)	62.50(35.50)
[2023-09-29 12:02:16 4splitDomains](trainer.py 286): INFO [70/91]	0.0971(0.1456)	0.0001(0.0071)	1.647(2.894)	59.38(39.35)
[2023-09-29 12:02:17 4splitDomains](trainer.py 286): INFO [80/91]	0.0976(0.1398)	0.0003(0.0063)	1.436(2.731)	62.50(42.59)
[2023-09-29 12:02:17 4splitDomains](trainer.py 286): INFO [90/91]	0.0776(0.1349)	0.0001(0.0056)	1.555(2.589)	47.37(45.57)
[2023-09-29 12:02:18 4splitDomains](trainer.py 288): INFO  * Train Acc 45.567
[2023-09-29 12:02:19 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.923, Total time 1.27
[2023-09-29 12:02:19 4splitDomains](my_trainer.py 328): INFO Epoch:1
[2023-09-29 12:02:19 4splitDomains](my_trainer.py 335): INFO LR:0.004000600000000001
[2023-09-29 12:02:19 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:02:19 4splitDomains](trainer.py 286): INFO [0/91]	0.5825(0.5825)	0.4721(0.4721)	1.070(1.070)	84.38(84.38)
[2023-09-29 12:02:20 4splitDomains](trainer.py 286): INFO [10/91]	0.0973(0.1433)	0.0002(0.0432)	0.877(1.066)	84.38(79.83)
[2023-09-29 12:02:21 4splitDomains](trainer.py 286): INFO [20/91]	0.0971(0.1216)	0.0002(0.0228)	0.733(1.023)	87.50(79.17)
[2023-09-29 12:02:22 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.1138)	0.0003(0.0155)	0.697(0.915)	84.38(80.04)
[2023-09-29 12:02:23 4splitDomains](trainer.py 286): INFO [40/91]	0.0973(0.1098)	0.0003(0.0118)	0.548(0.861)	84.38(80.79)
[2023-09-29 12:02:24 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.1075)	0.0003(0.0096)	0.435(0.808)	87.50(82.05)
[2023-09-29 12:02:25 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.1061)	0.0003(0.0080)	0.506(0.775)	87.50(82.48)
[2023-09-29 12:02:26 4splitDomains](trainer.py 286): INFO [70/91]	0.0980(0.1049)	0.0003(0.0069)	0.744(0.762)	84.38(82.57)
[2023-09-29 12:02:27 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1041)	0.0003(0.0061)	0.486(0.733)	90.62(83.26)
[2023-09-29 12:02:28 4splitDomains](trainer.py 286): INFO [90/91]	0.0783(0.1032)	0.0001(0.0055)	0.518(0.704)	89.47(83.86)
[2023-09-29 12:02:28 4splitDomains](trainer.py 288): INFO  * Train Acc 83.857
[2023-09-29 12:02:30 4splitDomains](my_trainer.py 503): INFO  * Val Acc 87.268, Total time 1.31
[2023-09-29 12:02:30 4splitDomains](my_trainer.py 328): INFO Epoch:2
[2023-09-29 12:02:30 4splitDomains](my_trainer.py 335): INFO LR:0.0060004
[2023-09-29 12:02:30 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:02:30 4splitDomains](trainer.py 286): INFO [0/91]	0.5932(0.5932)	0.4893(0.4893)	0.274(0.274)	96.88(96.88)
[2023-09-29 12:02:31 4splitDomains](trainer.py 286): INFO [10/91]	0.1059(0.1454)	0.0002(0.0449)	0.287(0.350)	93.75(92.05)
[2023-09-29 12:02:32 4splitDomains](trainer.py 286): INFO [20/91]	0.0975(0.1229)	0.0002(0.0237)	0.287(0.343)	96.88(92.56)
[2023-09-29 12:02:33 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1148)	0.0003(0.0161)	0.525(0.322)	84.38(92.64)
[2023-09-29 12:02:34 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1108)	0.0003(0.0123)	0.593(0.326)	81.25(92.23)
[2023-09-29 12:02:35 4splitDomains](trainer.py 286): INFO [50/91]	0.0977(0.1082)	0.0003(0.0099)	0.406(0.325)	84.38(92.22)
[2023-09-29 12:02:36 4splitDomains](trainer.py 286): INFO [60/91]	0.0978(0.1066)	0.0002(0.0083)	0.407(0.331)	84.38(92.16)
[2023-09-29 12:02:37 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1053)	0.0002(0.0072)	0.448(0.327)	90.62(92.39)
[2023-09-29 12:02:38 4splitDomains](trainer.py 286): INFO [80/91]	0.0978(0.1045)	0.0003(0.0064)	0.285(0.324)	93.75(92.55)
[2023-09-29 12:02:39 4splitDomains](trainer.py 286): INFO [90/91]	0.0776(0.1036)	0.0001(0.0057)	0.285(0.331)	100.00(92.34)
[2023-09-29 12:02:39 4splitDomains](trainer.py 288): INFO  * Train Acc 92.342
[2023-09-29 12:02:40 4splitDomains](my_trainer.py 503): INFO  * Val Acc 84.350, Total time 1.29
[2023-09-29 12:02:40 4splitDomains](my_trainer.py 328): INFO Epoch:3
[2023-09-29 12:02:40 4splitDomains](my_trainer.py 335): INFO LR:0.0080002
[2023-09-29 12:02:40 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:02:41 4splitDomains](trainer.py 286): INFO [0/91]	0.6576(0.6576)	0.5542(0.5542)	0.305(0.305)	90.62(90.62)
[2023-09-29 12:02:42 4splitDomains](trainer.py 286): INFO [10/91]	0.0978(0.1500)	0.0003(0.0507)	0.337(0.255)	87.50(92.05)
[2023-09-29 12:02:43 4splitDomains](trainer.py 286): INFO [20/91]	0.0971(0.1251)	0.0002(0.0267)	0.179(0.266)	100.00(92.26)
[2023-09-29 12:02:44 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.1162)	0.0002(0.0182)	0.265(0.293)	90.62(91.94)
[2023-09-29 12:02:45 4splitDomains](trainer.py 286): INFO [40/91]	0.0972(0.1117)	0.0002(0.0138)	0.323(0.319)	84.38(91.01)
[2023-09-29 12:02:46 4splitDomains](trainer.py 286): INFO [50/91]	0.0971(0.1088)	0.0002(0.0112)	0.336(0.321)	93.75(90.99)
[2023-09-29 12:02:47 4splitDomains](trainer.py 286): INFO [60/91]	0.0973(0.1072)	0.0003(0.0094)	0.513(0.319)	87.50(91.24)
[2023-09-29 12:02:48 4splitDomains](trainer.py 286): INFO [70/91]	0.0988(0.1060)	0.0003(0.0081)	0.440(0.325)	84.38(91.07)
[2023-09-29 12:02:49 4splitDomains](trainer.py 286): INFO [80/91]	0.0972(0.1049)	0.0003(0.0071)	0.352(0.341)	90.62(90.47)
[2023-09-29 12:02:50 4splitDomains](trainer.py 286): INFO [90/91]	0.0778(0.1039)	0.0001(0.0064)	0.462(0.342)	89.47(90.44)
[2023-09-29 12:02:50 4splitDomains](trainer.py 288): INFO  * Train Acc 90.445
[2023-09-29 12:02:51 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.637, Total time 1.26
[2023-09-29 12:02:51 4splitDomains](my_trainer.py 328): INFO Epoch:4
[2023-09-29 12:02:51 4splitDomains](my_trainer.py 335): INFO LR:0.01
[2023-09-29 12:02:51 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:02:52 4splitDomains](trainer.py 286): INFO [0/91]	0.5619(0.5619)	0.4565(0.4565)	0.399(0.399)	93.75(93.75)
[2023-09-29 12:02:53 4splitDomains](trainer.py 286): INFO [10/91]	0.1118(0.1438)	0.0006(0.0418)	0.192(0.312)	96.88(91.76)
[2023-09-29 12:02:54 4splitDomains](trainer.py 286): INFO [20/91]	0.0972(0.1224)	0.0002(0.0220)	0.353(0.320)	84.38(90.92)
[2023-09-29 12:02:55 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.1148)	0.0003(0.0150)	0.174(0.325)	96.88(91.33)
[2023-09-29 12:02:56 4splitDomains](trainer.py 286): INFO [40/91]	0.0971(0.1107)	0.0002(0.0114)	0.188(0.348)	96.88(90.70)
[2023-09-29 12:02:57 4splitDomains](trainer.py 286): INFO [50/91]	0.0971(0.1085)	0.0001(0.0093)	0.550(0.378)	84.38(89.95)
[2023-09-29 12:02:58 4splitDomains](trainer.py 286): INFO [60/91]	0.0971(0.1075)	0.0002(0.0078)	0.331(0.389)	87.50(89.55)
[2023-09-29 12:02:59 4splitDomains](trainer.py 286): INFO [70/91]	0.0972(0.1064)	0.0002(0.0068)	0.305(0.395)	93.75(88.95)
[2023-09-29 12:03:00 4splitDomains](trainer.py 286): INFO [80/91]	0.0973(0.1053)	0.0003(0.0060)	0.774(0.414)	75.00(88.46)
[2023-09-29 12:03:01 4splitDomains](trainer.py 286): INFO [90/91]	0.0781(0.1042)	0.0001(0.0053)	0.898(0.436)	78.95(87.89)
[2023-09-29 12:03:01 4splitDomains](trainer.py 288): INFO  * Train Acc 87.892
[2023-09-29 12:03:02 4splitDomains](my_trainer.py 503): INFO  * Val Acc 68.700, Total time 1.31
[2023-09-29 12:03:02 4splitDomains](my_trainer.py 328): INFO Epoch:5
[2023-09-29 12:03:02 4splitDomains](my_trainer.py 335): INFO LR:0.009890748929868663
[2023-09-29 12:03:02 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:03:03 4splitDomains](trainer.py 286): INFO [0/91]	0.5773(0.5773)	0.4562(0.4562)	0.118(0.118)	96.88(96.88)
[2023-09-29 12:03:04 4splitDomains](trainer.py 286): INFO [10/91]	0.0970(0.1417)	0.0001(0.0418)	0.164(0.326)	96.88(89.49)
[2023-09-29 12:03:05 4splitDomains](trainer.py 286): INFO [20/91]	0.0974(0.1208)	0.0002(0.0220)	0.125(0.346)	96.88(89.43)
[2023-09-29 12:03:06 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1134)	0.0003(0.0150)	0.192(0.335)	96.88(90.22)
[2023-09-29 12:03:07 4splitDomains](trainer.py 286): INFO [40/91]	0.0978(0.1097)	0.0007(0.0115)	0.442(0.345)	90.62(90.02)
[2023-09-29 12:03:08 4splitDomains](trainer.py 286): INFO [50/91]	0.1002(0.1082)	0.0006(0.0093)	0.361(0.342)	84.38(89.64)
[2023-09-29 12:03:09 4splitDomains](trainer.py 286): INFO [60/91]	0.0978(0.1067)	0.0002(0.0079)	0.385(0.349)	90.62(89.40)
[2023-09-29 12:03:10 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1055)	0.0002(0.0068)	0.448(0.372)	87.50(88.64)
[2023-09-29 12:03:11 4splitDomains](trainer.py 286): INFO [80/91]	0.1086(0.1052)	0.0005(0.0060)	0.490(0.393)	84.38(88.00)
[2023-09-29 12:03:12 4splitDomains](trainer.py 286): INFO [90/91]	0.0814(0.1041)	0.0001(0.0054)	0.209(0.404)	89.47(87.62)
[2023-09-29 12:03:12 4splitDomains](trainer.py 288): INFO  * Train Acc 87.616
[2023-09-29 12:03:13 4splitDomains](my_trainer.py 503): INFO  * Val Acc 72.149, Total time 1.32
[2023-09-29 12:03:13 4splitDomains](my_trainer.py 328): INFO Epoch:6
[2023-09-29 12:03:13 4splitDomains](my_trainer.py 335): INFO LR:0.009567770515484183
[2023-09-29 12:03:13 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:03:14 4splitDomains](trainer.py 286): INFO [0/91]	0.6124(0.6124)	0.4870(0.4870)	0.182(0.182)	96.88(96.88)
[2023-09-29 12:03:15 4splitDomains](trainer.py 286): INFO [10/91]	0.0984(0.1492)	0.0003(0.0446)	0.267(0.380)	87.50(89.49)
[2023-09-29 12:03:16 4splitDomains](trainer.py 286): INFO [20/91]	0.1007(0.1248)	0.0002(0.0235)	0.449(0.355)	90.62(89.88)
[2023-09-29 12:03:17 4splitDomains](trainer.py 286): INFO [30/91]	0.0976(0.1162)	0.0003(0.0160)	0.431(0.371)	81.25(88.71)
[2023-09-29 12:03:18 4splitDomains](trainer.py 286): INFO [40/91]	0.0975(0.1118)	0.0003(0.0122)	0.270(0.370)	87.50(88.72)
[2023-09-29 12:03:19 4splitDomains](trainer.py 286): INFO [50/91]	0.0974(0.1094)	0.0001(0.0099)	0.233(0.357)	96.88(89.34)
[2023-09-29 12:03:20 4splitDomains](trainer.py 286): INFO [60/91]	0.1019(0.1080)	0.0002(0.0083)	0.483(0.357)	84.38(89.19)
[2023-09-29 12:03:21 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1066)	0.0003(0.0072)	0.184(0.344)	93.75(89.57)
[2023-09-29 12:03:22 4splitDomains](trainer.py 286): INFO [80/91]	0.1006(0.1055)	0.0003(0.0063)	0.282(0.348)	90.62(89.47)
[2023-09-29 12:03:23 4splitDomains](trainer.py 286): INFO [90/91]	0.0783(0.1044)	0.0001(0.0057)	0.321(0.338)	94.74(89.79)
[2023-09-29 12:03:23 4splitDomains](trainer.py 288): INFO  * Train Acc 89.790
[2023-09-29 12:03:24 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.902, Total time 1.29
[2023-09-29 12:03:24 4splitDomains](my_trainer.py 328): INFO Epoch:7
[2023-09-29 12:03:24 4splitDomains](my_trainer.py 335): INFO LR:0.00904518046337755
[2023-09-29 12:03:24 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:03:25 4splitDomains](trainer.py 286): INFO [0/91]	0.5898(0.5898)	0.4807(0.4807)	0.168(0.168)	96.88(96.88)
[2023-09-29 12:03:26 4splitDomains](trainer.py 286): INFO [10/91]	0.0977(0.1434)	0.0002(0.0440)	0.161(0.144)	90.62(95.45)
[2023-09-29 12:03:27 4splitDomains](trainer.py 286): INFO [20/91]	0.0974(0.1221)	0.0002(0.0231)	0.225(0.183)	90.62(94.20)
[2023-09-29 12:03:28 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.1144)	0.0002(0.0158)	0.142(0.187)	96.88(94.66)
[2023-09-29 12:03:29 4splitDomains](trainer.py 286): INFO [40/91]	0.0974(0.1106)	0.0002(0.0120)	0.210(0.188)	93.75(94.66)
[2023-09-29 12:03:30 4splitDomains](trainer.py 286): INFO [50/91]	0.1015(0.1084)	0.0002(0.0097)	0.096(0.186)	100.00(94.67)
[2023-09-29 12:03:31 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1067)	0.0002(0.0082)	0.117(0.187)	96.88(94.83)
[2023-09-29 12:03:32 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1056)	0.0002(0.0071)	0.209(0.183)	96.88(94.81)
[2023-09-29 12:03:33 4splitDomains](trainer.py 286): INFO [80/91]	0.0972(0.1048)	0.0001(0.0062)	0.051(0.187)	100.00(94.79)
[2023-09-29 12:03:33 4splitDomains](trainer.py 286): INFO [90/91]	0.0773(0.1038)	0.0001(0.0056)	0.122(0.189)	94.74(94.69)
[2023-09-29 12:03:34 4splitDomains](trainer.py 288): INFO  * Train Acc 94.688
[2023-09-29 12:03:35 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.637, Total time 1.30
[2023-09-29 12:03:35 4splitDomains](my_trainer.py 328): INFO Epoch:8
[2023-09-29 12:03:35 4splitDomains](my_trainer.py 335): INFO LR:0.008345818466491111
[2023-09-29 12:03:35 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:03:35 4splitDomains](trainer.py 286): INFO [0/91]	0.5759(0.5759)	0.4742(0.4742)	0.186(0.186)	90.62(90.62)
[2023-09-29 12:03:37 4splitDomains](trainer.py 286): INFO [10/91]	0.0973(0.1448)	0.0002(0.0434)	0.197(0.126)	96.88(95.74)
[2023-09-29 12:03:37 4splitDomains](trainer.py 286): INFO [20/91]	0.0973(0.1225)	0.0001(0.0229)	0.034(0.158)	100.00(94.79)
[2023-09-29 12:03:38 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.1146)	0.0002(0.0156)	0.216(0.156)	90.62(94.86)
[2023-09-29 12:03:39 4splitDomains](trainer.py 286): INFO [40/91]	0.0974(0.1116)	0.0002(0.0119)	0.122(0.151)	96.88(95.12)
[2023-09-29 12:03:40 4splitDomains](trainer.py 286): INFO [50/91]	0.0974(0.1093)	0.0002(0.0096)	0.027(0.140)	100.00(95.53)
[2023-09-29 12:03:41 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.1074)	0.0002(0.0081)	0.078(0.142)	96.88(95.65)
[2023-09-29 12:03:42 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.1062)	0.0002(0.0070)	0.375(0.143)	84.38(95.51)
[2023-09-29 12:03:43 4splitDomains](trainer.py 286): INFO [80/91]	0.0973(0.1052)	0.0002(0.0062)	0.085(0.138)	96.88(95.56)
[2023-09-29 12:03:44 4splitDomains](trainer.py 286): INFO [90/91]	0.0822(0.1042)	0.0001(0.0055)	0.023(0.136)	100.00(95.62)
[2023-09-29 12:03:44 4splitDomains](trainer.py 288): INFO  * Train Acc 95.619
[2023-09-29 12:03:46 4splitDomains](my_trainer.py 503): INFO  * Val Acc 83.289, Total time 1.24
[2023-09-29 12:03:46 4splitDomains](my_trainer.py 328): INFO Epoch:9
[2023-09-29 12:03:46 4splitDomains](my_trainer.py 335): INFO LR:0.007500250000000001
[2023-09-29 12:03:46 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:03:46 4splitDomains](trainer.py 286): INFO [0/91]	0.6389(0.6389)	0.5341(0.5341)	0.049(0.049)	100.00(100.00)
[2023-09-29 12:03:47 4splitDomains](trainer.py 286): INFO [10/91]	0.0973(0.1479)	0.0002(0.0489)	0.047(0.064)	100.00(98.58)
[2023-09-29 12:03:48 4splitDomains](trainer.py 286): INFO [20/91]	0.0974(0.1239)	0.0002(0.0258)	0.082(0.066)	96.88(98.36)
[2023-09-29 12:03:49 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1164)	0.0002(0.0176)	0.006(0.054)	100.00(98.79)
[2023-09-29 12:03:50 4splitDomains](trainer.py 286): INFO [40/91]	0.0972(0.1121)	0.0002(0.0134)	0.013(0.063)	100.00(98.40)
[2023-09-29 12:03:51 4splitDomains](trainer.py 286): INFO [50/91]	0.0974(0.1099)	0.0002(0.0108)	0.027(0.063)	100.00(98.35)
[2023-09-29 12:03:52 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.1079)	0.0002(0.0091)	0.016(0.065)	100.00(98.31)
[2023-09-29 12:03:53 4splitDomains](trainer.py 286): INFO [70/91]	0.0973(0.1065)	0.0002(0.0078)	0.003(0.072)	100.00(98.20)
[2023-09-29 12:03:54 4splitDomains](trainer.py 286): INFO [80/91]	0.1029(0.1055)	0.0004(0.0069)	0.044(0.076)	96.88(97.92)
[2023-09-29 12:03:55 4splitDomains](trainer.py 286): INFO [90/91]	0.0779(0.1044)	0.0001(0.0062)	0.024(0.078)	100.00(97.90)
[2023-09-29 12:03:55 4splitDomains](trainer.py 288): INFO  * Train Acc 97.896
[2023-09-29 12:03:57 4splitDomains](my_trainer.py 503): INFO  * Val Acc 85.146, Total time 1.32
[2023-09-29 12:03:57 4splitDomains](my_trainer.py 328): INFO Epoch:10
[2023-09-29 12:03:57 4splitDomains](my_trainer.py 335): INFO LR:0.00654543046337755
[2023-09-29 12:03:57 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:03:57 4splitDomains](trainer.py 286): INFO [0/91]	0.5715(0.5715)	0.4540(0.4540)	0.053(0.053)	100.00(100.00)
[2023-09-29 12:03:58 4splitDomains](trainer.py 286): INFO [10/91]	0.0975(0.1410)	0.0003(0.0416)	0.007(0.029)	100.00(99.43)
[2023-09-29 12:03:59 4splitDomains](trainer.py 286): INFO [20/91]	0.1050(0.1207)	0.0001(0.0219)	0.048(0.032)	100.00(99.26)
[2023-09-29 12:04:00 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.1140)	0.0002(0.0150)	0.011(0.030)	100.00(99.40)
[2023-09-29 12:04:01 4splitDomains](trainer.py 286): INFO [40/91]	0.0972(0.1101)	0.0001(0.0114)	0.054(0.035)	96.88(99.16)
[2023-09-29 12:04:02 4splitDomains](trainer.py 286): INFO [50/91]	0.0978(0.1076)	0.0003(0.0092)	0.139(0.038)	93.75(99.02)
[2023-09-29 12:04:03 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.1060)	0.0003(0.0077)	0.010(0.039)	100.00(98.92)
[2023-09-29 12:04:04 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.1048)	0.0002(0.0067)	0.030(0.040)	100.00(98.90)
[2023-09-29 12:04:05 4splitDomains](trainer.py 286): INFO [80/91]	0.0981(0.1039)	0.0003(0.0059)	0.185(0.041)	93.75(98.88)
[2023-09-29 12:04:06 4splitDomains](trainer.py 286): INFO [90/91]	0.0782(0.1030)	0.0001(0.0053)	0.017(0.041)	100.00(98.90)
[2023-09-29 12:04:06 4splitDomains](trainer.py 288): INFO  * Train Acc 98.896
[2023-09-29 12:04:08 4splitDomains](my_trainer.py 503): INFO  * Val Acc 87.268, Total time 1.32
[2023-09-29 12:04:08 4splitDomains](my_trainer.py 328): INFO Epoch:11
[2023-09-29 12:04:08 4splitDomains](my_trainer.py 335): INFO LR:0.005523090052106635
[2023-09-29 12:04:08 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:04:08 4splitDomains](trainer.py 286): INFO [0/91]	0.5942(0.5942)	0.4870(0.4870)	0.010(0.010)	100.00(100.00)
[2023-09-29 12:04:09 4splitDomains](trainer.py 286): INFO [10/91]	0.0974(0.1445)	0.0002(0.0445)	0.060(0.045)	96.88(98.58)
[2023-09-29 12:04:10 4splitDomains](trainer.py 286): INFO [20/91]	0.0981(0.1225)	0.0005(0.0235)	0.025(0.030)	100.00(99.11)
[2023-09-29 12:04:11 4splitDomains](trainer.py 286): INFO [30/91]	0.0976(0.1147)	0.0002(0.0160)	0.045(0.030)	100.00(99.09)
[2023-09-29 12:04:12 4splitDomains](trainer.py 286): INFO [40/91]	0.0975(0.1106)	0.0002(0.0122)	0.006(0.026)	100.00(99.31)
[2023-09-29 12:04:13 4splitDomains](trainer.py 286): INFO [50/91]	0.0977(0.1082)	0.0003(0.0098)	0.007(0.027)	100.00(99.26)
[2023-09-29 12:04:14 4splitDomains](trainer.py 286): INFO [60/91]	0.0973(0.1065)	0.0002(0.0083)	0.002(0.026)	100.00(99.28)
[2023-09-29 12:04:15 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1053)	0.0002(0.0071)	0.007(0.025)	100.00(99.30)
[2023-09-29 12:04:16 4splitDomains](trainer.py 286): INFO [80/91]	0.0980(0.1043)	0.0003(0.0063)	0.003(0.024)	100.00(99.34)
[2023-09-29 12:04:17 4splitDomains](trainer.py 286): INFO [90/91]	0.0774(0.1034)	0.0001(0.0056)	0.008(0.023)	100.00(99.38)
[2023-09-29 12:04:17 4splitDomains](trainer.py 288): INFO  * Train Acc 99.379
[2023-09-29 12:04:18 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.064, Total time 1.28
[2023-09-29 12:04:18 4splitDomains](my_trainer.py 328): INFO Epoch:12
[2023-09-29 12:04:18 4splitDomains](my_trainer.py 335): INFO LR:0.0044779099478933675
[2023-09-29 12:04:18 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:04:19 4splitDomains](trainer.py 286): INFO [0/91]	0.5887(0.5887)	0.4782(0.4782)	0.002(0.002)	100.00(100.00)
[2023-09-29 12:04:20 4splitDomains](trainer.py 286): INFO [10/91]	0.0976(0.1439)	0.0003(0.0437)	0.010(0.016)	100.00(99.72)
[2023-09-29 12:04:21 4splitDomains](trainer.py 286): INFO [20/91]	0.0979(0.1224)	0.0003(0.0231)	0.101(0.018)	96.88(99.55)
[2023-09-29 12:04:22 4splitDomains](trainer.py 286): INFO [30/91]	0.0978(0.1146)	0.0002(0.0157)	0.003(0.020)	100.00(99.60)
[2023-09-29 12:04:23 4splitDomains](trainer.py 286): INFO [40/91]	0.0977(0.1106)	0.0003(0.0119)	0.003(0.018)	100.00(99.62)
[2023-09-29 12:04:24 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1081)	0.0003(0.0097)	0.004(0.020)	100.00(99.57)
[2023-09-29 12:04:25 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1066)	0.0003(0.0081)	0.002(0.019)	100.00(99.54)
[2023-09-29 12:04:26 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1054)	0.0002(0.0070)	0.003(0.019)	100.00(99.52)
[2023-09-29 12:04:27 4splitDomains](trainer.py 286): INFO [80/91]	0.1051(0.1045)	0.0003(0.0062)	0.002(0.019)	100.00(99.50)
[2023-09-29 12:04:28 4splitDomains](trainer.py 286): INFO [90/91]	0.0806(0.1036)	0.0001(0.0055)	0.044(0.019)	100.00(99.45)
[2023-09-29 12:04:28 4splitDomains](trainer.py 288): INFO  * Train Acc 99.448
[2023-09-29 12:04:29 4splitDomains](my_trainer.py 503): INFO  * Val Acc 87.798, Total time 1.28
[2023-09-29 12:04:29 4splitDomains](my_trainer.py 328): INFO Epoch:13
[2023-09-29 12:04:29 4splitDomains](my_trainer.py 335): INFO LR:0.0034555695366224513
[2023-09-29 12:04:29 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:04:30 4splitDomains](trainer.py 286): INFO [0/91]	0.5951(0.5951)	0.4909(0.4909)	0.004(0.004)	100.00(100.00)
[2023-09-29 12:04:31 4splitDomains](trainer.py 286): INFO [10/91]	0.0980(0.1449)	0.0003(0.0449)	0.028(0.008)	100.00(100.00)
[2023-09-29 12:04:32 4splitDomains](trainer.py 286): INFO [20/91]	0.0977(0.1231)	0.0002(0.0237)	0.005(0.007)	100.00(100.00)
[2023-09-29 12:04:33 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.1149)	0.0002(0.0161)	0.001(0.009)	100.00(99.90)
[2023-09-29 12:04:34 4splitDomains](trainer.py 286): INFO [40/91]	0.0978(0.1108)	0.0003(0.0123)	0.004(0.010)	100.00(99.85)
[2023-09-29 12:04:35 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1082)	0.0002(0.0099)	0.003(0.012)	100.00(99.69)
[2023-09-29 12:04:36 4splitDomains](trainer.py 286): INFO [60/91]	0.0977(0.1065)	0.0002(0.0083)	0.002(0.011)	100.00(99.74)
[2023-09-29 12:04:37 4splitDomains](trainer.py 286): INFO [70/91]	0.1057(0.1055)	0.0003(0.0072)	0.026(0.011)	100.00(99.78)
[2023-09-29 12:04:38 4splitDomains](trainer.py 286): INFO [80/91]	0.1003(0.1046)	0.0003(0.0063)	0.009(0.012)	100.00(99.73)
[2023-09-29 12:04:39 4splitDomains](trainer.py 286): INFO [90/91]	0.0777(0.1037)	0.0001(0.0057)	0.021(0.011)	100.00(99.76)
[2023-09-29 12:04:39 4splitDomains](trainer.py 288): INFO  * Train Acc 99.759
[2023-09-29 12:04:40 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.594, Total time 1.30
[2023-09-29 12:04:40 4splitDomains](my_trainer.py 328): INFO Epoch:14
[2023-09-29 12:04:40 4splitDomains](my_trainer.py 335): INFO LR:0.0025007500000000017
[2023-09-29 12:04:40 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:04:41 4splitDomains](trainer.py 286): INFO [0/91]	0.5890(0.5890)	0.4709(0.4709)	0.001(0.001)	100.00(100.00)
[2023-09-29 12:04:42 4splitDomains](trainer.py 286): INFO [10/91]	0.0977(0.1438)	0.0002(0.0431)	0.065(0.009)	96.88(99.72)
[2023-09-29 12:04:43 4splitDomains](trainer.py 286): INFO [20/91]	0.0976(0.1221)	0.0002(0.0227)	0.009(0.007)	100.00(99.85)
[2023-09-29 12:04:44 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1146)	0.0002(0.0155)	0.096(0.011)	93.75(99.60)
[2023-09-29 12:04:45 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1105)	0.0002(0.0118)	0.004(0.010)	100.00(99.62)
[2023-09-29 12:04:46 4splitDomains](trainer.py 286): INFO [50/91]	0.0978(0.1083)	0.0003(0.0095)	0.003(0.010)	100.00(99.63)
[2023-09-29 12:04:46 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.1066)	0.0002(0.0080)	0.004(0.009)	100.00(99.69)
[2023-09-29 12:04:47 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.1053)	0.0002(0.0069)	0.015(0.009)	100.00(99.74)
[2023-09-29 12:04:48 4splitDomains](trainer.py 286): INFO [80/91]	0.0978(0.1044)	0.0002(0.0061)	0.005(0.009)	100.00(99.73)
[2023-09-29 12:04:49 4splitDomains](trainer.py 286): INFO [90/91]	0.0780(0.1035)	0.0001(0.0055)	0.005(0.009)	100.00(99.72)
[2023-09-29 12:04:50 4splitDomains](trainer.py 288): INFO  * Train Acc 99.724
[2023-09-29 12:04:51 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.594, Total time 1.29
[2023-09-29 12:04:51 4splitDomains](my_trainer.py 328): INFO Epoch:15
[2023-09-29 12:04:51 4splitDomains](my_trainer.py 335): INFO LR:0.00165518153350889
[2023-09-29 12:04:51 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:04:51 4splitDomains](trainer.py 286): INFO [0/91]	0.6381(0.6381)	0.5231(0.5231)	0.002(0.002)	100.00(100.00)
[2023-09-29 12:04:53 4splitDomains](trainer.py 286): INFO [10/91]	0.1080(0.1515)	0.0007(0.0479)	0.003(0.004)	100.00(100.00)
[2023-09-29 12:04:53 4splitDomains](trainer.py 286): INFO [20/91]	0.0981(0.1260)	0.0002(0.0252)	0.002(0.004)	100.00(100.00)
[2023-09-29 12:04:54 4splitDomains](trainer.py 286): INFO [30/91]	0.0971(0.1169)	0.0002(0.0172)	0.002(0.006)	100.00(100.00)
[2023-09-29 12:04:55 4splitDomains](trainer.py 286): INFO [40/91]	0.0973(0.1122)	0.0001(0.0131)	0.059(0.007)	96.88(99.92)
[2023-09-29 12:04:56 4splitDomains](trainer.py 286): INFO [50/91]	0.0973(0.1095)	0.0002(0.0106)	0.002(0.006)	100.00(99.94)
[2023-09-29 12:04:57 4splitDomains](trainer.py 286): INFO [60/91]	0.0977(0.1076)	0.0002(0.0089)	0.001(0.005)	100.00(99.95)
[2023-09-29 12:04:58 4splitDomains](trainer.py 286): INFO [70/91]	0.0979(0.1062)	0.0002(0.0077)	0.001(0.006)	100.00(99.96)
[2023-09-29 12:04:59 4splitDomains](trainer.py 286): INFO [80/91]	0.0976(0.1056)	0.0001(0.0068)	0.021(0.007)	100.00(99.85)
[2023-09-29 12:05:00 4splitDomains](trainer.py 286): INFO [90/91]	0.0833(0.1046)	0.0001(0.0060)	0.006(0.007)	100.00(99.83)
[2023-09-29 12:05:00 4splitDomains](trainer.py 288): INFO  * Train Acc 99.828
[2023-09-29 12:05:02 4splitDomains](my_trainer.py 503): INFO  * Val Acc 87.798, Total time 1.31
[2023-09-29 12:05:02 4splitDomains](my_trainer.py 328): INFO Epoch:16
[2023-09-29 12:05:02 4splitDomains](my_trainer.py 335): INFO LR:0.0009558195366224509
[2023-09-29 12:05:02 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:05:02 4splitDomains](trainer.py 286): INFO [0/91]	0.6024(0.6024)	0.4992(0.4992)	0.001(0.001)	100.00(100.00)
[2023-09-29 12:05:03 4splitDomains](trainer.py 286): INFO [10/91]	0.0976(0.1453)	0.0002(0.0456)	0.005(0.005)	100.00(100.00)
[2023-09-29 12:05:04 4splitDomains](trainer.py 286): INFO [20/91]	0.0972(0.1235)	0.0002(0.0241)	0.002(0.006)	100.00(99.85)
[2023-09-29 12:05:05 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.1152)	0.0003(0.0164)	0.005(0.006)	100.00(99.90)
[2023-09-29 12:05:06 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1109)	0.0002(0.0125)	0.003(0.006)	100.00(99.92)
[2023-09-29 12:05:07 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.1084)	0.0002(0.0101)	0.002(0.007)	100.00(99.88)
[2023-09-29 12:05:08 4splitDomains](trainer.py 286): INFO [60/91]	0.1064(0.1077)	0.0005(0.0085)	0.005(0.009)	100.00(99.80)
[2023-09-29 12:05:09 4splitDomains](trainer.py 286): INFO [70/91]	0.0996(0.1064)	0.0003(0.0073)	0.002(0.009)	100.00(99.82)
[2023-09-29 12:05:10 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1054)	0.0002(0.0065)	0.001(0.008)	100.00(99.85)
[2023-09-29 12:05:11 4splitDomains](trainer.py 286): INFO [90/91]	0.0791(0.1043)	0.0001(0.0058)	0.012(0.008)	100.00(99.86)
[2023-09-29 12:05:11 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862
[2023-09-29 12:05:13 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.064, Total time 1.27
[2023-09-29 12:05:13 4splitDomains](my_trainer.py 328): INFO Epoch:17
[2023-09-29 12:05:13 4splitDomains](my_trainer.py 335): INFO LR:0.0004332294845158165
[2023-09-29 12:05:13 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:05:13 4splitDomains](trainer.py 286): INFO [0/91]	0.5924(0.5924)	0.4878(0.4878)	0.002(0.002)	100.00(100.00)
[2023-09-29 12:05:14 4splitDomains](trainer.py 286): INFO [10/91]	0.0998(0.1445)	0.0001(0.0446)	0.002(0.003)	100.00(100.00)
[2023-09-29 12:05:15 4splitDomains](trainer.py 286): INFO [20/91]	0.0977(0.1222)	0.0002(0.0235)	0.002(0.004)	100.00(100.00)
[2023-09-29 12:05:16 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1156)	0.0002(0.0160)	0.001(0.003)	100.00(100.00)
[2023-09-29 12:05:17 4splitDomains](trainer.py 286): INFO [40/91]	0.0981(0.1113)	0.0004(0.0122)	0.001(0.004)	100.00(99.92)
[2023-09-29 12:05:18 4splitDomains](trainer.py 286): INFO [50/91]	0.0978(0.1088)	0.0003(0.0099)	0.029(0.005)	100.00(99.94)
[2023-09-29 12:05:19 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1070)	0.0001(0.0083)	0.001(0.005)	100.00(99.90)
[2023-09-29 12:05:20 4splitDomains](trainer.py 286): INFO [70/91]	0.0977(0.1057)	0.0002(0.0072)	0.006(0.005)	100.00(99.91)
[2023-09-29 12:05:21 4splitDomains](trainer.py 286): INFO [80/91]	0.0976(0.1049)	0.0002(0.0063)	0.002(0.005)	100.00(99.88)
[2023-09-29 12:05:22 4splitDomains](trainer.py 286): INFO [90/91]	0.0774(0.1039)	0.0001(0.0056)	0.001(0.005)	100.00(99.90)
[2023-09-29 12:05:22 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897
[2023-09-29 12:05:24 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.064, Total time 1.26
[2023-09-29 12:05:24 4splitDomains](my_trainer.py 328): INFO Epoch:18
[2023-09-29 12:05:24 4splitDomains](my_trainer.py 335): INFO LR:0.00011025107013133847
[2023-09-29 12:05:24 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:05:24 4splitDomains](trainer.py 286): INFO [0/91]	0.5935(0.5935)	0.4834(0.4834)	0.001(0.001)	100.00(100.00)
[2023-09-29 12:05:25 4splitDomains](trainer.py 286): INFO [10/91]	0.0973(0.1433)	0.0002(0.0442)	0.001(0.002)	100.00(100.00)
[2023-09-29 12:05:26 4splitDomains](trainer.py 286): INFO [20/91]	0.0977(0.1217)	0.0002(0.0232)	0.002(0.004)	100.00(100.00)
[2023-09-29 12:05:27 4splitDomains](trainer.py 286): INFO [30/91]	0.0979(0.1145)	0.0003(0.0159)	0.004(0.004)	100.00(100.00)
[2023-09-29 12:05:28 4splitDomains](trainer.py 286): INFO [40/91]	0.0983(0.1105)	0.0001(0.0121)	0.003(0.004)	100.00(100.00)
[2023-09-29 12:05:29 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.1081)	0.0001(0.0097)	0.002(0.004)	100.00(100.00)
[2023-09-29 12:05:30 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1065)	0.0002(0.0082)	0.034(0.004)	96.88(99.95)
[2023-09-29 12:05:31 4splitDomains](trainer.py 286): INFO [70/91]	0.0978(0.1053)	0.0002(0.0071)	0.004(0.004)	100.00(99.96)
[2023-09-29 12:05:32 4splitDomains](trainer.py 286): INFO [80/91]	0.0978(0.1047)	0.0003(0.0062)	0.003(0.004)	100.00(99.96)
[2023-09-29 12:05:33 4splitDomains](trainer.py 286): INFO [90/91]	0.0793(0.1037)	0.0001(0.0056)	0.036(0.004)	100.00(99.97)
[2023-09-29 12:05:33 4splitDomains](trainer.py 288): INFO  * Train Acc 99.966
[2023-09-29 12:05:34 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.064, Total time 1.32
[2023-09-29 12:05:34 4splitDomains](my_trainer.py 328): INFO Epoch:19
[2023-09-29 12:05:34 4splitDomains](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 12:05:34 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:05:35 4splitDomains](trainer.py 286): INFO [0/91]	0.6343(0.6343)	0.5096(0.5096)	0.007(0.007)	100.00(100.00)
[2023-09-29 12:05:36 4splitDomains](trainer.py 286): INFO [10/91]	0.0976(0.1480)	0.0003(0.0466)	0.001(0.008)	100.00(99.72)
[2023-09-29 12:05:37 4splitDomains](trainer.py 286): INFO [20/91]	0.0975(0.1246)	0.0003(0.0246)	0.009(0.010)	100.00(99.70)
[2023-09-29 12:05:38 4splitDomains](trainer.py 286): INFO [30/91]	0.0978(0.1159)	0.0002(0.0168)	0.003(0.009)	100.00(99.70)
[2023-09-29 12:05:39 4splitDomains](trainer.py 286): INFO [40/91]	0.0977(0.1116)	0.0002(0.0127)	0.004(0.008)	100.00(99.77)
[2023-09-29 12:05:40 4splitDomains](trainer.py 286): INFO [50/91]	0.0977(0.1089)	0.0002(0.0103)	0.001(0.008)	100.00(99.82)
[2023-09-29 12:05:41 4splitDomains](trainer.py 286): INFO [60/91]	0.0981(0.1071)	0.0004(0.0087)	0.001(0.008)	100.00(99.80)
[2023-09-29 12:05:42 4splitDomains](trainer.py 286): INFO [70/91]	0.1011(0.1059)	0.0006(0.0075)	0.035(0.008)	96.88(99.78)
[2023-09-29 12:05:43 4splitDomains](trainer.py 286): INFO [80/91]	0.0982(0.1050)	0.0004(0.0066)	0.005(0.007)	100.00(99.81)
[2023-09-29 12:05:44 4splitDomains](trainer.py 286): INFO [90/91]	0.0790(0.1040)	0.0001(0.0059)	0.010(0.007)	100.00(99.83)
[2023-09-29 12:05:44 4splitDomains](trainer.py 288): INFO  * Train Acc 99.828
[2023-09-29 12:05:45 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.125, Total time 1.28
[2023-09-29 12:05:45 4splitDomains](my_trainer.py 206): INFO Pruning for task0
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 7056/9408 (75.00%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 3072/4096 (75.00%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 27648/36864 (75.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 12288/16384 (75.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 12288/16384 (75.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 12288/16384 (75.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 27648/36864 (75.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 12288/16384 (75.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 12288/16384 (75.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 27648/36864 (75.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 12288/16384 (75.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 24576/32768 (75.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 110592/147456 (75.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 49152/65536 (75.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 98304/131072 (75.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 49152/65536 (75.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 110592/147456 (75.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 49152/65536 (75.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 49152/65536 (75.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 110592/147456 (75.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 49152/65536 (75.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 49152/65536 (75.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 110592/147456 (75.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 49152/65536 (75.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 98304/131072 (75.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 442368/589824 (75.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 393216/524288 (75.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 442368/589824 (75.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 442368/589824 (75.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 442368/589824 (75.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 442368/589824 (75.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 442368/589824 (75.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 196608/262144 (75.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 393216/524288 (75.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 1769472/2359296 (75.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 786432/1048576 (75.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 1572864/2097152 (75.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 786432/1048576 (75.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 1769472/2359296 (75.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 786432/1048576 (75.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 786432/1048576 (75.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 1769472/2359296 (75.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 786433/1048576 (75.00%) (Total in layer: 1048576)
Layer #last.All, pruned 92160/122880 (75.00%) (Total in layer: 122880)
[2023-09-29 12:05:46 4splitDomains](my_trainer.py 298): INFO start retrain model
[2023-09-29 12:05:46 4splitDomains](my_trainer.py 302): INFO Epoch:0
[2023-09-29 12:05:46 4splitDomains](my_trainer.py 308): INFO LR:0.01
[2023-09-29 12:05:46 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:05:46 4splitDomains](trainer.py 286): INFO [0/91]	0.5719(0.5719)	0.4616(0.4616)	0.132(0.132)	100.00(100.00)
[2023-09-29 12:05:47 4splitDomains](trainer.py 286): INFO [10/91]	0.0973(0.1421)	0.0003(0.0422)	0.138(0.109)	96.88(99.43)
[2023-09-29 12:05:48 4splitDomains](trainer.py 286): INFO [20/91]	0.0975(0.1216)	0.0002(0.0222)	0.068(0.097)	100.00(99.55)
[2023-09-29 12:05:49 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.1138)	0.0003(0.0152)	0.086(0.088)	100.00(99.60)
[2023-09-29 12:05:50 4splitDomains](trainer.py 286): INFO [40/91]	0.0975(0.1098)	0.0002(0.0115)	0.035(0.082)	100.00(99.62)
[2023-09-29 12:05:51 4splitDomains](trainer.py 286): INFO [50/91]	0.1011(0.1075)	0.0002(0.0093)	0.014(0.074)	100.00(99.63)
[2023-09-29 12:05:52 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1060)	0.0002(0.0078)	0.042(0.069)	96.88(99.59)
[2023-09-29 12:05:53 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1050)	0.0002(0.0068)	0.044(0.068)	100.00(99.56)
[2023-09-29 12:05:54 4splitDomains](trainer.py 286): INFO [80/91]	0.0976(0.1041)	0.0003(0.0060)	0.040(0.064)	100.00(99.61)
[2023-09-29 12:05:55 4splitDomains](trainer.py 286): INFO [90/91]	0.0773(0.1031)	0.0001(0.0053)	0.044(0.062)	100.00(99.59)
[2023-09-29 12:05:55 4splitDomains](trainer.py 288): INFO  * Train Acc 99.586
[2023-09-29 12:05:57 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.594, Total time 1.25
[2023-09-29 12:05:57 4splitDomains](my_trainer.py 302): INFO Epoch:1
[2023-09-29 12:05:57 4splitDomains](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 12:05:57 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:05:57 4splitDomains](trainer.py 286): INFO [0/91]	0.5592(0.5592)	0.4487(0.4487)	0.017(0.017)	100.00(100.00)
[2023-09-29 12:05:58 4splitDomains](trainer.py 286): INFO [10/91]	0.0975(0.1402)	0.0003(0.0411)	0.016(0.025)	100.00(100.00)
[2023-09-29 12:05:59 4splitDomains](trainer.py 286): INFO [20/91]	0.0972(0.1198)	0.0002(0.0216)	0.013(0.024)	100.00(99.85)
[2023-09-29 12:06:00 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.1132)	0.0002(0.0148)	0.024(0.025)	100.00(99.70)
[2023-09-29 12:06:01 4splitDomains](trainer.py 286): INFO [40/91]	0.0980(0.1095)	0.0003(0.0112)	0.026(0.025)	100.00(99.70)
[2023-09-29 12:06:02 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1074)	0.0002(0.0091)	0.017(0.024)	100.00(99.75)
[2023-09-29 12:06:03 4splitDomains](trainer.py 286): INFO [60/91]	0.0977(0.1059)	0.0003(0.0077)	0.015(0.025)	100.00(99.74)
[2023-09-29 12:06:04 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1048)	0.0002(0.0066)	0.016(0.026)	100.00(99.69)
[2023-09-29 12:06:05 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1039)	0.0003(0.0058)	0.014(0.027)	100.00(99.61)
[2023-09-29 12:06:06 4splitDomains](trainer.py 286): INFO [90/91]	0.0786(0.1030)	0.0001(0.0052)	0.047(0.027)	100.00(99.62)
[2023-09-29 12:06:06 4splitDomains](trainer.py 288): INFO  * Train Acc 99.621
[2023-09-29 12:06:07 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.064, Total time 1.31
[2023-09-29 12:06:07 4splitDomains](my_trainer.py 302): INFO Epoch:2
[2023-09-29 12:06:07 4splitDomains](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 12:06:07 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:06:08 4splitDomains](trainer.py 286): INFO [0/91]	0.5769(0.5769)	0.4562(0.4562)	0.011(0.011)	100.00(100.00)
[2023-09-29 12:06:09 4splitDomains](trainer.py 286): INFO [10/91]	0.0978(0.1418)	0.0002(0.0418)	0.009(0.018)	100.00(99.72)
[2023-09-29 12:06:10 4splitDomains](trainer.py 286): INFO [20/91]	0.0972(0.1207)	0.0002(0.0220)	0.022(0.017)	100.00(99.85)
[2023-09-29 12:06:11 4splitDomains](trainer.py 286): INFO [30/91]	0.0971(0.1133)	0.0001(0.0150)	0.128(0.020)	96.88(99.80)
[2023-09-29 12:06:12 4splitDomains](trainer.py 286): INFO [40/91]	0.1077(0.1098)	0.0007(0.0114)	0.019(0.020)	100.00(99.77)
[2023-09-29 12:06:13 4splitDomains](trainer.py 286): INFO [50/91]	0.1044(0.1075)	0.0006(0.0092)	0.031(0.019)	96.88(99.75)
[2023-09-29 12:06:14 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.1060)	0.0002(0.0078)	0.011(0.018)	100.00(99.80)
[2023-09-29 12:06:15 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1053)	0.0003(0.0067)	0.025(0.018)	100.00(99.78)
[2023-09-29 12:06:16 4splitDomains](trainer.py 286): INFO [80/91]	0.0974(0.1043)	0.0001(0.0059)	0.020(0.017)	100.00(99.81)
[2023-09-29 12:06:17 4splitDomains](trainer.py 286): INFO [90/91]	0.0806(0.1034)	0.0001(0.0053)	0.006(0.019)	100.00(99.76)
[2023-09-29 12:06:17 4splitDomains](trainer.py 288): INFO  * Train Acc 99.759
[2023-09-29 12:06:18 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.125, Total time 1.25
[2023-09-29 12:06:18 4splitDomains](my_trainer.py 302): INFO Epoch:3
[2023-09-29 12:06:18 4splitDomains](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 12:06:18 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:06:19 4splitDomains](trainer.py 286): INFO [0/91]	0.5548(0.5548)	0.4454(0.4454)	0.058(0.058)	96.88(96.88)
[2023-09-29 12:06:20 4splitDomains](trainer.py 286): INFO [10/91]	0.0970(0.1417)	0.0001(0.0428)	0.014(0.025)	100.00(99.43)
[2023-09-29 12:06:21 4splitDomains](trainer.py 286): INFO [20/91]	0.0974(0.1211)	0.0002(0.0226)	0.015(0.022)	100.00(99.55)
[2023-09-29 12:06:22 4splitDomains](trainer.py 286): INFO [30/91]	0.0976(0.1134)	0.0002(0.0154)	0.007(0.020)	100.00(99.60)
[2023-09-29 12:06:23 4splitDomains](trainer.py 286): INFO [40/91]	0.0974(0.1096)	0.0002(0.0117)	0.028(0.019)	100.00(99.70)
[2023-09-29 12:06:24 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.1072)	0.0002(0.0095)	0.056(0.018)	96.88(99.69)
[2023-09-29 12:06:25 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.1056)	0.0002(0.0080)	0.007(0.020)	100.00(99.59)
[2023-09-29 12:06:26 4splitDomains](trainer.py 286): INFO [70/91]	0.0991(0.1047)	0.0004(0.0069)	0.045(0.019)	100.00(99.65)
[2023-09-29 12:06:27 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1039)	0.0003(0.0061)	0.005(0.019)	100.00(99.65)
[2023-09-29 12:06:28 4splitDomains](trainer.py 286): INFO [90/91]	0.0775(0.1030)	0.0001(0.0054)	0.028(0.018)	100.00(99.69)
[2023-09-29 12:06:28 4splitDomains](trainer.py 288): INFO  * Train Acc 99.690
[2023-09-29 12:06:29 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.920, Total time 1.26
[2023-09-29 12:06:29 4splitDomains](my_trainer.py 302): INFO Epoch:4
[2023-09-29 12:06:29 4splitDomains](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 12:06:29 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:06:30 4splitDomains](trainer.py 286): INFO [0/91]	0.5661(0.5661)	0.4572(0.4572)	0.043(0.043)	100.00(100.00)
[2023-09-29 12:06:31 4splitDomains](trainer.py 286): INFO [10/91]	0.0974(0.1405)	0.0003(0.0418)	0.010(0.016)	100.00(99.72)
[2023-09-29 12:06:31 4splitDomains](trainer.py 286): INFO [20/91]	0.0971(0.1199)	0.0001(0.0220)	0.003(0.016)	100.00(99.55)
[2023-09-29 12:06:32 4splitDomains](trainer.py 286): INFO [30/91]	0.0971(0.1127)	0.0001(0.0150)	0.007(0.015)	100.00(99.70)
[2023-09-29 12:06:33 4splitDomains](trainer.py 286): INFO [40/91]	0.0974(0.1090)	0.0002(0.0114)	0.015(0.014)	100.00(99.77)
[2023-09-29 12:06:34 4splitDomains](trainer.py 286): INFO [50/91]	0.0974(0.1069)	0.0002(0.0092)	0.026(0.014)	100.00(99.82)
[2023-09-29 12:06:35 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.1054)	0.0001(0.0077)	0.060(0.014)	96.88(99.80)
[2023-09-29 12:06:36 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1043)	0.0002(0.0067)	0.014(0.016)	100.00(99.78)
[2023-09-29 12:06:37 4splitDomains](trainer.py 286): INFO [80/91]	0.0975(0.1035)	0.0002(0.0059)	0.009(0.015)	100.00(99.81)
[2023-09-29 12:06:38 4splitDomains](trainer.py 286): INFO [90/91]	0.0774(0.1027)	0.0001(0.0053)	0.009(0.015)	100.00(99.83)
[2023-09-29 12:06:38 4splitDomains](trainer.py 288): INFO  * Train Acc 99.828
[2023-09-29 12:06:40 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.594, Total time 1.27
[2023-09-29 12:06:40 4splitDomains](my_trainer.py 302): INFO Epoch:5
[2023-09-29 12:06:40 4splitDomains](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 12:06:40 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:06:40 4splitDomains](trainer.py 286): INFO [0/91]	0.5689(0.5689)	0.4683(0.4683)	0.030(0.030)	100.00(100.00)
[2023-09-29 12:06:41 4splitDomains](trainer.py 286): INFO [10/91]	0.0973(0.1415)	0.0002(0.0428)	0.032(0.013)	100.00(100.00)
[2023-09-29 12:06:42 4splitDomains](trainer.py 286): INFO [20/91]	0.0972(0.1225)	0.0001(0.0226)	0.005(0.010)	100.00(100.00)
[2023-09-29 12:06:43 4splitDomains](trainer.py 286): INFO [30/91]	0.0978(0.1144)	0.0003(0.0154)	0.005(0.009)	100.00(100.00)
[2023-09-29 12:06:44 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1103)	0.0003(0.0117)	0.030(0.010)	100.00(100.00)
[2023-09-29 12:06:45 4splitDomains](trainer.py 286): INFO [50/91]	0.1032(0.1080)	0.0003(0.0095)	0.011(0.011)	100.00(99.94)
[2023-09-29 12:06:46 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.1062)	0.0001(0.0080)	0.002(0.010)	100.00(99.95)
[2023-09-29 12:06:47 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1050)	0.0003(0.0069)	0.010(0.011)	100.00(99.91)
[2023-09-29 12:06:48 4splitDomains](trainer.py 286): INFO [80/91]	0.0976(0.1041)	0.0003(0.0061)	0.046(0.011)	100.00(99.92)
[2023-09-29 12:06:49 4splitDomains](trainer.py 286): INFO [90/91]	0.0777(0.1032)	0.0001(0.0054)	0.004(0.011)	100.00(99.93)
[2023-09-29 12:06:49 4splitDomains](trainer.py 288): INFO  * Train Acc 99.931
[2023-09-29 12:06:50 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.655, Total time 1.28
[2023-09-29 12:06:50 4splitDomains](my_trainer.py 302): INFO Epoch:6
[2023-09-29 12:06:50 4splitDomains](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 12:06:50 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:06:51 4splitDomains](trainer.py 286): INFO [0/91]	0.5912(0.5912)	0.4752(0.4752)	0.003(0.003)	100.00(100.00)
[2023-09-29 12:06:52 4splitDomains](trainer.py 286): INFO [10/91]	0.1047(0.1429)	0.0002(0.0434)	0.003(0.007)	100.00(100.00)
[2023-09-29 12:06:53 4splitDomains](trainer.py 286): INFO [20/91]	0.0971(0.1212)	0.0001(0.0229)	0.043(0.008)	96.88(99.85)
[2023-09-29 12:06:54 4splitDomains](trainer.py 286): INFO [30/91]	0.0991(0.1136)	0.0003(0.0156)	0.006(0.011)	100.00(99.80)
[2023-09-29 12:06:55 4splitDomains](trainer.py 286): INFO [40/91]	0.0975(0.1097)	0.0002(0.0118)	0.006(0.011)	100.00(99.77)
[2023-09-29 12:06:56 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1073)	0.0002(0.0096)	0.008(0.011)	100.00(99.82)
[2023-09-29 12:06:57 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1057)	0.0003(0.0080)	0.003(0.010)	100.00(99.85)
[2023-09-29 12:06:58 4splitDomains](trainer.py 286): INFO [70/91]	0.1013(0.1046)	0.0003(0.0070)	0.014(0.009)	100.00(99.87)
[2023-09-29 12:06:59 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1038)	0.0003(0.0061)	0.003(0.009)	100.00(99.85)
[2023-09-29 12:07:00 4splitDomains](trainer.py 286): INFO [90/91]	0.0774(0.1029)	0.0001(0.0055)	0.007(0.009)	100.00(99.86)
[2023-09-29 12:07:00 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862
[2023-09-29 12:07:01 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.390, Total time 1.27
[2023-09-29 12:07:01 4splitDomains](my_trainer.py 302): INFO Epoch:7
[2023-09-29 12:07:01 4splitDomains](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 12:07:01 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:07:02 4splitDomains](trainer.py 286): INFO [0/91]	0.6053(0.6053)	0.5049(0.5049)	0.006(0.006)	100.00(100.00)
[2023-09-29 12:07:03 4splitDomains](trainer.py 286): INFO [10/91]	0.0974(0.1459)	0.0003(0.0465)	0.003(0.005)	100.00(100.00)
[2023-09-29 12:07:04 4splitDomains](trainer.py 286): INFO [20/91]	0.0973(0.1229)	0.0002(0.0245)	0.003(0.009)	100.00(99.85)
[2023-09-29 12:07:05 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.1148)	0.0002(0.0167)	0.004(0.009)	100.00(99.90)
[2023-09-29 12:07:06 4splitDomains](trainer.py 286): INFO [40/91]	0.0974(0.1107)	0.0002(0.0127)	0.010(0.009)	100.00(99.92)
[2023-09-29 12:07:07 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.1081)	0.0003(0.0103)	0.002(0.009)	100.00(99.94)
[2023-09-29 12:07:08 4splitDomains](trainer.py 286): INFO [60/91]	0.1007(0.1065)	0.0002(0.0086)	0.008(0.009)	100.00(99.90)
[2023-09-29 12:07:09 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1053)	0.0002(0.0074)	0.003(0.008)	100.00(99.91)
[2023-09-29 12:07:10 4splitDomains](trainer.py 286): INFO [80/91]	0.0974(0.1045)	0.0002(0.0066)	0.004(0.009)	100.00(99.88)
[2023-09-29 12:07:11 4splitDomains](trainer.py 286): INFO [90/91]	0.0780(0.1035)	0.0001(0.0059)	0.007(0.009)	100.00(99.90)
[2023-09-29 12:07:11 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897
[2023-09-29 12:07:12 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.655, Total time 1.26
[2023-09-29 12:07:12 4splitDomains](my_trainer.py 302): INFO Epoch:8
[2023-09-29 12:07:12 4splitDomains](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 12:07:12 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:07:13 4splitDomains](trainer.py 286): INFO [0/91]	0.5768(0.5768)	0.4678(0.4678)	0.004(0.004)	100.00(100.00)
[2023-09-29 12:07:14 4splitDomains](trainer.py 286): INFO [10/91]	0.0976(0.1425)	0.0002(0.0428)	0.005(0.012)	100.00(99.72)
[2023-09-29 12:07:15 4splitDomains](trainer.py 286): INFO [20/91]	0.0996(0.1212)	0.0002(0.0226)	0.017(0.010)	100.00(99.85)
[2023-09-29 12:07:16 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.1144)	0.0002(0.0154)	0.004(0.008)	100.00(99.90)
[2023-09-29 12:07:17 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1107)	0.0002(0.0117)	0.013(0.007)	100.00(99.92)
[2023-09-29 12:07:18 4splitDomains](trainer.py 286): INFO [50/91]	0.1001(0.1085)	0.0002(0.0095)	0.004(0.007)	100.00(99.94)
[2023-09-29 12:07:19 4splitDomains](trainer.py 286): INFO [60/91]	0.0979(0.1069)	0.0002(0.0081)	0.004(0.007)	100.00(99.95)
[2023-09-29 12:07:20 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.1056)	0.0001(0.0070)	0.008(0.007)	100.00(99.96)
[2023-09-29 12:07:20 4splitDomains](trainer.py 286): INFO [80/91]	0.0973(0.1046)	0.0001(0.0062)	0.005(0.007)	100.00(99.96)
[2023-09-29 12:07:21 4splitDomains](trainer.py 286): INFO [90/91]	0.0785(0.1037)	0.0001(0.0055)	0.051(0.007)	100.00(99.97)
[2023-09-29 12:07:22 4splitDomains](trainer.py 288): INFO  * Train Acc 99.966
[2023-09-29 12:07:23 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.125, Total time 1.28
[2023-09-29 12:07:23 4splitDomains](my_trainer.py 302): INFO Epoch:9
[2023-09-29 12:07:23 4splitDomains](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 12:07:23 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:07:23 4splitDomains](trainer.py 286): INFO [0/91]	0.6084(0.6084)	0.4927(0.4927)	0.006(0.006)	100.00(100.00)
[2023-09-29 12:07:24 4splitDomains](trainer.py 286): INFO [10/91]	0.1007(0.1461)	0.0002(0.0451)	0.002(0.003)	100.00(100.00)
[2023-09-29 12:07:25 4splitDomains](trainer.py 286): INFO [20/91]	0.0980(0.1243)	0.0003(0.0237)	0.003(0.004)	100.00(100.00)
[2023-09-29 12:07:26 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.1162)	0.0002(0.0162)	0.005(0.007)	100.00(99.90)
[2023-09-29 12:07:27 4splitDomains](trainer.py 286): INFO [40/91]	0.0975(0.1121)	0.0002(0.0123)	0.002(0.008)	100.00(99.85)
[2023-09-29 12:07:28 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1094)	0.0002(0.0100)	0.003(0.008)	100.00(99.88)
[2023-09-29 12:07:29 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.1083)	0.0002(0.0084)	0.002(0.007)	100.00(99.90)
[2023-09-29 12:07:30 4splitDomains](trainer.py 286): INFO [70/91]	0.1097(0.1073)	0.0006(0.0073)	0.003(0.007)	100.00(99.91)
[2023-09-29 12:07:31 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1063)	0.0002(0.0064)	0.002(0.007)	100.00(99.92)
[2023-09-29 12:07:32 4splitDomains](trainer.py 286): INFO [90/91]	0.0781(0.1052)	0.0001(0.0057)	0.080(0.007)	94.74(99.90)
[2023-09-29 12:07:32 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897
[2023-09-29 12:07:34 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.329, Total time 1.26
[2023-09-29 12:07:34 4splitDomains](my_trainer.py 302): INFO Epoch:10
[2023-09-29 12:07:34 4splitDomains](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 12:07:34 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:07:34 4splitDomains](trainer.py 286): INFO [0/91]	0.5880(0.5880)	0.4784(0.4784)	0.020(0.020)	100.00(100.00)
[2023-09-29 12:07:35 4splitDomains](trainer.py 286): INFO [10/91]	0.1059(0.1445)	0.0002(0.0437)	0.004(0.008)	100.00(100.00)
[2023-09-29 12:07:36 4splitDomains](trainer.py 286): INFO [20/91]	0.1051(0.1236)	0.0006(0.0231)	0.011(0.008)	100.00(99.85)
[2023-09-29 12:07:37 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.1154)	0.0002(0.0157)	0.009(0.007)	100.00(99.90)
[2023-09-29 12:07:38 4splitDomains](trainer.py 286): INFO [40/91]	0.0978(0.1112)	0.0003(0.0120)	0.006(0.006)	100.00(99.92)
[2023-09-29 12:07:39 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.1088)	0.0002(0.0098)	0.002(0.006)	100.00(99.94)
[2023-09-29 12:07:40 4splitDomains](trainer.py 286): INFO [60/91]	0.0978(0.1074)	0.0003(0.0083)	0.003(0.007)	100.00(99.90)
[2023-09-29 12:07:41 4splitDomains](trainer.py 286): INFO [70/91]	0.0979(0.1065)	0.0003(0.0072)	0.003(0.007)	100.00(99.91)
[2023-09-29 12:07:42 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1054)	0.0003(0.0063)	0.004(0.008)	100.00(99.85)
[2023-09-29 12:07:43 4splitDomains](trainer.py 286): INFO [90/91]	0.0781(0.1043)	0.0001(0.0057)	0.007(0.007)	100.00(99.86)
[2023-09-29 12:07:43 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862
[2023-09-29 12:07:45 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.859, Total time 1.26
[2023-09-29 12:07:45 4splitDomains](my_trainer.py 302): INFO Epoch:11
[2023-09-29 12:07:45 4splitDomains](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 12:07:45 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:07:45 4splitDomains](trainer.py 286): INFO [0/91]	0.5974(0.5974)	0.4814(0.4814)	0.014(0.014)	100.00(100.00)
[2023-09-29 12:07:46 4splitDomains](trainer.py 286): INFO [10/91]	0.0972(0.1455)	0.0002(0.0440)	0.002(0.005)	100.00(100.00)
[2023-09-29 12:07:47 4splitDomains](trainer.py 286): INFO [20/91]	0.0977(0.1226)	0.0004(0.0232)	0.003(0.004)	100.00(100.00)
[2023-09-29 12:07:48 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1145)	0.0002(0.0158)	0.018(0.005)	100.00(100.00)
[2023-09-29 12:07:49 4splitDomains](trainer.py 286): INFO [40/91]	0.0973(0.1104)	0.0002(0.0120)	0.021(0.007)	100.00(100.00)
[2023-09-29 12:07:50 4splitDomains](trainer.py 286): INFO [50/91]	0.0973(0.1078)	0.0002(0.0097)	0.005(0.007)	100.00(100.00)
[2023-09-29 12:07:51 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.1061)	0.0002(0.0081)	0.001(0.007)	100.00(100.00)
[2023-09-29 12:07:52 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.1049)	0.0003(0.0070)	0.006(0.008)	100.00(99.96)
[2023-09-29 12:07:53 4splitDomains](trainer.py 286): INFO [80/91]	0.0974(0.1040)	0.0002(0.0062)	0.002(0.007)	100.00(99.96)
[2023-09-29 12:07:54 4splitDomains](trainer.py 286): INFO [90/91]	0.0782(0.1031)	0.0001(0.0055)	0.011(0.007)	100.00(99.97)
[2023-09-29 12:07:54 4splitDomains](trainer.py 288): INFO  * Train Acc 99.966
[2023-09-29 12:07:55 4splitDomains](my_trainer.py 503): INFO  * Val Acc 90.186, Total time 1.29
[2023-09-29 12:07:55 4splitDomains](my_trainer.py 302): INFO Epoch:12
[2023-09-29 12:07:55 4splitDomains](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 12:07:55 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:07:56 4splitDomains](trainer.py 286): INFO [0/91]	0.6130(0.6130)	0.4951(0.4951)	0.016(0.016)	100.00(100.00)
[2023-09-29 12:07:57 4splitDomains](trainer.py 286): INFO [10/91]	0.0982(0.1464)	0.0011(0.0454)	0.078(0.022)	96.88(99.43)
[2023-09-29 12:07:58 4splitDomains](trainer.py 286): INFO [20/91]	0.0977(0.1231)	0.0002(0.0239)	0.002(0.014)	100.00(99.70)
[2023-09-29 12:07:59 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.1148)	0.0002(0.0163)	0.003(0.011)	100.00(99.80)
[2023-09-29 12:08:00 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1106)	0.0003(0.0124)	0.012(0.012)	100.00(99.77)
[2023-09-29 12:08:01 4splitDomains](trainer.py 286): INFO [50/91]	0.0984(0.1080)	0.0001(0.0100)	0.018(0.011)	100.00(99.82)
[2023-09-29 12:08:02 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1064)	0.0002(0.0084)	0.004(0.011)	100.00(99.80)
[2023-09-29 12:08:03 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1054)	0.0002(0.0073)	0.002(0.010)	100.00(99.82)
[2023-09-29 12:08:04 4splitDomains](trainer.py 286): INFO [80/91]	0.0975(0.1048)	0.0003(0.0064)	0.007(0.009)	100.00(99.85)
[2023-09-29 12:08:05 4splitDomains](trainer.py 286): INFO [90/91]	0.0776(0.1038)	0.0001(0.0057)	0.008(0.009)	100.00(99.86)
[2023-09-29 12:08:05 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862
[2023-09-29 12:08:06 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.125, Total time 1.27
[2023-09-29 12:08:06 4splitDomains](my_trainer.py 302): INFO Epoch:13
[2023-09-29 12:08:06 4splitDomains](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 12:08:06 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:08:07 4splitDomains](trainer.py 286): INFO [0/91]	0.6058(0.6058)	0.4782(0.4782)	0.003(0.003)	100.00(100.00)
[2023-09-29 12:08:08 4splitDomains](trainer.py 286): INFO [10/91]	0.0998(0.1457)	0.0007(0.0438)	0.003(0.005)	100.00(100.00)
[2023-09-29 12:08:09 4splitDomains](trainer.py 286): INFO [20/91]	0.0973(0.1226)	0.0002(0.0231)	0.013(0.007)	100.00(99.85)
[2023-09-29 12:08:10 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.1145)	0.0002(0.0157)	0.004(0.006)	100.00(99.90)
[2023-09-29 12:08:11 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1104)	0.0003(0.0119)	0.002(0.006)	100.00(99.92)
[2023-09-29 12:08:12 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1079)	0.0002(0.0096)	0.004(0.005)	100.00(99.94)
[2023-09-29 12:08:13 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.1064)	0.0002(0.0081)	0.002(0.005)	100.00(99.95)
[2023-09-29 12:08:14 4splitDomains](trainer.py 286): INFO [70/91]	0.0977(0.1051)	0.0003(0.0070)	0.013(0.006)	100.00(99.96)
[2023-09-29 12:08:15 4splitDomains](trainer.py 286): INFO [80/91]	0.0975(0.1042)	0.0002(0.0062)	0.082(0.007)	96.88(99.92)
[2023-09-29 12:08:16 4splitDomains](trainer.py 286): INFO [90/91]	0.0778(0.1032)	0.0001(0.0055)	0.002(0.007)	100.00(99.86)
[2023-09-29 12:08:16 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862
[2023-09-29 12:08:17 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.859, Total time 1.28
[2023-09-29 12:08:17 4splitDomains](my_trainer.py 302): INFO Epoch:14
[2023-09-29 12:08:17 4splitDomains](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 12:08:17 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:08:18 4splitDomains](trainer.py 286): INFO [0/91]	0.6026(0.6026)	0.4987(0.4987)	0.001(0.001)	100.00(100.00)
[2023-09-29 12:08:19 4splitDomains](trainer.py 286): INFO [10/91]	0.0971(0.1455)	0.0001(0.0456)	0.058(0.009)	96.88(99.72)
[2023-09-29 12:08:20 4splitDomains](trainer.py 286): INFO [20/91]	0.0973(0.1226)	0.0001(0.0240)	0.002(0.007)	100.00(99.85)
[2023-09-29 12:08:21 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1145)	0.0002(0.0163)	0.021(0.007)	100.00(99.90)
[2023-09-29 12:08:22 4splitDomains](trainer.py 286): INFO [40/91]	0.0973(0.1104)	0.0001(0.0124)	0.006(0.009)	100.00(99.77)
[2023-09-29 12:08:23 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1079)	0.0002(0.0100)	0.004(0.009)	100.00(99.75)
[2023-09-29 12:08:24 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.1062)	0.0001(0.0084)	0.001(0.008)	100.00(99.80)
[2023-09-29 12:08:25 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.1050)	0.0002(0.0072)	0.003(0.007)	100.00(99.82)
[2023-09-29 12:08:26 4splitDomains](trainer.py 286): INFO [80/91]	0.0975(0.1041)	0.0002(0.0064)	0.005(0.007)	100.00(99.85)
[2023-09-29 12:08:26 4splitDomains](trainer.py 286): INFO [90/91]	0.0774(0.1031)	0.0001(0.0057)	0.012(0.007)	100.00(99.86)
[2023-09-29 12:08:27 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862
[2023-09-29 12:08:28 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.390, Total time 1.30
[2023-09-29 12:08:28 4splitDomains](my_trainer.py 302): INFO Epoch:15
[2023-09-29 12:08:28 4splitDomains](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 12:08:28 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:08:28 4splitDomains](trainer.py 286): INFO [0/91]	0.5541(0.5541)	0.4376(0.4376)	0.007(0.007)	100.00(100.00)
[2023-09-29 12:08:29 4splitDomains](trainer.py 286): INFO [10/91]	0.1034(0.1402)	0.0003(0.0402)	0.018(0.008)	100.00(99.72)
[2023-09-29 12:08:30 4splitDomains](trainer.py 286): INFO [20/91]	0.0973(0.1199)	0.0001(0.0212)	0.007(0.008)	100.00(99.70)
[2023-09-29 12:08:31 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.1127)	0.0002(0.0144)	0.003(0.007)	100.00(99.80)
[2023-09-29 12:08:32 4splitDomains](trainer.py 286): INFO [40/91]	0.0973(0.1090)	0.0001(0.0110)	0.003(0.008)	100.00(99.77)
[2023-09-29 12:08:33 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.1068)	0.0003(0.0089)	0.003(0.007)	100.00(99.82)
[2023-09-29 12:08:34 4splitDomains](trainer.py 286): INFO [60/91]	0.0978(0.1055)	0.0005(0.0075)	0.008(0.007)	100.00(99.85)
[2023-09-29 12:08:35 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1044)	0.0003(0.0065)	0.002(0.007)	100.00(99.87)
[2023-09-29 12:08:36 4splitDomains](trainer.py 286): INFO [80/91]	0.0974(0.1035)	0.0002(0.0057)	0.003(0.006)	100.00(99.88)
[2023-09-29 12:08:37 4splitDomains](trainer.py 286): INFO [90/91]	0.0772(0.1026)	0.0001(0.0051)	0.005(0.007)	100.00(99.86)
[2023-09-29 12:08:37 4splitDomains](trainer.py 288): INFO  * Train Acc 99.862
[2023-09-29 12:08:39 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.655, Total time 1.27
[2023-09-29 12:08:39 4splitDomains](my_trainer.py 302): INFO Epoch:16
[2023-09-29 12:08:39 4splitDomains](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 12:08:39 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:08:39 4splitDomains](trainer.py 286): INFO [0/91]	0.5797(0.5797)	0.4716(0.4716)	0.004(0.004)	100.00(100.00)
[2023-09-29 12:08:40 4splitDomains](trainer.py 286): INFO [10/91]	0.0971(0.1433)	0.0002(0.0432)	0.003(0.007)	100.00(100.00)
[2023-09-29 12:08:41 4splitDomains](trainer.py 286): INFO [20/91]	0.0976(0.1215)	0.0002(0.0227)	0.002(0.005)	100.00(100.00)
[2023-09-29 12:08:42 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.1141)	0.0002(0.0155)	0.003(0.005)	100.00(100.00)
[2023-09-29 12:08:43 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1102)	0.0003(0.0118)	0.032(0.007)	96.88(99.77)
[2023-09-29 12:08:44 4splitDomains](trainer.py 286): INFO [50/91]	0.0972(0.1078)	0.0001(0.0095)	0.003(0.007)	100.00(99.82)
[2023-09-29 12:08:45 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.1061)	0.0002(0.0080)	0.005(0.006)	100.00(99.85)
[2023-09-29 12:08:46 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.1049)	0.0002(0.0069)	0.002(0.006)	100.00(99.87)
[2023-09-29 12:08:47 4splitDomains](trainer.py 286): INFO [80/91]	0.0975(0.1040)	0.0002(0.0061)	0.002(0.006)	100.00(99.88)
[2023-09-29 12:08:48 4splitDomains](trainer.py 286): INFO [90/91]	0.0773(0.1031)	0.0001(0.0055)	0.055(0.006)	100.00(99.90)
[2023-09-29 12:08:48 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897
[2023-09-29 12:08:49 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.859, Total time 1.31
[2023-09-29 12:08:49 4splitDomains](my_trainer.py 302): INFO Epoch:17
[2023-09-29 12:08:49 4splitDomains](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 12:08:49 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:08:50 4splitDomains](trainer.py 286): INFO [0/91]	0.5669(0.5669)	0.4563(0.4563)	0.002(0.002)	100.00(100.00)
[2023-09-29 12:08:51 4splitDomains](trainer.py 286): INFO [10/91]	0.1045(0.1410)	0.0005(0.0418)	0.003(0.005)	100.00(100.00)
[2023-09-29 12:08:52 4splitDomains](trainer.py 286): INFO [20/91]	0.0973(0.1203)	0.0002(0.0220)	0.005(0.005)	100.00(100.00)
[2023-09-29 12:08:53 4splitDomains](trainer.py 286): INFO [30/91]	0.0978(0.1133)	0.0004(0.0150)	0.001(0.004)	100.00(100.00)
[2023-09-29 12:08:54 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.1104)	0.0002(0.0115)	0.001(0.004)	100.00(100.00)
[2023-09-29 12:08:55 4splitDomains](trainer.py 286): INFO [50/91]	0.0980(0.1084)	0.0003(0.0093)	0.021(0.005)	100.00(100.00)
[2023-09-29 12:08:56 4splitDomains](trainer.py 286): INFO [60/91]	0.1000(0.1067)	0.0002(0.0078)	0.001(0.005)	100.00(100.00)
[2023-09-29 12:08:57 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.1055)	0.0003(0.0068)	0.008(0.005)	100.00(100.00)
[2023-09-29 12:08:58 4splitDomains](trainer.py 286): INFO [80/91]	0.0976(0.1048)	0.0003(0.0060)	0.002(0.005)	100.00(100.00)
[2023-09-29 12:08:59 4splitDomains](trainer.py 286): INFO [90/91]	0.0775(0.1038)	0.0001(0.0053)	0.023(0.005)	100.00(100.00)
[2023-09-29 12:08:59 4splitDomains](trainer.py 288): INFO  * Train Acc 100.000
[2023-09-29 12:09:00 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.655, Total time 1.30
[2023-09-29 12:09:00 4splitDomains](my_trainer.py 302): INFO Epoch:18
[2023-09-29 12:09:00 4splitDomains](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 12:09:00 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:09:01 4splitDomains](trainer.py 286): INFO [0/91]	0.5760(0.5760)	0.4712(0.4712)	0.006(0.006)	100.00(100.00)
[2023-09-29 12:09:02 4splitDomains](trainer.py 286): INFO [10/91]	0.0971(0.1416)	0.0001(0.0431)	0.012(0.005)	100.00(100.00)
[2023-09-29 12:09:03 4splitDomains](trainer.py 286): INFO [20/91]	0.0975(0.1208)	0.0003(0.0227)	0.008(0.004)	100.00(100.00)
[2023-09-29 12:09:04 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.1138)	0.0001(0.0155)	0.009(0.005)	100.00(100.00)
[2023-09-29 12:09:05 4splitDomains](trainer.py 286): INFO [40/91]	0.0973(0.1099)	0.0002(0.0118)	0.003(0.005)	100.00(100.00)
[2023-09-29 12:09:06 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1075)	0.0002(0.0095)	0.002(0.004)	100.00(100.00)
[2023-09-29 12:09:07 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.1059)	0.0003(0.0080)	0.001(0.005)	100.00(100.00)
[2023-09-29 12:09:08 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.1047)	0.0001(0.0069)	0.003(0.005)	100.00(99.96)
[2023-09-29 12:09:09 4splitDomains](trainer.py 286): INFO [80/91]	0.0976(0.1044)	0.0003(0.0061)	0.006(0.006)	100.00(99.92)
[2023-09-29 12:09:10 4splitDomains](trainer.py 286): INFO [90/91]	0.0777(0.1040)	0.0001(0.0055)	0.049(0.006)	100.00(99.90)
[2023-09-29 12:09:10 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897
[2023-09-29 12:09:11 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.920, Total time 1.28
[2023-09-29 12:09:11 4splitDomains](my_trainer.py 302): INFO Epoch:19
[2023-09-29 12:09:11 4splitDomains](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 12:09:11 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:09:12 4splitDomains](trainer.py 286): INFO [0/91]	0.5898(0.5898)	0.4794(0.4794)	0.001(0.001)	100.00(100.00)
[2023-09-29 12:09:13 4splitDomains](trainer.py 286): INFO [10/91]	0.1027(0.1437)	0.0005(0.0439)	0.003(0.003)	100.00(100.00)
[2023-09-29 12:09:14 4splitDomains](trainer.py 286): INFO [20/91]	0.0996(0.1229)	0.0005(0.0232)	0.003(0.003)	100.00(100.00)
[2023-09-29 12:09:15 4splitDomains](trainer.py 286): INFO [30/91]	0.0978(0.1150)	0.0004(0.0158)	0.003(0.003)	100.00(100.00)
[2023-09-29 12:09:16 4splitDomains](trainer.py 286): INFO [40/91]	0.0980(0.1108)	0.0003(0.0120)	0.004(0.003)	100.00(100.00)
[2023-09-29 12:09:17 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.1082)	0.0002(0.0097)	0.002(0.004)	100.00(100.00)
[2023-09-29 12:09:18 4splitDomains](trainer.py 286): INFO [60/91]	0.0973(0.1065)	0.0002(0.0082)	0.005(0.005)	100.00(99.95)
[2023-09-29 12:09:19 4splitDomains](trainer.py 286): INFO [70/91]	0.0973(0.1053)	0.0002(0.0071)	0.003(0.005)	100.00(99.91)
[2023-09-29 12:09:20 4splitDomains](trainer.py 286): INFO [80/91]	0.0977(0.1046)	0.0003(0.0062)	0.002(0.005)	100.00(99.92)
[2023-09-29 12:09:21 4splitDomains](trainer.py 286): INFO [90/91]	0.0778(0.1036)	0.0001(0.0056)	0.014(0.005)	100.00(99.93)
[2023-09-29 12:09:21 4splitDomains](trainer.py 288): INFO  * Train Acc 99.931
[2023-09-29 12:09:22 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.125, Total time 1.28
=> Saving model to: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-0.pth
=> Save Done
[2023-09-29 12:09:22 4splitDomains](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 12:09:24 4splitDomains](my_trainer.py 503): INFO  * Val Acc 89.125, Total time 1.47
[2023-09-29 12:09:24 4splitDomains](trainer.py 335): INFO saving storage...
[2023-09-29 12:09:24 4splitDomains](trainer.py 341): INFO done
[2023-09-29 12:09:24 4splitDomains](iBatchLearn.py 155): INFO Acc:89.12466843501326; BWT:0;
=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:09:27 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:09:27 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:09:27 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 0, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-0.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-0.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_0.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:09:27 4splitDomains](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-0.pth
[2023-09-29 12:09:28 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:09:30 4splitDomains](my_trainer.py 67): INFO load storage...
[2023-09-29 12:09:30 4splitDomains](my_trainer.py 71): INFO done
[2023-09-29 12:09:30 4splitDomains](my_trainer.py 64): INFO tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 12:09:30 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:09:30 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:09:30 4splitDomains](iBatchLearn.py 167): INFO test split name:0
--------------------------------Official Evaluation--------------------------------
0 92.23918575063614
=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:09:38 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:09:38 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:09:38 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 1, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-0.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-1.pth", "storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-0.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-1.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:09:38 4splitDomains](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-0.pth
[2023-09-29 12:09:39 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:09:41 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:09:41 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:09:41 4splitDomains](my_trainer.py 67): INFO load storage...
[2023-09-29 12:09:41 4splitDomains](my_trainer.py 71): INFO done
[2023-09-29 12:09:41 4splitDomains](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 12:09:41 4splitDomains](iBatchLearn.py 92): INFO ====================== 1 =======================
[2023-09-29 12:09:41 4splitDomains](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 12:09:41 4splitDomains](my_trainer.py 328): INFO Epoch:0
[2023-09-29 12:09:41 4splitDomains](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 12:09:41 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:09:44 4splitDomains](trainer.py 286): INFO [0/90]	3.3830(3.3830)	0.4322(0.4322)	3.122(3.122)	37.50(37.50)
[2023-09-29 12:09:45 4splitDomains](trainer.py 286): INFO [10/90]	0.0971(0.3972)	0.0002(0.0395)	2.605(2.576)	46.88(43.75)
[2023-09-29 12:09:46 4splitDomains](trainer.py 286): INFO [20/90]	0.0972(0.2548)	0.0002(0.0208)	1.885(2.451)	50.00(45.24)
[2023-09-29 12:09:47 4splitDomains](trainer.py 286): INFO [30/90]	0.0972(0.2040)	0.0003(0.0142)	1.572(2.291)	56.25(47.68)
[2023-09-29 12:09:48 4splitDomains](trainer.py 286): INFO [40/90]	0.0969(0.1779)	0.0001(0.0108)	1.865(2.172)	50.00(48.93)
[2023-09-29 12:09:49 4splitDomains](trainer.py 286): INFO [50/90]	0.0972(0.1623)	0.0002(0.0087)	1.375(2.041)	65.62(51.29)
[2023-09-29 12:09:50 4splitDomains](trainer.py 286): INFO [60/90]	0.0973(0.1517)	0.0003(0.0073)	0.946(1.933)	71.88(53.12)
[2023-09-29 12:09:51 4splitDomains](trainer.py 286): INFO [70/90]	0.1016(0.1443)	0.0002(0.0063)	1.746(1.893)	62.50(54.14)
[2023-09-29 12:09:52 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1388)	0.0003(0.0056)	1.581(1.836)	56.25(55.29)
[2023-09-29 12:09:53 4splitDomains](trainer.py 286): INFO [89/90]	0.0920(0.1346)	0.0001(0.0051)	1.406(1.776)	62.07(56.13)
[2023-09-29 12:09:53 4splitDomains](trainer.py 288): INFO  * Train Acc 56.135
[2023-09-29 12:09:55 4splitDomains](my_trainer.py 503): INFO  * Val Acc 62.169, Total time 1.88
[2023-09-29 12:09:55 4splitDomains](my_trainer.py 328): INFO Epoch:1
[2023-09-29 12:09:55 4splitDomains](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 12:09:55 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:09:56 4splitDomains](trainer.py 286): INFO [0/90]	0.6085(0.6085)	0.5021(0.5021)	0.543(0.543)	90.62(90.62)
[2023-09-29 12:09:57 4splitDomains](trainer.py 286): INFO [10/90]	0.0975(0.1446)	0.0003(0.0460)	0.850(0.778)	75.00(80.97)
[2023-09-29 12:09:58 4splitDomains](trainer.py 286): INFO [20/90]	0.0972(0.1222)	0.0001(0.0242)	0.525(0.797)	84.38(79.02)
[2023-09-29 12:09:59 4splitDomains](trainer.py 286): INFO [30/90]	0.0972(0.1145)	0.0002(0.0165)	0.742(0.818)	78.12(78.53)
[2023-09-29 12:10:00 4splitDomains](trainer.py 286): INFO [40/90]	0.0974(0.1106)	0.0002(0.0125)	1.474(0.869)	68.75(76.83)
[2023-09-29 12:10:01 4splitDomains](trainer.py 286): INFO [50/90]	0.0983(0.1080)	0.0002(0.0101)	0.817(0.881)	78.12(76.29)
[2023-09-29 12:10:02 4splitDomains](trainer.py 286): INFO [60/90]	0.0977(0.1064)	0.0002(0.0085)	0.811(0.913)	75.00(75.05)
[2023-09-29 12:10:03 4splitDomains](trainer.py 286): INFO [70/90]	0.1008(0.1053)	0.0003(0.0074)	0.754(0.922)	78.12(74.56)
[2023-09-29 12:10:04 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1045)	0.0003(0.0065)	0.955(0.943)	71.88(73.73)
[2023-09-29 12:10:04 4splitDomains](trainer.py 286): INFO [89/90]	0.0920(0.1037)	0.0001(0.0059)	1.032(0.959)	72.41(73.38)
[2023-09-29 12:10:04 4splitDomains](trainer.py 288): INFO  * Train Acc 73.375
[2023-09-29 12:10:06 4splitDomains](my_trainer.py 503): INFO  * Val Acc 62.434, Total time 1.89
[2023-09-29 12:10:06 4splitDomains](my_trainer.py 328): INFO Epoch:2
[2023-09-29 12:10:06 4splitDomains](my_trainer.py 335): INFO LR:0.01
[2023-09-29 12:10:06 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:10:07 4splitDomains](trainer.py 286): INFO [0/90]	0.5752(0.5752)	0.4726(0.4726)	0.992(0.992)	75.00(75.00)
[2023-09-29 12:10:08 4splitDomains](trainer.py 286): INFO [10/90]	0.0973(0.1444)	0.0002(0.0433)	0.962(0.634)	68.75(82.95)
[2023-09-29 12:10:09 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1241)	0.0003(0.0229)	0.472(0.706)	90.62(80.36)
[2023-09-29 12:10:10 4splitDomains](trainer.py 286): INFO [30/90]	0.0975(0.1163)	0.0003(0.0156)	0.971(0.738)	71.88(78.23)
[2023-09-29 12:10:11 4splitDomains](trainer.py 286): INFO [40/90]	0.0977(0.1118)	0.0004(0.0119)	1.057(0.772)	68.75(77.29)
[2023-09-29 12:10:12 4splitDomains](trainer.py 286): INFO [50/90]	0.1015(0.1091)	0.0003(0.0096)	1.205(0.843)	65.62(75.55)
[2023-09-29 12:10:13 4splitDomains](trainer.py 286): INFO [60/90]	0.0975(0.1074)	0.0003(0.0081)	1.311(0.892)	71.88(74.28)
[2023-09-29 12:10:14 4splitDomains](trainer.py 286): INFO [70/90]	0.0991(0.1061)	0.0002(0.0070)	1.074(0.929)	71.88(74.08)
[2023-09-29 12:10:15 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1050)	0.0003(0.0062)	1.009(0.958)	75.00(73.42)
[2023-09-29 12:10:16 4splitDomains](trainer.py 286): INFO [89/90]	0.0918(0.1043)	0.0001(0.0056)	0.774(0.979)	79.31(72.85)
[2023-09-29 12:10:16 4splitDomains](trainer.py 288): INFO  * Train Acc 72.854
[2023-09-29 12:10:18 4splitDomains](my_trainer.py 503): INFO  * Val Acc 59.788, Total time 1.89
[2023-09-29 12:10:18 4splitDomains](my_trainer.py 328): INFO Epoch:3
[2023-09-29 12:10:18 4splitDomains](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 12:10:18 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:10:18 4splitDomains](trainer.py 286): INFO [0/90]	0.5204(0.5204)	0.4101(0.4101)	0.621(0.621)	78.12(78.12)
[2023-09-29 12:10:19 4splitDomains](trainer.py 286): INFO [10/90]	0.1004(0.1395)	0.0002(0.0376)	0.949(0.912)	65.62(72.16)
[2023-09-29 12:10:20 4splitDomains](trainer.py 286): INFO [20/90]	0.0973(0.1202)	0.0002(0.0198)	0.733(0.878)	68.75(73.07)
[2023-09-29 12:10:21 4splitDomains](trainer.py 286): INFO [30/90]	0.1001(0.1136)	0.0003(0.0135)	0.764(0.885)	71.88(73.59)
[2023-09-29 12:10:22 4splitDomains](trainer.py 286): INFO [40/90]	0.0974(0.1101)	0.0002(0.0103)	0.814(0.868)	75.00(74.62)
[2023-09-29 12:10:23 4splitDomains](trainer.py 286): INFO [50/90]	0.1010(0.1078)	0.0002(0.0083)	1.011(0.855)	62.50(74.88)
[2023-09-29 12:10:24 4splitDomains](trainer.py 286): INFO [60/90]	0.0975(0.1062)	0.0002(0.0070)	0.893(0.851)	75.00(75.46)
[2023-09-29 12:10:25 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1050)	0.0003(0.0061)	0.615(0.818)	84.38(76.01)
[2023-09-29 12:10:26 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1042)	0.0002(0.0053)	0.564(0.812)	84.38(76.00)
[2023-09-29 12:10:27 4splitDomains](trainer.py 286): INFO [89/90]	0.0919(0.1035)	0.0001(0.0048)	0.777(0.808)	75.86(76.19)
[2023-09-29 12:10:27 4splitDomains](trainer.py 288): INFO  * Train Acc 76.190
[2023-09-29 12:10:29 4splitDomains](my_trainer.py 503): INFO  * Val Acc 65.608, Total time 1.89
[2023-09-29 12:10:29 4splitDomains](my_trainer.py 328): INFO Epoch:4
[2023-09-29 12:10:29 4splitDomains](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 12:10:29 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:10:30 4splitDomains](trainer.py 286): INFO [0/90]	0.5241(0.5241)	0.4025(0.4025)	0.745(0.745)	81.25(81.25)
[2023-09-29 12:10:31 4splitDomains](trainer.py 286): INFO [10/90]	0.1031(0.1409)	0.0002(0.0369)	0.585(0.495)	81.25(85.51)
[2023-09-29 12:10:32 4splitDomains](trainer.py 286): INFO [20/90]	0.0974(0.1203)	0.0002(0.0195)	0.114(0.440)	96.88(87.65)
[2023-09-29 12:10:33 4splitDomains](trainer.py 286): INFO [30/90]	0.0973(0.1133)	0.0002(0.0133)	0.494(0.463)	84.38(86.29)
[2023-09-29 12:10:34 4splitDomains](trainer.py 286): INFO [40/90]	0.1002(0.1097)	0.0002(0.0101)	0.231(0.457)	93.75(86.51)
[2023-09-29 12:10:35 4splitDomains](trainer.py 286): INFO [50/90]	0.0976(0.1075)	0.0003(0.0082)	0.661(0.452)	84.38(86.64)
[2023-09-29 12:10:36 4splitDomains](trainer.py 286): INFO [60/90]	0.0977(0.1059)	0.0003(0.0069)	0.313(0.448)	90.62(86.73)
[2023-09-29 12:10:37 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1048)	0.0002(0.0060)	0.404(0.435)	84.38(86.93)
[2023-09-29 12:10:38 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1041)	0.0002(0.0053)	0.438(0.424)	84.38(87.38)
[2023-09-29 12:10:38 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1034)	0.0001(0.0048)	0.276(0.421)	93.10(87.59)
[2023-09-29 12:10:39 4splitDomains](trainer.py 288): INFO  * Train Acc 87.591
[2023-09-29 12:10:40 4splitDomains](my_trainer.py 503): INFO  * Val Acc 73.810, Total time 1.91
[2023-09-29 12:10:40 4splitDomains](my_trainer.py 328): INFO Epoch:5
[2023-09-29 12:10:40 4splitDomains](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 12:10:40 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:10:41 4splitDomains](trainer.py 286): INFO [0/90]	0.5907(0.5907)	0.4855(0.4855)	0.177(0.177)	93.75(93.75)
[2023-09-29 12:10:42 4splitDomains](trainer.py 286): INFO [10/90]	0.0998(0.1449)	0.0002(0.0444)	0.108(0.207)	96.88(92.90)
[2023-09-29 12:10:43 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1230)	0.0003(0.0234)	0.178(0.239)	93.75(92.86)
[2023-09-29 12:10:44 4splitDomains](trainer.py 286): INFO [30/90]	0.0974(0.1150)	0.0002(0.0160)	0.161(0.246)	96.88(92.24)
[2023-09-29 12:10:45 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1108)	0.0002(0.0121)	0.288(0.236)	93.75(92.76)
[2023-09-29 12:10:46 4splitDomains](trainer.py 286): INFO [50/90]	0.1023(0.1085)	0.0003(0.0098)	0.248(0.223)	93.75(93.32)
[2023-09-29 12:10:47 4splitDomains](trainer.py 286): INFO [60/90]	0.0974(0.1068)	0.0003(0.0082)	0.115(0.216)	96.88(93.65)
[2023-09-29 12:10:48 4splitDomains](trainer.py 286): INFO [70/90]	0.0975(0.1056)	0.0002(0.0071)	0.172(0.208)	93.75(93.93)
[2023-09-29 12:10:49 4splitDomains](trainer.py 286): INFO [80/90]	0.0974(0.1047)	0.0003(0.0063)	0.267(0.206)	93.75(94.06)
[2023-09-29 12:10:50 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1039)	0.0001(0.0057)	0.310(0.205)	89.66(94.13)
[2023-09-29 12:10:50 4splitDomains](trainer.py 288): INFO  * Train Acc 94.126
[2023-09-29 12:10:52 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.190, Total time 1.89
[2023-09-29 12:10:52 4splitDomains](my_trainer.py 328): INFO Epoch:6
[2023-09-29 12:10:52 4splitDomains](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 12:10:52 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:10:52 4splitDomains](trainer.py 286): INFO [0/90]	0.5919(0.5919)	0.4829(0.4829)	0.192(0.192)	93.75(93.75)
[2023-09-29 12:10:53 4splitDomains](trainer.py 286): INFO [10/90]	0.0975(0.1465)	0.0003(0.0442)	0.077(0.116)	96.88(96.31)
[2023-09-29 12:10:54 4splitDomains](trainer.py 286): INFO [20/90]	0.0994(0.1238)	0.0004(0.0233)	0.097(0.112)	96.88(96.43)
[2023-09-29 12:10:55 4splitDomains](trainer.py 286): INFO [30/90]	0.0994(0.1155)	0.0003(0.0159)	0.065(0.115)	96.88(96.37)
[2023-09-29 12:10:56 4splitDomains](trainer.py 286): INFO [40/90]	0.0981(0.1112)	0.0003(0.0121)	0.179(0.122)	96.88(96.19)
[2023-09-29 12:10:57 4splitDomains](trainer.py 286): INFO [50/90]	0.0978(0.1086)	0.0002(0.0098)	0.027(0.120)	100.00(96.26)
[2023-09-29 12:10:58 4splitDomains](trainer.py 286): INFO [60/90]	0.1053(0.1070)	0.0003(0.0082)	0.112(0.113)	96.88(96.67)
[2023-09-29 12:10:59 4splitDomains](trainer.py 286): INFO [70/90]	0.0977(0.1057)	0.0003(0.0071)	0.198(0.116)	90.62(96.35)
[2023-09-29 12:11:00 4splitDomains](trainer.py 286): INFO [80/90]	0.0978(0.1049)	0.0002(0.0063)	0.272(0.120)	93.75(96.37)
[2023-09-29 12:11:01 4splitDomains](trainer.py 286): INFO [89/90]	0.0923(0.1042)	0.0002(0.0057)	0.119(0.122)	96.55(96.32)
[2023-09-29 12:11:01 4splitDomains](trainer.py 288): INFO  * Train Acc 96.316
[2023-09-29 12:11:03 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.101, Total time 1.92
[2023-09-29 12:11:03 4splitDomains](my_trainer.py 328): INFO Epoch:7
[2023-09-29 12:11:03 4splitDomains](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 12:11:03 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:11:04 4splitDomains](trainer.py 286): INFO [0/90]	0.5999(0.5999)	0.4915(0.4915)	0.121(0.121)	96.88(96.88)
[2023-09-29 12:11:05 4splitDomains](trainer.py 286): INFO [10/90]	0.0973(0.1459)	0.0001(0.0450)	0.115(0.079)	96.88(98.01)
[2023-09-29 12:11:06 4splitDomains](trainer.py 286): INFO [20/90]	0.0977(0.1230)	0.0002(0.0237)	0.038(0.092)	100.00(97.77)
[2023-09-29 12:11:07 4splitDomains](trainer.py 286): INFO [30/90]	0.0990(0.1149)	0.0003(0.0161)	0.195(0.085)	96.88(97.98)
[2023-09-29 12:11:08 4splitDomains](trainer.py 286): INFO [40/90]	0.0977(0.1111)	0.0002(0.0123)	0.092(0.082)	100.00(97.94)
[2023-09-29 12:11:09 4splitDomains](trainer.py 286): INFO [50/90]	0.0974(0.1086)	0.0001(0.0099)	0.022(0.075)	100.00(98.04)
[2023-09-29 12:11:10 4splitDomains](trainer.py 286): INFO [60/90]	0.0980(0.1069)	0.0004(0.0083)	0.154(0.081)	93.75(97.59)
[2023-09-29 12:11:11 4splitDomains](trainer.py 286): INFO [70/90]	0.0977(0.1056)	0.0002(0.0072)	0.131(0.082)	96.88(97.62)
[2023-09-29 12:11:12 4splitDomains](trainer.py 286): INFO [80/90]	0.0978(0.1047)	0.0002(0.0064)	0.124(0.081)	100.00(97.80)
[2023-09-29 12:11:13 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1039)	0.0001(0.0057)	0.058(0.079)	100.00(97.84)
[2023-09-29 12:11:13 4splitDomains](trainer.py 288): INFO  * Train Acc 97.845
[2023-09-29 12:11:15 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.307, Total time 1.88
[2023-09-29 12:11:15 4splitDomains](my_trainer.py 328): INFO Epoch:8
[2023-09-29 12:11:15 4splitDomains](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 12:11:15 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:11:15 4splitDomains](trainer.py 286): INFO [0/90]	0.5804(0.5804)	0.4766(0.4766)	0.025(0.025)	100.00(100.00)
[2023-09-29 12:11:16 4splitDomains](trainer.py 286): INFO [10/90]	0.0976(0.1425)	0.0001(0.0436)	0.021(0.050)	100.00(98.58)
[2023-09-29 12:11:17 4splitDomains](trainer.py 286): INFO [20/90]	0.0978(0.1213)	0.0002(0.0229)	0.178(0.061)	93.75(98.66)
[2023-09-29 12:11:18 4splitDomains](trainer.py 286): INFO [30/90]	0.0976(0.1137)	0.0002(0.0156)	0.048(0.063)	100.00(98.19)
[2023-09-29 12:11:19 4splitDomains](trainer.py 286): INFO [40/90]	0.1023(0.1104)	0.0002(0.0119)	0.098(0.064)	93.75(98.09)
[2023-09-29 12:11:20 4splitDomains](trainer.py 286): INFO [50/90]	0.1050(0.1085)	0.0003(0.0096)	0.064(0.071)	96.88(98.04)
[2023-09-29 12:11:21 4splitDomains](trainer.py 286): INFO [60/90]	0.1030(0.1070)	0.0003(0.0081)	0.088(0.070)	96.88(98.00)
[2023-09-29 12:11:22 4splitDomains](trainer.py 286): INFO [70/90]	0.1074(0.1060)	0.0003(0.0070)	0.066(0.069)	100.00(98.02)
[2023-09-29 12:11:23 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1051)	0.0002(0.0062)	0.041(0.067)	100.00(98.15)
[2023-09-29 12:11:24 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1043)	0.0001(0.0056)	0.050(0.066)	96.55(98.16)
[2023-09-29 12:11:24 4splitDomains](trainer.py 288): INFO  * Train Acc 98.158
[2023-09-29 12:11:26 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.630, Total time 1.87
[2023-09-29 12:11:26 4splitDomains](my_trainer.py 328): INFO Epoch:9
[2023-09-29 12:11:26 4splitDomains](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 12:11:26 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:11:26 4splitDomains](trainer.py 286): INFO [0/90]	0.5323(0.5323)	0.4175(0.4175)	0.070(0.070)	96.88(96.88)
[2023-09-29 12:11:27 4splitDomains](trainer.py 286): INFO [10/90]	0.0976(0.1391)	0.0001(0.0382)	0.103(0.058)	96.88(97.73)
[2023-09-29 12:11:28 4splitDomains](trainer.py 286): INFO [20/90]	0.0978(0.1197)	0.0003(0.0201)	0.087(0.058)	100.00(98.36)
[2023-09-29 12:11:29 4splitDomains](trainer.py 286): INFO [30/90]	0.0995(0.1131)	0.0002(0.0137)	0.023(0.056)	100.00(98.39)
[2023-09-29 12:11:30 4splitDomains](trainer.py 286): INFO [40/90]	0.0978(0.1096)	0.0003(0.0105)	0.186(0.063)	93.75(98.09)
[2023-09-29 12:11:31 4splitDomains](trainer.py 286): INFO [50/90]	0.0978(0.1074)	0.0002(0.0085)	0.036(0.059)	100.00(98.28)
[2023-09-29 12:11:32 4splitDomains](trainer.py 286): INFO [60/90]	0.0978(0.1058)	0.0002(0.0071)	0.032(0.059)	100.00(98.41)
[2023-09-29 12:11:33 4splitDomains](trainer.py 286): INFO [70/90]	0.0977(0.1048)	0.0002(0.0062)	0.052(0.059)	100.00(98.50)
[2023-09-29 12:11:34 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1040)	0.0001(0.0055)	0.078(0.061)	100.00(98.50)
[2023-09-29 12:11:35 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1035)	0.0001(0.0049)	0.019(0.061)	100.00(98.47)
[2023-09-29 12:11:35 4splitDomains](trainer.py 288): INFO  * Train Acc 98.471
[2023-09-29 12:11:37 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.836, Total time 1.85
[2023-09-29 12:11:37 4splitDomains](my_trainer.py 206): INFO Pruning for task1
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 4704/7056 (66.67%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 2048/3072 (66.67%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 18432/27648 (66.67%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 8192/12288 (66.67%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 8192/12288 (66.67%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 8192/12288 (66.67%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 18432/27648 (66.67%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 8192/12288 (66.67%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 8192/12288 (66.67%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 18432/27648 (66.67%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 8192/12288 (66.67%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 16384/24576 (66.67%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 73728/110592 (66.67%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 32768/49152 (66.67%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 65536/98304 (66.67%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 32768/49152 (66.67%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 73728/110592 (66.67%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 32768/49152 (66.67%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 32768/49152 (66.67%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 73728/110592 (66.67%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 32768/49152 (66.67%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 32768/49152 (66.67%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 73728/110592 (66.67%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 32768/49152 (66.67%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 65536/98304 (66.67%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 294912/442368 (66.67%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 262144/393216 (66.67%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 294912/442368 (66.67%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 294912/442368 (66.67%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 294912/442368 (66.67%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 294912/442368 (66.67%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 294912/442368 (66.67%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 131072/196608 (66.67%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 262144/393216 (66.67%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 1179648/1769472 (66.67%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 524288/786432 (66.67%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 1048576/1572864 (66.67%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 524288/786432 (66.67%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 1179648/1769472 (66.67%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 524288/786432 (66.67%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 524288/786432 (66.67%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 1179648/1769472 (66.67%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 524288/786433 (66.67%) (Total in layer: 1048576)
Layer #last.All, pruned 61440/92160 (66.67%) (Total in layer: 122880)
[2023-09-29 12:11:38 4splitDomains](my_trainer.py 298): INFO start retrain model
[2023-09-29 12:11:38 4splitDomains](my_trainer.py 302): INFO Epoch:0
[2023-09-29 12:11:38 4splitDomains](my_trainer.py 308): INFO LR:0.01
[2023-09-29 12:11:38 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:11:38 4splitDomains](trainer.py 286): INFO [0/90]	0.6015(0.6015)	0.4988(0.4988)	0.042(0.042)	100.00(100.00)
[2023-09-29 12:11:39 4splitDomains](trainer.py 286): INFO [10/90]	0.0974(0.1458)	0.0003(0.0456)	0.060(0.054)	96.88(98.86)
[2023-09-29 12:11:40 4splitDomains](trainer.py 286): INFO [20/90]	0.1018(0.1236)	0.0003(0.0241)	0.038(0.069)	100.00(98.36)
[2023-09-29 12:11:41 4splitDomains](trainer.py 286): INFO [30/90]	0.0977(0.1154)	0.0002(0.0164)	0.054(0.077)	100.00(97.68)
[2023-09-29 12:11:42 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1111)	0.0002(0.0125)	0.074(0.075)	96.88(97.79)
[2023-09-29 12:11:43 4splitDomains](trainer.py 286): INFO [50/90]	0.0978(0.1085)	0.0003(0.0102)	0.211(0.083)	93.75(97.37)
[2023-09-29 12:11:44 4splitDomains](trainer.py 286): INFO [60/90]	0.0978(0.1069)	0.0002(0.0085)	0.034(0.087)	100.00(97.39)
[2023-09-29 12:11:45 4splitDomains](trainer.py 286): INFO [70/90]	0.0976(0.1056)	0.0002(0.0074)	0.054(0.087)	100.00(97.40)
[2023-09-29 12:11:46 4splitDomains](trainer.py 286): INFO [80/90]	0.1003(0.1048)	0.0002(0.0065)	0.082(0.087)	96.88(97.49)
[2023-09-29 12:11:47 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1040)	0.0001(0.0059)	0.139(0.085)	96.55(97.60)
[2023-09-29 12:11:47 4splitDomains](trainer.py 288): INFO  * Train Acc 97.602
[2023-09-29 12:11:49 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.042, Total time 1.93
[2023-09-29 12:11:49 4splitDomains](my_trainer.py 302): INFO Epoch:1
[2023-09-29 12:11:49 4splitDomains](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 12:11:49 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:11:50 4splitDomains](trainer.py 286): INFO [0/90]	0.5564(0.5564)	0.4486(0.4486)	0.077(0.077)	96.88(96.88)
[2023-09-29 12:11:51 4splitDomains](trainer.py 286): INFO [10/90]	0.1015(0.1413)	0.0003(0.0411)	0.041(0.064)	100.00(98.58)
[2023-09-29 12:11:52 4splitDomains](trainer.py 286): INFO [20/90]	0.0977(0.1213)	0.0002(0.0217)	0.069(0.060)	96.88(98.51)
[2023-09-29 12:11:53 4splitDomains](trainer.py 286): INFO [30/90]	0.1026(0.1153)	0.0002(0.0148)	0.017(0.065)	100.00(98.19)
[2023-09-29 12:11:54 4splitDomains](trainer.py 286): INFO [40/90]	0.0974(0.1111)	0.0002(0.0113)	0.044(0.069)	100.00(98.09)
[2023-09-29 12:11:55 4splitDomains](trainer.py 286): INFO [50/90]	0.0974(0.1086)	0.0001(0.0091)	0.033(0.075)	100.00(97.79)
[2023-09-29 12:11:56 4splitDomains](trainer.py 286): INFO [60/90]	0.0977(0.1071)	0.0002(0.0077)	0.103(0.081)	96.88(97.44)
[2023-09-29 12:11:57 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1058)	0.0001(0.0066)	0.149(0.082)	93.75(97.40)
[2023-09-29 12:11:58 4splitDomains](trainer.py 286): INFO [80/90]	0.0977(0.1048)	0.0002(0.0058)	0.039(0.085)	100.00(97.26)
[2023-09-29 12:11:58 4splitDomains](trainer.py 286): INFO [89/90]	0.0924(0.1041)	0.0003(0.0053)	0.046(0.087)	100.00(97.22)
[2023-09-29 12:11:58 4splitDomains](trainer.py 288): INFO  * Train Acc 97.219
[2023-09-29 12:12:00 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.836, Total time 1.90
[2023-09-29 12:12:00 4splitDomains](my_trainer.py 302): INFO Epoch:2
[2023-09-29 12:12:00 4splitDomains](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 12:12:00 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:12:01 4splitDomains](trainer.py 286): INFO [0/90]	0.5746(0.5746)	0.4601(0.4601)	0.025(0.025)	100.00(100.00)
[2023-09-29 12:12:02 4splitDomains](trainer.py 286): INFO [10/90]	0.0977(0.1422)	0.0002(0.0421)	0.051(0.070)	100.00(97.73)
[2023-09-29 12:12:03 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1213)	0.0001(0.0222)	0.031(0.061)	100.00(98.07)
[2023-09-29 12:12:04 4splitDomains](trainer.py 286): INFO [30/90]	0.0988(0.1140)	0.0003(0.0151)	0.061(0.069)	100.00(97.98)
[2023-09-29 12:12:05 4splitDomains](trainer.py 286): INFO [40/90]	0.0976(0.1101)	0.0002(0.0115)	0.018(0.068)	100.00(98.02)
[2023-09-29 12:12:06 4splitDomains](trainer.py 286): INFO [50/90]	0.0976(0.1077)	0.0002(0.0093)	0.122(0.072)	93.75(97.86)
[2023-09-29 12:12:07 4splitDomains](trainer.py 286): INFO [60/90]	0.0977(0.1061)	0.0003(0.0078)	0.019(0.069)	100.00(97.90)
[2023-09-29 12:12:08 4splitDomains](trainer.py 286): INFO [70/90]	0.0979(0.1050)	0.0002(0.0068)	0.025(0.071)	100.00(97.84)
[2023-09-29 12:12:09 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1041)	0.0002(0.0060)	0.046(0.072)	100.00(97.80)
[2023-09-29 12:12:10 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1034)	0.0001(0.0054)	0.013(0.071)	100.00(97.78)
[2023-09-29 12:12:10 4splitDomains](trainer.py 288): INFO  * Train Acc 97.775
[2023-09-29 12:12:12 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.159, Total time 1.89
[2023-09-29 12:12:12 4splitDomains](my_trainer.py 302): INFO Epoch:3
[2023-09-29 12:12:12 4splitDomains](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 12:12:12 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:12:12 4splitDomains](trainer.py 286): INFO [0/90]	0.5796(0.5796)	0.4708(0.4708)	0.080(0.080)	96.88(96.88)
[2023-09-29 12:12:13 4splitDomains](trainer.py 286): INFO [10/90]	0.1006(0.1427)	0.0004(0.0431)	0.012(0.037)	100.00(99.15)
[2023-09-29 12:12:14 4splitDomains](trainer.py 286): INFO [20/90]	0.0974(0.1214)	0.0001(0.0227)	0.143(0.061)	93.75(97.92)
[2023-09-29 12:12:15 4splitDomains](trainer.py 286): INFO [30/90]	0.0976(0.1137)	0.0003(0.0154)	0.124(0.058)	93.75(98.19)
[2023-09-29 12:12:16 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1098)	0.0002(0.0117)	0.027(0.055)	100.00(98.25)
[2023-09-29 12:12:17 4splitDomains](trainer.py 286): INFO [50/90]	0.0996(0.1077)	0.0002(0.0095)	0.162(0.061)	96.88(97.98)
[2023-09-29 12:12:18 4splitDomains](trainer.py 286): INFO [60/90]	0.1004(0.1062)	0.0002(0.0080)	0.096(0.060)	96.88(98.05)
[2023-09-29 12:12:19 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1053)	0.0001(0.0069)	0.121(0.062)	93.75(97.98)
[2023-09-29 12:12:20 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1044)	0.0002(0.0061)	0.119(0.066)	96.88(97.84)
[2023-09-29 12:12:21 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1037)	0.0001(0.0055)	0.020(0.066)	100.00(97.88)
[2023-09-29 12:12:21 4splitDomains](trainer.py 288): INFO  * Train Acc 97.880
[2023-09-29 12:12:23 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.307, Total time 1.90
[2023-09-29 12:12:23 4splitDomains](my_trainer.py 302): INFO Epoch:4
[2023-09-29 12:12:23 4splitDomains](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 12:12:23 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:12:24 4splitDomains](trainer.py 286): INFO [0/90]	0.5731(0.5731)	0.4646(0.4646)	0.035(0.035)	100.00(100.00)
[2023-09-29 12:12:25 4splitDomains](trainer.py 286): INFO [10/90]	0.0980(0.1435)	0.0002(0.0426)	0.016(0.043)	100.00(98.58)
[2023-09-29 12:12:26 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1228)	0.0002(0.0225)	0.072(0.051)	93.75(98.07)
[2023-09-29 12:12:27 4splitDomains](trainer.py 286): INFO [30/90]	0.0974(0.1148)	0.0002(0.0153)	0.027(0.043)	100.00(98.49)
[2023-09-29 12:12:28 4splitDomains](trainer.py 286): INFO [40/90]	0.0977(0.1112)	0.0002(0.0117)	0.205(0.048)	96.88(98.55)
[2023-09-29 12:12:29 4splitDomains](trainer.py 286): INFO [50/90]	0.0975(0.1086)	0.0001(0.0094)	0.056(0.053)	100.00(98.47)
[2023-09-29 12:12:30 4splitDomains](trainer.py 286): INFO [60/90]	0.0976(0.1068)	0.0002(0.0079)	0.039(0.056)	100.00(98.36)
[2023-09-29 12:12:31 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1055)	0.0001(0.0069)	0.029(0.057)	100.00(98.28)
[2023-09-29 12:12:32 4splitDomains](trainer.py 286): INFO [80/90]	0.0977(0.1046)	0.0003(0.0060)	0.026(0.057)	100.00(98.26)
[2023-09-29 12:12:32 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1039)	0.0001(0.0055)	0.029(0.056)	100.00(98.30)
[2023-09-29 12:12:32 4splitDomains](trainer.py 288): INFO  * Train Acc 98.297
[2023-09-29 12:12:35 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.365, Total time 2.00
[2023-09-29 12:12:35 4splitDomains](my_trainer.py 302): INFO Epoch:5
[2023-09-29 12:12:35 4splitDomains](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 12:12:35 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:12:35 4splitDomains](trainer.py 286): INFO [0/90]	0.5891(0.5891)	0.4658(0.4658)	0.019(0.019)	100.00(100.00)
[2023-09-29 12:12:36 4splitDomains](trainer.py 286): INFO [10/90]	0.0974(0.1463)	0.0001(0.0426)	0.032(0.027)	100.00(99.72)
[2023-09-29 12:12:37 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1235)	0.0002(0.0225)	0.173(0.035)	96.88(99.26)
[2023-09-29 12:12:38 4splitDomains](trainer.py 286): INFO [30/90]	0.0998(0.1153)	0.0002(0.0153)	0.029(0.040)	100.00(99.09)
[2023-09-29 12:12:39 4splitDomains](trainer.py 286): INFO [40/90]	0.1014(0.1111)	0.0006(0.0116)	0.020(0.036)	100.00(99.16)
[2023-09-29 12:12:40 4splitDomains](trainer.py 286): INFO [50/90]	0.0974(0.1090)	0.0001(0.0094)	0.075(0.040)	96.88(98.90)
[2023-09-29 12:12:41 4splitDomains](trainer.py 286): INFO [60/90]	0.0977(0.1075)	0.0003(0.0079)	0.006(0.039)	100.00(98.92)
[2023-09-29 12:12:42 4splitDomains](trainer.py 286): INFO [70/90]	0.0987(0.1062)	0.0002(0.0068)	0.014(0.044)	100.00(98.81)
[2023-09-29 12:12:43 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1052)	0.0002(0.0060)	0.090(0.045)	96.88(98.77)
[2023-09-29 12:12:44 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1044)	0.0001(0.0055)	0.054(0.048)	96.55(98.61)
[2023-09-29 12:12:44 4splitDomains](trainer.py 288): INFO  * Train Acc 98.610
[2023-09-29 12:12:46 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.571, Total time 1.91
[2023-09-29 12:12:46 4splitDomains](my_trainer.py 302): INFO Epoch:6
[2023-09-29 12:12:46 4splitDomains](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 12:12:46 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:12:47 4splitDomains](trainer.py 286): INFO [0/90]	0.5792(0.5792)	0.4765(0.4765)	0.149(0.149)	96.88(96.88)
[2023-09-29 12:12:48 4splitDomains](trainer.py 286): INFO [10/90]	0.0976(0.1430)	0.0002(0.0435)	0.075(0.050)	96.88(98.58)
[2023-09-29 12:12:48 4splitDomains](trainer.py 286): INFO [20/90]	0.0987(0.1221)	0.0001(0.0229)	0.063(0.049)	96.88(98.51)
[2023-09-29 12:12:49 4splitDomains](trainer.py 286): INFO [30/90]	0.0977(0.1145)	0.0002(0.0156)	0.025(0.044)	100.00(98.59)
[2023-09-29 12:12:50 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1104)	0.0002(0.0119)	0.041(0.041)	100.00(98.78)
[2023-09-29 12:12:51 4splitDomains](trainer.py 286): INFO [50/90]	0.0994(0.1080)	0.0005(0.0096)	0.038(0.042)	100.00(98.71)
[2023-09-29 12:12:52 4splitDomains](trainer.py 286): INFO [60/90]	0.0974(0.1064)	0.0002(0.0081)	0.065(0.041)	100.00(98.82)
[2023-09-29 12:12:53 4splitDomains](trainer.py 286): INFO [70/90]	0.0975(0.1052)	0.0001(0.0070)	0.027(0.040)	100.00(98.81)
[2023-09-29 12:12:54 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1044)	0.0002(0.0061)	0.079(0.042)	96.88(98.65)
[2023-09-29 12:12:55 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1036)	0.0001(0.0056)	0.151(0.047)	93.10(98.44)
[2023-09-29 12:12:55 4splitDomains](trainer.py 288): INFO  * Train Acc 98.436
[2023-09-29 12:12:57 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.101, Total time 1.89
[2023-09-29 12:12:57 4splitDomains](my_trainer.py 302): INFO Epoch:7
[2023-09-29 12:12:57 4splitDomains](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 12:12:57 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:12:58 4splitDomains](trainer.py 286): INFO [0/90]	0.5387(0.5387)	0.4330(0.4330)	0.013(0.013)	100.00(100.00)
[2023-09-29 12:12:59 4splitDomains](trainer.py 286): INFO [10/90]	0.1043(0.1405)	0.0005(0.0397)	0.029(0.049)	100.00(98.58)
[2023-09-29 12:13:00 4splitDomains](trainer.py 286): INFO [20/90]	0.0987(0.1201)	0.0003(0.0209)	0.009(0.052)	100.00(98.21)
[2023-09-29 12:13:01 4splitDomains](trainer.py 286): INFO [30/90]	0.0976(0.1129)	0.0002(0.0142)	0.078(0.049)	93.75(98.39)
[2023-09-29 12:13:02 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1093)	0.0002(0.0108)	0.065(0.045)	100.00(98.55)
[2023-09-29 12:13:03 4splitDomains](trainer.py 286): INFO [50/90]	0.0974(0.1070)	0.0002(0.0088)	0.010(0.046)	100.00(98.41)
[2023-09-29 12:13:04 4splitDomains](trainer.py 286): INFO [60/90]	0.0976(0.1055)	0.0002(0.0074)	0.019(0.047)	100.00(98.36)
[2023-09-29 12:13:05 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1052)	0.0002(0.0064)	0.053(0.046)	96.88(98.33)
[2023-09-29 12:13:06 4splitDomains](trainer.py 286): INFO [80/90]	0.0980(0.1043)	0.0002(0.0056)	0.009(0.043)	100.00(98.46)
[2023-09-29 12:13:07 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1036)	0.0001(0.0051)	0.017(0.045)	100.00(98.40)
[2023-09-29 12:13:07 4splitDomains](trainer.py 288): INFO  * Train Acc 98.401
[2023-09-29 12:13:09 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.101, Total time 1.87
[2023-09-29 12:13:09 4splitDomains](my_trainer.py 302): INFO Epoch:8
[2023-09-29 12:13:09 4splitDomains](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 12:13:09 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:13:09 4splitDomains](trainer.py 286): INFO [0/90]	0.5779(0.5779)	0.4781(0.4781)	0.077(0.077)	96.88(96.88)
[2023-09-29 12:13:10 4splitDomains](trainer.py 286): INFO [10/90]	0.0976(0.1439)	0.0003(0.0438)	0.010(0.026)	100.00(99.43)
[2023-09-29 12:13:11 4splitDomains](trainer.py 286): INFO [20/90]	0.0976(0.1219)	0.0002(0.0231)	0.010(0.027)	100.00(99.40)
[2023-09-29 12:13:12 4splitDomains](trainer.py 286): INFO [30/90]	0.0997(0.1144)	0.0003(0.0157)	0.017(0.029)	100.00(99.09)
[2023-09-29 12:13:13 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1103)	0.0002(0.0119)	0.007(0.032)	100.00(98.86)
[2023-09-29 12:13:14 4splitDomains](trainer.py 286): INFO [50/90]	0.0975(0.1079)	0.0002(0.0097)	0.057(0.033)	100.00(98.77)
[2023-09-29 12:13:15 4splitDomains](trainer.py 286): INFO [60/90]	0.0976(0.1068)	0.0002(0.0081)	0.021(0.035)	100.00(98.72)
[2023-09-29 12:13:16 4splitDomains](trainer.py 286): INFO [70/90]	0.0986(0.1059)	0.0002(0.0070)	0.114(0.036)	93.75(98.72)
[2023-09-29 12:13:17 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1049)	0.0002(0.0062)	0.005(0.037)	100.00(98.61)
[2023-09-29 12:13:18 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1041)	0.0001(0.0056)	0.005(0.039)	100.00(98.57)
[2023-09-29 12:13:18 4splitDomains](trainer.py 288): INFO  * Train Acc 98.575
[2023-09-29 12:13:20 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.894, Total time 1.87
[2023-09-29 12:13:20 4splitDomains](my_trainer.py 302): INFO Epoch:9
[2023-09-29 12:13:20 4splitDomains](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 12:13:20 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:13:20 4splitDomains](trainer.py 286): INFO [0/90]	0.5449(0.5449)	0.4410(0.4410)	0.028(0.028)	100.00(100.00)
[2023-09-29 12:13:21 4splitDomains](trainer.py 286): INFO [10/90]	0.0978(0.1403)	0.0002(0.0404)	0.059(0.042)	96.88(98.58)
[2023-09-29 12:13:22 4splitDomains](trainer.py 286): INFO [20/90]	0.0974(0.1200)	0.0001(0.0213)	0.100(0.046)	93.75(98.51)
[2023-09-29 12:13:23 4splitDomains](trainer.py 286): INFO [30/90]	0.0977(0.1132)	0.0003(0.0145)	0.079(0.045)	96.88(98.59)
[2023-09-29 12:13:24 4splitDomains](trainer.py 286): INFO [40/90]	0.0999(0.1094)	0.0001(0.0110)	0.054(0.043)	96.88(98.63)
[2023-09-29 12:13:25 4splitDomains](trainer.py 286): INFO [50/90]	0.0977(0.1073)	0.0003(0.0089)	0.004(0.038)	100.00(98.84)
[2023-09-29 12:13:26 4splitDomains](trainer.py 286): INFO [60/90]	0.0975(0.1057)	0.0002(0.0075)	0.014(0.038)	100.00(98.72)
[2023-09-29 12:13:27 4splitDomains](trainer.py 286): INFO [70/90]	0.0985(0.1048)	0.0003(0.0065)	0.028(0.039)	96.88(98.50)
[2023-09-29 12:13:28 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1040)	0.0002(0.0057)	0.052(0.038)	96.88(98.57)
[2023-09-29 12:13:29 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1033)	0.0002(0.0052)	0.010(0.040)	100.00(98.54)
[2023-09-29 12:13:29 4splitDomains](trainer.py 288): INFO  * Train Acc 98.540
[2023-09-29 12:13:31 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.307, Total time 1.86
[2023-09-29 12:13:31 4splitDomains](my_trainer.py 302): INFO Epoch:10
[2023-09-29 12:13:31 4splitDomains](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 12:13:31 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:13:32 4splitDomains](trainer.py 286): INFO [0/90]	0.5574(0.5574)	0.4457(0.4457)	0.106(0.106)	93.75(93.75)
[2023-09-29 12:13:33 4splitDomains](trainer.py 286): INFO [10/90]	0.1081(0.1417)	0.0002(0.0408)	0.093(0.050)	96.88(97.44)
[2023-09-29 12:13:34 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1214)	0.0001(0.0215)	0.044(0.037)	100.00(98.51)
[2023-09-29 12:13:35 4splitDomains](trainer.py 286): INFO [30/90]	0.0977(0.1137)	0.0003(0.0146)	0.011(0.034)	100.00(98.79)
[2023-09-29 12:13:36 4splitDomains](trainer.py 286): INFO [40/90]	0.0974(0.1104)	0.0002(0.0111)	0.030(0.031)	100.00(98.93)
[2023-09-29 12:13:37 4splitDomains](trainer.py 286): INFO [50/90]	0.0977(0.1079)	0.0002(0.0090)	0.008(0.029)	100.00(99.08)
[2023-09-29 12:13:38 4splitDomains](trainer.py 286): INFO [60/90]	0.0974(0.1062)	0.0001(0.0076)	0.068(0.032)	93.75(98.87)
[2023-09-29 12:13:39 4splitDomains](trainer.py 286): INFO [70/90]	0.1000(0.1051)	0.0003(0.0066)	0.058(0.035)	96.88(98.68)
[2023-09-29 12:13:40 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1046)	0.0002(0.0058)	0.012(0.034)	100.00(98.65)
[2023-09-29 12:13:41 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1038)	0.0001(0.0052)	0.005(0.034)	100.00(98.71)
[2023-09-29 12:13:41 4splitDomains](trainer.py 288): INFO  * Train Acc 98.714
[2023-09-29 12:13:43 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.159, Total time 1.89
[2023-09-29 12:13:43 4splitDomains](my_trainer.py 302): INFO Epoch:11
[2023-09-29 12:13:43 4splitDomains](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 12:13:43 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:13:43 4splitDomains](trainer.py 286): INFO [0/90]	0.5764(0.5764)	0.4613(0.4613)	0.007(0.007)	100.00(100.00)
[2023-09-29 12:13:44 4splitDomains](trainer.py 286): INFO [10/90]	0.1016(0.1430)	0.0002(0.0422)	0.011(0.024)	100.00(99.43)
[2023-09-29 12:13:45 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1213)	0.0002(0.0222)	0.007(0.024)	100.00(99.40)
[2023-09-29 12:13:46 4splitDomains](trainer.py 286): INFO [30/90]	0.0976(0.1139)	0.0002(0.0151)	0.005(0.026)	100.00(99.19)
[2023-09-29 12:13:47 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1100)	0.0001(0.0115)	0.026(0.030)	100.00(99.09)
[2023-09-29 12:13:48 4splitDomains](trainer.py 286): INFO [50/90]	0.0974(0.1077)	0.0001(0.0093)	0.003(0.030)	100.00(99.08)
[2023-09-29 12:13:49 4splitDomains](trainer.py 286): INFO [60/90]	0.1041(0.1065)	0.0003(0.0078)	0.065(0.032)	96.88(98.98)
[2023-09-29 12:13:50 4splitDomains](trainer.py 286): INFO [70/90]	0.0976(0.1052)	0.0002(0.0068)	0.011(0.034)	100.00(98.81)
[2023-09-29 12:13:51 4splitDomains](trainer.py 286): INFO [80/90]	0.0974(0.1043)	0.0001(0.0060)	0.037(0.035)	96.88(98.69)
[2023-09-29 12:13:52 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1036)	0.0001(0.0054)	0.031(0.035)	96.55(98.57)
[2023-09-29 12:13:52 4splitDomains](trainer.py 288): INFO  * Train Acc 98.575
[2023-09-29 12:13:54 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.630, Total time 1.90
[2023-09-29 12:13:54 4splitDomains](my_trainer.py 302): INFO Epoch:12
[2023-09-29 12:13:54 4splitDomains](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 12:13:54 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:13:54 4splitDomains](trainer.py 286): INFO [0/90]	0.5885(0.5885)	0.4790(0.4790)	0.078(0.078)	96.88(96.88)
[2023-09-29 12:13:55 4splitDomains](trainer.py 286): INFO [10/90]	0.0977(0.1434)	0.0003(0.0438)	0.006(0.028)	100.00(98.86)
[2023-09-29 12:13:56 4splitDomains](trainer.py 286): INFO [20/90]	0.0990(0.1224)	0.0002(0.0231)	0.072(0.032)	96.88(98.81)
[2023-09-29 12:13:57 4splitDomains](trainer.py 286): INFO [30/90]	0.0982(0.1145)	0.0003(0.0157)	0.004(0.030)	100.00(98.79)
[2023-09-29 12:13:58 4splitDomains](trainer.py 286): INFO [40/90]	0.0978(0.1104)	0.0003(0.0120)	0.040(0.028)	96.88(98.86)
[2023-09-29 12:13:59 4splitDomains](trainer.py 286): INFO [50/90]	0.0976(0.1081)	0.0002(0.0097)	0.012(0.027)	100.00(98.90)
[2023-09-29 12:14:00 4splitDomains](trainer.py 286): INFO [60/90]	0.0999(0.1066)	0.0003(0.0081)	0.088(0.029)	93.75(98.77)
[2023-09-29 12:14:01 4splitDomains](trainer.py 286): INFO [70/90]	0.0977(0.1054)	0.0003(0.0070)	0.199(0.030)	90.62(98.81)
[2023-09-29 12:14:02 4splitDomains](trainer.py 286): INFO [80/90]	0.0974(0.1045)	0.0001(0.0062)	0.030(0.030)	96.88(98.73)
[2023-09-29 12:14:03 4splitDomains](trainer.py 286): INFO [89/90]	0.0923(0.1037)	0.0002(0.0056)	0.028(0.031)	100.00(98.68)
[2023-09-29 12:14:03 4splitDomains](trainer.py 288): INFO  * Train Acc 98.679
[2023-09-29 12:14:05 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.688, Total time 1.88
[2023-09-29 12:14:05 4splitDomains](my_trainer.py 302): INFO Epoch:13
[2023-09-29 12:14:05 4splitDomains](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 12:14:05 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:14:06 4splitDomains](trainer.py 286): INFO [0/90]	0.5689(0.5689)	0.4598(0.4598)	0.020(0.020)	100.00(100.00)
[2023-09-29 12:14:07 4splitDomains](trainer.py 286): INFO [10/90]	0.0974(0.1441)	0.0001(0.0422)	0.010(0.031)	100.00(98.01)
[2023-09-29 12:14:08 4splitDomains](trainer.py 286): INFO [20/90]	0.1110(0.1230)	0.0006(0.0222)	0.014(0.031)	100.00(98.36)
[2023-09-29 12:14:09 4splitDomains](trainer.py 286): INFO [30/90]	0.0974(0.1149)	0.0001(0.0152)	0.008(0.031)	100.00(98.49)
[2023-09-29 12:14:10 4splitDomains](trainer.py 286): INFO [40/90]	0.0977(0.1106)	0.0003(0.0115)	0.006(0.030)	100.00(98.70)
[2023-09-29 12:14:11 4splitDomains](trainer.py 286): INFO [50/90]	0.1066(0.1085)	0.0005(0.0093)	0.040(0.031)	100.00(98.65)
[2023-09-29 12:14:12 4splitDomains](trainer.py 286): INFO [60/90]	0.0985(0.1067)	0.0003(0.0078)	0.006(0.029)	100.00(98.77)
[2023-09-29 12:14:13 4splitDomains](trainer.py 286): INFO [70/90]	0.0977(0.1054)	0.0003(0.0068)	0.016(0.029)	100.00(98.72)
[2023-09-29 12:14:14 4splitDomains](trainer.py 286): INFO [80/90]	0.0977(0.1048)	0.0003(0.0060)	0.031(0.029)	96.88(98.80)
[2023-09-29 12:14:15 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1040)	0.0001(0.0054)	0.008(0.030)	100.00(98.82)
[2023-09-29 12:14:15 4splitDomains](trainer.py 288): INFO  * Train Acc 98.818
[2023-09-29 12:14:17 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.894, Total time 1.87
[2023-09-29 12:14:17 4splitDomains](my_trainer.py 302): INFO Epoch:14
[2023-09-29 12:14:17 4splitDomains](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 12:14:17 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:14:17 4splitDomains](trainer.py 286): INFO [0/90]	0.6026(0.6026)	0.4903(0.4903)	0.006(0.006)	100.00(100.00)
[2023-09-29 12:14:18 4splitDomains](trainer.py 286): INFO [10/90]	0.0983(0.1440)	0.0002(0.0448)	0.011(0.017)	100.00(99.72)
[2023-09-29 12:14:19 4splitDomains](trainer.py 286): INFO [20/90]	0.0978(0.1222)	0.0003(0.0236)	0.044(0.020)	96.88(99.40)
[2023-09-29 12:14:20 4splitDomains](trainer.py 286): INFO [30/90]	0.0977(0.1148)	0.0002(0.0162)	0.021(0.026)	100.00(98.89)
[2023-09-29 12:14:21 4splitDomains](trainer.py 286): INFO [40/90]	0.0978(0.1108)	0.0003(0.0123)	0.011(0.026)	100.00(98.78)
[2023-09-29 12:14:22 4splitDomains](trainer.py 286): INFO [50/90]	0.0978(0.1082)	0.0003(0.0099)	0.053(0.030)	100.00(98.84)
[2023-09-29 12:14:23 4splitDomains](trainer.py 286): INFO [60/90]	0.0976(0.1065)	0.0001(0.0084)	0.008(0.030)	100.00(98.77)
[2023-09-29 12:14:24 4splitDomains](trainer.py 286): INFO [70/90]	0.0978(0.1053)	0.0003(0.0072)	0.012(0.030)	100.00(98.77)
[2023-09-29 12:14:25 4splitDomains](trainer.py 286): INFO [80/90]	0.0977(0.1045)	0.0002(0.0064)	0.071(0.029)	96.88(98.84)
[2023-09-29 12:14:26 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1037)	0.0001(0.0057)	0.020(0.028)	100.00(98.92)
[2023-09-29 12:14:26 4splitDomains](trainer.py 288): INFO  * Train Acc 98.922
[2023-09-29 12:14:28 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.630, Total time 1.93
[2023-09-29 12:14:28 4splitDomains](my_trainer.py 302): INFO Epoch:15
[2023-09-29 12:14:28 4splitDomains](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 12:14:28 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:14:28 4splitDomains](trainer.py 286): INFO [0/90]	0.5935(0.5935)	0.4929(0.4929)	0.027(0.027)	96.88(96.88)
[2023-09-29 12:14:30 4splitDomains](trainer.py 286): INFO [10/90]	0.0980(0.1481)	0.0005(0.0451)	0.005(0.023)	100.00(98.86)
[2023-09-29 12:14:31 4splitDomains](trainer.py 286): INFO [20/90]	0.0978(0.1245)	0.0002(0.0238)	0.006(0.022)	100.00(99.26)
[2023-09-29 12:14:32 4splitDomains](trainer.py 286): INFO [30/90]	0.1095(0.1164)	0.0006(0.0162)	0.061(0.026)	96.88(98.99)
[2023-09-29 12:14:32 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1118)	0.0001(0.0123)	0.034(0.025)	96.88(99.01)
[2023-09-29 12:14:33 4splitDomains](trainer.py 286): INFO [50/90]	0.0977(0.1093)	0.0003(0.0100)	0.004(0.024)	100.00(99.02)
[2023-09-29 12:14:34 4splitDomains](trainer.py 286): INFO [60/90]	0.0992(0.1075)	0.0004(0.0084)	0.007(0.026)	100.00(98.98)
[2023-09-29 12:14:35 4splitDomains](trainer.py 286): INFO [70/90]	0.0977(0.1063)	0.0003(0.0072)	0.013(0.027)	100.00(98.90)
[2023-09-29 12:14:36 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1054)	0.0001(0.0064)	0.069(0.028)	96.88(98.92)
[2023-09-29 12:14:37 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1045)	0.0001(0.0058)	0.011(0.027)	100.00(99.03)
[2023-09-29 12:14:37 4splitDomains](trainer.py 288): INFO  * Train Acc 99.027
[2023-09-29 12:14:39 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.307, Total time 1.89
[2023-09-29 12:14:39 4splitDomains](my_trainer.py 302): INFO Epoch:16
[2023-09-29 12:14:39 4splitDomains](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 12:14:39 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:14:40 4splitDomains](trainer.py 286): INFO [0/90]	0.5685(0.5685)	0.4593(0.4593)	0.014(0.014)	100.00(100.00)
[2023-09-29 12:14:41 4splitDomains](trainer.py 286): INFO [10/90]	0.0978(0.1425)	0.0003(0.0420)	0.062(0.029)	93.75(98.30)
[2023-09-29 12:14:42 4splitDomains](trainer.py 286): INFO [20/90]	0.0977(0.1212)	0.0003(0.0222)	0.025(0.029)	100.00(98.51)
[2023-09-29 12:14:43 4splitDomains](trainer.py 286): INFO [30/90]	0.0976(0.1136)	0.0002(0.0151)	0.024(0.031)	100.00(98.49)
[2023-09-29 12:14:44 4splitDomains](trainer.py 286): INFO [40/90]	0.1013(0.1098)	0.0002(0.0115)	0.013(0.029)	100.00(98.63)
[2023-09-29 12:14:45 4splitDomains](trainer.py 286): INFO [50/90]	0.0977(0.1075)	0.0002(0.0093)	0.021(0.028)	100.00(98.77)
[2023-09-29 12:14:46 4splitDomains](trainer.py 286): INFO [60/90]	0.1022(0.1060)	0.0003(0.0078)	0.064(0.030)	96.88(98.67)
[2023-09-29 12:14:47 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1049)	0.0002(0.0067)	0.003(0.029)	100.00(98.68)
[2023-09-29 12:14:48 4splitDomains](trainer.py 286): INFO [80/90]	0.0977(0.1042)	0.0003(0.0060)	0.004(0.029)	100.00(98.77)
[2023-09-29 12:14:49 4splitDomains](trainer.py 286): INFO [89/90]	0.0922(0.1035)	0.0001(0.0054)	0.026(0.029)	100.00(98.82)
[2023-09-29 12:14:49 4splitDomains](trainer.py 288): INFO  * Train Acc 98.818
[2023-09-29 12:14:51 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.630, Total time 1.89
[2023-09-29 12:14:51 4splitDomains](my_trainer.py 302): INFO Epoch:17
[2023-09-29 12:14:51 4splitDomains](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 12:14:51 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:14:51 4splitDomains](trainer.py 286): INFO [0/90]	0.5784(0.5784)	0.4771(0.4771)	0.017(0.017)	100.00(100.00)
[2023-09-29 12:14:52 4splitDomains](trainer.py 286): INFO [10/90]	0.0974(0.1418)	0.0001(0.0436)	0.040(0.023)	100.00(99.43)
[2023-09-29 12:14:53 4splitDomains](trainer.py 286): INFO [20/90]	0.0978(0.1208)	0.0003(0.0230)	0.027(0.021)	100.00(99.55)
[2023-09-29 12:14:54 4splitDomains](trainer.py 286): INFO [30/90]	0.0976(0.1138)	0.0002(0.0157)	0.031(0.022)	96.88(99.50)
[2023-09-29 12:14:55 4splitDomains](trainer.py 286): INFO [40/90]	0.0975(0.1103)	0.0002(0.0119)	0.011(0.021)	100.00(99.54)
[2023-09-29 12:14:56 4splitDomains](trainer.py 286): INFO [50/90]	0.0975(0.1080)	0.0001(0.0096)	0.018(0.021)	100.00(99.39)
[2023-09-29 12:14:57 4splitDomains](trainer.py 286): INFO [60/90]	0.1016(0.1067)	0.0003(0.0081)	0.005(0.020)	100.00(99.39)
[2023-09-29 12:14:58 4splitDomains](trainer.py 286): INFO [70/90]	0.0977(0.1056)	0.0002(0.0070)	0.083(0.023)	96.88(99.30)
[2023-09-29 12:14:59 4splitDomains](trainer.py 286): INFO [80/90]	0.0975(0.1046)	0.0002(0.0062)	0.028(0.023)	100.00(99.27)
[2023-09-29 12:15:00 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1039)	0.0001(0.0056)	0.014(0.025)	100.00(99.10)
[2023-09-29 12:15:00 4splitDomains](trainer.py 288): INFO  * Train Acc 99.096
[2023-09-29 12:15:02 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.571, Total time 1.91
[2023-09-29 12:15:02 4splitDomains](my_trainer.py 302): INFO Epoch:18
[2023-09-29 12:15:02 4splitDomains](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 12:15:02 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:15:03 4splitDomains](trainer.py 286): INFO [0/90]	0.5766(0.5766)	0.4724(0.4724)	0.004(0.004)	100.00(100.00)
[2023-09-29 12:15:04 4splitDomains](trainer.py 286): INFO [10/90]	0.0974(0.1412)	0.0001(0.0432)	0.031(0.018)	100.00(99.72)
[2023-09-29 12:15:05 4splitDomains](trainer.py 286): INFO [20/90]	0.0973(0.1204)	0.0001(0.0227)	0.006(0.029)	100.00(98.81)
[2023-09-29 12:15:06 4splitDomains](trainer.py 286): INFO [30/90]	0.0977(0.1131)	0.0002(0.0155)	0.004(0.028)	100.00(98.79)
[2023-09-29 12:15:06 4splitDomains](trainer.py 286): INFO [40/90]	0.0986(0.1095)	0.0001(0.0118)	0.005(0.027)	100.00(98.86)
[2023-09-29 12:15:07 4splitDomains](trainer.py 286): INFO [50/90]	0.0975(0.1072)	0.0002(0.0095)	0.008(0.027)	100.00(98.84)
[2023-09-29 12:15:08 4splitDomains](trainer.py 286): INFO [60/90]	0.0983(0.1059)	0.0003(0.0080)	0.043(0.026)	96.88(98.92)
[2023-09-29 12:15:09 4splitDomains](trainer.py 286): INFO [70/90]	0.0976(0.1048)	0.0001(0.0069)	0.003(0.027)	100.00(98.81)
[2023-09-29 12:15:10 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1040)	0.0003(0.0061)	0.065(0.028)	93.75(98.73)
[2023-09-29 12:15:11 4splitDomains](trainer.py 286): INFO [89/90]	0.0921(0.1033)	0.0001(0.0055)	0.056(0.027)	96.55(98.75)
[2023-09-29 12:15:11 4splitDomains](trainer.py 288): INFO  * Train Acc 98.749
[2023-09-29 12:15:13 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.423, Total time 1.91
[2023-09-29 12:15:13 4splitDomains](my_trainer.py 302): INFO Epoch:19
[2023-09-29 12:15:13 4splitDomains](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 12:15:13 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:15:14 4splitDomains](trainer.py 286): INFO [0/90]	0.5389(0.5389)	0.4313(0.4313)	0.018(0.018)	100.00(100.00)
[2023-09-29 12:15:15 4splitDomains](trainer.py 286): INFO [10/90]	0.0974(0.1408)	0.0001(0.0395)	0.006(0.024)	100.00(99.15)
[2023-09-29 12:15:16 4splitDomains](trainer.py 286): INFO [20/90]	0.0975(0.1206)	0.0002(0.0208)	0.017(0.033)	100.00(98.21)
[2023-09-29 12:15:17 4splitDomains](trainer.py 286): INFO [30/90]	0.0975(0.1132)	0.0002(0.0142)	0.044(0.039)	96.88(97.98)
[2023-09-29 12:15:18 4splitDomains](trainer.py 286): INFO [40/90]	0.0976(0.1097)	0.0002(0.0108)	0.003(0.033)	100.00(98.40)
[2023-09-29 12:15:19 4splitDomains](trainer.py 286): INFO [50/90]	0.0977(0.1077)	0.0003(0.0087)	0.035(0.031)	96.88(98.47)
[2023-09-29 12:15:20 4splitDomains](trainer.py 286): INFO [60/90]	0.0976(0.1071)	0.0002(0.0074)	0.019(0.031)	100.00(98.46)
[2023-09-29 12:15:21 4splitDomains](trainer.py 286): INFO [70/90]	0.0974(0.1058)	0.0001(0.0064)	0.006(0.030)	100.00(98.46)
[2023-09-29 12:15:22 4splitDomains](trainer.py 286): INFO [80/90]	0.0976(0.1049)	0.0002(0.0056)	0.008(0.030)	100.00(98.53)
[2023-09-29 12:15:23 4splitDomains](trainer.py 286): INFO [89/90]	0.0923(0.1041)	0.0001(0.0051)	0.073(0.029)	96.55(98.64)
[2023-09-29 12:15:23 4splitDomains](trainer.py 288): INFO  * Train Acc 98.644
[2023-09-29 12:15:25 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.894, Total time 1.86
=> Saving model to: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-1.pth
=> Save Done
[2023-09-29 12:15:25 4splitDomains](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 12:15:27 4splitDomains](my_trainer.py 503): INFO  * Val Acc 87.003, Total time 2.34
[2023-09-29 12:15:27 4splitDomains](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 12:15:30 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.571, Total time 2.31
[2023-09-29 12:15:30 4splitDomains](trainer.py 335): INFO saving storage...
[2023-09-29 12:15:30 4splitDomains](trainer.py 341): INFO done
[2023-09-29 12:15:30 4splitDomains](iBatchLearn.py 155): INFO Acc:82.78704064657916; BWT:-2.1220159151193627;
=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:15:33 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:15:33 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:15:33 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 1, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-1.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-1.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_1.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:15:34 4splitDomains](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-1.pth
[2023-09-29 12:15:34 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:15:36 4splitDomains](my_trainer.py 67): INFO load storage...
[2023-09-29 12:15:36 4splitDomains](my_trainer.py 71): INFO done
[2023-09-29 12:15:36 4splitDomains](my_trainer.py 64): INFO tensor([[2, 2, 2, 2, 2, 2, 2],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 2, 2, 0, 1, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 1, 0, 0],
        [2, 0, 0, 0, 0, 0, 0],
        [2, 0, 0, 0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 12:15:36 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:15:36 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:15:36 4splitDomains](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 12:15:43 4splitDomains](iBatchLearn.py 167): INFO test split name:1
--------------------------------Official Evaluation--------------------------------
1 83.9926198902245
=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:15:52 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:15:52 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:15:52 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 2, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-1.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-2.pth", "storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-1.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-2.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:15:52 4splitDomains](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-1.pth
[2023-09-29 12:15:52 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:15:55 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:15:55 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:15:55 4splitDomains](my_trainer.py 67): INFO load storage...
[2023-09-29 12:15:55 4splitDomains](my_trainer.py 71): INFO done
[2023-09-29 12:15:55 4splitDomains](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 12:15:55 4splitDomains](iBatchLearn.py 92): INFO ====================== 2 =======================
[2023-09-29 12:15:55 4splitDomains](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 12:15:55 4splitDomains](my_trainer.py 328): INFO Epoch:0
[2023-09-29 12:15:55 4splitDomains](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 12:15:55 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:15:59 4splitDomains](trainer.py 286): INFO [0/91]	4.4079(4.4079)	1.1200(1.1200)	1.591(1.591)	59.38(59.38)
[2023-09-29 12:16:01 4splitDomains](trainer.py 286): INFO [10/91]	0.2622(0.5140)	0.1637(0.1254)	1.387(1.605)	62.50(60.23)
[2023-09-29 12:16:03 4splitDomains](trainer.py 286): INFO [20/91]	0.0972(0.3822)	0.0003(0.1323)	1.228(1.632)	59.38(59.97)
[2023-09-29 12:16:06 4splitDomains](trainer.py 286): INFO [30/91]	0.7050(0.3560)	0.5997(0.1551)	1.335(1.529)	65.62(61.90)
[2023-09-29 12:16:09 4splitDomains](trainer.py 286): INFO [40/91]	0.0978(0.3342)	0.0003(0.1583)	1.133(1.449)	68.75(63.26)
[2023-09-29 12:16:11 4splitDomains](trainer.py 286): INFO [50/91]	0.3163(0.3225)	0.2188(0.1611)	0.951(1.404)	65.62(64.40)
[2023-09-29 12:16:14 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.3167)	0.0003(0.1650)	1.098(1.352)	78.12(65.73)
[2023-09-29 12:16:17 4splitDomains](trainer.py 286): INFO [70/91]	0.3174(0.3056)	0.2136(0.1613)	0.785(1.305)	71.88(66.37)
[2023-09-29 12:16:19 4splitDomains](trainer.py 286): INFO [80/91]	0.1162(0.2988)	0.0187(0.1597)	0.762(1.294)	71.88(66.59)
[2023-09-29 12:16:21 4splitDomains](trainer.py 286): INFO [90/91]	0.1155(0.2900)	0.0209(0.1553)	1.369(1.277)	54.84(66.75)
[2023-09-29 12:16:21 4splitDomains](trainer.py 288): INFO  * Train Acc 66.747
[2023-09-29 12:16:26 4splitDomains](my_trainer.py 503): INFO  * Val Acc 70.526, Total time 4.28
[2023-09-29 12:16:26 4splitDomains](my_trainer.py 328): INFO Epoch:1
[2023-09-29 12:16:26 4splitDomains](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 12:16:26 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:16:27 4splitDomains](trainer.py 286): INFO [0/91]	1.2477(1.2477)	1.1344(1.1344)	1.070(1.070)	71.88(71.88)
[2023-09-29 12:16:29 4splitDomains](trainer.py 286): INFO [10/91]	0.2944(0.3373)	0.1862(0.2343)	0.535(0.632)	75.00(81.25)
[2023-09-29 12:16:34 4splitDomains](trainer.py 286): INFO [20/91]	1.3603(0.4037)	1.2629(0.3013)	0.752(0.623)	75.00(81.99)
[2023-09-29 12:16:37 4splitDomains](trainer.py 286): INFO [30/91]	0.1228(0.3500)	0.0006(0.2476)	0.716(0.608)	84.38(82.96)
[2023-09-29 12:16:39 4splitDomains](trainer.py 286): INFO [40/91]	0.3807(0.3280)	0.2756(0.2260)	0.300(0.613)	93.75(82.62)
[2023-09-29 12:16:41 4splitDomains](trainer.py 286): INFO [50/91]	0.1121(0.2985)	0.0004(0.1956)	0.648(0.620)	84.38(82.60)
[2023-09-29 12:16:45 4splitDomains](trainer.py 286): INFO [60/91]	0.8249(0.3093)	0.7264(0.2065)	0.367(0.604)	90.62(83.04)
[2023-09-29 12:16:47 4splitDomains](trainer.py 286): INFO [70/91]	0.1020(0.2944)	0.0003(0.1917)	0.647(0.614)	78.12(82.61)
[2023-09-29 12:16:49 4splitDomains](trainer.py 286): INFO [80/91]	0.6990(0.2922)	0.5973(0.1884)	0.488(0.631)	84.38(82.06)
[2023-09-29 12:16:51 4splitDomains](trainer.py 286): INFO [90/91]	0.1045(0.2810)	0.0094(0.1778)	0.590(0.617)	77.42(82.38)
[2023-09-29 12:16:51 4splitDomains](trainer.py 288): INFO  * Train Acc 82.377
[2023-09-29 12:16:56 4splitDomains](my_trainer.py 503): INFO  * Val Acc 74.737, Total time 4.28
[2023-09-29 12:16:56 4splitDomains](my_trainer.py 328): INFO Epoch:2
[2023-09-29 12:16:56 4splitDomains](my_trainer.py 335): INFO LR:0.01
[2023-09-29 12:16:56 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:16:57 4splitDomains](trainer.py 286): INFO [0/91]	1.7285(1.7285)	1.6238(1.6238)	0.188(0.188)	96.88(96.88)
[2023-09-29 12:16:59 4splitDomains](trainer.py 286): INFO [10/91]	0.1017(0.3235)	0.0003(0.2232)	0.320(0.342)	93.75(90.91)
[2023-09-29 12:17:02 4splitDomains](trainer.py 286): INFO [20/91]	0.4193(0.3044)	0.3028(0.2019)	0.200(0.365)	96.88(90.77)
[2023-09-29 12:17:05 4splitDomains](trainer.py 286): INFO [30/91]	0.1229(0.2889)	0.0005(0.1865)	0.221(0.383)	93.75(89.01)
[2023-09-29 12:17:08 4splitDomains](trainer.py 286): INFO [40/91]	0.6261(0.3063)	0.5247(0.2046)	0.341(0.384)	90.62(89.18)
[2023-09-29 12:17:10 4splitDomains](trainer.py 286): INFO [50/91]	0.2571(0.2865)	0.1478(0.1836)	0.162(0.386)	93.75(89.03)
[2023-09-29 12:17:13 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.2809)	0.0002(0.1782)	0.363(0.411)	90.62(88.37)
[2023-09-29 12:17:16 4splitDomains](trainer.py 286): INFO [70/91]	0.1535(0.2804)	0.0274(0.1773)	0.485(0.410)	78.12(88.16)
[2023-09-29 12:17:18 4splitDomains](trainer.py 286): INFO [80/91]	0.0978(0.2705)	0.0004(0.1673)	0.798(0.429)	75.00(87.46)
[2023-09-29 12:17:21 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2809)	0.0001(0.1782)	0.683(0.438)	90.32(87.32)
[2023-09-29 12:17:21 4splitDomains](trainer.py 288): INFO  * Train Acc 87.324
[2023-09-29 12:17:26 4splitDomains](my_trainer.py 503): INFO  * Val Acc 71.316, Total time 4.33
[2023-09-29 12:17:26 4splitDomains](my_trainer.py 328): INFO Epoch:3
[2023-09-29 12:17:26 4splitDomains](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 12:17:26 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:17:27 4splitDomains](trainer.py 286): INFO [0/91]	1.1863(1.1863)	1.0708(1.0708)	0.219(0.219)	96.88(96.88)
[2023-09-29 12:17:29 4splitDomains](trainer.py 286): INFO [10/91]	0.1258(0.3160)	0.0178(0.2146)	0.229(0.318)	93.75(89.20)
[2023-09-29 12:17:33 4splitDomains](trainer.py 286): INFO [20/91]	1.1694(0.3310)	1.0720(0.2309)	0.391(0.292)	84.38(90.33)
[2023-09-29 12:17:35 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.2934)	0.0001(0.1939)	0.258(0.292)	93.75(90.93)
[2023-09-29 12:17:37 4splitDomains](trainer.py 286): INFO [40/91]	0.5292(0.2774)	0.4317(0.1779)	0.397(0.288)	90.62(91.54)
[2023-09-29 12:17:39 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.2624)	0.0003(0.1634)	0.226(0.290)	90.62(91.48)
[2023-09-29 12:17:42 4splitDomains](trainer.py 286): INFO [60/91]	0.7454(0.2648)	0.5999(0.1652)	0.136(0.292)	96.88(91.29)
[2023-09-29 12:17:45 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.2657)	0.0003(0.1663)	0.298(0.289)	96.88(91.46)
[2023-09-29 12:17:48 4splitDomains](trainer.py 286): INFO [80/91]	0.6700(0.2712)	0.5588(0.1718)	0.399(0.292)	84.38(91.36)
[2023-09-29 12:17:50 4splitDomains](trainer.py 286): INFO [90/91]	0.1841(0.2622)	0.0891(0.1631)	0.160(0.288)	90.32(91.48)
[2023-09-29 12:17:50 4splitDomains](trainer.py 288): INFO  * Train Acc 91.481
[2023-09-29 12:17:54 4splitDomains](my_trainer.py 503): INFO  * Val Acc 70.526, Total time 4.31
[2023-09-29 12:17:54 4splitDomains](my_trainer.py 328): INFO Epoch:4
[2023-09-29 12:17:54 4splitDomains](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 12:17:54 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:17:55 4splitDomains](trainer.py 286): INFO [0/91]	1.2629(1.2629)	1.1561(1.1561)	0.264(0.264)	96.88(96.88)
[2023-09-29 12:17:58 4splitDomains](trainer.py 286): INFO [10/91]	0.1015(0.3676)	0.0003(0.2661)	0.163(0.192)	96.88(94.03)
[2023-09-29 12:18:01 4splitDomains](trainer.py 286): INFO [20/91]	0.9473(0.3183)	0.8499(0.2167)	0.047(0.198)	100.00(94.20)
[2023-09-29 12:18:03 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.2932)	0.0001(0.1926)	0.125(0.188)	93.75(94.05)
[2023-09-29 12:18:05 4splitDomains](trainer.py 286): INFO [40/91]	0.5382(0.2805)	0.4408(0.1806)	0.122(0.175)	96.88(94.82)
[2023-09-29 12:18:08 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.2737)	0.0002(0.1742)	0.157(0.172)	93.75(94.91)
[2023-09-29 12:18:10 4splitDomains](trainer.py 286): INFO [60/91]	0.3330(0.2679)	0.2352(0.1687)	0.179(0.170)	93.75(94.88)
[2023-09-29 12:18:13 4splitDomains](trainer.py 286): INFO [70/91]	0.0973(0.2690)	0.0001(0.1700)	0.162(0.168)	93.75(95.07)
[2023-09-29 12:18:16 4splitDomains](trainer.py 286): INFO [80/91]	0.6480(0.2739)	0.5507(0.1750)	0.149(0.166)	93.75(95.22)
[2023-09-29 12:18:18 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2681)	0.0001(0.1695)	0.222(0.166)	93.55(95.23)
[2023-09-29 12:18:18 4splitDomains](trainer.py 288): INFO  * Train Acc 95.225
[2023-09-29 12:18:23 4splitDomains](my_trainer.py 503): INFO  * Val Acc 73.158, Total time 4.50
[2023-09-29 12:18:23 4splitDomains](my_trainer.py 328): INFO Epoch:5
[2023-09-29 12:18:23 4splitDomains](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 12:18:23 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:18:24 4splitDomains](trainer.py 286): INFO [0/91]	1.4120(1.4120)	1.3075(1.3075)	0.074(0.074)	100.00(100.00)
[2023-09-29 12:18:27 4splitDomains](trainer.py 286): INFO [10/91]	0.5141(0.3477)	0.4167(0.2441)	0.124(0.090)	93.75(98.58)
[2023-09-29 12:18:30 4splitDomains](trainer.py 286): INFO [20/91]	0.6846(0.3326)	0.5534(0.2237)	0.210(0.120)	96.88(97.02)
[2023-09-29 12:18:32 4splitDomains](trainer.py 286): INFO [30/91]	0.0976(0.2905)	0.0003(0.1843)	0.132(0.118)	100.00(97.28)
[2023-09-29 12:18:35 4splitDomains](trainer.py 286): INFO [40/91]	0.4952(0.2841)	0.3943(0.1782)	0.049(0.111)	100.00(97.26)
[2023-09-29 12:18:37 4splitDomains](trainer.py 286): INFO [50/91]	0.1003(0.2699)	0.0002(0.1648)	0.122(0.107)	96.88(97.30)
[2023-09-29 12:18:39 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.2640)	0.0002(0.1601)	0.053(0.107)	100.00(97.28)
[2023-09-29 12:18:42 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.2681)	0.0001(0.1651)	0.084(0.105)	100.00(97.27)
[2023-09-29 12:18:45 4splitDomains](trainer.py 286): INFO [80/91]	0.0979(0.2651)	0.0002(0.1628)	0.084(0.102)	96.88(97.34)
[2023-09-29 12:18:47 4splitDomains](trainer.py 286): INFO [90/91]	0.2873(0.2628)	0.1926(0.1608)	0.055(0.098)	100.00(97.49)
[2023-09-29 12:18:47 4splitDomains](trainer.py 288): INFO  * Train Acc 97.492
[2023-09-29 12:18:51 4splitDomains](my_trainer.py 503): INFO  * Val Acc 73.947, Total time 4.31
[2023-09-29 12:18:51 4splitDomains](my_trainer.py 328): INFO Epoch:6
[2023-09-29 12:18:51 4splitDomains](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 12:18:51 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:18:52 4splitDomains](trainer.py 286): INFO [0/91]	1.1305(1.1305)	1.0254(1.0254)	0.067(0.067)	100.00(100.00)
[2023-09-29 12:18:55 4splitDomains](trainer.py 286): INFO [10/91]	0.0980(0.3459)	0.0004(0.2428)	0.125(0.062)	100.00(99.15)
[2023-09-29 12:18:58 4splitDomains](trainer.py 286): INFO [20/91]	0.9231(0.3150)	0.8088(0.2128)	0.047(0.054)	100.00(99.26)
[2023-09-29 12:19:01 4splitDomains](trainer.py 286): INFO [30/91]	0.1031(0.3013)	0.0004(0.1997)	0.043(0.051)	100.00(99.40)
[2023-09-29 12:19:03 4splitDomains](trainer.py 286): INFO [40/91]	0.3716(0.2946)	0.2599(0.1924)	0.065(0.047)	100.00(99.54)
[2023-09-29 12:19:06 4splitDomains](trainer.py 286): INFO [50/91]	0.1007(0.2948)	0.0002(0.1922)	0.031(0.048)	100.00(99.45)
[2023-09-29 12:19:09 4splitDomains](trainer.py 286): INFO [60/91]	0.5309(0.2845)	0.4333(0.1816)	0.040(0.049)	100.00(99.28)
[2023-09-29 12:19:11 4splitDomains](trainer.py 286): INFO [70/91]	0.2419(0.2710)	0.1426(0.1689)	0.051(0.047)	96.88(99.30)
[2023-09-29 12:19:13 4splitDomains](trainer.py 286): INFO [80/91]	0.4064(0.2721)	0.3088(0.1696)	0.030(0.045)	100.00(99.34)
[2023-09-29 12:19:16 4splitDomains](trainer.py 286): INFO [90/91]	0.6583(0.2702)	0.5634(0.1678)	0.024(0.045)	100.00(99.31)
[2023-09-29 12:19:16 4splitDomains](trainer.py 288): INFO  * Train Acc 99.313
[2023-09-29 12:19:20 4splitDomains](my_trainer.py 503): INFO  * Val Acc 74.737, Total time 4.31
[2023-09-29 12:19:20 4splitDomains](my_trainer.py 328): INFO Epoch:7
[2023-09-29 12:19:20 4splitDomains](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 12:19:20 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:19:22 4splitDomains](trainer.py 286): INFO [0/91]	1.2675(1.2675)	1.1585(1.1585)	0.016(0.016)	100.00(100.00)
[2023-09-29 12:19:24 4splitDomains](trainer.py 286): INFO [10/91]	0.0975(0.3116)	0.0002(0.2118)	0.023(0.036)	100.00(99.15)
[2023-09-29 12:19:28 4splitDomains](trainer.py 286): INFO [20/91]	0.9658(0.3483)	0.8683(0.2484)	0.073(0.046)	100.00(98.96)
[2023-09-29 12:19:30 4splitDomains](trainer.py 286): INFO [30/91]	0.0976(0.3047)	0.0003(0.2046)	0.062(0.041)	96.88(99.19)
[2023-09-29 12:19:33 4splitDomains](trainer.py 286): INFO [40/91]	0.7102(0.3128)	0.6127(0.2116)	0.038(0.039)	100.00(99.31)
[2023-09-29 12:19:35 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.2902)	0.0002(0.1876)	0.013(0.040)	100.00(99.33)
[2023-09-29 12:19:38 4splitDomains](trainer.py 286): INFO [60/91]	1.1033(0.2944)	0.9978(0.1920)	0.033(0.039)	100.00(99.33)
[2023-09-29 12:19:41 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.2871)	0.0003(0.1852)	0.020(0.038)	100.00(99.38)
[2023-09-29 12:19:43 4splitDomains](trainer.py 286): INFO [80/91]	0.6932(0.2821)	0.5946(0.1804)	0.039(0.038)	100.00(99.38)
[2023-09-29 12:19:45 4splitDomains](trainer.py 286): INFO [90/91]	0.0953(0.2729)	0.0003(0.1716)	0.012(0.037)	100.00(99.35)
[2023-09-29 12:19:45 4splitDomains](trainer.py 288): INFO  * Train Acc 99.347
[2023-09-29 12:19:50 4splitDomains](my_trainer.py 503): INFO  * Val Acc 75.789, Total time 4.27
[2023-09-29 12:19:50 4splitDomains](my_trainer.py 328): INFO Epoch:8
[2023-09-29 12:19:50 4splitDomains](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 12:19:50 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:19:51 4splitDomains](trainer.py 286): INFO [0/91]	1.2125(1.2125)	1.0907(1.0907)	0.061(0.061)	96.88(96.88)
[2023-09-29 12:19:53 4splitDomains](trainer.py 286): INFO [10/91]	0.2849(0.3183)	0.1661(0.2145)	0.016(0.037)	100.00(99.15)
[2023-09-29 12:19:57 4splitDomains](trainer.py 286): INFO [20/91]	1.0344(0.3288)	0.9368(0.2276)	0.013(0.036)	100.00(99.40)
[2023-09-29 12:19:59 4splitDomains](trainer.py 286): INFO [30/91]	0.3854(0.2937)	0.2633(0.1917)	0.057(0.037)	96.88(99.19)
[2023-09-29 12:20:01 4splitDomains](trainer.py 286): INFO [40/91]	0.6053(0.2833)	0.5078(0.1806)	0.040(0.036)	100.00(99.24)
[2023-09-29 12:20:03 4splitDomains](trainer.py 286): INFO [50/91]	0.4461(0.2646)	0.3486(0.1627)	0.108(0.040)	93.75(98.96)
[2023-09-29 12:20:06 4splitDomains](trainer.py 286): INFO [60/91]	0.1229(0.2617)	0.0002(0.1594)	0.017(0.038)	100.00(99.03)
[2023-09-29 12:20:09 4splitDomains](trainer.py 286): INFO [70/91]	0.6331(0.2675)	0.5357(0.1646)	0.027(0.037)	100.00(99.12)
[2023-09-29 12:20:11 4splitDomains](trainer.py 286): INFO [80/91]	0.1152(0.2670)	0.0002(0.1634)	0.016(0.037)	100.00(99.15)
[2023-09-29 12:20:14 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2665)	0.0001(0.1634)	0.016(0.036)	100.00(99.24)
[2023-09-29 12:20:14 4splitDomains](trainer.py 288): INFO  * Train Acc 99.244
[2023-09-29 12:20:18 4splitDomains](my_trainer.py 503): INFO  * Val Acc 75.526, Total time 4.32
[2023-09-29 12:20:18 4splitDomains](my_trainer.py 328): INFO Epoch:9
[2023-09-29 12:20:18 4splitDomains](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 12:20:18 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:20:20 4splitDomains](trainer.py 286): INFO [0/91]	1.7618(1.7618)	1.6574(1.6574)	0.015(0.015)	100.00(100.00)
[2023-09-29 12:20:22 4splitDomains](trainer.py 286): INFO [10/91]	0.0976(0.3374)	0.0002(0.2388)	0.036(0.031)	100.00(99.43)
[2023-09-29 12:20:26 4splitDomains](trainer.py 286): INFO [20/91]	0.7200(0.3440)	0.6226(0.2456)	0.014(0.028)	100.00(99.55)
[2023-09-29 12:20:27 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.2926)	0.0002(0.1945)	0.048(0.032)	100.00(99.50)
[2023-09-29 12:20:30 4splitDomains](trainer.py 286): INFO [40/91]	0.6324(0.2906)	0.5350(0.1927)	0.022(0.030)	100.00(99.62)
[2023-09-29 12:20:33 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.2793)	0.0002(0.1813)	0.016(0.031)	100.00(99.63)
[2023-09-29 12:20:35 4splitDomains](trainer.py 286): INFO [60/91]	0.0980(0.2793)	0.0006(0.1813)	0.008(0.029)	100.00(99.69)
[2023-09-29 12:20:38 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.2812)	0.0002(0.1833)	0.017(0.029)	100.00(99.74)
[2023-09-29 12:20:40 4splitDomains](trainer.py 286): INFO [80/91]	0.0975(0.2678)	0.0002(0.1699)	0.028(0.029)	100.00(99.65)
[2023-09-29 12:20:43 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2721)	0.0001(0.1742)	0.068(0.031)	96.77(99.52)
[2023-09-29 12:20:43 4splitDomains](trainer.py 288): INFO  * Train Acc 99.519
[2023-09-29 12:20:47 4splitDomains](my_trainer.py 503): INFO  * Val Acc 75.263, Total time 4.28
[2023-09-29 12:20:47 4splitDomains](my_trainer.py 206): INFO Pruning for task2
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 2352/4704 (50.00%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 1024/2048 (50.00%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 9216/18432 (50.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 4096/8192 (50.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 4096/8192 (50.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 4096/8192 (50.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 9216/18432 (50.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 4096/8192 (50.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 4096/8192 (50.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 9216/18432 (50.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 4096/8192 (50.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 8192/16384 (50.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 36864/73728 (50.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 16384/32768 (50.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 32768/65536 (50.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 16384/32768 (50.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 36864/73728 (50.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 16384/32768 (50.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 16384/32768 (50.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 36864/73728 (50.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 16384/32768 (50.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 16384/32768 (50.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 36864/73728 (50.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 16384/32768 (50.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 32768/65536 (50.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 147456/294912 (50.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 131072/262144 (50.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 147456/294912 (50.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 147456/294912 (50.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 147456/294912 (50.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 147456/294912 (50.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 147456/294912 (50.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 65536/131072 (50.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 131072/262144 (50.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 589824/1179648 (50.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 262144/524288 (50.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 524288/1048576 (50.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 262144/524288 (50.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 589824/1179648 (50.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 262144/524288 (50.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 262144/524288 (50.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 589824/1179648 (50.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 262144/524288 (50.00%) (Total in layer: 1048576)
Layer #last.All, pruned 30720/61440 (50.00%) (Total in layer: 122880)
[2023-09-29 12:20:48 4splitDomains](my_trainer.py 298): INFO start retrain model
[2023-09-29 12:20:48 4splitDomains](my_trainer.py 302): INFO Epoch:0
[2023-09-29 12:20:48 4splitDomains](my_trainer.py 308): INFO LR:0.01
[2023-09-29 12:20:48 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:20:49 4splitDomains](trainer.py 286): INFO [0/91]	1.2705(1.2705)	1.1208(1.1208)	0.006(0.006)	100.00(100.00)
[2023-09-29 12:20:52 4splitDomains](trainer.py 286): INFO [10/91]	0.1066(0.3414)	0.0003(0.2382)	0.028(0.020)	100.00(99.72)
[2023-09-29 12:20:54 4splitDomains](trainer.py 286): INFO [20/91]	0.0976(0.2928)	0.0002(0.1914)	0.023(0.028)	100.00(99.40)
[2023-09-29 12:20:58 4splitDomains](trainer.py 286): INFO [30/91]	0.0975(0.3266)	0.0002(0.2265)	0.017(0.033)	100.00(99.09)
[2023-09-29 12:21:00 4splitDomains](trainer.py 286): INFO [40/91]	0.0977(0.2970)	0.0003(0.1955)	0.088(0.034)	96.88(99.16)
[2023-09-29 12:21:03 4splitDomains](trainer.py 286): INFO [50/91]	0.0972(0.2961)	0.0001(0.1950)	0.049(0.033)	96.88(99.20)
[2023-09-29 12:21:05 4splitDomains](trainer.py 286): INFO [60/91]	0.1179(0.2847)	0.0003(0.1833)	0.031(0.036)	100.00(99.23)
[2023-09-29 12:21:08 4splitDomains](trainer.py 286): INFO [70/91]	0.4621(0.2902)	0.3645(0.1887)	0.014(0.038)	100.00(99.16)
[2023-09-29 12:21:10 4splitDomains](trainer.py 286): INFO [80/91]	0.1020(0.2803)	0.0003(0.1783)	0.026(0.039)	100.00(99.19)
[2023-09-29 12:21:14 4splitDomains](trainer.py 286): INFO [90/91]	0.5761(0.2838)	0.4811(0.1821)	0.064(0.040)	96.77(99.11)
[2023-09-29 12:21:14 4splitDomains](trainer.py 288): INFO  * Train Acc 99.107
[2023-09-29 12:21:18 4splitDomains](my_trainer.py 503): INFO  * Val Acc 75.526, Total time 4.33
[2023-09-29 12:21:18 4splitDomains](my_trainer.py 302): INFO Epoch:1
[2023-09-29 12:21:18 4splitDomains](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 12:21:18 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:21:19 4splitDomains](trainer.py 286): INFO [0/91]	1.1251(1.1251)	1.0128(1.0128)	0.014(0.014)	100.00(100.00)
[2023-09-29 12:21:22 4splitDomains](trainer.py 286): INFO [10/91]	0.0974(0.3602)	0.0002(0.2523)	0.216(0.053)	93.75(98.58)
[2023-09-29 12:21:25 4splitDomains](trainer.py 286): INFO [20/91]	0.8181(0.3306)	0.7139(0.2249)	0.005(0.038)	100.00(99.11)
[2023-09-29 12:21:27 4splitDomains](trainer.py 286): INFO [30/91]	0.0972(0.3030)	0.0001(0.1998)	0.079(0.042)	96.88(98.89)
[2023-09-29 12:21:30 4splitDomains](trainer.py 286): INFO [40/91]	0.6262(0.2957)	0.5287(0.1925)	0.190(0.047)	96.88(98.86)
[2023-09-29 12:21:33 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.2853)	0.0002(0.1831)	0.038(0.051)	100.00(98.77)
[2023-09-29 12:21:35 4splitDomains](trainer.py 286): INFO [60/91]	0.6582(0.2829)	0.5609(0.1815)	0.049(0.050)	96.88(98.87)
[2023-09-29 12:21:38 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.2759)	0.0002(0.1751)	0.025(0.048)	100.00(98.90)
[2023-09-29 12:21:40 4splitDomains](trainer.py 286): INFO [80/91]	0.2015(0.2739)	0.1039(0.1735)	0.019(0.047)	100.00(98.96)
[2023-09-29 12:21:43 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2706)	0.0001(0.1705)	0.037(0.045)	100.00(99.07)
[2023-09-29 12:21:43 4splitDomains](trainer.py 288): INFO  * Train Acc 99.072
[2023-09-29 12:21:47 4splitDomains](my_trainer.py 503): INFO  * Val Acc 75.789, Total time 4.31
[2023-09-29 12:21:47 4splitDomains](my_trainer.py 302): INFO Epoch:2
[2023-09-29 12:21:47 4splitDomains](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 12:21:47 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:21:48 4splitDomains](trainer.py 286): INFO [0/91]	1.2768(1.2768)	1.1763(1.1763)	0.013(0.013)	100.00(100.00)
[2023-09-29 12:21:51 4splitDomains](trainer.py 286): INFO [10/91]	0.0996(0.3235)	0.0002(0.2218)	0.013(0.031)	100.00(99.43)
[2023-09-29 12:21:54 4splitDomains](trainer.py 286): INFO [20/91]	1.0301(0.3461)	0.9227(0.2459)	0.023(0.026)	100.00(99.40)
[2023-09-29 12:21:57 4splitDomains](trainer.py 286): INFO [30/91]	0.0979(0.3184)	0.0002(0.2189)	0.008(0.030)	100.00(99.19)
[2023-09-29 12:22:00 4splitDomains](trainer.py 286): INFO [40/91]	0.7679(0.3151)	0.6631(0.2155)	0.020(0.033)	100.00(99.16)
[2023-09-29 12:22:02 4splitDomains](trainer.py 286): INFO [50/91]	0.0974(0.3000)	0.0002(0.2008)	0.016(0.032)	100.00(99.20)
[2023-09-29 12:22:05 4splitDomains](trainer.py 286): INFO [60/91]	0.4929(0.2866)	0.3954(0.1878)	0.005(0.033)	100.00(99.23)
[2023-09-29 12:22:07 4splitDomains](trainer.py 286): INFO [70/91]	0.1043(0.2793)	0.0003(0.1805)	0.032(0.033)	100.00(99.25)
[2023-09-29 12:22:10 4splitDomains](trainer.py 286): INFO [80/91]	0.6415(0.2784)	0.5441(0.1794)	0.072(0.038)	96.88(99.19)
[2023-09-29 12:22:12 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2697)	0.0001(0.1706)	0.066(0.036)	100.00(99.28)
[2023-09-29 12:22:12 4splitDomains](trainer.py 288): INFO  * Train Acc 99.279
[2023-09-29 12:22:16 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.316, Total time 4.29
[2023-09-29 12:22:16 4splitDomains](my_trainer.py 302): INFO Epoch:3
[2023-09-29 12:22:16 4splitDomains](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 12:22:16 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:22:17 4splitDomains](trainer.py 286): INFO [0/91]	0.9401(0.9401)	0.8340(0.8340)	0.059(0.059)	96.88(96.88)
[2023-09-29 12:22:20 4splitDomains](trainer.py 286): INFO [10/91]	0.7414(0.3714)	0.6440(0.2698)	0.015(0.026)	100.00(99.43)
[2023-09-29 12:22:22 4splitDomains](trainer.py 286): INFO [20/91]	0.0974(0.2976)	0.0002(0.1936)	0.066(0.027)	96.88(99.40)
[2023-09-29 12:22:25 4splitDomains](trainer.py 286): INFO [30/91]	0.3020(0.2910)	0.2045(0.1878)	0.011(0.027)	100.00(99.50)
[2023-09-29 12:22:28 4splitDomains](trainer.py 286): INFO [40/91]	1.0883(0.2928)	0.9909(0.1901)	0.118(0.029)	93.75(99.39)
[2023-09-29 12:22:30 4splitDomains](trainer.py 286): INFO [50/91]	0.1145(0.2800)	0.0002(0.1763)	0.023(0.029)	100.00(99.45)
[2023-09-29 12:22:33 4splitDomains](trainer.py 286): INFO [60/91]	0.5123(0.2796)	0.3923(0.1759)	0.018(0.029)	100.00(99.39)
[2023-09-29 12:22:35 4splitDomains](trainer.py 286): INFO [70/91]	0.0981(0.2727)	0.0006(0.1693)	0.022(0.030)	100.00(99.38)
[2023-09-29 12:22:38 4splitDomains](trainer.py 286): INFO [80/91]	0.8595(0.2759)	0.7622(0.1726)	0.028(0.029)	100.00(99.42)
[2023-09-29 12:22:41 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2708)	0.0001(0.1682)	0.028(0.029)	100.00(99.38)
[2023-09-29 12:22:41 4splitDomains](trainer.py 288): INFO  * Train Acc 99.382
[2023-09-29 12:22:45 4splitDomains](my_trainer.py 503): INFO  * Val Acc 75.526, Total time 4.30
[2023-09-29 12:22:45 4splitDomains](my_trainer.py 302): INFO Epoch:4
[2023-09-29 12:22:45 4splitDomains](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 12:22:45 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:22:46 4splitDomains](trainer.py 286): INFO [0/91]	1.1598(1.1598)	1.0544(1.0544)	0.015(0.015)	100.00(100.00)
[2023-09-29 12:22:49 4splitDomains](trainer.py 286): INFO [10/91]	0.7734(0.3518)	0.6761(0.2530)	0.016(0.015)	100.00(100.00)
[2023-09-29 12:22:51 4splitDomains](trainer.py 286): INFO [20/91]	0.4089(0.2695)	0.3103(0.1709)	0.042(0.021)	96.88(99.55)
[2023-09-29 12:22:54 4splitDomains](trainer.py 286): INFO [30/91]	0.5682(0.2806)	0.4707(0.1819)	0.011(0.020)	100.00(99.70)
[2023-09-29 12:22:57 4splitDomains](trainer.py 286): INFO [40/91]	0.4312(0.2816)	0.3337(0.1832)	0.005(0.019)	100.00(99.77)
[2023-09-29 12:22:59 4splitDomains](trainer.py 286): INFO [50/91]	0.0974(0.2677)	0.0002(0.1691)	0.016(0.021)	100.00(99.69)
[2023-09-29 12:23:01 4splitDomains](trainer.py 286): INFO [60/91]	0.3635(0.2630)	0.2612(0.1645)	0.005(0.020)	100.00(99.69)
[2023-09-29 12:23:04 4splitDomains](trainer.py 286): INFO [70/91]	0.2616(0.2611)	0.1643(0.1622)	0.010(0.021)	100.00(99.69)
[2023-09-29 12:23:06 4splitDomains](trainer.py 286): INFO [80/91]	0.6299(0.2623)	0.5325(0.1632)	0.024(0.021)	100.00(99.65)
[2023-09-29 12:23:09 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2604)	0.0001(0.1612)	0.022(0.022)	100.00(99.62)
[2023-09-29 12:23:09 4splitDomains](trainer.py 288): INFO  * Train Acc 99.622
[2023-09-29 12:23:13 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.579, Total time 4.39
[2023-09-29 12:23:13 4splitDomains](my_trainer.py 302): INFO Epoch:5
[2023-09-29 12:23:13 4splitDomains](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 12:23:13 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:23:15 4splitDomains](trainer.py 286): INFO [0/91]	1.2684(1.2684)	1.1531(1.1531)	0.018(0.018)	100.00(100.00)
[2023-09-29 12:23:17 4splitDomains](trainer.py 286): INFO [10/91]	0.2610(0.3307)	0.1635(0.2274)	0.010(0.020)	100.00(99.72)
[2023-09-29 12:23:20 4splitDomains](trainer.py 286): INFO [20/91]	1.0055(0.3207)	0.9080(0.2195)	0.022(0.025)	100.00(99.55)
[2023-09-29 12:23:22 4splitDomains](trainer.py 286): INFO [30/91]	0.2911(0.2933)	0.1931(0.1924)	0.021(0.024)	100.00(99.70)
[2023-09-29 12:23:25 4splitDomains](trainer.py 286): INFO [40/91]	0.6622(0.2842)	0.5646(0.1841)	0.024(0.028)	100.00(99.47)
[2023-09-29 12:23:27 4splitDomains](trainer.py 286): INFO [50/91]	0.5919(0.2775)	0.4945(0.1779)	0.016(0.028)	100.00(99.51)
[2023-09-29 12:23:30 4splitDomains](trainer.py 286): INFO [60/91]	0.1352(0.2681)	0.0377(0.1689)	0.008(0.026)	100.00(99.54)
[2023-09-29 12:23:33 4splitDomains](trainer.py 286): INFO [70/91]	0.7318(0.2707)	0.6344(0.1715)	0.013(0.025)	100.00(99.56)
[2023-09-29 12:23:35 4splitDomains](trainer.py 286): INFO [80/91]	0.1069(0.2681)	0.0003(0.1689)	0.035(0.024)	100.00(99.61)
[2023-09-29 12:23:38 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2690)	0.0001(0.1700)	0.003(0.024)	100.00(99.59)
[2023-09-29 12:23:38 4splitDomains](trainer.py 288): INFO  * Train Acc 99.588
[2023-09-29 12:23:42 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.842, Total time 4.31
[2023-09-29 12:23:42 4splitDomains](my_trainer.py 302): INFO Epoch:6
[2023-09-29 12:23:42 4splitDomains](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 12:23:42 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:23:43 4splitDomains](trainer.py 286): INFO [0/91]	0.6646(0.6646)	0.5488(0.5488)	0.006(0.006)	100.00(100.00)
[2023-09-29 12:23:45 4splitDomains](trainer.py 286): INFO [10/91]	0.4519(0.2928)	0.3544(0.1876)	0.005(0.020)	100.00(99.72)
[2023-09-29 12:23:48 4splitDomains](trainer.py 286): INFO [20/91]	0.5059(0.2936)	0.3950(0.1909)	0.019(0.016)	100.00(99.85)
[2023-09-29 12:23:51 4splitDomains](trainer.py 286): INFO [30/91]	0.4700(0.2937)	0.3725(0.1913)	0.019(0.015)	100.00(99.90)
[2023-09-29 12:23:54 4splitDomains](trainer.py 286): INFO [40/91]	0.1102(0.2776)	0.0002(0.1738)	0.010(0.016)	100.00(99.77)
[2023-09-29 12:23:57 4splitDomains](trainer.py 286): INFO [50/91]	1.4437(0.2933)	1.3460(0.1894)	0.010(0.015)	100.00(99.82)
[2023-09-29 12:23:59 4splitDomains](trainer.py 286): INFO [60/91]	0.0981(0.2819)	0.0003(0.1789)	0.050(0.018)	96.88(99.69)
[2023-09-29 12:24:03 4splitDomains](trainer.py 286): INFO [70/91]	1.0857(0.2859)	0.9882(0.1831)	0.018(0.017)	100.00(99.74)
[2023-09-29 12:24:05 4splitDomains](trainer.py 286): INFO [80/91]	0.0981(0.2769)	0.0002(0.1744)	0.007(0.017)	100.00(99.77)
[2023-09-29 12:24:07 4splitDomains](trainer.py 286): INFO [90/91]	0.5787(0.2770)	0.4835(0.1750)	0.005(0.017)	100.00(99.79)
[2023-09-29 12:24:08 4splitDomains](trainer.py 288): INFO  * Train Acc 99.794
[2023-09-29 12:24:12 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.579, Total time 4.32
[2023-09-29 12:24:12 4splitDomains](my_trainer.py 302): INFO Epoch:7
[2023-09-29 12:24:12 4splitDomains](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 12:24:12 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:24:13 4splitDomains](trainer.py 286): INFO [0/91]	1.3475(1.3475)	1.2320(1.2320)	0.025(0.025)	100.00(100.00)
[2023-09-29 12:24:15 4splitDomains](trainer.py 286): INFO [10/91]	0.1097(0.3207)	0.0003(0.2096)	0.006(0.018)	100.00(99.43)
[2023-09-29 12:24:18 4splitDomains](trainer.py 286): INFO [20/91]	0.5938(0.3028)	0.4963(0.1964)	0.004(0.016)	100.00(99.70)
[2023-09-29 12:24:21 4splitDomains](trainer.py 286): INFO [30/91]	0.0981(0.2853)	0.0001(0.1793)	0.010(0.014)	100.00(99.80)
[2023-09-29 12:24:24 4splitDomains](trainer.py 286): INFO [40/91]	0.7971(0.2866)	0.6962(0.1819)	0.005(0.014)	100.00(99.85)
[2023-09-29 12:24:26 4splitDomains](trainer.py 286): INFO [50/91]	0.1290(0.2761)	0.0025(0.1702)	0.012(0.014)	100.00(99.88)
[2023-09-29 12:24:30 4splitDomains](trainer.py 286): INFO [60/91]	1.1638(0.2986)	1.0660(0.1942)	0.088(0.016)	96.88(99.80)
[2023-09-29 12:24:32 4splitDomains](trainer.py 286): INFO [70/91]	0.0977(0.2865)	0.0002(0.1828)	0.116(0.017)	96.88(99.78)
[2023-09-29 12:24:36 4splitDomains](trainer.py 286): INFO [80/91]	1.0137(0.2956)	0.9126(0.1927)	0.013(0.016)	100.00(99.77)
[2023-09-29 12:24:37 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2814)	0.0001(0.1792)	0.008(0.016)	100.00(99.79)
[2023-09-29 12:24:38 4splitDomains](trainer.py 288): INFO  * Train Acc 99.794
[2023-09-29 12:24:42 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.316, Total time 4.31
[2023-09-29 12:24:42 4splitDomains](my_trainer.py 302): INFO Epoch:8
[2023-09-29 12:24:42 4splitDomains](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 12:24:42 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:24:44 4splitDomains](trainer.py 286): INFO [0/91]	1.6928(1.6928)	1.5812(1.5812)	0.034(0.034)	100.00(100.00)
[2023-09-29 12:24:46 4splitDomains](trainer.py 286): INFO [10/91]	0.0976(0.3532)	0.0002(0.2510)	0.019(0.010)	100.00(100.00)
[2023-09-29 12:24:48 4splitDomains](trainer.py 286): INFO [20/91]	0.4869(0.3089)	0.3895(0.2085)	0.015(0.010)	100.00(100.00)
[2023-09-29 12:24:51 4splitDomains](trainer.py 286): INFO [30/91]	0.0994(0.2826)	0.0003(0.1831)	0.032(0.015)	100.00(99.70)
[2023-09-29 12:24:53 4splitDomains](trainer.py 286): INFO [40/91]	0.1048(0.2669)	0.0003(0.1673)	0.004(0.014)	100.00(99.77)
[2023-09-29 12:24:56 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.2702)	0.0002(0.1707)	0.004(0.014)	100.00(99.75)
[2023-09-29 12:24:58 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.2610)	0.0003(0.1618)	0.006(0.014)	100.00(99.74)
[2023-09-29 12:25:01 4splitDomains](trainer.py 286): INFO [70/91]	0.0974(0.2661)	0.0002(0.1670)	0.005(0.013)	100.00(99.78)
[2023-09-29 12:25:03 4splitDomains](trainer.py 286): INFO [80/91]	0.5568(0.2637)	0.4593(0.1648)	0.080(0.013)	96.88(99.77)
[2023-09-29 12:25:06 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2639)	0.0001(0.1652)	0.013(0.013)	100.00(99.76)
[2023-09-29 12:25:06 4splitDomains](trainer.py 288): INFO  * Train Acc 99.760
[2023-09-29 12:25:10 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.895, Total time 4.28
[2023-09-29 12:25:10 4splitDomains](my_trainer.py 302): INFO Epoch:9
[2023-09-29 12:25:10 4splitDomains](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 12:25:10 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:25:12 4splitDomains](trainer.py 286): INFO [0/91]	1.2719(1.2719)	1.1709(1.1709)	0.056(0.056)	96.88(96.88)
[2023-09-29 12:25:14 4splitDomains](trainer.py 286): INFO [10/91]	0.2490(0.3205)	0.1427(0.2174)	0.001(0.024)	100.00(99.15)
[2023-09-29 12:25:17 4splitDomains](trainer.py 286): INFO [20/91]	0.0976(0.3043)	0.0002(0.2002)	0.003(0.023)	100.00(99.11)
[2023-09-29 12:25:20 4splitDomains](trainer.py 286): INFO [30/91]	0.7531(0.3070)	0.6557(0.2036)	0.008(0.018)	100.00(99.40)
[2023-09-29 12:25:22 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.2887)	0.0002(0.1859)	0.004(0.018)	100.00(99.47)
[2023-09-29 12:25:25 4splitDomains](trainer.py 286): INFO [50/91]	0.8319(0.2865)	0.7310(0.1838)	0.018(0.018)	100.00(99.51)
[2023-09-29 12:25:28 4splitDomains](trainer.py 286): INFO [60/91]	0.3552(0.2858)	0.2578(0.1836)	0.034(0.018)	96.88(99.49)
[2023-09-29 12:25:30 4splitDomains](trainer.py 286): INFO [70/91]	0.1023(0.2751)	0.0004(0.1726)	0.013(0.018)	100.00(99.52)
[2023-09-29 12:25:32 4splitDomains](trainer.py 286): INFO [80/91]	0.6317(0.2739)	0.5342(0.1714)	0.008(0.017)	100.00(99.54)
[2023-09-29 12:25:35 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2693)	0.0001(0.1674)	0.006(0.017)	100.00(99.59)
[2023-09-29 12:25:35 4splitDomains](trainer.py 288): INFO  * Train Acc 99.588
[2023-09-29 12:25:39 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.368, Total time 4.32
[2023-09-29 12:25:39 4splitDomains](my_trainer.py 302): INFO Epoch:10
[2023-09-29 12:25:39 4splitDomains](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 12:25:39 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:25:40 4splitDomains](trainer.py 286): INFO [0/91]	1.0957(1.0957)	0.9736(0.9736)	0.012(0.012)	100.00(100.00)
[2023-09-29 12:25:43 4splitDomains](trainer.py 286): INFO [10/91]	0.2185(0.3288)	0.1083(0.2241)	0.006(0.012)	100.00(99.72)
[2023-09-29 12:25:46 4splitDomains](trainer.py 286): INFO [20/91]	0.1236(0.3038)	0.0005(0.1975)	0.002(0.010)	100.00(99.85)
[2023-09-29 12:25:48 4splitDomains](trainer.py 286): INFO [30/91]	0.1284(0.2930)	0.0308(0.1888)	0.017(0.015)	100.00(99.70)
[2023-09-29 12:25:51 4splitDomains](trainer.py 286): INFO [40/91]	0.2907(0.2747)	0.1932(0.1713)	0.006(0.013)	100.00(99.77)
[2023-09-29 12:25:53 4splitDomains](trainer.py 286): INFO [50/91]	0.1082(0.2772)	0.0002(0.1743)	0.006(0.013)	100.00(99.75)
[2023-09-29 12:25:57 4splitDomains](trainer.py 286): INFO [60/91]	0.6679(0.2830)	0.5702(0.1808)	0.006(0.012)	100.00(99.80)
[2023-09-29 12:25:59 4splitDomains](trainer.py 286): INFO [70/91]	0.0976(0.2793)	0.0002(0.1760)	0.003(0.014)	100.00(99.74)
[2023-09-29 12:26:01 4splitDomains](trainer.py 286): INFO [80/91]	0.4694(0.2711)	0.3708(0.1684)	0.017(0.013)	100.00(99.77)
[2023-09-29 12:26:03 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2656)	0.0001(0.1636)	0.027(0.014)	100.00(99.76)
[2023-09-29 12:26:03 4splitDomains](trainer.py 288): INFO  * Train Acc 99.760
[2023-09-29 12:26:08 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.579, Total time 4.32
[2023-09-29 12:26:08 4splitDomains](my_trainer.py 302): INFO Epoch:11
[2023-09-29 12:26:08 4splitDomains](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 12:26:08 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:26:09 4splitDomains](trainer.py 286): INFO [0/91]	1.2316(1.2316)	1.1250(1.1250)	0.020(0.020)	100.00(100.00)
[2023-09-29 12:26:12 4splitDomains](trainer.py 286): INFO [10/91]	0.4399(0.3459)	0.3422(0.2437)	0.002(0.007)	100.00(100.00)
[2023-09-29 12:26:14 4splitDomains](trainer.py 286): INFO [20/91]	0.3450(0.2932)	0.2403(0.1903)	0.005(0.010)	100.00(99.85)
[2023-09-29 12:26:17 4splitDomains](trainer.py 286): INFO [30/91]	0.1042(0.2928)	0.0003(0.1909)	0.018(0.013)	100.00(99.70)
[2023-09-29 12:26:20 4splitDomains](trainer.py 286): INFO [40/91]	0.3801(0.2940)	0.2781(0.1920)	0.004(0.013)	100.00(99.70)
[2023-09-29 12:26:22 4splitDomains](trainer.py 286): INFO [50/91]	0.0975(0.2765)	0.0001(0.1736)	0.014(0.012)	100.00(99.75)
[2023-09-29 12:26:25 4splitDomains](trainer.py 286): INFO [60/91]	0.6410(0.2839)	0.5435(0.1813)	0.040(0.013)	100.00(99.80)
[2023-09-29 12:26:27 4splitDomains](trainer.py 286): INFO [70/91]	0.0975(0.2717)	0.0002(0.1698)	0.002(0.012)	100.00(99.82)
[2023-09-29 12:26:30 4splitDomains](trainer.py 286): INFO [80/91]	0.0974(0.2769)	0.0001(0.1748)	0.005(0.013)	100.00(99.81)
[2023-09-29 12:26:33 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2749)	0.0001(0.1733)	0.021(0.012)	100.00(99.83)
[2023-09-29 12:26:33 4splitDomains](trainer.py 288): INFO  * Train Acc 99.828
[2023-09-29 12:26:37 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.632, Total time 4.30
[2023-09-29 12:26:37 4splitDomains](my_trainer.py 302): INFO Epoch:12
[2023-09-29 12:26:37 4splitDomains](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 12:26:37 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:26:39 4splitDomains](trainer.py 286): INFO [0/91]	1.6520(1.6520)	1.5421(1.5421)	0.022(0.022)	100.00(100.00)
[2023-09-29 12:26:41 4splitDomains](trainer.py 286): INFO [10/91]	0.1367(0.3152)	0.0393(0.2056)	0.015(0.012)	100.00(100.00)
[2023-09-29 12:26:43 4splitDomains](trainer.py 286): INFO [20/91]	0.0975(0.2857)	0.0003(0.1775)	0.002(0.012)	100.00(99.70)
[2023-09-29 12:26:46 4splitDomains](trainer.py 286): INFO [30/91]	0.1224(0.2729)	0.0002(0.1644)	0.002(0.012)	100.00(99.70)
[2023-09-29 12:26:49 4splitDomains](trainer.py 286): INFO [40/91]	0.0977(0.2743)	0.0003(0.1658)	0.006(0.011)	100.00(99.77)
[2023-09-29 12:26:51 4splitDomains](trainer.py 286): INFO [50/91]	0.4317(0.2720)	0.3344(0.1641)	0.007(0.011)	100.00(99.82)
[2023-09-29 12:26:54 4splitDomains](trainer.py 286): INFO [60/91]	0.1465(0.2765)	0.0338(0.1683)	0.004(0.011)	100.00(99.85)
[2023-09-29 12:26:57 4splitDomains](trainer.py 286): INFO [70/91]	0.5048(0.2768)	0.4074(0.1695)	0.004(0.010)	100.00(99.87)
[2023-09-29 12:27:00 4splitDomains](trainer.py 286): INFO [80/91]	0.1197(0.2748)	0.0002(0.1677)	0.002(0.010)	100.00(99.88)
[2023-09-29 12:27:02 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2681)	0.0001(0.1620)	0.002(0.010)	100.00(99.86)
[2023-09-29 12:27:02 4splitDomains](trainer.py 288): INFO  * Train Acc 99.863
[2023-09-29 12:27:06 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.368, Total time 4.31
[2023-09-29 12:27:06 4splitDomains](my_trainer.py 302): INFO Epoch:13
[2023-09-29 12:27:06 4splitDomains](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 12:27:06 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:27:07 4splitDomains](trainer.py 286): INFO [0/91]	1.0719(1.0719)	0.9687(0.9687)	0.012(0.012)	100.00(100.00)
[2023-09-29 12:27:10 4splitDomains](trainer.py 286): INFO [10/91]	0.2783(0.3417)	0.1809(0.2399)	0.004(0.008)	100.00(100.00)
[2023-09-29 12:27:12 4splitDomains](trainer.py 286): INFO [20/91]	0.0977(0.2820)	0.0003(0.1787)	0.045(0.013)	100.00(100.00)
[2023-09-29 12:27:16 4splitDomains](trainer.py 286): INFO [30/91]	0.9083(0.3038)	0.8109(0.2015)	0.005(0.012)	100.00(99.90)
[2023-09-29 12:27:18 4splitDomains](trainer.py 286): INFO [40/91]	0.0987(0.2941)	0.0002(0.1920)	0.005(0.011)	100.00(99.92)
[2023-09-29 12:27:21 4splitDomains](trainer.py 286): INFO [50/91]	0.4789(0.2950)	0.3816(0.1929)	0.041(0.011)	96.88(99.88)
[2023-09-29 12:27:23 4splitDomains](trainer.py 286): INFO [60/91]	0.0974(0.2811)	0.0002(0.1795)	0.003(0.012)	100.00(99.90)
[2023-09-29 12:27:26 4splitDomains](trainer.py 286): INFO [70/91]	0.5170(0.2845)	0.4197(0.1836)	0.034(0.012)	100.00(99.91)
[2023-09-29 12:27:28 4splitDomains](trainer.py 286): INFO [80/91]	0.0972(0.2745)	0.0003(0.1740)	0.005(0.011)	100.00(99.92)
[2023-09-29 12:27:31 4splitDomains](trainer.py 286): INFO [90/91]	0.1794(0.2697)	0.0848(0.1696)	0.006(0.010)	100.00(99.93)
[2023-09-29 12:27:31 4splitDomains](trainer.py 288): INFO  * Train Acc 99.931
[2023-09-29 12:27:35 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.632, Total time 4.36
[2023-09-29 12:27:35 4splitDomains](my_trainer.py 302): INFO Epoch:14
[2023-09-29 12:27:35 4splitDomains](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 12:27:35 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:27:37 4splitDomains](trainer.py 286): INFO [0/91]	1.6248(1.6248)	1.5013(1.5013)	0.015(0.015)	100.00(100.00)
[2023-09-29 12:27:39 4splitDomains](trainer.py 286): INFO [10/91]	0.0975(0.3634)	0.0002(0.2609)	0.002(0.008)	100.00(100.00)
[2023-09-29 12:27:42 4splitDomains](trainer.py 286): INFO [20/91]	0.5939(0.3346)	0.4964(0.2342)	0.007(0.009)	100.00(100.00)
[2023-09-29 12:27:45 4splitDomains](trainer.py 286): INFO [30/91]	0.0973(0.3176)	0.0001(0.2181)	0.009(0.008)	100.00(100.00)
[2023-09-29 12:27:48 4splitDomains](trainer.py 286): INFO [40/91]	0.5900(0.3242)	0.4926(0.2253)	0.014(0.010)	100.00(99.92)
[2023-09-29 12:27:50 4splitDomains](trainer.py 286): INFO [50/91]	0.0973(0.3003)	0.0002(0.2017)	0.005(0.009)	100.00(99.94)
[2023-09-29 12:27:53 4splitDomains](trainer.py 286): INFO [60/91]	0.4794(0.2982)	0.3820(0.1998)	0.004(0.010)	100.00(99.90)
[2023-09-29 12:27:56 4splitDomains](trainer.py 286): INFO [70/91]	0.0980(0.2882)	0.0003(0.1897)	0.005(0.010)	100.00(99.91)
[2023-09-29 12:27:59 4splitDomains](trainer.py 286): INFO [80/91]	0.6899(0.2884)	0.5921(0.1898)	0.008(0.010)	100.00(99.85)
[2023-09-29 12:28:01 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2813)	0.0001(0.1825)	0.003(0.010)	100.00(99.83)
[2023-09-29 12:28:01 4splitDomains](trainer.py 288): INFO  * Train Acc 99.828
[2023-09-29 12:28:05 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.842, Total time 4.28
[2023-09-29 12:28:05 4splitDomains](my_trainer.py 302): INFO Epoch:15
[2023-09-29 12:28:05 4splitDomains](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 12:28:05 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:28:06 4splitDomains](trainer.py 286): INFO [0/91]	1.2871(1.2871)	1.1846(1.1846)	0.004(0.004)	100.00(100.00)
[2023-09-29 12:28:09 4splitDomains](trainer.py 286): INFO [10/91]	0.2403(0.3187)	0.1413(0.2193)	0.017(0.017)	100.00(99.43)
[2023-09-29 12:28:12 4splitDomains](trainer.py 286): INFO [20/91]	0.2131(0.3100)	0.1105(0.2083)	0.005(0.014)	100.00(99.70)
[2023-09-29 12:28:14 4splitDomains](trainer.py 286): INFO [30/91]	0.0980(0.2745)	0.0004(0.1737)	0.005(0.012)	100.00(99.80)
[2023-09-29 12:28:17 4splitDomains](trainer.py 286): INFO [40/91]	0.1085(0.2779)	0.0003(0.1755)	0.010(0.012)	100.00(99.77)
[2023-09-29 12:28:19 4splitDomains](trainer.py 286): INFO [50/91]	0.0976(0.2668)	0.0003(0.1642)	0.007(0.011)	100.00(99.82)
[2023-09-29 12:28:22 4splitDomains](trainer.py 286): INFO [60/91]	0.0977(0.2682)	0.0003(0.1662)	0.009(0.011)	100.00(99.80)
[2023-09-29 12:28:24 4splitDomains](trainer.py 286): INFO [70/91]	0.1040(0.2612)	0.0002(0.1597)	0.005(0.011)	100.00(99.82)
[2023-09-29 12:28:27 4splitDomains](trainer.py 286): INFO [80/91]	1.0357(0.2732)	0.9381(0.1722)	0.005(0.010)	100.00(99.85)
[2023-09-29 12:28:29 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2659)	0.0001(0.1653)	0.004(0.010)	100.00(99.86)
[2023-09-29 12:28:29 4splitDomains](trainer.py 288): INFO  * Train Acc 99.863
[2023-09-29 12:28:34 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.158, Total time 4.33
[2023-09-29 12:28:34 4splitDomains](my_trainer.py 302): INFO Epoch:16
[2023-09-29 12:28:34 4splitDomains](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 12:28:34 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:28:35 4splitDomains](trainer.py 286): INFO [0/91]	1.5285(1.5285)	1.4235(1.4235)	0.005(0.005)	100.00(100.00)
[2023-09-29 12:28:37 4splitDomains](trainer.py 286): INFO [10/91]	0.1264(0.3136)	0.0002(0.2082)	0.015(0.010)	100.00(100.00)
[2023-09-29 12:28:40 4splitDomains](trainer.py 286): INFO [20/91]	0.7004(0.3183)	0.6028(0.2155)	0.015(0.011)	100.00(99.85)
[2023-09-29 12:28:43 4splitDomains](trainer.py 286): INFO [30/91]	0.6058(0.3016)	0.5084(0.1988)	0.002(0.011)	100.00(99.80)
[2023-09-29 12:28:46 4splitDomains](trainer.py 286): INFO [40/91]	0.0975(0.3030)	0.0002(0.2013)	0.004(0.010)	100.00(99.85)
[2023-09-29 12:28:49 4splitDomains](trainer.py 286): INFO [50/91]	0.5561(0.3008)	0.4585(0.1997)	0.007(0.010)	100.00(99.88)
[2023-09-29 12:28:51 4splitDomains](trainer.py 286): INFO [60/91]	0.0975(0.2891)	0.0003(0.1884)	0.004(0.010)	100.00(99.90)
[2023-09-29 12:28:54 4splitDomains](trainer.py 286): INFO [70/91]	0.5850(0.2833)	0.4875(0.1829)	0.003(0.010)	100.00(99.87)
[2023-09-29 12:28:57 4splitDomains](trainer.py 286): INFO [80/91]	1.1704(0.2857)	1.0731(0.1854)	0.011(0.010)	100.00(99.88)
[2023-09-29 12:29:00 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2841)	0.0001(0.1841)	0.004(0.010)	100.00(99.90)
[2023-09-29 12:29:00 4splitDomains](trainer.py 288): INFO  * Train Acc 99.897
[2023-09-29 12:29:04 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.632, Total time 4.32
[2023-09-29 12:29:04 4splitDomains](my_trainer.py 302): INFO Epoch:17
[2023-09-29 12:29:04 4splitDomains](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 12:29:04 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:29:05 4splitDomains](trainer.py 286): INFO [0/91]	1.2215(1.2215)	1.0974(1.0974)	0.005(0.005)	100.00(100.00)
[2023-09-29 12:29:08 4splitDomains](trainer.py 286): INFO [10/91]	0.1091(0.3528)	0.0002(0.2483)	0.003(0.010)	100.00(99.72)
[2023-09-29 12:29:11 4splitDomains](trainer.py 286): INFO [20/91]	0.3865(0.3187)	0.2804(0.2162)	0.020(0.010)	100.00(99.85)
[2023-09-29 12:29:14 4splitDomains](trainer.py 286): INFO [30/91]	0.0974(0.3101)	0.0002(0.2092)	0.002(0.008)	100.00(99.90)
[2023-09-29 12:29:16 4splitDomains](trainer.py 286): INFO [40/91]	0.1821(0.2853)	0.0850(0.1851)	0.006(0.008)	100.00(99.92)
[2023-09-29 12:29:18 4splitDomains](trainer.py 286): INFO [50/91]	0.0978(0.2783)	0.0004(0.1786)	0.033(0.008)	100.00(99.88)
[2023-09-29 12:29:21 4splitDomains](trainer.py 286): INFO [60/91]	0.0977(0.2723)	0.0002(0.1729)	0.031(0.008)	96.88(99.85)
[2023-09-29 12:29:24 4splitDomains](trainer.py 286): INFO [70/91]	0.0982(0.2756)	0.0003(0.1761)	0.012(0.008)	100.00(99.87)
[2023-09-29 12:29:26 4splitDomains](trainer.py 286): INFO [80/91]	0.0973(0.2655)	0.0002(0.1662)	0.002(0.009)	100.00(99.88)
[2023-09-29 12:29:28 4splitDomains](trainer.py 286): INFO [90/91]	0.0948(0.2602)	0.0001(0.1611)	0.007(0.010)	100.00(99.79)
[2023-09-29 12:29:28 4splitDomains](trainer.py 288): INFO  * Train Acc 99.794
[2023-09-29 12:29:32 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.368, Total time 4.29
[2023-09-29 12:29:32 4splitDomains](my_trainer.py 302): INFO Epoch:18
[2023-09-29 12:29:32 4splitDomains](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 12:29:32 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:29:34 4splitDomains](trainer.py 286): INFO [0/91]	1.3983(1.3983)	1.2965(1.2965)	0.009(0.009)	100.00(100.00)
[2023-09-29 12:29:36 4splitDomains](trainer.py 286): INFO [10/91]	0.2315(0.3384)	0.1338(0.2333)	0.004(0.005)	100.00(100.00)
[2023-09-29 12:29:38 4splitDomains](trainer.py 286): INFO [20/91]	0.6226(0.2947)	0.5111(0.1924)	0.007(0.009)	100.00(99.85)
[2023-09-29 12:29:41 4splitDomains](trainer.py 286): INFO [30/91]	0.1011(0.2789)	0.0004(0.1775)	0.004(0.010)	100.00(99.80)
[2023-09-29 12:29:43 4splitDomains](trainer.py 286): INFO [40/91]	0.0975(0.2632)	0.0002(0.1624)	0.041(0.010)	100.00(99.77)
[2023-09-29 12:29:46 4splitDomains](trainer.py 286): INFO [50/91]	0.1594(0.2648)	0.0461(0.1633)	0.003(0.010)	100.00(99.82)
[2023-09-29 12:29:49 4splitDomains](trainer.py 286): INFO [60/91]	0.5696(0.2692)	0.4627(0.1661)	0.006(0.010)	100.00(99.80)
[2023-09-29 12:29:51 4splitDomains](trainer.py 286): INFO [70/91]	0.2911(0.2654)	0.1666(0.1617)	0.003(0.010)	100.00(99.82)
[2023-09-29 12:29:54 4splitDomains](trainer.py 286): INFO [80/91]	0.8096(0.2718)	0.7060(0.1684)	0.003(0.010)	100.00(99.77)
[2023-09-29 12:29:57 4splitDomains](trainer.py 286): INFO [90/91]	0.0949(0.2677)	0.0001(0.1648)	0.012(0.010)	100.00(99.79)
[2023-09-29 12:29:57 4splitDomains](trainer.py 288): INFO  * Train Acc 99.794
[2023-09-29 12:30:01 4splitDomains](my_trainer.py 503): INFO  * Val Acc 77.895, Total time 4.29
[2023-09-29 12:30:01 4splitDomains](my_trainer.py 302): INFO Epoch:19
[2023-09-29 12:30:01 4splitDomains](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 12:30:01 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:30:02 4splitDomains](trainer.py 286): INFO [0/91]	1.2876(1.2876)	1.1616(1.1616)	0.016(0.016)	100.00(100.00)
[2023-09-29 12:30:05 4splitDomains](trainer.py 286): INFO [10/91]	0.0986(0.3506)	0.0002(0.2490)	0.002(0.013)	100.00(99.72)
[2023-09-29 12:30:07 4splitDomains](trainer.py 286): INFO [20/91]	0.0978(0.3071)	0.0003(0.2069)	0.002(0.010)	100.00(99.85)
[2023-09-29 12:30:11 4splitDomains](trainer.py 286): INFO [30/91]	0.2090(0.3095)	0.1113(0.2094)	0.005(0.011)	100.00(99.80)
[2023-09-29 12:30:13 4splitDomains](trainer.py 286): INFO [40/91]	0.0976(0.2839)	0.0003(0.1841)	0.007(0.011)	100.00(99.85)
[2023-09-29 12:30:15 4splitDomains](trainer.py 286): INFO [50/91]	0.7802(0.2804)	0.6810(0.1811)	0.003(0.010)	100.00(99.88)
[2023-09-29 12:30:18 4splitDomains](trainer.py 286): INFO [60/91]	0.0976(0.2743)	0.0004(0.1752)	0.017(0.010)	100.00(99.90)
[2023-09-29 12:30:21 4splitDomains](trainer.py 286): INFO [70/91]	0.2497(0.2760)	0.1526(0.1768)	0.008(0.010)	100.00(99.91)
[2023-09-29 12:30:23 4splitDomains](trainer.py 286): INFO [80/91]	0.0978(0.2712)	0.0004(0.1721)	0.005(0.010)	100.00(99.92)
[2023-09-29 12:30:26 4splitDomains](trainer.py 286): INFO [90/91]	0.6347(0.2724)	0.5397(0.1735)	0.010(0.010)	100.00(99.93)
[2023-09-29 12:30:26 4splitDomains](trainer.py 288): INFO  * Train Acc 99.931
[2023-09-29 12:30:30 4splitDomains](my_trainer.py 503): INFO  * Val Acc 76.842, Total time 4.32
=> Saving model to: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-2.pth
=> Save Done
[2023-09-29 12:30:30 4splitDomains](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 12:30:34 4splitDomains](my_trainer.py 503): INFO  * Val Acc 88.329, Total time 3.15
[2023-09-29 12:30:34 4splitDomains](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 12:30:37 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.571, Total time 3.11
[2023-09-29 12:30:37 4splitDomains](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 12:30:41 4splitDomains](my_trainer.py 503): INFO  * Val Acc 79.737, Total time 4.58
[2023-09-29 12:30:41 4splitDomains](trainer.py 335): INFO saving storage...
[2023-09-29 12:30:41 4splitDomains](trainer.py 341): INFO done
[2023-09-29 12:30:41 4splitDomains](iBatchLearn.py 155): INFO Acc:82.21239458244769; BWT:-0.39787794371770957;
=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:30:45 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:30:45 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:30:45 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 2, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-2.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-2.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_2.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:30:46 4splitDomains](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-2.pth
[2023-09-29 12:30:46 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:30:48 4splitDomains](my_trainer.py 67): INFO load storage...
[2023-09-29 12:30:49 4splitDomains](my_trainer.py 71): INFO done
[2023-09-29 12:30:49 4splitDomains](my_trainer.py 64): INFO tensor([[2, 2, 2, 2, 2, 2, 2],
        [0, 0, 0, 0, 0, 3, 3],
        [3, 2, 2, 0, 1, 3, 3],
        [0, 0, 0, 0, 3, 3, 3],
        [0, 0, 0, 0, 1, 3, 3],
        [2, 0, 0, 0, 3, 3, 3],
        [2, 0, 0, 3, 3, 3, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 12:30:49 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:30:49 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:30:49 4splitDomains](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 12:30:57 4splitDomains](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 12:31:03 4splitDomains](iBatchLearn.py 167): INFO test split name:2
--------------------------------Official Evaluation--------------------------------
2 83.81702432768503
=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:31:14 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:31:14 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:31:14 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": false, "task_count": 3, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-2.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-3.pth", "storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-2.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-3.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:31:15 4splitDomains](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-2.pth
[2023-09-29 12:31:15 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:31:17 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:31:17 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:31:17 4splitDomains](my_trainer.py 67): INFO load storage...
[2023-09-29 12:31:17 4splitDomains](my_trainer.py 71): INFO done
[2023-09-29 12:31:17 4splitDomains](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 12:31:17 4splitDomains](iBatchLearn.py 92): INFO ====================== 3 =======================
[2023-09-29 12:31:17 4splitDomains](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 12:31:17 4splitDomains](my_trainer.py 328): INFO Epoch:0
[2023-09-29 12:31:17 4splitDomains](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 12:31:17 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:31:21 4splitDomains](trainer.py 286): INFO [0/51]	3.6857(3.6857)	0.6747(0.6747)	1.390(1.390)	62.50(62.50)
[2023-09-29 12:31:22 4splitDomains](trainer.py 286): INFO [10/51]	0.0976(0.4253)	0.0003(0.0616)	1.566(1.847)	59.38(58.52)
[2023-09-29 12:31:23 4splitDomains](trainer.py 286): INFO [20/51]	0.0978(0.2704)	0.0003(0.0324)	1.818(1.766)	68.75(58.63)
[2023-09-29 12:31:24 4splitDomains](trainer.py 286): INFO [30/51]	0.1048(0.2210)	0.0003(0.0269)	1.915(1.807)	62.50(58.17)
[2023-09-29 12:31:25 4splitDomains](trainer.py 286): INFO [40/51]	0.0999(0.1934)	0.0002(0.0224)	1.570(1.759)	56.25(58.92)
[2023-09-29 12:31:26 4splitDomains](trainer.py 286): INFO [50/51]	0.1070(0.1761)	0.0146(0.0194)	1.682(1.752)	63.33(58.90)
[2023-09-29 12:31:26 4splitDomains](trainer.py 288): INFO  * Train Acc 58.896
[2023-09-29 12:31:29 4splitDomains](my_trainer.py 503): INFO  * Val Acc 61.463, Total time 2.25
[2023-09-29 12:31:29 4splitDomains](my_trainer.py 328): INFO Epoch:1
[2023-09-29 12:31:29 4splitDomains](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 12:31:29 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:31:29 4splitDomains](trainer.py 286): INFO [0/51]	0.8458(0.8458)	0.7427(0.7427)	0.800(0.800)	90.62(90.62)
[2023-09-29 12:31:31 4splitDomains](trainer.py 286): INFO [10/51]	0.0990(0.1793)	0.0003(0.0810)	1.221(0.829)	71.88(79.26)
[2023-09-29 12:31:32 4splitDomains](trainer.py 286): INFO [20/51]	0.0973(0.1471)	0.0003(0.0483)	1.385(0.865)	71.88(77.83)
[2023-09-29 12:31:33 4splitDomains](trainer.py 286): INFO [30/51]	0.0972(0.1342)	0.0003(0.0357)	0.818(0.885)	75.00(75.91)
[2023-09-29 12:31:34 4splitDomains](trainer.py 286): INFO [40/51]	0.0972(0.1318)	0.0002(0.0333)	1.043(0.879)	75.00(75.84)
[2023-09-29 12:31:35 4splitDomains](trainer.py 286): INFO [50/51]	0.0918(0.1269)	0.0001(0.0288)	0.613(0.883)	83.33(75.58)
[2023-09-29 12:31:35 4splitDomains](trainer.py 288): INFO  * Train Acc 75.583
[2023-09-29 12:31:37 4splitDomains](my_trainer.py 503): INFO  * Val Acc 62.927, Total time 2.26
[2023-09-29 12:31:37 4splitDomains](my_trainer.py 328): INFO Epoch:2
[2023-09-29 12:31:37 4splitDomains](my_trainer.py 335): INFO LR:0.01
[2023-09-29 12:31:37 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:31:38 4splitDomains](trainer.py 286): INFO [0/51]	0.7602(0.7602)	0.6590(0.6590)	0.587(0.587)	84.38(84.38)
[2023-09-29 12:31:39 4splitDomains](trainer.py 286): INFO [10/51]	0.0973(0.1696)	0.0002(0.0718)	0.689(0.514)	81.25(88.35)
[2023-09-29 12:31:41 4splitDomains](trainer.py 286): INFO [20/51]	0.2325(0.1479)	0.1338(0.0499)	0.472(0.497)	87.50(87.35)
[2023-09-29 12:31:42 4splitDomains](trainer.py 286): INFO [30/51]	0.1091(0.1416)	0.0003(0.0434)	0.809(0.553)	81.25(85.18)
[2023-09-29 12:31:43 4splitDomains](trainer.py 286): INFO [40/51]	0.1882(0.1368)	0.0911(0.0379)	0.483(0.548)	84.38(84.91)
[2023-09-29 12:31:44 4splitDomains](trainer.py 286): INFO [50/51]	0.0920(0.1302)	0.0001(0.0316)	0.433(0.556)	83.33(84.23)
[2023-09-29 12:31:44 4splitDomains](trainer.py 288): INFO  * Train Acc 84.233
[2023-09-29 12:31:46 4splitDomains](my_trainer.py 503): INFO  * Val Acc 61.463, Total time 2.30
[2023-09-29 12:31:46 4splitDomains](my_trainer.py 328): INFO Epoch:3
[2023-09-29 12:31:46 4splitDomains](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 12:31:46 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:31:47 4splitDomains](trainer.py 286): INFO [0/51]	0.7798(0.7798)	0.6664(0.6664)	0.309(0.309)	90.62(90.62)
[2023-09-29 12:31:48 4splitDomains](trainer.py 286): INFO [10/51]	0.0974(0.1611)	0.0003(0.0622)	0.295(0.306)	87.50(91.19)
[2023-09-29 12:31:49 4splitDomains](trainer.py 286): INFO [20/51]	0.1006(0.1365)	0.0003(0.0378)	0.370(0.314)	90.62(90.77)
[2023-09-29 12:31:51 4splitDomains](trainer.py 286): INFO [30/51]	0.0974(0.1336)	0.0003(0.0352)	0.243(0.315)	90.62(90.52)
[2023-09-29 12:31:52 4splitDomains](trainer.py 286): INFO [40/51]	0.0999(0.1286)	0.0004(0.0304)	0.281(0.311)	90.62(90.70)
[2023-09-29 12:31:53 4splitDomains](trainer.py 286): INFO [50/51]	0.0920(0.1241)	0.0001(0.0258)	0.312(0.326)	96.67(90.37)
[2023-09-29 12:31:53 4splitDomains](trainer.py 288): INFO  * Train Acc 90.368
[2023-09-29 12:31:55 4splitDomains](my_trainer.py 503): INFO  * Val Acc 63.902, Total time 2.34
[2023-09-29 12:31:55 4splitDomains](my_trainer.py 328): INFO Epoch:4
[2023-09-29 12:31:55 4splitDomains](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 12:31:55 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:31:56 4splitDomains](trainer.py 286): INFO [0/51]	0.8038(0.8038)	0.6970(0.6970)	0.290(0.290)	90.62(90.62)
[2023-09-29 12:31:57 4splitDomains](trainer.py 286): INFO [10/51]	0.0980(0.1689)	0.0002(0.0698)	0.270(0.229)	96.88(94.60)
[2023-09-29 12:31:58 4splitDomains](trainer.py 286): INFO [20/51]	0.0973(0.1385)	0.0003(0.0400)	0.112(0.214)	100.00(94.35)
[2023-09-29 12:31:59 4splitDomains](trainer.py 286): INFO [30/51]	0.0973(0.1354)	0.0002(0.0359)	0.157(0.202)	96.88(95.26)
[2023-09-29 12:32:01 4splitDomains](trainer.py 286): INFO [40/51]	0.0984(0.1282)	0.0003(0.0289)	0.312(0.200)	90.62(95.35)
[2023-09-29 12:32:02 4splitDomains](trainer.py 286): INFO [50/51]	0.0918(0.1293)	0.0001(0.0301)	0.185(0.207)	96.67(95.21)
[2023-09-29 12:32:02 4splitDomains](trainer.py 288): INFO  * Train Acc 95.215
[2023-09-29 12:32:04 4splitDomains](my_trainer.py 503): INFO  * Val Acc 66.829, Total time 2.25
[2023-09-29 12:32:04 4splitDomains](my_trainer.py 328): INFO Epoch:5
[2023-09-29 12:32:04 4splitDomains](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 12:32:04 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:32:05 4splitDomains](trainer.py 286): INFO [0/51]	0.7888(0.7888)	0.6815(0.6815)	0.207(0.207)	93.75(93.75)
[2023-09-29 12:32:06 4splitDomains](trainer.py 286): INFO [10/51]	0.0986(0.1746)	0.0002(0.0750)	0.038(0.121)	100.00(97.73)
[2023-09-29 12:32:07 4splitDomains](trainer.py 286): INFO [20/51]	0.1388(0.1401)	0.0366(0.0412)	0.027(0.109)	100.00(98.21)
[2023-09-29 12:32:09 4splitDomains](trainer.py 286): INFO [30/51]	0.0973(0.1389)	0.0003(0.0384)	0.146(0.114)	96.88(97.98)
[2023-09-29 12:32:10 4splitDomains](trainer.py 286): INFO [40/51]	0.1744(0.1325)	0.0676(0.0322)	0.204(0.119)	93.75(97.94)
[2023-09-29 12:32:11 4splitDomains](trainer.py 286): INFO [50/51]	0.0917(0.1275)	0.0001(0.0277)	0.071(0.118)	100.00(97.98)
[2023-09-29 12:32:11 4splitDomains](trainer.py 288): INFO  * Train Acc 97.975
[2023-09-29 12:32:13 4splitDomains](my_trainer.py 503): INFO  * Val Acc 68.293, Total time 2.29
[2023-09-29 12:32:13 4splitDomains](my_trainer.py 328): INFO Epoch:6
[2023-09-29 12:32:13 4splitDomains](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 12:32:13 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:32:14 4splitDomains](trainer.py 286): INFO [0/51]	0.7346(0.7346)	0.6291(0.6291)	0.108(0.108)	96.88(96.88)
[2023-09-29 12:32:15 4splitDomains](trainer.py 286): INFO [10/51]	0.1030(0.1775)	0.0003(0.0791)	0.046(0.095)	100.00(98.01)
[2023-09-29 12:32:16 4splitDomains](trainer.py 286): INFO [20/51]	0.2035(0.1547)	0.1050(0.0561)	0.062(0.091)	100.00(98.36)
[2023-09-29 12:32:18 4splitDomains](trainer.py 286): INFO [30/51]	0.0989(0.1402)	0.0002(0.0419)	0.091(0.082)	100.00(98.49)
[2023-09-29 12:32:19 4splitDomains](trainer.py 286): INFO [40/51]	0.1082(0.1333)	0.0110(0.0347)	0.054(0.078)	100.00(98.70)
[2023-09-29 12:32:20 4splitDomains](trainer.py 286): INFO [50/51]	0.0921(0.1289)	0.0001(0.0307)	0.073(0.079)	96.67(98.77)
[2023-09-29 12:32:20 4splitDomains](trainer.py 288): INFO  * Train Acc 98.773
[2023-09-29 12:32:22 4splitDomains](my_trainer.py 503): INFO  * Val Acc 69.268, Total time 2.24
[2023-09-29 12:32:22 4splitDomains](my_trainer.py 328): INFO Epoch:7
[2023-09-29 12:32:22 4splitDomains](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 12:32:22 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:32:23 4splitDomains](trainer.py 286): INFO [0/51]	0.9448(0.9448)	0.8388(0.8388)	0.046(0.046)	96.88(96.88)
[2023-09-29 12:32:24 4splitDomains](trainer.py 286): INFO [10/51]	0.0978(0.1790)	0.0003(0.0791)	0.034(0.071)	100.00(98.86)
[2023-09-29 12:32:25 4splitDomains](trainer.py 286): INFO [20/51]	0.0973(0.1436)	0.0003(0.0446)	0.086(0.072)	96.88(98.81)
[2023-09-29 12:32:26 4splitDomains](trainer.py 286): INFO [30/51]	0.0974(0.1316)	0.0003(0.0328)	0.068(0.071)	100.00(98.69)
[2023-09-29 12:32:27 4splitDomains](trainer.py 286): INFO [40/51]	0.1022(0.1271)	0.0003(0.0276)	0.086(0.068)	96.88(98.78)
[2023-09-29 12:32:28 4splitDomains](trainer.py 286): INFO [50/51]	0.0922(0.1247)	0.0001(0.0257)	0.032(0.067)	100.00(98.77)
[2023-09-29 12:32:29 4splitDomains](trainer.py 288): INFO  * Train Acc 98.773
[2023-09-29 12:32:31 4splitDomains](my_trainer.py 503): INFO  * Val Acc 66.341, Total time 2.27
[2023-09-29 12:32:31 4splitDomains](my_trainer.py 328): INFO Epoch:8
[2023-09-29 12:32:31 4splitDomains](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 12:32:31 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:32:32 4splitDomains](trainer.py 286): INFO [0/51]	0.7544(0.7544)	0.6545(0.6545)	0.133(0.133)	96.88(96.88)
[2023-09-29 12:32:33 4splitDomains](trainer.py 286): INFO [10/51]	0.0985(0.1782)	0.0002(0.0801)	0.100(0.063)	100.00(99.72)
[2023-09-29 12:32:34 4splitDomains](trainer.py 286): INFO [20/51]	0.1394(0.1468)	0.0310(0.0476)	0.031(0.059)	100.00(99.55)
[2023-09-29 12:32:35 4splitDomains](trainer.py 286): INFO [30/51]	0.0973(0.1345)	0.0002(0.0357)	0.076(0.061)	100.00(99.29)
[2023-09-29 12:32:36 4splitDomains](trainer.py 286): INFO [40/51]	0.1077(0.1306)	0.0102(0.0319)	0.034(0.061)	100.00(99.16)
[2023-09-29 12:32:37 4splitDomains](trainer.py 286): INFO [50/51]	0.0921(0.1266)	0.0001(0.0281)	0.056(0.058)	100.00(99.26)
[2023-09-29 12:32:37 4splitDomains](trainer.py 288): INFO  * Train Acc 99.264
[2023-09-29 12:32:40 4splitDomains](my_trainer.py 503): INFO  * Val Acc 68.293, Total time 2.27
[2023-09-29 12:32:40 4splitDomains](my_trainer.py 328): INFO Epoch:9
[2023-09-29 12:32:40 4splitDomains](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 12:32:40 4splitDomains](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:32:41 4splitDomains](trainer.py 286): INFO [0/51]	0.7871(0.7871)	0.6862(0.6862)	0.021(0.021)	100.00(100.00)
[2023-09-29 12:32:42 4splitDomains](trainer.py 286): INFO [10/51]	0.0975(0.1725)	0.0002(0.0745)	0.030(0.064)	100.00(99.15)
[2023-09-29 12:32:43 4splitDomains](trainer.py 286): INFO [20/51]	0.0980(0.1409)	0.0002(0.0426)	0.030(0.061)	100.00(98.96)
[2023-09-29 12:32:44 4splitDomains](trainer.py 286): INFO [30/51]	0.0976(0.1401)	0.0004(0.0406)	0.032(0.059)	100.00(99.19)
[2023-09-29 12:32:45 4splitDomains](trainer.py 286): INFO [40/51]	0.1197(0.1330)	0.0003(0.0329)	0.084(0.056)	96.88(99.24)
[2023-09-29 12:32:47 4splitDomains](trainer.py 286): INFO [50/51]	0.0921(0.1331)	0.0001(0.0336)	0.043(0.056)	100.00(99.26)
[2023-09-29 12:32:47 4splitDomains](trainer.py 288): INFO  * Train Acc 99.264
[2023-09-29 12:32:49 4splitDomains](my_trainer.py 503): INFO  * Val Acc 68.780, Total time 2.30
[2023-09-29 12:32:49 4splitDomains](my_trainer.py 206): INFO Pruning for task3
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 294/2352 (12.50%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 435/1024 (42.48%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 5004/9216 (54.30%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 2644/4096 (64.55%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 2180/4096 (53.22%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 2644/4096 (64.55%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 2304/9216 (25.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 512/4096 (12.50%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 512/4096 (12.50%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 1/9216 (0.01%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 1/4096 (0.02%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 1/8192 (0.01%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 1/36864 (0.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 8064/16384 (49.22%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 16128/32768 (49.22%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 8064/16384 (49.22%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 1152/36864 (3.12%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 1346/16384 (8.22%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 896/16384 (5.47%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 1/36864 (0.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 1/16384 (0.01%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 1/16384 (0.01%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 1/36864 (0.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 1/16384 (0.01%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 1/32768 (0.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 1/147456 (0.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 10240/65536 (15.62%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 20480/131072 (15.62%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 10240/65536 (15.62%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 1/147456 (0.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 1280/65536 (1.95%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 1280/65536 (1.95%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 1/147456 (0.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 256/65536 (0.39%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 256/65536 (0.39%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 1/147456 (0.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 256/65536 (0.39%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 256/65536 (0.39%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 1/147456 (0.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 1/65536 (0.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 1/65536 (0.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 1/147456 (0.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 1/65536 (0.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 1/131072 (0.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 1/589824 (0.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 1/262144 (0.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 1/524288 (0.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 1/262144 (0.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 1/589824 (0.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 1/262144 (0.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 1/262144 (0.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 1/589824 (0.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 1/262144 (0.00%) (Total in layer: 1048576)
Layer #last.All, pruned 1/30720 (0.00%) (Total in layer: 122880)
[2023-09-29 12:32:49 4splitDomains](my_trainer.py 298): INFO start retrain model
[2023-09-29 12:32:49 4splitDomains](my_trainer.py 302): INFO Epoch:0
[2023-09-29 12:32:49 4splitDomains](my_trainer.py 308): INFO LR:0.01
[2023-09-29 12:32:49 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:32:50 4splitDomains](trainer.py 286): INFO [0/51]	0.7633(0.7633)	0.6508(0.6508)	0.055(0.055)	96.88(96.88)
[2023-09-29 12:32:51 4splitDomains](trainer.py 286): INFO [10/51]	0.0976(0.1590)	0.0003(0.0594)	0.102(0.063)	96.88(98.01)
[2023-09-29 12:32:52 4splitDomains](trainer.py 286): INFO [20/51]	0.1788(0.1367)	0.0705(0.0370)	0.133(0.073)	93.75(97.77)
[2023-09-29 12:32:53 4splitDomains](trainer.py 286): INFO [30/51]	0.0974(0.1289)	0.0002(0.0297)	0.049(0.068)	100.00(98.29)
[2023-09-29 12:32:54 4splitDomains](trainer.py 286): INFO [40/51]	0.2074(0.1267)	0.1100(0.0273)	0.095(0.076)	93.75(98.17)
[2023-09-29 12:32:55 4splitDomains](trainer.py 286): INFO [50/51]	0.0921(0.1257)	0.0001(0.0265)	0.058(0.078)	100.00(98.28)
[2023-09-29 12:32:56 4splitDomains](trainer.py 288): INFO  * Train Acc 98.282
[2023-09-29 12:32:58 4splitDomains](my_trainer.py 503): INFO  * Val Acc 69.268, Total time 2.29
[2023-09-29 12:32:58 4splitDomains](my_trainer.py 302): INFO Epoch:1
[2023-09-29 12:32:58 4splitDomains](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 12:32:58 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:32:59 4splitDomains](trainer.py 286): INFO [0/51]	0.8806(0.8806)	0.7796(0.7796)	0.016(0.016)	100.00(100.00)
[2023-09-29 12:33:00 4splitDomains](trainer.py 286): INFO [10/51]	0.0975(0.1927)	0.0003(0.0927)	0.085(0.049)	96.88(99.43)
[2023-09-29 12:33:01 4splitDomains](trainer.py 286): INFO [20/51]	0.0975(0.1508)	0.0002(0.0510)	0.036(0.064)	100.00(99.11)
[2023-09-29 12:33:02 4splitDomains](trainer.py 286): INFO [30/51]	0.1047(0.1416)	0.0002(0.0421)	0.051(0.068)	100.00(98.79)
[2023-09-29 12:33:03 4splitDomains](trainer.py 286): INFO [40/51]	0.0976(0.1338)	0.0003(0.0347)	0.025(0.071)	100.00(98.70)
[2023-09-29 12:33:05 4splitDomains](trainer.py 286): INFO [50/51]	0.0923(0.1320)	0.0001(0.0330)	0.047(0.073)	100.00(98.65)
[2023-09-29 12:33:05 4splitDomains](trainer.py 288): INFO  * Train Acc 98.650
[2023-09-29 12:33:07 4splitDomains](my_trainer.py 503): INFO  * Val Acc 65.366, Total time 2.30
[2023-09-29 12:33:07 4splitDomains](my_trainer.py 302): INFO Epoch:2
[2023-09-29 12:33:07 4splitDomains](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 12:33:07 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:33:08 4splitDomains](trainer.py 286): INFO [0/51]	0.7642(0.7642)	0.6333(0.6333)	0.079(0.079)	96.88(96.88)
[2023-09-29 12:33:09 4splitDomains](trainer.py 286): INFO [10/51]	0.0975(0.1711)	0.0002(0.0702)	0.096(0.067)	96.88(98.30)
[2023-09-29 12:33:10 4splitDomains](trainer.py 286): INFO [20/51]	0.1638(0.1447)	0.0664(0.0450)	0.028(0.059)	100.00(98.81)
[2023-09-29 12:33:11 4splitDomains](trainer.py 286): INFO [30/51]	0.0976(0.1334)	0.0002(0.0335)	0.025(0.051)	100.00(99.19)
[2023-09-29 12:33:12 4splitDomains](trainer.py 286): INFO [40/51]	0.0980(0.1285)	0.0006(0.0289)	0.036(0.055)	100.00(99.01)
[2023-09-29 12:33:13 4splitDomains](trainer.py 286): INFO [50/51]	0.0920(0.1242)	0.0001(0.0250)	0.057(0.058)	96.67(98.71)
[2023-09-29 12:33:13 4splitDomains](trainer.py 288): INFO  * Train Acc 98.712
[2023-09-29 12:33:16 4splitDomains](my_trainer.py 503): INFO  * Val Acc 66.829, Total time 2.24
[2023-09-29 12:33:16 4splitDomains](my_trainer.py 302): INFO Epoch:3
[2023-09-29 12:33:16 4splitDomains](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 12:33:16 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:33:16 4splitDomains](trainer.py 286): INFO [0/51]	0.7387(0.7387)	0.6287(0.6287)	0.082(0.082)	100.00(100.00)
[2023-09-29 12:33:18 4splitDomains](trainer.py 286): INFO [10/51]	0.0975(0.1625)	0.0002(0.0641)	0.043(0.033)	100.00(100.00)
[2023-09-29 12:33:19 4splitDomains](trainer.py 286): INFO [20/51]	0.1035(0.1398)	0.0003(0.0404)	0.094(0.039)	100.00(99.55)
[2023-09-29 12:33:20 4splitDomains](trainer.py 286): INFO [30/51]	0.1833(0.1358)	0.0701(0.0360)	0.057(0.038)	100.00(99.60)
[2023-09-29 12:33:21 4splitDomains](trainer.py 286): INFO [40/51]	0.0978(0.1308)	0.0003(0.0311)	0.051(0.043)	96.88(99.31)
[2023-09-29 12:33:22 4splitDomains](trainer.py 286): INFO [50/51]	0.0923(0.1283)	0.0001(0.0285)	0.035(0.047)	100.00(99.20)
[2023-09-29 12:33:22 4splitDomains](trainer.py 288): INFO  * Train Acc 99.202
[2023-09-29 12:33:25 4splitDomains](my_trainer.py 503): INFO  * Val Acc 65.366, Total time 2.26
[2023-09-29 12:33:25 4splitDomains](my_trainer.py 302): INFO Epoch:4
[2023-09-29 12:33:25 4splitDomains](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 12:33:25 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:33:25 4splitDomains](trainer.py 286): INFO [0/51]	0.7158(0.7158)	0.6160(0.6160)	0.061(0.061)	100.00(100.00)
[2023-09-29 12:33:26 4splitDomains](trainer.py 286): INFO [10/51]	0.0976(0.1672)	0.0003(0.0697)	0.012(0.070)	100.00(98.58)
[2023-09-29 12:33:28 4splitDomains](trainer.py 286): INFO [20/51]	0.2860(0.1463)	0.1861(0.0484)	0.019(0.050)	100.00(99.26)
[2023-09-29 12:33:29 4splitDomains](trainer.py 286): INFO [30/51]	0.1103(0.1385)	0.0002(0.0396)	0.014(0.047)	100.00(99.19)
[2023-09-29 12:33:30 4splitDomains](trainer.py 286): INFO [40/51]	0.1521(0.1369)	0.0546(0.0378)	0.150(0.053)	93.75(98.93)
[2023-09-29 12:33:31 4splitDomains](trainer.py 286): INFO [50/51]	0.0923(0.1328)	0.0001(0.0340)	0.031(0.053)	100.00(98.96)
[2023-09-29 12:33:32 4splitDomains](trainer.py 288): INFO  * Train Acc 98.957
[2023-09-29 12:33:34 4splitDomains](my_trainer.py 503): INFO  * Val Acc 66.341, Total time 2.28
[2023-09-29 12:33:34 4splitDomains](my_trainer.py 302): INFO Epoch:5
[2023-09-29 12:33:34 4splitDomains](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 12:33:34 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:33:35 4splitDomains](trainer.py 286): INFO [0/51]	0.7369(0.7369)	0.6258(0.6258)	0.060(0.060)	96.88(96.88)
[2023-09-29 12:33:36 4splitDomains](trainer.py 286): INFO [10/51]	0.0978(0.1737)	0.0003(0.0740)	0.018(0.041)	100.00(99.15)
[2023-09-29 12:33:37 4splitDomains](trainer.py 286): INFO [20/51]	0.1273(0.1432)	0.0196(0.0440)	0.029(0.038)	100.00(99.26)
[2023-09-29 12:33:38 4splitDomains](trainer.py 286): INFO [30/51]	0.1092(0.1307)	0.0002(0.0308)	0.023(0.035)	100.00(99.40)
[2023-09-29 12:33:39 4splitDomains](trainer.py 286): INFO [40/51]	0.1319(0.1270)	0.0346(0.0273)	0.023(0.033)	100.00(99.47)
[2023-09-29 12:33:40 4splitDomains](trainer.py 286): INFO [50/51]	0.0922(0.1247)	0.0001(0.0253)	0.139(0.037)	93.33(99.14)
[2023-09-29 12:33:40 4splitDomains](trainer.py 288): INFO  * Train Acc 99.141
[2023-09-29 12:33:43 4splitDomains](my_trainer.py 503): INFO  * Val Acc 69.268, Total time 2.25
[2023-09-29 12:33:43 4splitDomains](my_trainer.py 302): INFO Epoch:6
[2023-09-29 12:33:43 4splitDomains](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 12:33:43 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:33:43 4splitDomains](trainer.py 286): INFO [0/51]	0.7185(0.7185)	0.6131(0.6131)	0.014(0.014)	100.00(100.00)
[2023-09-29 12:33:45 4splitDomains](trainer.py 286): INFO [10/51]	0.0996(0.1777)	0.0003(0.0786)	0.023(0.029)	100.00(99.43)
[2023-09-29 12:33:46 4splitDomains](trainer.py 286): INFO [20/51]	0.1577(0.1500)	0.0601(0.0517)	0.013(0.035)	100.00(99.11)
[2023-09-29 12:33:47 4splitDomains](trainer.py 286): INFO [30/51]	0.0998(0.1412)	0.0002(0.0428)	0.008(0.037)	100.00(99.19)
[2023-09-29 12:33:48 4splitDomains](trainer.py 286): INFO [40/51]	0.0975(0.1378)	0.0003(0.0392)	0.016(0.041)	100.00(99.09)
[2023-09-29 12:33:49 4splitDomains](trainer.py 286): INFO [50/51]	0.0920(0.1349)	0.0001(0.0366)	0.072(0.042)	100.00(99.14)
[2023-09-29 12:33:50 4splitDomains](trainer.py 288): INFO  * Train Acc 99.141
[2023-09-29 12:33:52 4splitDomains](my_trainer.py 503): INFO  * Val Acc 67.805, Total time 2.27
[2023-09-29 12:33:52 4splitDomains](my_trainer.py 302): INFO Epoch:7
[2023-09-29 12:33:52 4splitDomains](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 12:33:52 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:33:53 4splitDomains](trainer.py 286): INFO [0/51]	0.7533(0.7533)	0.6414(0.6414)	0.018(0.018)	100.00(100.00)
[2023-09-29 12:33:54 4splitDomains](trainer.py 286): INFO [10/51]	0.0976(0.1613)	0.0003(0.0624)	0.033(0.020)	96.88(99.43)
[2023-09-29 12:33:55 4splitDomains](trainer.py 286): INFO [20/51]	0.1025(0.1341)	0.0049(0.0352)	0.011(0.027)	100.00(99.26)
[2023-09-29 12:33:56 4splitDomains](trainer.py 286): INFO [30/51]	0.0977(0.1342)	0.0003(0.0355)	0.107(0.029)	96.88(99.40)
[2023-09-29 12:33:57 4splitDomains](trainer.py 286): INFO [40/51]	0.1015(0.1267)	0.0003(0.0282)	0.015(0.030)	100.00(99.24)
[2023-09-29 12:33:58 4splitDomains](trainer.py 286): INFO [50/51]	0.0921(0.1225)	0.0001(0.0242)	0.020(0.028)	100.00(99.39)
[2023-09-29 12:33:58 4splitDomains](trainer.py 288): INFO  * Train Acc 99.387
[2023-09-29 12:34:00 4splitDomains](my_trainer.py 503): INFO  * Val Acc 68.780, Total time 2.29
[2023-09-29 12:34:00 4splitDomains](my_trainer.py 302): INFO Epoch:8
[2023-09-29 12:34:00 4splitDomains](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 12:34:00 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:34:01 4splitDomains](trainer.py 286): INFO [0/51]	0.8268(0.8268)	0.7262(0.7262)	0.009(0.009)	100.00(100.00)
[2023-09-29 12:34:02 4splitDomains](trainer.py 286): INFO [10/51]	0.0974(0.1842)	0.0003(0.0850)	0.005(0.023)	100.00(99.72)
[2023-09-29 12:34:04 4splitDomains](trainer.py 286): INFO [20/51]	0.1445(0.1497)	0.0234(0.0496)	0.008(0.032)	100.00(99.26)
[2023-09-29 12:34:05 4splitDomains](trainer.py 286): INFO [30/51]	0.1720(0.1418)	0.0733(0.0414)	0.016(0.028)	100.00(99.40)
[2023-09-29 12:34:06 4splitDomains](trainer.py 286): INFO [40/51]	0.3287(0.1414)	0.2313(0.0415)	0.012(0.029)	100.00(99.39)
[2023-09-29 12:34:07 4splitDomains](trainer.py 286): INFO [50/51]	0.0924(0.1339)	0.0001(0.0344)	0.025(0.029)	100.00(99.45)
[2023-09-29 12:34:07 4splitDomains](trainer.py 288): INFO  * Train Acc 99.448
[2023-09-29 12:34:10 4splitDomains](my_trainer.py 503): INFO  * Val Acc 67.805, Total time 2.43
[2023-09-29 12:34:10 4splitDomains](my_trainer.py 302): INFO Epoch:9
[2023-09-29 12:34:10 4splitDomains](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 12:34:10 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:34:11 4splitDomains](trainer.py 286): INFO [0/51]	0.8454(0.8454)	0.7437(0.7437)	0.007(0.007)	100.00(100.00)
[2023-09-29 12:34:12 4splitDomains](trainer.py 286): INFO [10/51]	0.0974(0.1789)	0.0003(0.0794)	0.022(0.025)	100.00(99.43)
[2023-09-29 12:34:13 4splitDomains](trainer.py 286): INFO [20/51]	0.1674(0.1514)	0.0700(0.0516)	0.009(0.018)	100.00(99.70)
[2023-09-29 12:34:14 4splitDomains](trainer.py 286): INFO [30/51]	0.0979(0.1443)	0.0003(0.0444)	0.015(0.022)	100.00(99.60)
[2023-09-29 12:34:16 4splitDomains](trainer.py 286): INFO [40/51]	0.1159(0.1391)	0.0186(0.0396)	0.016(0.027)	100.00(99.54)
[2023-09-29 12:34:17 4splitDomains](trainer.py 286): INFO [50/51]	0.0921(0.1331)	0.0001(0.0335)	0.032(0.028)	100.00(99.57)
[2023-09-29 12:34:17 4splitDomains](trainer.py 288): INFO  * Train Acc 99.571
[2023-09-29 12:34:19 4splitDomains](my_trainer.py 503): INFO  * Val Acc 67.805, Total time 2.27
[2023-09-29 12:34:19 4splitDomains](my_trainer.py 302): INFO Epoch:10
[2023-09-29 12:34:19 4splitDomains](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 12:34:19 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:34:20 4splitDomains](trainer.py 286): INFO [0/51]	0.7581(0.7581)	0.6396(0.6396)	0.007(0.007)	100.00(100.00)
[2023-09-29 12:34:21 4splitDomains](trainer.py 286): INFO [10/51]	0.0974(0.1789)	0.0003(0.0798)	0.029(0.014)	100.00(100.00)
[2023-09-29 12:34:22 4splitDomains](trainer.py 286): INFO [20/51]	0.1471(0.1543)	0.0496(0.0550)	0.030(0.020)	100.00(99.70)
[2023-09-29 12:34:23 4splitDomains](trainer.py 286): INFO [30/51]	0.0988(0.1408)	0.0002(0.0419)	0.019(0.019)	100.00(99.80)
[2023-09-29 12:34:24 4splitDomains](trainer.py 286): INFO [40/51]	0.1228(0.1318)	0.0252(0.0329)	0.012(0.022)	100.00(99.70)
[2023-09-29 12:34:26 4splitDomains](trainer.py 286): INFO [50/51]	0.0922(0.1284)	0.0001(0.0297)	0.015(0.021)	100.00(99.69)
[2023-09-29 12:34:26 4splitDomains](trainer.py 288): INFO  * Train Acc 99.693
[2023-09-29 12:34:28 4splitDomains](my_trainer.py 503): INFO  * Val Acc 67.805, Total time 2.27
[2023-09-29 12:34:28 4splitDomains](my_trainer.py 302): INFO Epoch:11
[2023-09-29 12:34:28 4splitDomains](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 12:34:28 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:34:29 4splitDomains](trainer.py 286): INFO [0/51]	0.7783(0.7783)	0.6648(0.6648)	0.018(0.018)	100.00(100.00)
[2023-09-29 12:34:30 4splitDomains](trainer.py 286): INFO [10/51]	0.0977(0.1666)	0.0003(0.0680)	0.013(0.018)	100.00(99.72)
[2023-09-29 12:34:31 4splitDomains](trainer.py 286): INFO [20/51]	0.0975(0.1466)	0.0003(0.0476)	0.031(0.016)	100.00(99.85)
[2023-09-29 12:34:32 4splitDomains](trainer.py 286): INFO [30/51]	0.1075(0.1339)	0.0003(0.0344)	0.021(0.016)	100.00(99.80)
[2023-09-29 12:34:33 4splitDomains](trainer.py 286): INFO [40/51]	0.2108(0.1304)	0.1135(0.0310)	0.010(0.017)	100.00(99.77)
[2023-09-29 12:34:34 4splitDomains](trainer.py 286): INFO [50/51]	0.0922(0.1273)	0.0001(0.0283)	0.005(0.019)	100.00(99.69)
[2023-09-29 12:34:34 4splitDomains](trainer.py 288): INFO  * Train Acc 99.693
[2023-09-29 12:34:37 4splitDomains](my_trainer.py 503): INFO  * Val Acc 68.780, Total time 2.27
[2023-09-29 12:34:37 4splitDomains](my_trainer.py 302): INFO Epoch:12
[2023-09-29 12:34:37 4splitDomains](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 12:34:37 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:34:38 4splitDomains](trainer.py 286): INFO [0/51]	0.7606(0.7606)	0.6428(0.6428)	0.023(0.023)	100.00(100.00)
[2023-09-29 12:34:39 4splitDomains](trainer.py 286): INFO [10/51]	0.0976(0.1633)	0.0002(0.0619)	0.011(0.021)	100.00(99.43)
[2023-09-29 12:34:40 4splitDomains](trainer.py 286): INFO [20/51]	0.1017(0.1411)	0.0004(0.0398)	0.006(0.020)	100.00(99.55)
[2023-09-29 12:34:41 4splitDomains](trainer.py 286): INFO [30/51]	0.0976(0.1339)	0.0002(0.0336)	0.007(0.018)	100.00(99.60)
[2023-09-29 12:34:42 4splitDomains](trainer.py 286): INFO [40/51]	0.1634(0.1288)	0.0660(0.0287)	0.013(0.019)	100.00(99.54)
[2023-09-29 12:34:43 4splitDomains](trainer.py 286): INFO [50/51]	0.0923(0.1270)	0.0001(0.0272)	0.074(0.020)	96.67(99.45)
[2023-09-29 12:34:43 4splitDomains](trainer.py 288): INFO  * Train Acc 99.448
[2023-09-29 12:34:46 4splitDomains](my_trainer.py 503): INFO  * Val Acc 69.756, Total time 2.28
[2023-09-29 12:34:46 4splitDomains](my_trainer.py 302): INFO Epoch:13
[2023-09-29 12:34:46 4splitDomains](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 12:34:46 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:34:46 4splitDomains](trainer.py 286): INFO [0/51]	0.7270(0.7270)	0.6029(0.6029)	0.010(0.010)	100.00(100.00)
[2023-09-29 12:34:48 4splitDomains](trainer.py 286): INFO [10/51]	0.0972(0.1688)	0.0002(0.0683)	0.015(0.017)	100.00(99.72)
[2023-09-29 12:34:49 4splitDomains](trainer.py 286): INFO [20/51]	0.1814(0.1435)	0.0794(0.0437)	0.007(0.014)	100.00(99.85)
[2023-09-29 12:34:50 4splitDomains](trainer.py 286): INFO [30/51]	0.0993(0.1335)	0.0003(0.0342)	0.013(0.015)	100.00(99.80)
[2023-09-29 12:34:51 4splitDomains](trainer.py 286): INFO [40/51]	0.0976(0.1273)	0.0003(0.0283)	0.017(0.015)	100.00(99.85)
[2023-09-29 12:34:52 4splitDomains](trainer.py 286): INFO [50/51]	0.0921(0.1236)	0.0001(0.0248)	0.052(0.015)	100.00(99.88)
[2023-09-29 12:34:52 4splitDomains](trainer.py 288): INFO  * Train Acc 99.877
[2023-09-29 12:34:54 4splitDomains](my_trainer.py 503): INFO  * Val Acc 69.756, Total time 2.27
[2023-09-29 12:34:54 4splitDomains](my_trainer.py 302): INFO Epoch:14
[2023-09-29 12:34:54 4splitDomains](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 12:34:54 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:34:55 4splitDomains](trainer.py 286): INFO [0/51]	0.7426(0.7426)	0.6125(0.6125)	0.019(0.019)	100.00(100.00)
[2023-09-29 12:34:56 4splitDomains](trainer.py 286): INFO [10/51]	0.0979(0.1568)	0.0003(0.0560)	0.004(0.020)	100.00(99.43)
[2023-09-29 12:34:57 4splitDomains](trainer.py 286): INFO [20/51]	0.1934(0.1396)	0.0880(0.0387)	0.006(0.021)	100.00(99.40)
[2023-09-29 12:34:58 4splitDomains](trainer.py 286): INFO [30/51]	0.0976(0.1284)	0.0003(0.0282)	0.008(0.018)	100.00(99.60)
[2023-09-29 12:35:00 4splitDomains](trainer.py 286): INFO [40/51]	0.1311(0.1265)	0.0337(0.0267)	0.017(0.017)	100.00(99.70)
[2023-09-29 12:35:01 4splitDomains](trainer.py 286): INFO [50/51]	0.0922(0.1255)	0.0001(0.0264)	0.009(0.017)	100.00(99.63)
[2023-09-29 12:35:01 4splitDomains](trainer.py 288): INFO  * Train Acc 99.632
[2023-09-29 12:35:03 4splitDomains](my_trainer.py 503): INFO  * Val Acc 68.293, Total time 2.28
[2023-09-29 12:35:03 4splitDomains](my_trainer.py 302): INFO Epoch:15
[2023-09-29 12:35:03 4splitDomains](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 12:35:03 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:35:04 4splitDomains](trainer.py 286): INFO [0/51]	0.7499(0.7499)	0.6450(0.6450)	0.011(0.011)	100.00(100.00)
[2023-09-29 12:35:05 4splitDomains](trainer.py 286): INFO [10/51]	0.0977(0.1834)	0.0002(0.0846)	0.026(0.013)	100.00(100.00)
[2023-09-29 12:35:06 4splitDomains](trainer.py 286): INFO [20/51]	0.0977(0.1459)	0.0003(0.0472)	0.013(0.024)	100.00(99.40)
[2023-09-29 12:35:07 4splitDomains](trainer.py 286): INFO [30/51]	0.0976(0.1350)	0.0002(0.0354)	0.007(0.022)	100.00(99.50)
[2023-09-29 12:35:09 4splitDomains](trainer.py 286): INFO [40/51]	0.0975(0.1311)	0.0003(0.0315)	0.004(0.021)	100.00(99.54)
[2023-09-29 12:35:10 4splitDomains](trainer.py 286): INFO [50/51]	0.0927(0.1282)	0.0001(0.0291)	0.028(0.021)	100.00(99.57)
[2023-09-29 12:35:10 4splitDomains](trainer.py 288): INFO  * Train Acc 99.571
[2023-09-29 12:35:12 4splitDomains](my_trainer.py 503): INFO  * Val Acc 69.268, Total time 2.27
[2023-09-29 12:35:12 4splitDomains](my_trainer.py 302): INFO Epoch:16
[2023-09-29 12:35:12 4splitDomains](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 12:35:12 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:35:13 4splitDomains](trainer.py 286): INFO [0/51]	0.7917(0.7917)	0.6790(0.6790)	0.012(0.012)	100.00(100.00)
[2023-09-29 12:35:14 4splitDomains](trainer.py 286): INFO [10/51]	0.0976(0.1641)	0.0002(0.0648)	0.007(0.009)	100.00(100.00)
[2023-09-29 12:35:15 4splitDomains](trainer.py 286): INFO [20/51]	0.2037(0.1421)	0.1063(0.0424)	0.038(0.013)	100.00(100.00)
[2023-09-29 12:35:16 4splitDomains](trainer.py 286): INFO [30/51]	0.0976(0.1317)	0.0002(0.0325)	0.020(0.013)	100.00(100.00)
[2023-09-29 12:35:17 4splitDomains](trainer.py 286): INFO [40/51]	0.1696(0.1279)	0.0703(0.0290)	0.019(0.014)	100.00(99.85)
[2023-09-29 12:35:18 4splitDomains](trainer.py 286): INFO [50/51]	0.0922(0.1248)	0.0001(0.0263)	0.015(0.014)	100.00(99.88)
[2023-09-29 12:35:19 4splitDomains](trainer.py 288): INFO  * Train Acc 99.877
[2023-09-29 12:35:21 4splitDomains](my_trainer.py 503): INFO  * Val Acc 70.732, Total time 2.28
[2023-09-29 12:35:21 4splitDomains](my_trainer.py 302): INFO Epoch:17
[2023-09-29 12:35:21 4splitDomains](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 12:35:21 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:35:22 4splitDomains](trainer.py 286): INFO [0/51]	0.7792(0.7792)	0.6576(0.6576)	0.093(0.093)	93.75(93.75)
[2023-09-29 12:35:23 4splitDomains](trainer.py 286): INFO [10/51]	0.0976(0.1747)	0.0003(0.0750)	0.055(0.022)	96.88(99.15)
[2023-09-29 12:35:24 4splitDomains](trainer.py 286): INFO [20/51]	0.1356(0.1486)	0.0376(0.0478)	0.011(0.021)	100.00(99.26)
[2023-09-29 12:35:25 4splitDomains](trainer.py 286): INFO [30/51]	0.0976(0.1390)	0.0003(0.0389)	0.007(0.022)	100.00(99.19)
[2023-09-29 12:35:26 4splitDomains](trainer.py 286): INFO [40/51]	0.0977(0.1326)	0.0002(0.0330)	0.012(0.020)	100.00(99.39)
[2023-09-29 12:35:27 4splitDomains](trainer.py 286): INFO [50/51]	0.0923(0.1291)	0.0001(0.0300)	0.010(0.019)	100.00(99.51)
[2023-09-29 12:35:27 4splitDomains](trainer.py 288): INFO  * Train Acc 99.509
[2023-09-29 12:35:30 4splitDomains](my_trainer.py 503): INFO  * Val Acc 67.317, Total time 2.25
[2023-09-29 12:35:30 4splitDomains](my_trainer.py 302): INFO Epoch:18
[2023-09-29 12:35:30 4splitDomains](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 12:35:30 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:35:31 4splitDomains](trainer.py 286): INFO [0/51]	0.7761(0.7761)	0.6748(0.6748)	0.025(0.025)	100.00(100.00)
[2023-09-29 12:35:32 4splitDomains](trainer.py 286): INFO [10/51]	0.0977(0.1660)	0.0003(0.0664)	0.009(0.017)	100.00(99.72)
[2023-09-29 12:35:33 4splitDomains](trainer.py 286): INFO [20/51]	0.1200(0.1412)	0.0112(0.0407)	0.018(0.017)	100.00(99.55)
[2023-09-29 12:35:34 4splitDomains](trainer.py 286): INFO [30/51]	0.1029(0.1314)	0.0002(0.0312)	0.007(0.020)	100.00(99.40)
[2023-09-29 12:35:35 4splitDomains](trainer.py 286): INFO [40/51]	0.0976(0.1293)	0.0002(0.0287)	0.011(0.019)	100.00(99.47)
[2023-09-29 12:35:36 4splitDomains](trainer.py 286): INFO [50/51]	0.0923(0.1267)	0.0001(0.0268)	0.010(0.018)	100.00(99.57)
[2023-09-29 12:35:36 4splitDomains](trainer.py 288): INFO  * Train Acc 99.571
[2023-09-29 12:35:39 4splitDomains](my_trainer.py 503): INFO  * Val Acc 70.244, Total time 2.27
[2023-09-29 12:35:39 4splitDomains](my_trainer.py 302): INFO Epoch:19
[2023-09-29 12:35:39 4splitDomains](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 12:35:39 4splitDomains](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:35:39 4splitDomains](trainer.py 286): INFO [0/51]	0.6975(0.6975)	0.5906(0.5906)	0.019(0.019)	100.00(100.00)
[2023-09-29 12:35:40 4splitDomains](trainer.py 286): INFO [10/51]	0.0991(0.1569)	0.0003(0.0583)	0.006(0.021)	100.00(99.43)
[2023-09-29 12:35:42 4splitDomains](trainer.py 286): INFO [20/51]	0.1623(0.1380)	0.0628(0.0398)	0.007(0.017)	100.00(99.70)
[2023-09-29 12:35:43 4splitDomains](trainer.py 286): INFO [30/51]	0.1154(0.1319)	0.0002(0.0333)	0.011(0.018)	100.00(99.70)
[2023-09-29 12:35:44 4splitDomains](trainer.py 286): INFO [40/51]	0.1395(0.1282)	0.0403(0.0297)	0.011(0.017)	100.00(99.70)
[2023-09-29 12:35:45 4splitDomains](trainer.py 286): INFO [50/51]	0.0925(0.1254)	0.0001(0.0270)	0.008(0.016)	100.00(99.75)
[2023-09-29 12:35:45 4splitDomains](trainer.py 288): INFO  * Train Acc 99.755
[2023-09-29 12:35:47 4splitDomains](my_trainer.py 503): INFO  * Val Acc 70.244, Total time 2.28
=> Saving model to: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-3.pth
=> Save Done
[2023-09-29 12:35:48 4splitDomains](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 12:35:52 4splitDomains](my_trainer.py 503): INFO  * Val Acc 86.737, Total time 4.05
[2023-09-29 12:35:52 4splitDomains](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 12:35:56 4splitDomains](my_trainer.py 503): INFO  * Val Acc 78.042, Total time 4.00
[2023-09-29 12:35:56 4splitDomains](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 12:36:01 4splitDomains](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 5.35
[2023-09-29 12:36:01 4splitDomains](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 12:36:04 4splitDomains](my_trainer.py 503): INFO  * Val Acc 70.732, Total time 2.71
[2023-09-29 12:36:04 4splitDomains](trainer.py 335): INFO saving storage...
[2023-09-29 12:36:04 4splitDomains](trainer.py 341): INFO done
[2023-09-29 12:36:04 4splitDomains](iBatchLearn.py 155): INFO Acc:78.87785912764375; BWT:-0.8844035796697133;
=> merge config from utils/user_4splitDomains.yaml
=> merge config from ../official_eva/configs/4splitDomains.yaml
[2023-09-29 12:36:07 4splitDomains](iBatchLearn.py 230): INFO Full config saved to outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:36:07 4splitDomains](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 4splitDomains
  NUM_CLASSES: 60
  NUM_TASKS: 4
  NUM_WORKERS: 4
  ROOT: input/contest_data/4splitDomains
DOMAIN_INCR: true
GPUID:
- 0
LOGGER_PATH: outputs/4splitDomains/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: false

[2023-09-29 12:36:07 4splitDomains](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/4splitDomains.yaml", "user_cfg": "utils/user_4splitDomains.yaml", "test": true, "task_count": 3, "init_path": "input/init_models/4splitDomains.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-3.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/4splitDomains/storage-3.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_3.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:36:08 4splitDomains](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-3.pth
[2023-09-29 12:36:08 4splitDomains](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:36:10 4splitDomains](my_trainer.py 67): INFO load storage...
[2023-09-29 12:36:10 4splitDomains](my_trainer.py 71): INFO done
[2023-09-29 12:36:10 4splitDomains](my_trainer.py 64): INFO tensor([[2, 2, 2, 2, 2, 2, 2],
        [4, 4, 4, 4, 4, 3, 3],
        [3, 2, 2, 4, 1, 3, 3],
        [4, 4, 4, 4, 3, 3, 3],
        [4, 4, 4, 4, 1, 3, 3],
        [2, 4, 4, 4, 3, 3, 3],
        [2, 4, 4, 3, 3, 3, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 12:36:10 4splitDomains](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (All): Linear(in_features=2048, out_features=60, bias=False)
  )
)
[2023-09-29 12:36:10 4splitDomains](iBatchLearn.py 58): INFO #parameter of model:23630912
[2023-09-29 12:36:10 4splitDomains](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 12:36:21 4splitDomains](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 12:36:28 4splitDomains](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 12:36:37 4splitDomains](iBatchLearn.py 167): INFO test split name:3
--------------------------------Official Evaluation--------------------------------
3 80.42081781223662
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 12:36:47 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:36:47 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 12:36:47 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 0, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "input/init_models/10splitTasks.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-0.pth", "storage_path": "None", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-0.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:36:47 10splitTasks](my_trainer.py 108): INFO => Load model weights: input/init_models/10splitTasks.pth
[2023-09-29 12:36:48 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:36:50 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 12:36:50 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 12:36:50 10splitTasks](iBatchLearn.py 92): INFO ====================== 0 =======================
[2023-09-29 12:36:50 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 12:36:50 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 12:36:50 10splitTasks](my_trainer.py 335): INFO LR:0.0020008000000000005
[2023-09-29 12:36:50 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:36:53 10splitTasks](trainer.py 286): INFO [0/157]	3.5043(3.5043)	0.5383(0.5383)	2.617(2.617)	6.25(6.25)
[2023-09-29 12:36:54 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.4118)	0.0002(0.0492)	2.671(2.479)	15.62(12.78)
[2023-09-29 12:36:55 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.2648)	0.0002(0.0259)	2.550(2.481)	12.50(13.69)
[2023-09-29 12:36:57 10splitTasks](trainer.py 286): INFO [30/157]	0.1164(0.2141)	0.0002(0.0176)	2.511(2.471)	12.50(14.72)
[2023-09-29 12:36:58 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1869)	0.0003(0.0134)	2.339(2.455)	9.38(14.86)
[2023-09-29 12:36:59 10splitTasks](trainer.py 286): INFO [50/157]	0.1012(0.1702)	0.0003(0.0108)	2.202(2.464)	21.88(15.07)
[2023-09-29 12:37:00 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1592)	0.0002(0.0091)	2.057(2.486)	18.75(15.47)
[2023-09-29 12:37:01 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1512)	0.0003(0.0079)	2.156(2.463)	25.00(16.15)
[2023-09-29 12:37:02 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1454)	0.0003(0.0069)	2.159(2.451)	25.00(16.28)
[2023-09-29 12:37:03 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1406)	0.0002(0.0062)	2.162(2.457)	15.62(16.66)
[2023-09-29 12:37:04 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1368)	0.0002(0.0056)	2.405(2.439)	15.62(17.36)
[2023-09-29 12:37:05 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1337)	0.0002(0.0051)	2.339(2.421)	18.75(17.82)
[2023-09-29 12:37:06 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1311)	0.0003(0.0047)	2.345(2.412)	15.62(18.23)
[2023-09-29 12:37:07 10splitTasks](trainer.py 286): INFO [130/157]	0.1050(0.1291)	0.0005(0.0044)	2.029(2.398)	25.00(18.63)
[2023-09-29 12:37:08 10splitTasks](trainer.py 286): INFO [140/157]	0.1035(0.1272)	0.0002(0.0041)	1.917(2.384)	34.38(18.79)
[2023-09-29 12:37:09 10splitTasks](trainer.py 286): INFO [150/157]	0.1003(0.1255)	0.0001(0.0039)	1.970(2.361)	21.88(19.29)
[2023-09-29 12:37:09 10splitTasks](trainer.py 286): INFO [156/157]	0.0803(0.1244)	0.0001(0.0037)	1.763(2.352)	37.50(19.42)
[2023-09-29 12:37:09 10splitTasks](trainer.py 288): INFO  * Train Acc 19.420
[2023-09-29 12:37:11 10splitTasks](my_trainer.py 503): INFO  * Val Acc 25.000, Total time 1.64
[2023-09-29 12:37:11 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 12:37:11 10splitTasks](my_trainer.py 335): INFO LR:0.004000600000000001
[2023-09-29 12:37:11 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:37:12 10splitTasks](trainer.py 286): INFO [0/157]	0.6560(0.6560)	0.5498(0.5498)	1.989(1.989)	37.50(37.50)
[2023-09-29 12:37:13 10splitTasks](trainer.py 286): INFO [10/157]	0.1039(0.1535)	0.0003(0.0503)	3.247(2.186)	28.12(26.42)
[2023-09-29 12:37:14 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1290)	0.0003(0.0265)	1.725(2.314)	40.62(26.34)
[2023-09-29 12:37:15 10splitTasks](trainer.py 286): INFO [30/157]	0.1036(0.1204)	0.0002(0.0180)	1.691(2.240)	43.75(25.60)
[2023-09-29 12:37:16 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1164)	0.0002(0.0137)	1.920(2.170)	28.12(26.30)
[2023-09-29 12:37:17 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1141)	0.0002(0.0111)	1.831(2.155)	31.25(27.27)
[2023-09-29 12:37:18 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1121)	0.0002(0.0094)	2.611(2.157)	28.12(27.61)
[2023-09-29 12:37:19 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1108)	0.0002(0.0081)	2.488(2.176)	25.00(28.17)
[2023-09-29 12:37:20 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1099)	0.0003(0.0071)	1.907(2.170)	37.50(28.32)
[2023-09-29 12:37:21 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1090)	0.0003(0.0064)	1.854(2.155)	21.88(28.67)
[2023-09-29 12:37:22 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1084)	0.0003(0.0058)	1.976(2.140)	28.12(28.96)
[2023-09-29 12:37:23 10splitTasks](trainer.py 286): INFO [110/157]	0.1126(0.1079)	0.0002(0.0053)	1.796(2.111)	25.00(29.45)
[2023-09-29 12:37:24 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1075)	0.0002(0.0049)	2.007(2.082)	31.25(30.06)
[2023-09-29 12:37:25 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1073)	0.0003(0.0045)	1.706(2.081)	37.50(30.13)
[2023-09-29 12:37:26 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1070)	0.0003(0.0042)	1.924(2.072)	46.88(30.54)
[2023-09-29 12:37:27 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1067)	0.0001(0.0040)	1.634(2.060)	34.38(30.77)
[2023-09-29 12:37:28 10splitTasks](trainer.py 286): INFO [156/157]	0.0823(0.1063)	0.0001(0.0038)	1.904(2.055)	25.00(30.76)
[2023-09-29 12:37:28 10splitTasks](trainer.py 288): INFO  * Train Acc 30.760
[2023-09-29 12:37:30 10splitTasks](my_trainer.py 503): INFO  * Val Acc 33.000, Total time 1.57
[2023-09-29 12:37:30 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 12:37:30 10splitTasks](my_trainer.py 335): INFO LR:0.0060004
[2023-09-29 12:37:30 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:37:30 10splitTasks](trainer.py 286): INFO [0/157]	0.6052(0.6052)	0.4968(0.4968)	1.864(1.864)	28.12(28.12)
[2023-09-29 12:37:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1483)	0.0001(0.0454)	1.937(1.998)	28.12(31.82)
[2023-09-29 12:37:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1263)	0.0002(0.0239)	2.189(1.989)	28.12(31.10)
[2023-09-29 12:37:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1186)	0.0002(0.0163)	1.693(2.030)	34.38(30.04)
[2023-09-29 12:37:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1027(0.1148)	0.0007(0.0124)	1.900(2.034)	28.12(30.87)
[2023-09-29 12:37:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1042(0.1124)	0.0003(0.0100)	1.556(2.009)	46.88(31.56)
[2023-09-29 12:37:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1107)	0.0003(0.0084)	1.863(1.980)	37.50(32.43)
[2023-09-29 12:37:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1029(0.1096)	0.0003(0.0073)	1.505(1.934)	40.62(33.41)
[2023-09-29 12:37:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1041(0.1089)	0.0002(0.0064)	1.607(1.936)	50.00(33.95)
[2023-09-29 12:37:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1082)	0.0003(0.0058)	1.618(1.915)	46.88(34.48)
[2023-09-29 12:37:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1035(0.1078)	0.0003(0.0053)	1.672(1.914)	40.62(34.90)
[2023-09-29 12:37:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1074)	0.0003(0.0048)	1.531(1.897)	53.12(35.75)
[2023-09-29 12:37:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1071)	0.0002(0.0044)	2.112(1.877)	37.50(36.16)
[2023-09-29 12:37:44 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1068)	0.0003(0.0041)	1.597(1.870)	46.88(36.67)
[2023-09-29 12:37:45 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1065)	0.0003(0.0039)	2.061(1.870)	31.25(37.17)
[2023-09-29 12:37:46 10splitTasks](trainer.py 286): INFO [150/157]	0.1019(0.1062)	0.0001(0.0036)	1.686(1.869)	40.62(37.27)
[2023-09-29 12:37:46 10splitTasks](trainer.py 286): INFO [156/157]	0.0800(0.1059)	0.0001(0.0035)	2.156(1.860)	37.50(37.46)
[2023-09-29 12:37:46 10splitTasks](trainer.py 288): INFO  * Train Acc 37.460
[2023-09-29 12:37:48 10splitTasks](my_trainer.py 503): INFO  * Val Acc 47.000, Total time 1.64
[2023-09-29 12:37:48 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 12:37:48 10splitTasks](my_trainer.py 335): INFO LR:0.0080002
[2023-09-29 12:37:48 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:37:49 10splitTasks](trainer.py 286): INFO [0/157]	0.6202(0.6202)	0.5124(0.5124)	1.570(1.570)	40.62(40.62)
[2023-09-29 12:37:50 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1513)	0.0002(0.0469)	1.613(1.924)	40.62(44.32)
[2023-09-29 12:37:51 10splitTasks](trainer.py 286): INFO [20/157]	0.1117(0.1298)	0.0005(0.0247)	2.063(2.000)	37.50(40.48)
[2023-09-29 12:37:52 10splitTasks](trainer.py 286): INFO [30/157]	0.1053(0.1214)	0.0002(0.0169)	3.670(2.006)	34.38(39.62)
[2023-09-29 12:37:53 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1169)	0.0002(0.0128)	1.590(1.954)	40.62(40.09)
[2023-09-29 12:37:54 10splitTasks](trainer.py 286): INFO [50/157]	0.1050(0.1140)	0.0002(0.0104)	1.457(1.868)	46.88(41.18)
[2023-09-29 12:37:55 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1122)	0.0002(0.0087)	1.285(1.833)	43.75(41.55)
[2023-09-29 12:37:56 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1108)	0.0002(0.0075)	1.810(1.835)	43.75(41.73)
[2023-09-29 12:37:57 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1098)	0.0002(0.0066)	1.824(1.800)	34.38(42.21)
[2023-09-29 12:37:58 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1089)	0.0002(0.0059)	2.218(1.831)	25.00(42.14)
[2023-09-29 12:37:59 10splitTasks](trainer.py 286): INFO [100/157]	0.1032(0.1083)	0.0004(0.0054)	1.311(1.823)	59.38(42.26)
[2023-09-29 12:38:00 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1077)	0.0003(0.0049)	1.310(1.813)	56.25(42.79)
[2023-09-29 12:38:01 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1073)	0.0003(0.0045)	1.537(1.790)	43.75(43.13)
[2023-09-29 12:38:02 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1071)	0.0003(0.0042)	1.241(1.762)	62.50(43.85)
[2023-09-29 12:38:03 10splitTasks](trainer.py 286): INFO [140/157]	0.1029(0.1067)	0.0002(0.0039)	1.609(1.751)	37.50(44.37)
[2023-09-29 12:38:04 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1064)	0.0001(0.0037)	1.305(1.733)	62.50(44.87)
[2023-09-29 12:38:05 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1061)	0.0001(0.0036)	1.970(1.748)	25.00(44.94)
[2023-09-29 12:38:05 10splitTasks](trainer.py 288): INFO  * Train Acc 44.940
[2023-09-29 12:38:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 44.200, Total time 1.65
[2023-09-29 12:38:06 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 12:38:06 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 12:38:06 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:38:07 10splitTasks](trainer.py 286): INFO [0/157]	0.6591(0.6591)	0.5486(0.5486)	1.588(1.588)	43.75(43.75)
[2023-09-29 12:38:08 10splitTasks](trainer.py 286): INFO [10/157]	0.1044(0.1563)	0.0002(0.0502)	1.712(1.748)	31.25(41.19)
[2023-09-29 12:38:09 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1308)	0.0002(0.0264)	2.615(1.739)	46.88(43.90)
[2023-09-29 12:38:10 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1217)	0.0003(0.0180)	1.398(1.657)	62.50(44.76)
[2023-09-29 12:38:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1171)	0.0002(0.0137)	2.099(1.692)	34.38(44.82)
[2023-09-29 12:38:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1144)	0.0003(0.0111)	1.713(1.704)	43.75(46.02)
[2023-09-29 12:38:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1125)	0.0003(0.0094)	1.365(1.692)	50.00(46.06)
[2023-09-29 12:38:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1031(0.1114)	0.0002(0.0081)	1.655(1.675)	37.50(46.35)
[2023-09-29 12:38:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1103)	0.0003(0.0071)	1.627(1.675)	37.50(46.10)
[2023-09-29 12:38:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1045(0.1094)	0.0002(0.0064)	1.704(1.693)	37.50(45.40)
[2023-09-29 12:38:17 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1087)	0.0003(0.0058)	1.391(1.697)	56.25(44.89)
[2023-09-29 12:38:18 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1083)	0.0002(0.0053)	1.507(1.696)	56.25(44.96)
[2023-09-29 12:38:19 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1079)	0.0003(0.0049)	1.211(1.674)	40.62(45.12)
[2023-09-29 12:38:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1075)	0.0002(0.0045)	2.588(1.662)	43.75(45.40)
[2023-09-29 12:38:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1143(0.1074)	0.0006(0.0042)	2.021(1.650)	53.12(46.19)
[2023-09-29 12:38:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1071)	0.0002(0.0040)	1.325(1.630)	46.88(46.65)
[2023-09-29 12:38:23 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1067)	0.0001(0.0038)	1.539(1.622)	37.50(46.62)
[2023-09-29 12:38:23 10splitTasks](trainer.py 288): INFO  * Train Acc 46.620
[2023-09-29 12:38:25 10splitTasks](my_trainer.py 503): INFO  * Val Acc 45.000, Total time 1.58
[2023-09-29 12:38:25 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 12:38:25 10splitTasks](my_trainer.py 335): INFO LR:0.009890748929868663
[2023-09-29 12:38:25 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:38:25 10splitTasks](trainer.py 286): INFO [0/157]	0.6161(0.6161)	0.4988(0.4988)	1.267(1.267)	43.75(43.75)
[2023-09-29 12:38:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1032(0.1517)	0.0002(0.0457)	1.632(1.538)	53.12(49.15)
[2023-09-29 12:38:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1286)	0.0002(0.0240)	1.967(1.677)	31.25(47.92)
[2023-09-29 12:38:29 10splitTasks](trainer.py 286): INFO [30/157]	0.1111(0.1204)	0.0006(0.0164)	1.424(1.729)	59.38(47.28)
[2023-09-29 12:38:30 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1161)	0.0002(0.0125)	1.883(1.666)	56.25(47.56)
[2023-09-29 12:38:31 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1135)	0.0002(0.0101)	1.306(1.611)	56.25(48.77)
[2023-09-29 12:38:32 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1116)	0.0002(0.0085)	1.625(1.573)	40.62(49.33)
[2023-09-29 12:38:33 10splitTasks](trainer.py 286): INFO [70/157]	0.1057(0.1104)	0.0004(0.0073)	1.249(1.543)	56.25(50.40)
[2023-09-29 12:38:34 10splitTasks](trainer.py 286): INFO [80/157]	0.1033(0.1094)	0.0002(0.0065)	1.336(1.512)	65.62(51.47)
[2023-09-29 12:38:35 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1087)	0.0002(0.0058)	1.537(1.498)	53.12(51.96)
[2023-09-29 12:38:36 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1081)	0.0002(0.0052)	1.354(1.490)	46.88(51.95)
[2023-09-29 12:38:37 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1075)	0.0002(0.0048)	1.946(1.482)	53.12(52.22)
[2023-09-29 12:38:38 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1071)	0.0002(0.0044)	2.343(1.488)	28.12(52.20)
[2023-09-29 12:38:39 10splitTasks](trainer.py 286): INFO [130/157]	0.1074(0.1068)	0.0005(0.0041)	1.423(1.526)	50.00(51.46)
[2023-09-29 12:38:40 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1065)	0.0002(0.0038)	1.313(1.521)	65.62(51.93)
[2023-09-29 12:38:41 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1062)	0.0001(0.0036)	1.264(1.506)	59.38(52.24)
[2023-09-29 12:38:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0811(0.1059)	0.0001(0.0035)	1.622(1.512)	37.50(52.10)
[2023-09-29 12:38:41 10splitTasks](trainer.py 288): INFO  * Train Acc 52.100
[2023-09-29 12:38:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 44.400, Total time 1.69
[2023-09-29 12:38:43 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 12:38:43 10splitTasks](my_trainer.py 335): INFO LR:0.009567770515484183
[2023-09-29 12:38:43 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:38:44 10splitTasks](trainer.py 286): INFO [0/157]	0.6203(0.6203)	0.4903(0.4903)	1.393(1.393)	50.00(50.00)
[2023-09-29 12:38:45 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1539)	0.0002(0.0448)	1.252(1.705)	46.88(48.86)
[2023-09-29 12:38:46 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1291)	0.0002(0.0236)	1.583(1.587)	43.75(50.74)
[2023-09-29 12:38:47 10splitTasks](trainer.py 286): INFO [30/157]	0.1056(0.1204)	0.0002(0.0161)	2.555(1.698)	43.75(49.40)
[2023-09-29 12:38:48 10splitTasks](trainer.py 286): INFO [40/157]	0.1040(0.1167)	0.0002(0.0122)	1.552(1.780)	50.00(48.17)
[2023-09-29 12:38:49 10splitTasks](trainer.py 286): INFO [50/157]	0.1090(0.1140)	0.0002(0.0099)	1.591(1.791)	37.50(47.18)
[2023-09-29 12:38:50 10splitTasks](trainer.py 286): INFO [60/157]	0.1025(0.1124)	0.0002(0.0083)	1.471(1.748)	50.00(47.49)
[2023-09-29 12:38:51 10splitTasks](trainer.py 286): INFO [70/157]	0.1102(0.1111)	0.0004(0.0072)	1.419(1.704)	50.00(47.71)
[2023-09-29 12:38:52 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1100)	0.0002(0.0064)	1.429(1.666)	46.88(48.46)
[2023-09-29 12:38:53 10splitTasks](trainer.py 286): INFO [90/157]	0.1009(0.1092)	0.0002(0.0057)	1.549(1.655)	46.88(48.83)
[2023-09-29 12:38:54 10splitTasks](trainer.py 286): INFO [100/157]	0.1079(0.1088)	0.0003(0.0052)	0.959(1.636)	53.12(48.86)
[2023-09-29 12:38:55 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1084)	0.0004(0.0047)	1.262(1.612)	56.25(49.35)
[2023-09-29 12:38:56 10splitTasks](trainer.py 286): INFO [120/157]	0.1012(0.1079)	0.0002(0.0044)	1.105(1.581)	62.50(50.03)
[2023-09-29 12:38:57 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1075)	0.0002(0.0040)	1.160(1.563)	50.00(50.36)
[2023-09-29 12:38:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1011(0.1071)	0.0002(0.0038)	1.263(1.552)	50.00(50.69)
[2023-09-29 12:38:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1068)	0.0001(0.0036)	2.273(1.542)	46.88(50.89)
[2023-09-29 12:39:00 10splitTasks](trainer.py 286): INFO [156/157]	0.0847(0.1065)	0.0001(0.0034)	2.042(1.540)	12.50(50.82)
[2023-09-29 12:39:00 10splitTasks](trainer.py 288): INFO  * Train Acc 50.820
[2023-09-29 12:39:02 10splitTasks](my_trainer.py 503): INFO  * Val Acc 57.800, Total time 1.59
[2023-09-29 12:39:02 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 12:39:02 10splitTasks](my_trainer.py 335): INFO LR:0.00904518046337755
[2023-09-29 12:39:02 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:39:02 10splitTasks](trainer.py 286): INFO [0/157]	0.6831(0.6831)	0.5616(0.5616)	1.418(1.418)	50.00(50.00)
[2023-09-29 12:39:03 10splitTasks](trainer.py 286): INFO [10/157]	0.1037(0.1576)	0.0003(0.0513)	1.570(1.510)	46.88(53.12)
[2023-09-29 12:39:04 10splitTasks](trainer.py 286): INFO [20/157]	0.1028(0.1316)	0.0002(0.0270)	1.785(1.468)	53.12(52.38)
[2023-09-29 12:39:05 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1221)	0.0002(0.0184)	1.336(1.444)	59.38(53.12)
[2023-09-29 12:39:06 10splitTasks](trainer.py 286): INFO [40/157]	0.1046(0.1175)	0.0002(0.0140)	1.372(1.450)	53.12(53.12)
[2023-09-29 12:39:07 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1146)	0.0003(0.0113)	1.270(1.438)	50.00(53.31)
[2023-09-29 12:39:08 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1126)	0.0001(0.0095)	1.120(1.408)	68.75(54.05)
[2023-09-29 12:39:10 10splitTasks](trainer.py 286): INFO [70/157]	0.1027(0.1115)	0.0003(0.0082)	1.484(1.403)	53.12(54.45)
[2023-09-29 12:39:11 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1106)	0.0001(0.0072)	1.396(1.378)	50.00(55.21)
[2023-09-29 12:39:12 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1096)	0.0002(0.0065)	1.518(1.361)	53.12(55.39)
[2023-09-29 12:39:13 10splitTasks](trainer.py 286): INFO [100/157]	0.1039(0.1091)	0.0002(0.0059)	1.889(1.374)	46.88(55.45)
[2023-09-29 12:39:14 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1085)	0.0002(0.0054)	1.295(1.360)	62.50(55.91)
[2023-09-29 12:39:15 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1080)	0.0002(0.0049)	1.307(1.354)	59.38(56.04)
[2023-09-29 12:39:16 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1077)	0.0002(0.0046)	1.169(1.340)	59.38(56.42)
[2023-09-29 12:39:17 10splitTasks](trainer.py 286): INFO [140/157]	0.1059(0.1074)	0.0003(0.0043)	1.367(1.328)	50.00(56.67)
[2023-09-29 12:39:18 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1070)	0.0001(0.0040)	1.240(1.313)	56.25(57.14)
[2023-09-29 12:39:18 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1066)	0.0001(0.0039)	0.925(1.309)	75.00(57.22)
[2023-09-29 12:39:18 10splitTasks](trainer.py 288): INFO  * Train Acc 57.220
[2023-09-29 12:39:20 10splitTasks](my_trainer.py 503): INFO  * Val Acc 60.600, Total time 1.79
[2023-09-29 12:39:20 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 12:39:20 10splitTasks](my_trainer.py 335): INFO LR:0.008345818466491111
[2023-09-29 12:39:20 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:39:21 10splitTasks](trainer.py 286): INFO [0/157]	0.6335(0.6335)	0.5187(0.5187)	0.861(0.861)	71.88(71.88)
[2023-09-29 12:39:22 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1535)	0.0002(0.0474)	0.982(1.116)	68.75(63.07)
[2023-09-29 12:39:23 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1291)	0.0003(0.0250)	0.785(1.149)	71.88(59.23)
[2023-09-29 12:39:24 10splitTasks](trainer.py 286): INFO [30/157]	0.1048(0.1204)	0.0002(0.0170)	0.886(1.125)	68.75(60.69)
[2023-09-29 12:39:25 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1158)	0.0002(0.0129)	0.875(1.109)	65.62(61.20)
[2023-09-29 12:39:26 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1131)	0.0002(0.0104)	1.211(1.118)	65.62(60.85)
[2023-09-29 12:39:27 10splitTasks](trainer.py 286): INFO [60/157]	0.1052(0.1113)	0.0002(0.0088)	0.909(1.105)	65.62(61.58)
[2023-09-29 12:39:28 10splitTasks](trainer.py 286): INFO [70/157]	0.1068(0.1101)	0.0003(0.0076)	1.591(1.124)	43.75(61.36)
[2023-09-29 12:39:29 10splitTasks](trainer.py 286): INFO [80/157]	0.1039(0.1091)	0.0003(0.0067)	0.836(1.124)	71.88(61.50)
[2023-09-29 12:39:30 10splitTasks](trainer.py 286): INFO [90/157]	0.1025(0.1083)	0.0003(0.0060)	1.057(1.128)	71.88(61.37)
[2023-09-29 12:39:31 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1079)	0.0002(0.0054)	1.543(1.139)	50.00(61.48)
[2023-09-29 12:39:32 10splitTasks](trainer.py 286): INFO [110/157]	0.1029(0.1074)	0.0002(0.0049)	1.339(1.142)	50.00(61.26)
[2023-09-29 12:39:33 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1071)	0.0003(0.0046)	1.147(1.148)	53.12(60.92)
[2023-09-29 12:39:34 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1067)	0.0003(0.0042)	1.107(1.145)	65.62(60.73)
[2023-09-29 12:39:35 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1064)	0.0002(0.0040)	1.265(1.145)	62.50(60.93)
[2023-09-29 12:39:36 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1061)	0.0002(0.0037)	0.899(1.129)	62.50(61.49)
[2023-09-29 12:39:37 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1058)	0.0001(0.0036)	1.603(1.130)	62.50(61.58)
[2023-09-29 12:39:37 10splitTasks](trainer.py 288): INFO  * Train Acc 61.580
[2023-09-29 12:39:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 61.400, Total time 1.56
[2023-09-29 12:39:39 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 12:39:39 10splitTasks](my_trainer.py 335): INFO LR:0.007500250000000001
[2023-09-29 12:39:39 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:39:39 10splitTasks](trainer.py 286): INFO [0/157]	0.6347(0.6347)	0.5230(0.5230)	1.008(1.008)	62.50(62.50)
[2023-09-29 12:39:40 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1533)	0.0003(0.0478)	1.145(1.012)	65.62(63.64)
[2023-09-29 12:39:41 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1301)	0.0003(0.0252)	0.612(0.991)	78.12(65.03)
[2023-09-29 12:39:42 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1211)	0.0002(0.0171)	1.119(0.989)	59.38(65.32)
[2023-09-29 12:39:43 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1164)	0.0002(0.0130)	1.182(1.003)	71.88(65.85)
[2023-09-29 12:39:44 10splitTasks](trainer.py 286): INFO [50/157]	0.1022(0.1135)	0.0002(0.0105)	1.350(1.013)	62.50(65.56)
[2023-09-29 12:39:45 10splitTasks](trainer.py 286): INFO [60/157]	0.1054(0.1117)	0.0005(0.0089)	0.947(1.012)	62.50(65.11)
[2023-09-29 12:39:46 10splitTasks](trainer.py 286): INFO [70/157]	0.1028(0.1103)	0.0003(0.0077)	1.199(1.023)	46.88(65.23)
[2023-09-29 12:39:47 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1093)	0.0002(0.0067)	1.218(1.037)	65.62(64.85)
[2023-09-29 12:39:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1089)	0.0003(0.0060)	1.409(1.023)	59.38(65.38)
[2023-09-29 12:39:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1028(0.1082)	0.0002(0.0055)	0.832(1.021)	71.88(64.85)
[2023-09-29 12:39:51 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1080)	0.0002(0.0050)	0.878(1.023)	68.75(64.75)
[2023-09-29 12:39:52 10splitTasks](trainer.py 286): INFO [120/157]	0.1078(0.1075)	0.0006(0.0046)	1.101(1.036)	53.12(64.20)
[2023-09-29 12:39:53 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1071)	0.0002(0.0043)	1.093(1.029)	59.38(64.67)
[2023-09-29 12:39:54 10splitTasks](trainer.py 286): INFO [140/157]	0.1053(0.1068)	0.0002(0.0040)	1.099(1.032)	62.50(64.69)
[2023-09-29 12:39:55 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1065)	0.0001(0.0038)	1.111(1.027)	56.25(64.84)
[2023-09-29 12:39:55 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1061)	0.0001(0.0036)	1.459(1.022)	62.50(65.08)
[2023-09-29 12:39:55 10splitTasks](trainer.py 288): INFO  * Train Acc 65.080
[2023-09-29 12:39:57 10splitTasks](my_trainer.py 503): INFO  * Val Acc 64.000, Total time 1.68
[2023-09-29 12:39:57 10splitTasks](my_trainer.py 328): INFO Epoch:10
[2023-09-29 12:39:57 10splitTasks](my_trainer.py 335): INFO LR:0.00654543046337755
[2023-09-29 12:39:57 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:39:58 10splitTasks](trainer.py 286): INFO [0/157]	0.6050(0.6050)	0.4956(0.4956)	0.793(0.793)	81.25(81.25)
[2023-09-29 12:39:59 10splitTasks](trainer.py 286): INFO [10/157]	0.1010(0.1485)	0.0001(0.0453)	0.525(0.939)	81.25(67.61)
[2023-09-29 12:40:00 10splitTasks](trainer.py 286): INFO [20/157]	0.1043(0.1270)	0.0003(0.0239)	1.118(0.901)	59.38(69.64)
[2023-09-29 12:40:01 10splitTasks](trainer.py 286): INFO [30/157]	0.1010(0.1189)	0.0002(0.0163)	0.903(0.905)	65.62(68.95)
[2023-09-29 12:40:02 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1154)	0.0002(0.0124)	0.949(0.904)	65.62(68.52)
[2023-09-29 12:40:03 10splitTasks](trainer.py 286): INFO [50/157]	0.1044(0.1130)	0.0002(0.0100)	0.851(0.921)	75.00(67.83)
[2023-09-29 12:40:04 10splitTasks](trainer.py 286): INFO [60/157]	0.1060(0.1115)	0.0003(0.0084)	0.860(0.922)	65.62(68.24)
[2023-09-29 12:40:05 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1103)	0.0002(0.0073)	1.034(0.927)	68.75(68.18)
[2023-09-29 12:40:06 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1095)	0.0002(0.0064)	1.076(0.925)	62.50(68.21)
[2023-09-29 12:40:07 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1089)	0.0002(0.0057)	0.731(0.936)	68.75(67.89)
[2023-09-29 12:40:08 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1082)	0.0002(0.0052)	1.222(0.942)	62.50(67.64)
[2023-09-29 12:40:09 10splitTasks](trainer.py 286): INFO [110/157]	0.1048(0.1077)	0.0003(0.0048)	0.875(0.937)	75.00(67.54)
[2023-09-29 12:40:10 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1074)	0.0001(0.0044)	1.382(0.942)	62.50(67.41)
[2023-09-29 12:40:11 10splitTasks](trainer.py 286): INFO [130/157]	0.1033(0.1070)	0.0002(0.0041)	1.128(0.941)	62.50(67.41)
[2023-09-29 12:40:12 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1069)	0.0002(0.0038)	0.800(0.941)	68.75(67.33)
[2023-09-29 12:40:13 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1066)	0.0001(0.0036)	1.354(0.941)	53.12(67.45)
[2023-09-29 12:40:14 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1062)	0.0001(0.0035)	1.908(0.943)	37.50(67.36)
[2023-09-29 12:40:14 10splitTasks](trainer.py 288): INFO  * Train Acc 67.360
[2023-09-29 12:40:15 10splitTasks](my_trainer.py 503): INFO  * Val Acc 66.200, Total time 1.64
[2023-09-29 12:40:15 10splitTasks](my_trainer.py 328): INFO Epoch:11
[2023-09-29 12:40:15 10splitTasks](my_trainer.py 335): INFO LR:0.005523090052106635
[2023-09-29 12:40:15 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:40:16 10splitTasks](trainer.py 286): INFO [0/157]	0.6410(0.6410)	0.5132(0.5132)	0.728(0.728)	75.00(75.00)
[2023-09-29 12:40:17 10splitTasks](trainer.py 286): INFO [10/157]	0.1053(0.1531)	0.0001(0.0469)	0.879(1.007)	68.75(66.48)
[2023-09-29 12:40:18 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1297)	0.0003(0.0247)	0.721(0.929)	78.12(69.94)
[2023-09-29 12:40:19 10splitTasks](trainer.py 286): INFO [30/157]	0.1026(0.1212)	0.0004(0.0168)	0.932(0.936)	65.62(69.15)
[2023-09-29 12:40:20 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1172)	0.0003(0.0128)	0.831(0.918)	75.00(69.59)
[2023-09-29 12:40:21 10splitTasks](trainer.py 286): INFO [50/157]	0.1032(0.1143)	0.0003(0.0104)	0.707(0.917)	78.12(69.73)
[2023-09-29 12:40:22 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1129)	0.0003(0.0087)	1.032(0.916)	65.62(69.42)
[2023-09-29 12:40:23 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1115)	0.0003(0.0075)	0.964(0.917)	65.62(69.41)
[2023-09-29 12:40:24 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1106)	0.0002(0.0067)	1.172(0.917)	59.38(69.21)
[2023-09-29 12:40:25 10splitTasks](trainer.py 286): INFO [90/157]	0.1028(0.1097)	0.0004(0.0060)	0.708(0.904)	71.88(69.44)
[2023-09-29 12:40:26 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1091)	0.0003(0.0054)	0.946(0.898)	71.88(69.68)
[2023-09-29 12:40:28 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1085)	0.0002(0.0050)	1.088(0.902)	81.25(69.54)
[2023-09-29 12:40:29 10splitTasks](trainer.py 286): INFO [120/157]	0.1036(0.1082)	0.0003(0.0046)	0.670(0.900)	71.88(69.65)
[2023-09-29 12:40:30 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1077)	0.0002(0.0043)	0.994(0.905)	68.75(69.39)
[2023-09-29 12:40:31 10splitTasks](trainer.py 286): INFO [140/157]	0.1026(0.1074)	0.0003(0.0040)	0.709(0.899)	75.00(69.61)
[2023-09-29 12:40:32 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1071)	0.0001(0.0037)	0.955(0.894)	68.75(69.70)
[2023-09-29 12:40:32 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1068)	0.0001(0.0036)	0.631(0.896)	75.00(69.66)
[2023-09-29 12:40:32 10splitTasks](trainer.py 288): INFO  * Train Acc 69.660
[2023-09-29 12:40:34 10splitTasks](my_trainer.py 503): INFO  * Val Acc 64.200, Total time 1.58
[2023-09-29 12:40:34 10splitTasks](my_trainer.py 328): INFO Epoch:12
[2023-09-29 12:40:34 10splitTasks](my_trainer.py 335): INFO LR:0.0044779099478933675
[2023-09-29 12:40:34 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:40:35 10splitTasks](trainer.py 286): INFO [0/157]	0.5996(0.5996)	0.4850(0.4850)	0.829(0.829)	78.12(78.12)
[2023-09-29 12:40:36 10splitTasks](trainer.py 286): INFO [10/157]	0.1042(0.1517)	0.0002(0.0444)	1.101(0.770)	65.62(74.15)
[2023-09-29 12:40:37 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1285)	0.0002(0.0234)	0.986(0.810)	71.88(73.07)
[2023-09-29 12:40:38 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1201)	0.0003(0.0159)	0.975(0.822)	68.75(72.78)
[2023-09-29 12:40:39 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1159)	0.0002(0.0121)	0.990(0.845)	59.38(72.03)
[2023-09-29 12:40:40 10splitTasks](trainer.py 286): INFO [50/157]	0.1026(0.1132)	0.0002(0.0098)	1.073(0.869)	68.75(71.26)
[2023-09-29 12:40:41 10splitTasks](trainer.py 286): INFO [60/157]	0.1044(0.1115)	0.0003(0.0083)	0.589(0.847)	81.25(71.88)
[2023-09-29 12:40:42 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1101)	0.0003(0.0071)	0.973(0.841)	65.62(71.79)
[2023-09-29 12:40:43 10splitTasks](trainer.py 286): INFO [80/157]	0.1037(0.1093)	0.0003(0.0063)	0.863(0.847)	62.50(71.84)
[2023-09-29 12:40:44 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1087)	0.0002(0.0056)	0.880(0.841)	75.00(72.32)
[2023-09-29 12:40:45 10splitTasks](trainer.py 286): INFO [100/157]	0.1046(0.1081)	0.0002(0.0051)	0.967(0.844)	71.88(72.03)
[2023-09-29 12:40:46 10splitTasks](trainer.py 286): INFO [110/157]	0.1051(0.1078)	0.0003(0.0047)	0.764(0.839)	68.75(72.30)
[2023-09-29 12:40:47 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1073)	0.0003(0.0043)	0.659(0.831)	81.25(72.49)
[2023-09-29 12:40:48 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1069)	0.0002(0.0040)	0.825(0.825)	71.88(72.66)
[2023-09-29 12:40:49 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1066)	0.0003(0.0038)	0.828(0.821)	65.62(72.56)
[2023-09-29 12:40:50 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1064)	0.0001(0.0035)	0.870(0.827)	68.75(72.39)
[2023-09-29 12:40:51 10splitTasks](trainer.py 286): INFO [156/157]	0.0799(0.1060)	0.0001(0.0034)	0.517(0.822)	75.00(72.62)
[2023-09-29 12:40:51 10splitTasks](trainer.py 288): INFO  * Train Acc 72.620
[2023-09-29 12:40:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 67.800, Total time 1.55
[2023-09-29 12:40:52 10splitTasks](my_trainer.py 328): INFO Epoch:13
[2023-09-29 12:40:52 10splitTasks](my_trainer.py 335): INFO LR:0.0034555695366224513
[2023-09-29 12:40:52 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:40:53 10splitTasks](trainer.py 286): INFO [0/157]	0.6259(0.6259)	0.5088(0.5088)	0.669(0.669)	71.88(71.88)
[2023-09-29 12:40:54 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1531)	0.0002(0.0465)	0.730(0.662)	65.62(79.26)
[2023-09-29 12:40:55 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1290)	0.0002(0.0245)	0.544(0.723)	78.12(74.55)
[2023-09-29 12:40:56 10splitTasks](trainer.py 286): INFO [30/157]	0.1050(0.1208)	0.0003(0.0167)	0.745(0.702)	68.75(75.00)
[2023-09-29 12:40:57 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1164)	0.0003(0.0127)	0.638(0.721)	84.38(74.85)
[2023-09-29 12:40:58 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1136)	0.0002(0.0103)	1.099(0.726)	65.62(74.75)
[2023-09-29 12:40:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1042(0.1118)	0.0006(0.0087)	0.691(0.720)	75.00(74.74)
[2023-09-29 12:41:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1034(0.1108)	0.0005(0.0075)	0.688(0.729)	71.88(74.34)
[2023-09-29 12:41:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1098)	0.0003(0.0066)	0.648(0.733)	84.38(74.04)
[2023-09-29 12:41:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1036(0.1090)	0.0003(0.0059)	0.394(0.736)	87.50(74.35)
[2023-09-29 12:41:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1084)	0.0002(0.0054)	0.673(0.744)	75.00(73.82)
[2023-09-29 12:41:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1078)	0.0004(0.0049)	0.569(0.740)	75.00(73.96)
[2023-09-29 12:41:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1074)	0.0003(0.0045)	0.848(0.739)	65.62(74.10)
[2023-09-29 12:41:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1070)	0.0003(0.0042)	1.158(0.744)	56.25(74.00)
[2023-09-29 12:41:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1066)	0.0002(0.0039)	0.706(0.750)	71.88(74.07)
[2023-09-29 12:41:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1063)	0.0001(0.0037)	0.741(0.752)	68.75(74.09)
[2023-09-29 12:41:09 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1060)	0.0002(0.0035)	0.625(0.751)	75.00(74.04)
[2023-09-29 12:41:09 10splitTasks](trainer.py 288): INFO  * Train Acc 74.040
[2023-09-29 12:41:11 10splitTasks](my_trainer.py 503): INFO  * Val Acc 68.600, Total time 1.52
[2023-09-29 12:41:11 10splitTasks](my_trainer.py 328): INFO Epoch:14
[2023-09-29 12:41:11 10splitTasks](my_trainer.py 335): INFO LR:0.0025007500000000017
[2023-09-29 12:41:11 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:41:11 10splitTasks](trainer.py 286): INFO [0/157]	0.6617(0.6617)	0.5552(0.5552)	0.789(0.789)	65.62(65.62)
[2023-09-29 12:41:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1532)	0.0003(0.0508)	0.721(0.657)	71.88(76.14)
[2023-09-29 12:41:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1299)	0.0002(0.0268)	0.443(0.666)	90.62(75.89)
[2023-09-29 12:41:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1049(0.1212)	0.0002(0.0182)	1.055(0.681)	59.38(75.50)
[2023-09-29 12:41:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1047(0.1168)	0.0005(0.0139)	0.941(0.687)	75.00(75.99)
[2023-09-29 12:41:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1139)	0.0004(0.0112)	0.773(0.678)	71.88(75.86)
[2023-09-29 12:41:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1120)	0.0004(0.0095)	0.746(0.678)	71.88(76.28)
[2023-09-29 12:41:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1106)	0.0003(0.0082)	0.406(0.677)	87.50(76.32)
[2023-09-29 12:41:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1095)	0.0003(0.0072)	0.774(0.688)	68.75(75.96)
[2023-09-29 12:41:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1088)	0.0003(0.0064)	0.574(0.694)	84.38(75.79)
[2023-09-29 12:41:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1082)	0.0003(0.0058)	0.625(0.688)	81.25(76.11)
[2023-09-29 12:41:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1076)	0.0002(0.0053)	0.715(0.683)	75.00(76.18)
[2023-09-29 12:41:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1073)	0.0003(0.0049)	0.662(0.675)	81.25(76.55)
[2023-09-29 12:41:25 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1070)	0.0002(0.0046)	0.857(0.674)	71.88(76.46)
[2023-09-29 12:41:26 10splitTasks](trainer.py 286): INFO [140/157]	0.1034(0.1067)	0.0003(0.0043)	0.676(0.674)	71.88(76.37)
[2023-09-29 12:41:27 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1064)	0.0001(0.0040)	0.543(0.674)	87.50(76.39)
[2023-09-29 12:41:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1060)	0.0001(0.0039)	1.107(0.674)	62.50(76.46)
[2023-09-29 12:41:27 10splitTasks](trainer.py 288): INFO  * Train Acc 76.460
[2023-09-29 12:41:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 67.400, Total time 1.57
[2023-09-29 12:41:29 10splitTasks](my_trainer.py 328): INFO Epoch:15
[2023-09-29 12:41:29 10splitTasks](my_trainer.py 335): INFO LR:0.00165518153350889
[2023-09-29 12:41:29 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:41:29 10splitTasks](trainer.py 286): INFO [0/157]	0.6091(0.6091)	0.4880(0.4880)	0.498(0.498)	84.38(84.38)
[2023-09-29 12:41:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1125(0.1507)	0.0006(0.0447)	0.664(0.735)	75.00(72.73)
[2023-09-29 12:41:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1034(0.1276)	0.0003(0.0235)	0.654(0.680)	84.38(75.45)
[2023-09-29 12:41:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1192)	0.0002(0.0160)	0.396(0.634)	84.38(76.61)
[2023-09-29 12:41:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1042(0.1155)	0.0003(0.0122)	0.607(0.624)	78.12(77.13)
[2023-09-29 12:41:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1131)	0.0002(0.0099)	0.562(0.622)	81.25(77.45)
[2023-09-29 12:41:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1117)	0.0002(0.0083)	0.657(0.628)	75.00(77.56)
[2023-09-29 12:41:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1104)	0.0002(0.0072)	0.566(0.623)	78.12(77.95)
[2023-09-29 12:41:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1094)	0.0003(0.0063)	0.955(0.627)	65.62(77.82)
[2023-09-29 12:41:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1086)	0.0002(0.0057)	0.556(0.635)	78.12(77.44)
[2023-09-29 12:41:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1066(0.1081)	0.0002(0.0051)	0.552(0.626)	81.25(77.97)
[2023-09-29 12:41:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1075)	0.0003(0.0047)	0.434(0.632)	87.50(77.79)
[2023-09-29 12:41:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1049(0.1072)	0.0004(0.0043)	0.603(0.636)	71.88(77.84)
[2023-09-29 12:41:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1069)	0.0003(0.0040)	0.688(0.636)	78.12(77.96)
[2023-09-29 12:41:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1065)	0.0003(0.0038)	0.613(0.638)	71.88(77.97)
[2023-09-29 12:41:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1064)	0.0002(0.0035)	0.740(0.643)	65.62(77.67)
[2023-09-29 12:41:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0797(0.1060)	0.0001(0.0034)	0.828(0.646)	75.00(77.64)
[2023-09-29 12:41:46 10splitTasks](trainer.py 288): INFO  * Train Acc 77.640
[2023-09-29 12:41:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.000, Total time 1.64
[2023-09-29 12:41:47 10splitTasks](my_trainer.py 328): INFO Epoch:16
[2023-09-29 12:41:47 10splitTasks](my_trainer.py 335): INFO LR:0.0009558195366224509
[2023-09-29 12:41:47 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:41:48 10splitTasks](trainer.py 286): INFO [0/157]	0.6081(0.6081)	0.4925(0.4925)	0.430(0.430)	81.25(81.25)
[2023-09-29 12:41:49 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1499)	0.0002(0.0451)	0.593(0.581)	81.25(80.68)
[2023-09-29 12:41:50 10splitTasks](trainer.py 286): INFO [20/157]	0.1026(0.1274)	0.0003(0.0238)	0.421(0.540)	75.00(81.25)
[2023-09-29 12:41:51 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1197)	0.0002(0.0162)	0.549(0.562)	84.38(81.05)
[2023-09-29 12:41:52 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1157)	0.0003(0.0123)	0.485(0.564)	78.12(80.56)
[2023-09-29 12:41:53 10splitTasks](trainer.py 286): INFO [50/157]	0.1043(0.1133)	0.0003(0.0100)	0.396(0.562)	87.50(80.82)
[2023-09-29 12:41:54 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1117)	0.0003(0.0084)	0.551(0.558)	87.50(81.05)
[2023-09-29 12:41:55 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1103)	0.0002(0.0073)	0.386(0.557)	90.62(80.94)
[2023-09-29 12:41:56 10splitTasks](trainer.py 286): INFO [80/157]	0.1048(0.1095)	0.0002(0.0064)	0.575(0.572)	81.25(80.48)
[2023-09-29 12:41:57 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1087)	0.0003(0.0058)	0.379(0.575)	90.62(80.25)
[2023-09-29 12:41:58 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1081)	0.0003(0.0052)	0.474(0.569)	87.50(80.48)
[2023-09-29 12:41:59 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1076)	0.0002(0.0048)	0.860(0.570)	68.75(80.46)
[2023-09-29 12:42:00 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1071)	0.0001(0.0044)	0.430(0.570)	84.38(80.45)
[2023-09-29 12:42:01 10splitTasks](trainer.py 286): INFO [130/157]	0.1031(0.1067)	0.0002(0.0041)	0.616(0.570)	84.38(80.49)
[2023-09-29 12:42:02 10splitTasks](trainer.py 286): INFO [140/157]	0.1023(0.1064)	0.0002(0.0038)	0.476(0.571)	84.38(80.30)
[2023-09-29 12:42:03 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1061)	0.0001(0.0036)	0.808(0.571)	81.25(80.30)
[2023-09-29 12:42:04 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1058)	0.0002(0.0034)	0.195(0.573)	100.00(80.36)
[2023-09-29 12:42:04 10splitTasks](trainer.py 288): INFO  * Train Acc 80.360
[2023-09-29 12:42:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.400, Total time 1.58
[2023-09-29 12:42:06 10splitTasks](my_trainer.py 328): INFO Epoch:17
[2023-09-29 12:42:06 10splitTasks](my_trainer.py 335): INFO LR:0.0004332294845158165
[2023-09-29 12:42:06 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:42:06 10splitTasks](trainer.py 286): INFO [0/157]	0.5884(0.5884)	0.4670(0.4670)	0.469(0.469)	81.25(81.25)
[2023-09-29 12:42:07 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1484)	0.0002(0.0427)	0.547(0.498)	78.12(82.39)
[2023-09-29 12:42:08 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1278)	0.0003(0.0226)	0.682(0.482)	84.38(82.44)
[2023-09-29 12:42:09 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1195)	0.0003(0.0154)	0.426(0.484)	90.62(81.75)
[2023-09-29 12:42:10 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1155)	0.0002(0.0117)	0.575(0.495)	81.25(82.24)
[2023-09-29 12:42:11 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1132)	0.0004(0.0096)	0.980(0.514)	81.25(81.74)
[2023-09-29 12:42:12 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1115)	0.0002(0.0080)	0.599(0.521)	81.25(81.92)
[2023-09-29 12:42:13 10splitTasks](trainer.py 286): INFO [70/157]	0.1043(0.1103)	0.0003(0.0070)	0.460(0.521)	78.12(81.78)
[2023-09-29 12:42:14 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1093)	0.0003(0.0061)	0.612(0.524)	81.25(81.64)
[2023-09-29 12:42:15 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1086)	0.0003(0.0055)	0.709(0.537)	81.25(81.28)
[2023-09-29 12:42:16 10splitTasks](trainer.py 286): INFO [100/157]	0.1048(0.1081)	0.0003(0.0050)	0.671(0.533)	78.12(81.40)
[2023-09-29 12:42:18 10splitTasks](trainer.py 286): INFO [110/157]	0.1022(0.1076)	0.0002(0.0046)	0.808(0.539)	78.12(81.25)
[2023-09-29 12:42:19 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1072)	0.0003(0.0042)	0.592(0.548)	68.75(80.79)
[2023-09-29 12:42:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1069)	0.0002(0.0039)	0.515(0.549)	81.25(80.92)
[2023-09-29 12:42:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1066)	0.0003(0.0037)	0.512(0.551)	78.12(80.94)
[2023-09-29 12:42:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1063)	0.0001(0.0034)	0.487(0.542)	84.38(81.17)
[2023-09-29 12:42:22 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1060)	0.0001(0.0033)	1.506(0.550)	62.50(81.00)
[2023-09-29 12:42:22 10splitTasks](trainer.py 288): INFO  * Train Acc 81.000
[2023-09-29 12:42:24 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.400, Total time 1.55
[2023-09-29 12:42:24 10splitTasks](my_trainer.py 328): INFO Epoch:18
[2023-09-29 12:42:24 10splitTasks](my_trainer.py 335): INFO LR:0.00011025107013133847
[2023-09-29 12:42:24 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:42:25 10splitTasks](trainer.py 286): INFO [0/157]	0.6263(0.6263)	0.5211(0.5211)	0.686(0.686)	81.25(81.25)
[2023-09-29 12:42:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1572)	0.0002(0.0521)	1.056(0.587)	65.62(81.25)
[2023-09-29 12:42:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1028(0.1316)	0.0002(0.0274)	0.286(0.565)	93.75(81.10)
[2023-09-29 12:42:28 10splitTasks](trainer.py 286): INFO [30/157]	0.1045(0.1225)	0.0003(0.0187)	0.265(0.547)	90.62(81.55)
[2023-09-29 12:42:29 10splitTasks](trainer.py 286): INFO [40/157]	0.1109(0.1184)	0.0003(0.0143)	0.364(0.554)	87.50(81.25)
[2023-09-29 12:42:30 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1153)	0.0003(0.0116)	0.607(0.534)	71.88(81.74)
[2023-09-29 12:42:31 10splitTasks](trainer.py 286): INFO [60/157]	0.1145(0.1138)	0.0005(0.0097)	0.564(0.530)	84.38(82.12)
[2023-09-29 12:42:32 10splitTasks](trainer.py 286): INFO [70/157]	0.1066(0.1122)	0.0002(0.0084)	0.538(0.527)	81.25(82.13)
[2023-09-29 12:42:33 10splitTasks](trainer.py 286): INFO [80/157]	0.1050(0.1114)	0.0002(0.0074)	0.376(0.527)	87.50(82.02)
[2023-09-29 12:42:34 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1105)	0.0003(0.0066)	0.601(0.525)	78.12(82.18)
[2023-09-29 12:42:35 10splitTasks](trainer.py 286): INFO [100/157]	0.1037(0.1098)	0.0003(0.0060)	0.579(0.524)	75.00(82.24)
[2023-09-29 12:42:36 10splitTasks](trainer.py 286): INFO [110/157]	0.1047(0.1093)	0.0003(0.0055)	0.373(0.523)	90.62(82.35)
[2023-09-29 12:42:37 10splitTasks](trainer.py 286): INFO [120/157]	0.1041(0.1088)	0.0004(0.0051)	0.516(0.523)	81.25(82.28)
[2023-09-29 12:42:38 10splitTasks](trainer.py 286): INFO [130/157]	0.1078(0.1085)	0.0003(0.0047)	0.387(0.515)	84.38(82.47)
[2023-09-29 12:42:39 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1081)	0.0003(0.0044)	0.425(0.515)	90.62(82.56)
[2023-09-29 12:42:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1077)	0.0001(0.0041)	0.548(0.513)	84.38(82.64)
[2023-09-29 12:42:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1074)	0.0001(0.0040)	0.272(0.513)	87.50(82.56)
[2023-09-29 12:42:41 10splitTasks](trainer.py 288): INFO  * Train Acc 82.560
[2023-09-29 12:42:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.600, Total time 1.60
[2023-09-29 12:42:42 10splitTasks](my_trainer.py 328): INFO Epoch:19
[2023-09-29 12:42:42 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 12:42:42 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:42:43 10splitTasks](trainer.py 286): INFO [0/157]	0.5983(0.5983)	0.4711(0.4711)	0.558(0.558)	81.25(81.25)
[2023-09-29 12:42:44 10splitTasks](trainer.py 286): INFO [10/157]	0.1053(0.1500)	0.0002(0.0431)	0.549(0.521)	75.00(80.97)
[2023-09-29 12:42:45 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1273)	0.0002(0.0229)	0.587(0.541)	84.38(80.21)
[2023-09-29 12:42:46 10splitTasks](trainer.py 286): INFO [30/157]	0.1032(0.1192)	0.0003(0.0156)	0.407(0.519)	78.12(81.05)
[2023-09-29 12:42:47 10splitTasks](trainer.py 286): INFO [40/157]	0.1055(0.1158)	0.0003(0.0119)	0.756(0.525)	75.00(81.17)
[2023-09-29 12:42:48 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1133)	0.0002(0.0096)	0.569(0.515)	84.38(81.86)
[2023-09-29 12:42:49 10splitTasks](trainer.py 286): INFO [60/157]	0.1032(0.1117)	0.0002(0.0081)	0.221(0.513)	96.88(82.12)
[2023-09-29 12:42:50 10splitTasks](trainer.py 286): INFO [70/157]	0.1137(0.1107)	0.0007(0.0070)	0.294(0.500)	90.62(82.61)
[2023-09-29 12:42:51 10splitTasks](trainer.py 286): INFO [80/157]	0.1035(0.1097)	0.0002(0.0062)	0.162(0.499)	100.00(82.95)
[2023-09-29 12:42:52 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1091)	0.0003(0.0055)	0.570(0.497)	78.12(82.90)
[2023-09-29 12:42:53 10splitTasks](trainer.py 286): INFO [100/157]	0.1054(0.1085)	0.0003(0.0050)	0.173(0.499)	93.75(82.95)
[2023-09-29 12:42:54 10splitTasks](trainer.py 286): INFO [110/157]	0.1051(0.1081)	0.0002(0.0046)	0.447(0.493)	87.50(83.11)
[2023-09-29 12:42:55 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1076)	0.0003(0.0042)	0.573(0.493)	84.38(83.34)
[2023-09-29 12:42:57 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1073)	0.0003(0.0039)	0.592(0.498)	78.12(83.30)
[2023-09-29 12:42:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1044(0.1071)	0.0002(0.0037)	0.558(0.495)	87.50(83.31)
[2023-09-29 12:42:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1069)	0.0001(0.0035)	0.727(0.499)	81.25(83.15)
[2023-09-29 12:42:59 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1065)	0.0001(0.0033)	1.478(0.496)	37.50(83.30)
[2023-09-29 12:42:59 10splitTasks](trainer.py 288): INFO  * Train Acc 83.300
[2023-09-29 12:43:01 10splitTasks](my_trainer.py 503): INFO  * Val Acc 71.800, Total time 1.59
[2023-09-29 12:43:01 10splitTasks](my_trainer.py 206): INFO Pruning for task0
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 7997/9408 (85.00%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 3482/4096 (85.01%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 31334/36864 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 13926/16384 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 13926/16384 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 13926/16384 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 31334/36864 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 13926/16384 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 13926/16384 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 31334/36864 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 13926/16384 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 27853/32768 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 125338/147456 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 55706/65536 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 111411/131072 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 55706/65536 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 125338/147456 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 55706/65536 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 55706/65536 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 125338/147456 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 55706/65536 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 55706/65536 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 125338/147456 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 55706/65536 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 111411/131072 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 501350/589824 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 445645/524288 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 501350/589824 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 501350/589824 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 501350/589824 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 501350/589824 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 501350/589824 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 222822/262144 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 445645/524288 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 2005402/2359296 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 891290/1048576 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 1782579/2097152 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 891290/1048576 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 2005402/2359296 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 891290/1048576 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 891290/1048576 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 2005403/2359296 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 891290/1048576 (85.00%) (Total in layer: 1048576)
[2023-09-29 12:43:01 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 12:43:01 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 12:43:01 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 12:43:01 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:43:02 10splitTasks](trainer.py 286): INFO [0/157]	0.6700(0.6700)	0.5667(0.5667)	3.333(3.333)	3.12(3.12)
[2023-09-29 12:43:03 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1535)	0.0003(0.0518)	2.610(2.633)	21.88(24.43)
[2023-09-29 12:43:04 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1299)	0.0002(0.0272)	1.778(2.510)	37.50(27.38)
[2023-09-29 12:43:05 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1208)	0.0003(0.0185)	2.071(2.378)	31.25(29.23)
[2023-09-29 12:43:06 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1163)	0.0002(0.0141)	1.734(2.241)	43.75(32.85)
[2023-09-29 12:43:07 10splitTasks](trainer.py 286): INFO [50/157]	0.1024(0.1137)	0.0002(0.0114)	1.848(2.126)	43.75(36.09)
[2023-09-29 12:43:08 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1122)	0.0005(0.0096)	1.844(2.067)	37.50(36.68)
[2023-09-29 12:43:09 10splitTasks](trainer.py 286): INFO [70/157]	0.1028(0.1110)	0.0003(0.0083)	1.798(2.030)	46.88(37.10)
[2023-09-29 12:43:10 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1099)	0.0002(0.0073)	1.430(1.967)	46.88(38.50)
[2023-09-29 12:43:11 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1092)	0.0002(0.0065)	1.589(1.950)	56.25(38.91)
[2023-09-29 12:43:12 10splitTasks](trainer.py 286): INFO [100/157]	0.1030(0.1085)	0.0002(0.0059)	1.711(1.964)	40.62(38.61)
[2023-09-29 12:43:13 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1080)	0.0002(0.0054)	2.278(1.957)	40.62(38.99)
[2023-09-29 12:43:14 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1075)	0.0002(0.0050)	1.445(1.934)	37.50(39.72)
[2023-09-29 12:43:15 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1071)	0.0002(0.0046)	1.786(1.918)	46.88(40.12)
[2023-09-29 12:43:16 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1068)	0.0003(0.0043)	1.435(1.892)	37.50(40.45)
[2023-09-29 12:43:18 10splitTasks](trainer.py 286): INFO [150/157]	0.1041(0.1064)	0.0002(0.0040)	1.466(1.858)	50.00(41.16)
[2023-09-29 12:43:18 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1061)	0.0001(0.0039)	2.332(1.841)	25.00(41.66)
[2023-09-29 12:43:18 10splitTasks](trainer.py 288): INFO  * Train Acc 41.660
[2023-09-29 12:43:20 10splitTasks](my_trainer.py 503): INFO  * Val Acc 48.200, Total time 1.56
[2023-09-29 12:43:20 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 12:43:20 10splitTasks](my_trainer.py 308): INFO LR:0.009983788162016984
[2023-09-29 12:43:20 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:43:20 10splitTasks](trainer.py 286): INFO [0/157]	0.6220(0.6220)	0.5076(0.5076)	0.870(0.870)	78.12(78.12)
[2023-09-29 12:43:21 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1510)	0.0003(0.0465)	1.533(1.461)	46.88(53.41)
[2023-09-29 12:43:22 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1275)	0.0002(0.0245)	1.370(1.493)	53.12(52.38)
[2023-09-29 12:43:24 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1201)	0.0003(0.0168)	1.187(1.492)	43.75(50.91)
[2023-09-29 12:43:25 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1158)	0.0003(0.0128)	1.487(1.455)	34.38(51.22)
[2023-09-29 12:43:26 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1135)	0.0002(0.0103)	1.936(1.471)	31.25(51.53)
[2023-09-29 12:43:27 10splitTasks](trainer.py 286): INFO [60/157]	0.1044(0.1116)	0.0006(0.0087)	1.405(1.459)	56.25(51.90)
[2023-09-29 12:43:28 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1103)	0.0003(0.0075)	1.456(1.449)	59.38(52.16)
[2023-09-29 12:43:29 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1095)	0.0003(0.0066)	1.197(1.435)	59.38(52.74)
[2023-09-29 12:43:30 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1087)	0.0002(0.0059)	1.238(1.410)	65.62(53.54)
[2023-09-29 12:43:31 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1080)	0.0003(0.0054)	1.131(1.400)	53.12(53.71)
[2023-09-29 12:43:32 10splitTasks](trainer.py 286): INFO [110/157]	0.1069(0.1076)	0.0003(0.0049)	1.272(1.392)	59.38(53.89)
[2023-09-29 12:43:33 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1071)	0.0002(0.0045)	1.196(1.370)	59.38(54.57)
[2023-09-29 12:43:34 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1069)	0.0002(0.0042)	1.774(1.367)	46.88(54.99)
[2023-09-29 12:43:35 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1067)	0.0004(0.0039)	1.465(1.376)	53.12(54.63)
[2023-09-29 12:43:36 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1064)	0.0001(0.0037)	1.861(1.393)	34.38(54.72)
[2023-09-29 12:43:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1060)	0.0001(0.0036)	0.795(1.393)	75.00(54.76)
[2023-09-29 12:43:37 10splitTasks](trainer.py 288): INFO  * Train Acc 54.760
[2023-09-29 12:43:38 10splitTasks](my_trainer.py 503): INFO  * Val Acc 56.000, Total time 1.72
[2023-09-29 12:43:38 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 12:43:38 10splitTasks](my_trainer.py 308): INFO LR:0.009935257788058245
[2023-09-29 12:43:38 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:43:39 10splitTasks](trainer.py 286): INFO [0/157]	0.6600(0.6600)	0.5568(0.5568)	1.413(1.413)	78.12(78.12)
[2023-09-29 12:43:40 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1541)	0.0002(0.0509)	0.962(1.268)	71.88(57.67)
[2023-09-29 12:43:41 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1313)	0.0003(0.0268)	1.245(1.315)	56.25(56.99)
[2023-09-29 12:43:42 10splitTasks](trainer.py 286): INFO [30/157]	0.1040(0.1222)	0.0003(0.0183)	1.406(1.266)	50.00(57.56)
[2023-09-29 12:43:43 10splitTasks](trainer.py 286): INFO [40/157]	0.1050(0.1174)	0.0005(0.0139)	0.903(1.265)	68.75(58.84)
[2023-09-29 12:43:44 10splitTasks](trainer.py 286): INFO [50/157]	0.1040(0.1145)	0.0002(0.0112)	1.400(1.267)	46.88(58.21)
[2023-09-29 12:43:45 10splitTasks](trainer.py 286): INFO [60/157]	0.1030(0.1130)	0.0003(0.0095)	1.163(1.261)	53.12(57.89)
[2023-09-29 12:43:46 10splitTasks](trainer.py 286): INFO [70/157]	0.1034(0.1115)	0.0003(0.0082)	1.014(1.232)	75.00(58.93)
[2023-09-29 12:43:47 10splitTasks](trainer.py 286): INFO [80/157]	0.1042(0.1104)	0.0002(0.0072)	1.110(1.224)	53.12(59.45)
[2023-09-29 12:43:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1042(0.1096)	0.0003(0.0064)	1.399(1.218)	59.38(59.48)
[2023-09-29 12:43:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1089)	0.0002(0.0058)	2.006(1.234)	46.88(59.44)
[2023-09-29 12:43:50 10splitTasks](trainer.py 286): INFO [110/157]	0.1042(0.1083)	0.0002(0.0053)	0.841(1.240)	68.75(59.35)
[2023-09-29 12:43:51 10splitTasks](trainer.py 286): INFO [120/157]	0.1049(0.1078)	0.0002(0.0049)	0.962(1.270)	71.88(59.04)
[2023-09-29 12:43:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1030(0.1074)	0.0002(0.0046)	1.222(1.261)	56.25(59.04)
[2023-09-29 12:43:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1157(0.1071)	0.0005(0.0043)	1.123(1.256)	56.25(59.02)
[2023-09-29 12:43:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1069)	0.0001(0.0040)	0.971(1.266)	65.62(58.98)
[2023-09-29 12:43:55 10splitTasks](trainer.py 286): INFO [156/157]	0.0800(0.1065)	0.0001(0.0039)	2.071(1.263)	37.50(58.82)
[2023-09-29 12:43:55 10splitTasks](trainer.py 288): INFO  * Train Acc 58.820
[2023-09-29 12:43:57 10splitTasks](my_trainer.py 503): INFO  * Val Acc 57.200, Total time 1.58
[2023-09-29 12:43:57 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 12:43:57 10splitTasks](my_trainer.py 308): INFO LR:0.009854723616221547
[2023-09-29 12:43:57 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:43:57 10splitTasks](trainer.py 286): INFO [0/157]	0.6463(0.6463)	0.5348(0.5348)	0.986(0.986)	56.25(56.25)
[2023-09-29 12:43:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1045(0.1524)	0.0003(0.0489)	1.160(1.098)	62.50(59.09)
[2023-09-29 12:43:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1296)	0.0002(0.0258)	0.995(1.063)	68.75(62.80)
[2023-09-29 12:44:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1025(0.1211)	0.0002(0.0176)	1.313(1.057)	56.25(63.31)
[2023-09-29 12:44:02 10splitTasks](trainer.py 286): INFO [40/157]	0.1026(0.1167)	0.0002(0.0133)	1.633(1.129)	50.00(61.36)
[2023-09-29 12:44:03 10splitTasks](trainer.py 286): INFO [50/157]	0.1074(0.1139)	0.0003(0.0108)	0.845(1.099)	68.75(62.56)
[2023-09-29 12:44:04 10splitTasks](trainer.py 286): INFO [60/157]	0.1042(0.1122)	0.0002(0.0091)	0.954(1.097)	71.88(62.91)
[2023-09-29 12:44:05 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1107)	0.0002(0.0078)	0.865(1.105)	65.62(62.59)
[2023-09-29 12:44:06 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1097)	0.0003(0.0069)	0.964(1.107)	65.62(62.54)
[2023-09-29 12:44:07 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1089)	0.0003(0.0062)	1.298(1.114)	37.50(62.05)
[2023-09-29 12:44:08 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1083)	0.0002(0.0056)	1.148(1.109)	62.50(62.44)
[2023-09-29 12:44:09 10splitTasks](trainer.py 286): INFO [110/157]	0.1045(0.1079)	0.0002(0.0051)	1.239(1.108)	53.12(62.50)
[2023-09-29 12:44:10 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1074)	0.0003(0.0047)	1.080(1.105)	62.50(62.71)
[2023-09-29 12:44:11 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1071)	0.0003(0.0044)	0.959(1.102)	68.75(62.91)
[2023-09-29 12:44:12 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1068)	0.0003(0.0041)	1.049(1.102)	68.75(63.03)
[2023-09-29 12:44:13 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1065)	0.0001(0.0039)	0.843(1.099)	75.00(63.33)
[2023-09-29 12:44:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1061)	0.0001(0.0037)	0.706(1.095)	75.00(63.56)
[2023-09-29 12:44:13 10splitTasks](trainer.py 288): INFO  * Train Acc 63.560
[2023-09-29 12:44:15 10splitTasks](my_trainer.py 503): INFO  * Val Acc 61.200, Total time 1.62
[2023-09-29 12:44:15 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 12:44:15 10splitTasks](my_trainer.py 308): INFO LR:0.009742707941514753
[2023-09-29 12:44:15 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:44:16 10splitTasks](trainer.py 286): INFO [0/157]	0.6231(0.6231)	0.5069(0.5069)	1.108(1.108)	59.38(59.38)
[2023-09-29 12:44:17 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1543)	0.0001(0.0466)	1.071(1.021)	68.75(68.47)
[2023-09-29 12:44:18 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1295)	0.0002(0.0245)	0.973(1.005)	71.88(66.67)
[2023-09-29 12:44:19 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1208)	0.0003(0.0167)	0.888(0.987)	68.75(67.44)
[2023-09-29 12:44:20 10splitTasks](trainer.py 286): INFO [40/157]	0.1090(0.1167)	0.0006(0.0127)	0.545(0.965)	78.12(68.29)
[2023-09-29 12:44:21 10splitTasks](trainer.py 286): INFO [50/157]	0.1046(0.1141)	0.0003(0.0103)	0.857(0.974)	65.62(67.52)
[2023-09-29 12:44:22 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1122)	0.0003(0.0087)	1.099(0.990)	62.50(66.96)
[2023-09-29 12:44:23 10splitTasks](trainer.py 286): INFO [70/157]	0.1008(0.1108)	0.0002(0.0075)	1.833(1.001)	56.25(66.86)
[2023-09-29 12:44:24 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1097)	0.0002(0.0066)	0.851(1.007)	68.75(67.05)
[2023-09-29 12:44:25 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1091)	0.0003(0.0059)	1.221(1.014)	65.62(67.00)
[2023-09-29 12:44:26 10splitTasks](trainer.py 286): INFO [100/157]	0.1034(0.1085)	0.0002(0.0054)	0.877(1.017)	68.75(67.08)
[2023-09-29 12:44:27 10splitTasks](trainer.py 286): INFO [110/157]	0.1066(0.1080)	0.0002(0.0049)	1.072(1.010)	56.25(67.06)
[2023-09-29 12:44:28 10splitTasks](trainer.py 286): INFO [120/157]	0.1033(0.1076)	0.0003(0.0045)	0.963(1.020)	59.38(66.48)
[2023-09-29 12:44:29 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1074)	0.0002(0.0042)	0.704(1.019)	78.12(66.56)
[2023-09-29 12:44:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1039(0.1071)	0.0003(0.0039)	0.853(1.020)	68.75(66.38)
[2023-09-29 12:44:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1067)	0.0001(0.0037)	1.476(1.019)	65.62(66.29)
[2023-09-29 12:44:32 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1064)	0.0001(0.0036)	1.121(1.023)	50.00(66.24)
[2023-09-29 12:44:32 10splitTasks](trainer.py 288): INFO  * Train Acc 66.240
[2023-09-29 12:44:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 65.800, Total time 1.54
[2023-09-29 12:44:33 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 12:44:33 10splitTasks](my_trainer.py 308): INFO LR:0.009599937228572292
[2023-09-29 12:44:33 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:44:34 10splitTasks](trainer.py 286): INFO [0/157]	0.5865(0.5865)	0.4560(0.4560)	1.231(1.231)	56.25(56.25)
[2023-09-29 12:44:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1007(0.1476)	0.0002(0.0418)	1.105(1.010)	62.50(65.34)
[2023-09-29 12:44:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1038(0.1263)	0.0002(0.0220)	1.023(1.036)	65.62(65.77)
[2023-09-29 12:44:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1187)	0.0002(0.0150)	0.572(1.019)	84.38(66.73)
[2023-09-29 12:44:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1047(0.1148)	0.0002(0.0115)	1.236(1.045)	59.38(66.23)
[2023-09-29 12:44:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1132)	0.0002(0.0093)	1.416(1.063)	46.88(65.87)
[2023-09-29 12:44:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1040(0.1116)	0.0002(0.0078)	1.639(1.066)	59.38(65.52)
[2023-09-29 12:44:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1103)	0.0003(0.0068)	1.044(1.041)	75.00(66.59)
[2023-09-29 12:44:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1041(0.1095)	0.0003(0.0060)	1.012(1.035)	56.25(66.09)
[2023-09-29 12:44:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1089)	0.0002(0.0054)	1.107(1.043)	62.50(66.17)
[2023-09-29 12:44:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1054(0.1082)	0.0006(0.0049)	0.867(1.046)	75.00(66.06)
[2023-09-29 12:44:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1077)	0.0002(0.0044)	1.360(1.037)	56.25(66.47)
[2023-09-29 12:44:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1027(0.1072)	0.0002(0.0041)	0.932(1.026)	71.88(66.68)
[2023-09-29 12:44:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1069)	0.0002(0.0038)	1.059(1.021)	65.62(66.79)
[2023-09-29 12:44:49 10splitTasks](trainer.py 286): INFO [140/157]	0.1024(0.1067)	0.0003(0.0036)	1.059(1.011)	65.62(66.93)
[2023-09-29 12:44:50 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1064)	0.0001(0.0034)	0.876(1.005)	65.62(66.97)
[2023-09-29 12:44:50 10splitTasks](trainer.py 286): INFO [156/157]	0.0821(0.1061)	0.0001(0.0032)	0.608(0.999)	87.50(67.20)
[2023-09-29 12:44:50 10splitTasks](trainer.py 288): INFO  * Train Acc 67.200
[2023-09-29 12:44:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 65.000, Total time 1.60
[2023-09-29 12:44:52 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 12:44:52 10splitTasks](my_trainer.py 308): INFO LR:0.009427337400253222
[2023-09-29 12:44:52 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:44:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6186(0.6186)	0.5063(0.5063)	0.840(0.840)	68.75(68.75)
[2023-09-29 12:44:54 10splitTasks](trainer.py 286): INFO [10/157]	0.1190(0.1508)	0.0005(0.0463)	0.947(0.948)	68.75(70.74)
[2023-09-29 12:44:55 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1283)	0.0002(0.0244)	1.153(0.929)	71.88(70.39)
[2023-09-29 12:44:56 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1201)	0.0003(0.0166)	1.078(0.922)	56.25(69.56)
[2023-09-29 12:44:57 10splitTasks](trainer.py 286): INFO [40/157]	0.1052(0.1160)	0.0007(0.0127)	0.983(0.909)	65.62(69.97)
[2023-09-29 12:44:58 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1133)	0.0003(0.0102)	1.139(0.896)	71.88(70.34)
[2023-09-29 12:44:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1117)	0.0003(0.0086)	0.696(0.890)	78.12(70.34)
[2023-09-29 12:45:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1107)	0.0003(0.0075)	1.109(0.905)	68.75(70.11)
[2023-09-29 12:45:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1097)	0.0002(0.0066)	0.895(0.901)	65.62(70.22)
[2023-09-29 12:45:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1079(0.1092)	0.0002(0.0059)	0.963(0.900)	56.25(70.30)
[2023-09-29 12:45:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1086)	0.0002(0.0053)	1.001(0.903)	81.25(70.02)
[2023-09-29 12:45:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1035(0.1080)	0.0003(0.0049)	0.867(0.905)	78.12(70.16)
[2023-09-29 12:45:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1075)	0.0002(0.0045)	1.454(0.910)	65.62(70.14)
[2023-09-29 12:45:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1160(0.1073)	0.0006(0.0042)	0.764(0.920)	68.75(69.44)
[2023-09-29 12:45:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1056(0.1070)	0.0002(0.0039)	0.491(0.918)	84.38(69.41)
[2023-09-29 12:45:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1067)	0.0001(0.0037)	0.670(0.921)	68.75(69.39)
[2023-09-29 12:45:09 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1064)	0.0001(0.0036)	0.681(0.925)	75.00(69.36)
[2023-09-29 12:45:09 10splitTasks](trainer.py 288): INFO  * Train Acc 69.360
[2023-09-29 12:45:10 10splitTasks](my_trainer.py 503): INFO  * Val Acc 67.400, Total time 1.61
[2023-09-29 12:45:10 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 12:45:10 10splitTasks](my_trainer.py 308): INFO LR:0.009226027832676202
[2023-09-29 12:45:10 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:45:11 10splitTasks](trainer.py 286): INFO [0/157]	0.6278(0.6278)	0.5010(0.5010)	0.969(0.969)	71.88(71.88)
[2023-09-29 12:45:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1007(0.1514)	0.0001(0.0458)	0.889(0.851)	68.75(70.45)
[2023-09-29 12:45:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1287)	0.0002(0.0242)	0.563(0.820)	81.25(70.83)
[2023-09-29 12:45:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1205)	0.0002(0.0165)	1.044(0.823)	71.88(71.17)
[2023-09-29 12:45:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1037(0.1163)	0.0002(0.0125)	1.127(0.838)	62.50(70.58)
[2023-09-29 12:45:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1010(0.1134)	0.0002(0.0101)	0.841(0.832)	75.00(70.71)
[2023-09-29 12:45:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1116)	0.0003(0.0085)	1.631(0.843)	50.00(70.59)
[2023-09-29 12:45:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1104)	0.0002(0.0074)	0.801(0.850)	75.00(70.64)
[2023-09-29 12:45:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1094)	0.0002(0.0065)	0.978(0.860)	68.75(70.68)
[2023-09-29 12:45:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1088)	0.0002(0.0058)	1.228(0.867)	56.25(70.47)
[2023-09-29 12:45:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1025(0.1083)	0.0002(0.0053)	0.795(0.856)	75.00(70.73)
[2023-09-29 12:45:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1110(0.1078)	0.0006(0.0048)	0.856(0.861)	62.50(70.75)
[2023-09-29 12:45:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1119(0.1075)	0.0004(0.0045)	0.897(0.859)	75.00(71.02)
[2023-09-29 12:45:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1030(0.1070)	0.0003(0.0041)	1.085(0.860)	62.50(70.83)
[2023-09-29 12:45:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1030(0.1067)	0.0003(0.0039)	0.685(0.878)	78.12(70.52)
[2023-09-29 12:45:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1065)	0.0001(0.0036)	1.664(0.880)	53.12(70.45)
[2023-09-29 12:45:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1061)	0.0001(0.0035)	0.520(0.886)	75.00(70.40)
[2023-09-29 12:45:27 10splitTasks](trainer.py 288): INFO  * Train Acc 70.400
[2023-09-29 12:45:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 67.000, Total time 1.63
[2023-09-29 12:45:29 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 12:45:29 10splitTasks](my_trainer.py 308): INFO LR:0.008997314095635804
[2023-09-29 12:45:29 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:45:29 10splitTasks](trainer.py 286): INFO [0/157]	0.5860(0.5860)	0.4685(0.4685)	0.780(0.780)	62.50(62.50)
[2023-09-29 12:45:30 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1491)	0.0002(0.0432)	0.567(0.778)	84.38(71.88)
[2023-09-29 12:45:31 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1277)	0.0003(0.0227)	0.537(0.727)	81.25(75.15)
[2023-09-29 12:45:32 10splitTasks](trainer.py 286): INFO [30/157]	0.1094(0.1200)	0.0006(0.0155)	0.682(0.752)	71.88(74.40)
[2023-09-29 12:45:33 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1156)	0.0003(0.0118)	0.444(0.739)	78.12(74.85)
[2023-09-29 12:45:34 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1129)	0.0003(0.0096)	1.053(0.755)	75.00(74.02)
[2023-09-29 12:45:35 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1112)	0.0002(0.0080)	1.012(0.754)	65.62(74.13)
[2023-09-29 12:45:36 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1099)	0.0003(0.0069)	0.645(0.771)	71.88(73.64)
[2023-09-29 12:45:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1090)	0.0002(0.0061)	0.660(0.788)	78.12(72.80)
[2023-09-29 12:45:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1082)	0.0002(0.0055)	0.387(0.783)	87.50(73.15)
[2023-09-29 12:45:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1040(0.1077)	0.0002(0.0050)	0.724(0.787)	62.50(72.83)
[2023-09-29 12:45:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1058(0.1073)	0.0003(0.0046)	0.701(0.782)	75.00(73.11)
[2023-09-29 12:45:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1069)	0.0002(0.0042)	0.496(0.788)	81.25(73.06)
[2023-09-29 12:45:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1065)	0.0003(0.0039)	0.659(0.782)	78.12(73.14)
[2023-09-29 12:45:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1044(0.1063)	0.0002(0.0036)	1.055(0.780)	71.88(73.18)
[2023-09-29 12:45:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1061)	0.0001(0.0034)	0.635(0.788)	75.00(72.95)
[2023-09-29 12:45:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0798(0.1057)	0.0001(0.0033)	1.473(0.789)	50.00(72.90)
[2023-09-29 12:45:45 10splitTasks](trainer.py 288): INFO  * Train Acc 72.900
[2023-09-29 12:45:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 69.400, Total time 1.56
[2023-09-29 12:45:47 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 12:45:47 10splitTasks](my_trainer.py 308): INFO LR:0.00874267948548142
[2023-09-29 12:45:47 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:45:48 10splitTasks](trainer.py 286): INFO [0/157]	0.5632(0.5632)	0.4444(0.4444)	0.593(0.593)	84.38(84.38)
[2023-09-29 12:45:49 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1469)	0.0002(0.0407)	0.656(0.781)	78.12(72.44)
[2023-09-29 12:45:50 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1254)	0.0003(0.0214)	0.642(0.754)	78.12(72.92)
[2023-09-29 12:45:51 10splitTasks](trainer.py 286): INFO [30/157]	0.1032(0.1179)	0.0003(0.0146)	0.935(0.774)	65.62(72.78)
[2023-09-29 12:45:52 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1143)	0.0003(0.0111)	0.896(0.777)	68.75(72.94)
[2023-09-29 12:45:53 10splitTasks](trainer.py 286): INFO [50/157]	0.1040(0.1119)	0.0002(0.0090)	0.447(0.753)	81.25(73.47)
[2023-09-29 12:45:54 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1103)	0.0003(0.0076)	1.454(0.782)	59.38(72.54)
[2023-09-29 12:45:55 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1092)	0.0002(0.0065)	0.536(0.773)	78.12(72.84)
[2023-09-29 12:45:56 10splitTasks](trainer.py 286): INFO [80/157]	0.1040(0.1084)	0.0002(0.0058)	0.960(0.796)	65.62(72.18)
[2023-09-29 12:45:57 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1077)	0.0002(0.0052)	0.955(0.797)	68.75(72.12)
[2023-09-29 12:45:58 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1071)	0.0002(0.0047)	0.495(0.791)	90.62(72.37)
[2023-09-29 12:45:59 10splitTasks](trainer.py 286): INFO [110/157]	0.1033(0.1066)	0.0002(0.0043)	0.676(0.784)	75.00(72.49)
[2023-09-29 12:46:00 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1062)	0.0002(0.0039)	1.054(0.781)	65.62(72.73)
[2023-09-29 12:46:01 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1059)	0.0002(0.0037)	0.791(0.775)	65.62(73.09)
[2023-09-29 12:46:02 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1056)	0.0002(0.0034)	0.901(0.784)	71.88(72.89)
[2023-09-29 12:46:03 10splitTasks](trainer.py 286): INFO [150/157]	0.1005(0.1053)	0.0001(0.0032)	0.942(0.779)	75.00(73.10)
[2023-09-29 12:46:03 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1050)	0.0001(0.0031)	0.879(0.781)	62.50(73.04)
[2023-09-29 12:46:04 10splitTasks](trainer.py 288): INFO  * Train Acc 73.040
[2023-09-29 12:46:05 10splitTasks](my_trainer.py 503): INFO  * Val Acc 68.200, Total time 1.60
[2023-09-29 12:46:05 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 12:46:05 10splitTasks](my_trainer.py 308): INFO LR:0.008463775405371243
[2023-09-29 12:46:05 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:46:06 10splitTasks](trainer.py 286): INFO [0/157]	0.6320(0.6320)	0.5271(0.5271)	0.586(0.586)	81.25(81.25)
[2023-09-29 12:46:07 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1508)	0.0002(0.0483)	0.409(0.727)	87.50(75.57)
[2023-09-29 12:46:08 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1277)	0.0003(0.0255)	0.579(0.750)	87.50(74.55)
[2023-09-29 12:46:09 10splitTasks](trainer.py 286): INFO [30/157]	0.1049(0.1197)	0.0002(0.0174)	1.043(0.771)	59.38(72.98)
[2023-09-29 12:46:10 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1157)	0.0002(0.0133)	0.676(0.754)	71.88(73.17)
[2023-09-29 12:46:11 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1130)	0.0002(0.0107)	0.579(0.753)	84.38(73.28)
[2023-09-29 12:46:12 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1111)	0.0002(0.0090)	0.564(0.728)	71.88(74.08)
[2023-09-29 12:46:13 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1099)	0.0001(0.0078)	0.859(0.747)	68.75(73.64)
[2023-09-29 12:46:14 10splitTasks](trainer.py 286): INFO [80/157]	0.1053(0.1090)	0.0002(0.0069)	1.097(0.752)	68.75(73.50)
[2023-09-29 12:46:15 10splitTasks](trainer.py 286): INFO [90/157]	0.1011(0.1082)	0.0002(0.0061)	0.735(0.754)	81.25(73.63)
[2023-09-29 12:46:16 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1075)	0.0002(0.0056)	0.793(0.759)	75.00(73.61)
[2023-09-29 12:46:17 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1070)	0.0004(0.0051)	0.539(0.756)	78.12(73.90)
[2023-09-29 12:46:18 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1066)	0.0002(0.0047)	0.775(0.755)	75.00(74.07)
[2023-09-29 12:46:19 10splitTasks](trainer.py 286): INFO [130/157]	0.1025(0.1063)	0.0002(0.0043)	0.513(0.743)	81.25(74.33)
[2023-09-29 12:46:20 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1060)	0.0003(0.0041)	1.063(0.741)	68.75(74.31)
[2023-09-29 12:46:21 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1058)	0.0002(0.0038)	0.550(0.738)	81.25(74.34)
[2023-09-29 12:46:22 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1055)	0.0001(0.0037)	0.878(0.737)	75.00(74.28)
[2023-09-29 12:46:22 10splitTasks](trainer.py 288): INFO  * Train Acc 74.280
[2023-09-29 12:46:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 70.000, Total time 1.57
[2023-09-29 12:46:23 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 12:46:23 10splitTasks](my_trainer.py 308): INFO LR:0.008162410655289089
[2023-09-29 12:46:23 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:46:24 10splitTasks](trainer.py 286): INFO [0/157]	0.7228(0.7228)	0.6187(0.6187)	0.634(0.634)	81.25(81.25)
[2023-09-29 12:46:25 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1599)	0.0003(0.0565)	0.670(0.751)	71.88(72.16)
[2023-09-29 12:46:26 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1325)	0.0005(0.0298)	0.781(0.753)	81.25(73.21)
[2023-09-29 12:46:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1230)	0.0002(0.0203)	0.635(0.739)	78.12(74.29)
[2023-09-29 12:46:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1178)	0.0002(0.0154)	0.584(0.734)	81.25(74.92)
[2023-09-29 12:46:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1029(0.1151)	0.0003(0.0124)	0.406(0.733)	84.38(75.37)
[2023-09-29 12:46:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1130)	0.0003(0.0105)	0.561(0.737)	87.50(75.31)
[2023-09-29 12:46:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1114)	0.0003(0.0090)	1.031(0.739)	62.50(75.31)
[2023-09-29 12:46:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1104)	0.0003(0.0079)	0.656(0.725)	78.12(75.73)
[2023-09-29 12:46:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1095)	0.0003(0.0071)	0.754(0.732)	71.88(75.41)
[2023-09-29 12:46:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1088)	0.0003(0.0064)	0.755(0.732)	68.75(75.37)
[2023-09-29 12:46:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1104(0.1082)	0.0002(0.0059)	0.497(0.723)	81.25(75.65)
[2023-09-29 12:46:36 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1077)	0.0002(0.0054)	1.066(0.726)	71.88(75.59)
[2023-09-29 12:46:37 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1072)	0.0003(0.0050)	0.584(0.725)	78.12(75.62)
[2023-09-29 12:46:38 10splitTasks](trainer.py 286): INFO [140/157]	0.1008(0.1068)	0.0002(0.0047)	0.623(0.721)	84.38(75.58)
[2023-09-29 12:46:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1066)	0.0001(0.0044)	0.538(0.719)	71.88(75.64)
[2023-09-29 12:46:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0808(0.1062)	0.0001(0.0042)	0.987(0.720)	62.50(75.60)
[2023-09-29 12:46:40 10splitTasks](trainer.py 288): INFO  * Train Acc 75.600
[2023-09-29 12:46:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 68.600, Total time 1.60
[2023-09-29 12:46:42 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 12:46:42 10splitTasks](my_trainer.py 308): INFO LR:0.007840539701282413
[2023-09-29 12:46:42 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:46:42 10splitTasks](trainer.py 286): INFO [0/157]	0.6490(0.6490)	0.5355(0.5355)	0.720(0.720)	71.88(71.88)
[2023-09-29 12:46:44 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1537)	0.0002(0.0489)	0.457(0.754)	84.38(73.30)
[2023-09-29 12:46:45 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1296)	0.0003(0.0258)	1.016(0.761)	59.38(72.77)
[2023-09-29 12:46:46 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1209)	0.0002(0.0176)	0.784(0.717)	75.00(75.10)
[2023-09-29 12:46:47 10splitTasks](trainer.py 286): INFO [40/157]	0.1084(0.1164)	0.0003(0.0133)	0.716(0.682)	81.25(76.45)
[2023-09-29 12:46:48 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1137)	0.0004(0.0108)	0.690(0.667)	78.12(77.45)
[2023-09-29 12:46:49 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1120)	0.0002(0.0091)	0.825(0.672)	78.12(77.87)
[2023-09-29 12:46:50 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1107)	0.0002(0.0079)	0.482(0.666)	90.62(77.90)
[2023-09-29 12:46:51 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1099)	0.0003(0.0069)	0.631(0.665)	78.12(77.93)
[2023-09-29 12:46:52 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1091)	0.0004(0.0062)	0.603(0.659)	71.88(78.06)
[2023-09-29 12:46:53 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1086)	0.0002(0.0056)	0.896(0.670)	68.75(77.72)
[2023-09-29 12:46:54 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1080)	0.0003(0.0051)	0.857(0.666)	68.75(77.76)
[2023-09-29 12:46:55 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1075)	0.0003(0.0047)	1.209(0.684)	53.12(77.22)
[2023-09-29 12:46:56 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1071)	0.0002(0.0044)	0.682(0.687)	78.12(77.05)
[2023-09-29 12:46:57 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1067)	0.0003(0.0041)	0.618(0.689)	78.12(76.97)
[2023-09-29 12:46:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1065)	0.0001(0.0039)	0.782(0.690)	75.00(76.80)
[2023-09-29 12:46:58 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1062)	0.0001(0.0037)	1.321(0.694)	50.00(76.56)
[2023-09-29 12:46:59 10splitTasks](trainer.py 288): INFO  * Train Acc 76.560
[2023-09-29 12:47:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.000, Total time 1.57
[2023-09-29 12:47:00 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 12:47:00 10splitTasks](my_trainer.py 308): INFO LR:0.007500250000000001
[2023-09-29 12:47:00 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:47:01 10splitTasks](trainer.py 286): INFO [0/157]	0.6378(0.6378)	0.5285(0.5285)	0.509(0.509)	84.38(84.38)
[2023-09-29 12:47:02 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1516)	0.0002(0.0483)	1.010(0.728)	62.50(75.00)
[2023-09-29 12:47:03 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1281)	0.0002(0.0255)	0.794(0.732)	78.12(75.15)
[2023-09-29 12:47:04 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1198)	0.0003(0.0173)	1.314(0.763)	43.75(73.79)
[2023-09-29 12:47:05 10splitTasks](trainer.py 286): INFO [40/157]	0.1045(0.1154)	0.0002(0.0132)	0.817(0.760)	78.12(74.09)
[2023-09-29 12:47:06 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1127)	0.0001(0.0107)	0.819(0.748)	62.50(74.39)
[2023-09-29 12:47:07 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1109)	0.0003(0.0090)	0.662(0.745)	78.12(74.69)
[2023-09-29 12:47:08 10splitTasks](trainer.py 286): INFO [70/157]	0.1062(0.1099)	0.0003(0.0077)	0.670(0.729)	75.00(75.31)
[2023-09-29 12:47:09 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1089)	0.0002(0.0068)	0.553(0.712)	78.12(76.16)
[2023-09-29 12:47:10 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1082)	0.0003(0.0061)	0.434(0.710)	90.62(76.34)
[2023-09-29 12:47:11 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1077)	0.0003(0.0055)	0.856(0.709)	78.12(76.33)
[2023-09-29 12:47:12 10splitTasks](trainer.py 286): INFO [110/157]	0.1051(0.1073)	0.0018(0.0051)	0.782(0.707)	78.12(76.32)
[2023-09-29 12:47:13 10splitTasks](trainer.py 286): INFO [120/157]	0.1037(0.1070)	0.0006(0.0047)	0.887(0.707)	65.62(76.21)
[2023-09-29 12:47:14 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1066)	0.0002(0.0044)	0.883(0.702)	65.62(76.55)
[2023-09-29 12:47:15 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1064)	0.0002(0.0041)	0.609(0.693)	75.00(76.60)
[2023-09-29 12:47:16 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1061)	0.0001(0.0038)	0.581(0.689)	75.00(76.61)
[2023-09-29 12:47:17 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1058)	0.0001(0.0037)	1.213(0.697)	75.00(76.30)
[2023-09-29 12:47:17 10splitTasks](trainer.py 288): INFO  * Train Acc 76.300
[2023-09-29 12:47:18 10splitTasks](my_trainer.py 503): INFO  * Val Acc 70.800, Total time 1.49
[2023-09-29 12:47:18 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 12:47:18 10splitTasks](my_trainer.py 308): INFO LR:0.0071437484607345695
[2023-09-29 12:47:18 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:47:19 10splitTasks](trainer.py 286): INFO [0/157]	0.6273(0.6273)	0.5220(0.5220)	0.324(0.324)	87.50(87.50)
[2023-09-29 12:47:20 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1516)	0.0002(0.0477)	0.815(0.591)	71.88(80.97)
[2023-09-29 12:47:21 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1278)	0.0002(0.0251)	0.422(0.559)	93.75(81.55)
[2023-09-29 12:47:22 10splitTasks](trainer.py 286): INFO [30/157]	0.1034(0.1199)	0.0002(0.0171)	0.798(0.572)	75.00(80.14)
[2023-09-29 12:47:23 10splitTasks](trainer.py 286): INFO [40/157]	0.1065(0.1156)	0.0002(0.0130)	0.609(0.586)	78.12(79.50)
[2023-09-29 12:47:24 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1129)	0.0003(0.0105)	0.734(0.609)	78.12(79.29)
[2023-09-29 12:47:25 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1111)	0.0004(0.0088)	0.889(0.619)	78.12(78.94)
[2023-09-29 12:47:26 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1101)	0.0003(0.0077)	0.658(0.606)	71.88(79.09)
[2023-09-29 12:47:27 10splitTasks](trainer.py 286): INFO [80/157]	0.1050(0.1092)	0.0001(0.0067)	0.604(0.614)	81.25(78.86)
[2023-09-29 12:47:28 10splitTasks](trainer.py 286): INFO [90/157]	0.1055(0.1086)	0.0002(0.0060)	1.032(0.615)	65.62(78.78)
[2023-09-29 12:47:29 10splitTasks](trainer.py 286): INFO [100/157]	0.1028(0.1082)	0.0002(0.0055)	0.664(0.620)	75.00(78.65)
[2023-09-29 12:47:30 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1078)	0.0002(0.0050)	0.729(0.628)	75.00(78.43)
[2023-09-29 12:47:31 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1075)	0.0002(0.0046)	0.641(0.627)	81.25(78.59)
[2023-09-29 12:47:32 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1071)	0.0003(0.0043)	0.465(0.630)	81.25(78.46)
[2023-09-29 12:47:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1071)	0.0002(0.0040)	0.502(0.626)	87.50(78.44)
[2023-09-29 12:47:35 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1068)	0.0001(0.0038)	0.610(0.645)	90.62(78.17)
[2023-09-29 12:47:35 10splitTasks](trainer.py 286): INFO [156/157]	0.0800(0.1064)	0.0001(0.0037)	1.219(0.647)	62.50(78.18)
[2023-09-29 12:47:35 10splitTasks](trainer.py 288): INFO  * Train Acc 78.180
[2023-09-29 12:47:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 67.400, Total time 1.59
[2023-09-29 12:47:37 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 12:47:37 10splitTasks](my_trainer.py 308): INFO LR:0.0067733471327691575
[2023-09-29 12:47:37 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:47:37 10splitTasks](trainer.py 286): INFO [0/157]	0.6547(0.6547)	0.5512(0.5512)	0.512(0.512)	84.38(84.38)
[2023-09-29 12:47:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1539)	0.0002(0.0504)	0.461(0.693)	90.62(77.56)
[2023-09-29 12:47:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1027(0.1299)	0.0001(0.0266)	0.751(0.657)	81.25(78.87)
[2023-09-29 12:47:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1024(0.1211)	0.0003(0.0181)	0.584(0.645)	78.12(79.64)
[2023-09-29 12:47:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1168)	0.0002(0.0137)	0.513(0.623)	78.12(79.95)
[2023-09-29 12:47:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1012(0.1138)	0.0003(0.0111)	1.504(0.621)	65.62(80.09)
[2023-09-29 12:47:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1122)	0.0002(0.0093)	0.781(0.623)	65.62(79.76)
[2023-09-29 12:47:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1111)	0.0003(0.0081)	0.355(0.620)	78.12(79.49)
[2023-09-29 12:47:46 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1100)	0.0003(0.0071)	0.725(0.615)	78.12(79.71)
[2023-09-29 12:47:47 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1092)	0.0004(0.0064)	0.622(0.609)	81.25(79.81)
[2023-09-29 12:47:48 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1084)	0.0002(0.0058)	0.654(0.612)	71.88(79.89)
[2023-09-29 12:47:49 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1078)	0.0001(0.0053)	0.560(0.607)	75.00(79.98)
[2023-09-29 12:47:50 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1073)	0.0002(0.0049)	0.413(0.615)	84.38(79.75)
[2023-09-29 12:47:51 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1069)	0.0002(0.0045)	0.777(0.619)	71.88(79.58)
[2023-09-29 12:47:52 10splitTasks](trainer.py 286): INFO [140/157]	0.1056(0.1066)	0.0003(0.0042)	0.697(0.616)	78.12(79.68)
[2023-09-29 12:47:53 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1063)	0.0001(0.0040)	0.505(0.613)	78.12(79.68)
[2023-09-29 12:47:53 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1060)	0.0001(0.0038)	0.560(0.612)	87.50(79.72)
[2023-09-29 12:47:54 10splitTasks](trainer.py 288): INFO  * Train Acc 79.720
[2023-09-29 12:47:55 10splitTasks](my_trainer.py 503): INFO  * Val Acc 71.000, Total time 1.60
[2023-09-29 12:47:55 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 12:47:55 10splitTasks](my_trainer.py 308): INFO LR:0.006391448210850306
[2023-09-29 12:47:55 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:47:56 10splitTasks](trainer.py 286): INFO [0/157]	0.5755(0.5755)	0.4454(0.4454)	0.409(0.409)	81.25(81.25)
[2023-09-29 12:47:57 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1460)	0.0002(0.0407)	0.594(0.521)	75.00(81.25)
[2023-09-29 12:47:58 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1247)	0.0002(0.0215)	0.592(0.506)	78.12(81.85)
[2023-09-29 12:47:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1175)	0.0002(0.0147)	0.916(0.580)	71.88(81.75)
[2023-09-29 12:48:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1038(0.1137)	0.0004(0.0111)	0.412(0.568)	84.38(81.40)
[2023-09-29 12:48:01 10splitTasks](trainer.py 286): INFO [50/157]	0.1024(0.1116)	0.0003(0.0090)	0.326(0.561)	87.50(81.13)
[2023-09-29 12:48:02 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1106)	0.0004(0.0076)	0.630(0.540)	78.12(81.76)
[2023-09-29 12:48:03 10splitTasks](trainer.py 286): INFO [70/157]	0.1008(0.1097)	0.0002(0.0066)	0.775(0.541)	75.00(81.82)
[2023-09-29 12:48:04 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1088)	0.0003(0.0058)	0.542(0.547)	75.00(81.71)
[2023-09-29 12:48:05 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1083)	0.0003(0.0052)	0.776(0.547)	78.12(81.80)
[2023-09-29 12:48:06 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1077)	0.0003(0.0048)	0.748(0.539)	71.88(82.09)
[2023-09-29 12:48:07 10splitTasks](trainer.py 286): INFO [110/157]	0.1028(0.1072)	0.0004(0.0044)	0.470(0.541)	84.38(82.07)
[2023-09-29 12:48:08 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1069)	0.0002(0.0041)	0.578(0.541)	75.00(81.84)
[2023-09-29 12:48:09 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1065)	0.0002(0.0038)	0.441(0.546)	84.38(81.66)
[2023-09-29 12:48:10 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1062)	0.0002(0.0035)	0.343(0.546)	87.50(81.58)
[2023-09-29 12:48:11 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1060)	0.0002(0.0033)	0.583(0.553)	78.12(81.35)
[2023-09-29 12:48:12 10splitTasks](trainer.py 286): INFO [156/157]	0.0827(0.1057)	0.0001(0.0032)	0.439(0.551)	75.00(81.38)
[2023-09-29 12:48:12 10splitTasks](trainer.py 288): INFO  * Train Acc 81.380
[2023-09-29 12:48:13 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.000, Total time 1.61
[2023-09-29 12:48:13 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 12:48:13 10splitTasks](my_trainer.py 308): INFO LR:0.006000528456033335
[2023-09-29 12:48:13 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:48:14 10splitTasks](trainer.py 286): INFO [0/157]	0.7243(0.7243)	0.6133(0.6133)	0.323(0.323)	90.62(90.62)
[2023-09-29 12:48:15 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1625)	0.0002(0.0561)	0.578(0.514)	78.12(81.25)
[2023-09-29 12:48:16 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1338)	0.0002(0.0295)	0.504(0.541)	90.62(81.85)
[2023-09-29 12:48:17 10splitTasks](trainer.py 286): INFO [30/157]	0.1052(0.1240)	0.0002(0.0201)	0.212(0.499)	93.75(82.86)
[2023-09-29 12:48:18 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1191)	0.0002(0.0153)	0.545(0.506)	81.25(82.16)
[2023-09-29 12:48:19 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1157)	0.0002(0.0124)	0.705(0.501)	68.75(82.66)
[2023-09-29 12:48:20 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1138)	0.0003(0.0104)	0.257(0.500)	93.75(82.74)
[2023-09-29 12:48:21 10splitTasks](trainer.py 286): INFO [70/157]	0.1085(0.1123)	0.0002(0.0090)	0.451(0.498)	87.50(82.97)
[2023-09-29 12:48:22 10splitTasks](trainer.py 286): INFO [80/157]	0.1097(0.1112)	0.0006(0.0079)	0.608(0.511)	78.12(82.68)
[2023-09-29 12:48:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1104)	0.0002(0.0071)	0.358(0.519)	87.50(82.38)
[2023-09-29 12:48:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1096)	0.0003(0.0064)	0.721(0.528)	75.00(82.15)
[2023-09-29 12:48:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1089)	0.0003(0.0059)	0.576(0.530)	78.12(82.01)
[2023-09-29 12:48:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1086)	0.0002(0.0054)	0.351(0.522)	87.50(82.21)
[2023-09-29 12:48:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1081)	0.0002(0.0050)	0.646(0.524)	84.38(82.16)
[2023-09-29 12:48:29 10splitTasks](trainer.py 286): INFO [140/157]	0.1023(0.1077)	0.0002(0.0047)	0.835(0.529)	71.88(82.09)
[2023-09-29 12:48:30 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1074)	0.0002(0.0044)	0.237(0.524)	93.75(82.20)
[2023-09-29 12:48:30 10splitTasks](trainer.py 286): INFO [156/157]	0.0795(0.1070)	0.0001(0.0042)	0.963(0.525)	87.50(82.28)
[2023-09-29 12:48:30 10splitTasks](trainer.py 288): INFO  * Train Acc 82.280
[2023-09-29 12:48:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 71.800, Total time 1.63
[2023-09-29 12:48:32 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 12:48:32 10splitTasks](my_trainer.py 308): INFO LR:0.005603123132936488
[2023-09-29 12:48:32 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:48:33 10splitTasks](trainer.py 286): INFO [0/157]	0.6514(0.6514)	0.5464(0.5464)	0.279(0.279)	93.75(93.75)
[2023-09-29 12:48:34 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1533)	0.0003(0.0501)	0.278(0.450)	87.50(84.66)
[2023-09-29 12:48:35 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1292)	0.0003(0.0264)	0.483(0.461)	84.38(84.08)
[2023-09-29 12:48:36 10splitTasks](trainer.py 286): INFO [30/157]	0.1044(0.1206)	0.0003(0.0180)	0.330(0.459)	90.62(84.48)
[2023-09-29 12:48:37 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1159)	0.0003(0.0137)	0.896(0.479)	68.75(83.77)
[2023-09-29 12:48:38 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1132)	0.0001(0.0110)	0.733(0.485)	65.62(83.76)
[2023-09-29 12:48:39 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1117)	0.0003(0.0093)	0.416(0.487)	81.25(83.61)
[2023-09-29 12:48:40 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1105)	0.0003(0.0080)	0.445(0.491)	87.50(83.45)
[2023-09-29 12:48:41 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1095)	0.0002(0.0071)	0.542(0.495)	71.88(83.29)
[2023-09-29 12:48:42 10splitTasks](trainer.py 286): INFO [90/157]	0.1103(0.1089)	0.0003(0.0063)	0.473(0.509)	84.38(82.90)
[2023-09-29 12:48:43 10splitTasks](trainer.py 286): INFO [100/157]	0.1046(0.1084)	0.0004(0.0057)	0.344(0.505)	87.50(82.98)
[2023-09-29 12:48:44 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1081)	0.0003(0.0053)	0.709(0.509)	84.38(82.80)
[2023-09-29 12:48:45 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1078)	0.0002(0.0049)	0.853(0.516)	78.12(82.77)
[2023-09-29 12:48:46 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1074)	0.0001(0.0045)	1.027(0.521)	65.62(82.51)
[2023-09-29 12:48:47 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1073)	0.0002(0.0042)	0.634(0.527)	81.25(82.18)
[2023-09-29 12:48:48 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1070)	0.0002(0.0040)	0.501(0.527)	90.62(82.20)
[2023-09-29 12:48:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1067)	0.0001(0.0038)	0.263(0.524)	87.50(82.24)
[2023-09-29 12:48:49 10splitTasks](trainer.py 288): INFO  * Train Acc 82.240
[2023-09-29 12:48:50 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.600, Total time 1.59
[2023-09-29 12:48:50 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 12:48:50 10splitTasks](my_trainer.py 308): INFO LR:0.005201809567577022
[2023-09-29 12:48:50 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:48:51 10splitTasks](trainer.py 286): INFO [0/157]	0.5934(0.5934)	0.4720(0.4720)	0.428(0.428)	75.00(75.00)
[2023-09-29 12:48:52 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1477)	0.0003(0.0432)	0.305(0.508)	87.50(80.11)
[2023-09-29 12:48:53 10splitTasks](trainer.py 286): INFO [20/157]	0.1008(0.1256)	0.0002(0.0228)	0.646(0.450)	78.12(83.63)
[2023-09-29 12:48:54 10splitTasks](trainer.py 286): INFO [30/157]	0.1009(0.1178)	0.0002(0.0155)	0.491(0.482)	84.38(82.96)
[2023-09-29 12:48:55 10splitTasks](trainer.py 286): INFO [40/157]	0.1009(0.1139)	0.0002(0.0118)	0.644(0.478)	84.38(83.69)
[2023-09-29 12:48:56 10splitTasks](trainer.py 286): INFO [50/157]	0.1007(0.1118)	0.0001(0.0096)	0.313(0.473)	84.38(83.64)
[2023-09-29 12:48:57 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1103)	0.0002(0.0080)	0.499(0.472)	87.50(83.56)
[2023-09-29 12:48:58 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1091)	0.0002(0.0069)	0.389(0.469)	93.75(83.93)
[2023-09-29 12:48:59 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1082)	0.0002(0.0061)	0.326(0.477)	84.38(83.68)
[2023-09-29 12:49:00 10splitTasks](trainer.py 286): INFO [90/157]	0.1073(0.1076)	0.0007(0.0055)	0.422(0.481)	87.50(83.48)
[2023-09-29 12:49:01 10splitTasks](trainer.py 286): INFO [100/157]	0.1049(0.1072)	0.0002(0.0050)	0.270(0.480)	96.88(83.79)
[2023-09-29 12:49:02 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1067)	0.0001(0.0045)	0.518(0.483)	81.25(83.67)
[2023-09-29 12:49:03 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1065)	0.0002(0.0042)	0.338(0.489)	87.50(83.42)
[2023-09-29 12:49:04 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1061)	0.0002(0.0039)	0.562(0.487)	84.38(83.56)
[2023-09-29 12:49:05 10splitTasks](trainer.py 286): INFO [140/157]	0.1036(0.1059)	0.0002(0.0037)	0.280(0.482)	87.50(83.58)
[2023-09-29 12:49:06 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1058)	0.0001(0.0034)	0.417(0.482)	84.38(83.49)
[2023-09-29 12:49:07 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1054)	0.0001(0.0033)	0.137(0.479)	100.00(83.58)
[2023-09-29 12:49:07 10splitTasks](trainer.py 288): INFO  * Train Acc 83.580
[2023-09-29 12:49:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.800, Total time 1.67
[2023-09-29 12:49:09 10splitTasks](my_trainer.py 302): INFO Epoch:20
[2023-09-29 12:49:09 10splitTasks](my_trainer.py 308): INFO LR:0.004799190432422981
[2023-09-29 12:49:09 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:49:09 10splitTasks](trainer.py 286): INFO [0/157]	0.5836(0.5836)	0.4664(0.4664)	0.417(0.417)	78.12(78.12)
[2023-09-29 12:49:10 10splitTasks](trainer.py 286): INFO [10/157]	0.1009(0.1468)	0.0002(0.0427)	0.666(0.456)	81.25(84.66)
[2023-09-29 12:49:11 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1253)	0.0002(0.0225)	0.434(0.408)	84.38(86.01)
[2023-09-29 12:49:12 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1178)	0.0002(0.0153)	0.385(0.429)	81.25(85.48)
[2023-09-29 12:49:14 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1142)	0.0002(0.0117)	0.352(0.435)	87.50(85.44)
[2023-09-29 12:49:15 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1121)	0.0002(0.0094)	0.228(0.422)	93.75(85.97)
[2023-09-29 12:49:16 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1105)	0.0002(0.0080)	0.271(0.415)	93.75(86.22)
[2023-09-29 12:49:17 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1095)	0.0002(0.0069)	0.708(0.429)	68.75(85.87)
[2023-09-29 12:49:18 10splitTasks](trainer.py 286): INFO [80/157]	0.1044(0.1088)	0.0002(0.0061)	0.249(0.429)	90.62(85.96)
[2023-09-29 12:49:19 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1082)	0.0002(0.0054)	0.440(0.423)	84.38(86.16)
[2023-09-29 12:49:20 10splitTasks](trainer.py 286): INFO [100/157]	0.1057(0.1077)	0.0002(0.0049)	0.271(0.426)	93.75(85.98)
[2023-09-29 12:49:21 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1073)	0.0003(0.0045)	0.892(0.434)	65.62(85.61)
[2023-09-29 12:49:22 10splitTasks](trainer.py 286): INFO [120/157]	0.1195(0.1071)	0.0003(0.0042)	0.254(0.434)	90.62(85.51)
[2023-09-29 12:49:23 10splitTasks](trainer.py 286): INFO [130/157]	0.1038(0.1067)	0.0003(0.0039)	0.488(0.436)	81.25(85.42)
[2023-09-29 12:49:24 10splitTasks](trainer.py 286): INFO [140/157]	0.1046(0.1064)	0.0003(0.0036)	0.338(0.445)	87.50(85.15)
[2023-09-29 12:49:25 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1064)	0.0002(0.0034)	0.230(0.451)	96.88(84.89)
[2023-09-29 12:49:25 10splitTasks](trainer.py 286): INFO [156/157]	0.0812(0.1061)	0.0001(0.0033)	0.088(0.450)	100.00(84.88)
[2023-09-29 12:49:26 10splitTasks](trainer.py 288): INFO  * Train Acc 84.880
[2023-09-29 12:49:27 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.400, Total time 1.60
[2023-09-29 12:49:27 10splitTasks](my_trainer.py 302): INFO Epoch:21
[2023-09-29 12:49:27 10splitTasks](my_trainer.py 308): INFO LR:0.004397876867063513
[2023-09-29 12:49:27 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:49:28 10splitTasks](trainer.py 286): INFO [0/157]	0.6004(0.6004)	0.4957(0.4957)	0.282(0.282)	93.75(93.75)
[2023-09-29 12:49:29 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1485)	0.0002(0.0454)	0.411(0.350)	84.38(88.35)
[2023-09-29 12:49:30 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1267)	0.0003(0.0240)	0.539(0.402)	87.50(85.86)
[2023-09-29 12:49:31 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1188)	0.0002(0.0164)	0.419(0.398)	84.38(85.99)
[2023-09-29 12:49:32 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1154)	0.0002(0.0125)	0.447(0.399)	81.25(86.28)
[2023-09-29 12:49:33 10splitTasks](trainer.py 286): INFO [50/157]	0.1012(0.1128)	0.0002(0.0101)	0.328(0.399)	87.50(85.91)
[2023-09-29 12:49:34 10splitTasks](trainer.py 286): INFO [60/157]	0.1046(0.1110)	0.0002(0.0085)	0.492(0.396)	87.50(86.12)
[2023-09-29 12:49:35 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1097)	0.0002(0.0073)	0.314(0.397)	87.50(85.92)
[2023-09-29 12:49:36 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1088)	0.0002(0.0065)	0.517(0.404)	78.12(85.69)
[2023-09-29 12:49:37 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1080)	0.0003(0.0058)	0.265(0.410)	96.88(85.82)
[2023-09-29 12:49:38 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1077)	0.0002(0.0052)	0.368(0.414)	90.62(85.61)
[2023-09-29 12:49:39 10splitTasks](trainer.py 286): INFO [110/157]	0.1041(0.1073)	0.0002(0.0048)	0.397(0.420)	87.50(85.42)
[2023-09-29 12:49:40 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1070)	0.0002(0.0044)	0.268(0.415)	90.62(85.67)
[2023-09-29 12:49:41 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1068)	0.0002(0.0041)	0.567(0.415)	78.12(85.69)
[2023-09-29 12:49:42 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1066)	0.0002(0.0039)	0.433(0.412)	81.25(85.75)
[2023-09-29 12:49:43 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1063)	0.0001(0.0036)	0.687(0.419)	87.50(85.53)
[2023-09-29 12:49:44 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1060)	0.0001(0.0035)	0.939(0.419)	62.50(85.50)
[2023-09-29 12:49:44 10splitTasks](trainer.py 288): INFO  * Train Acc 85.500
[2023-09-29 12:49:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 75.000, Total time 1.56
[2023-09-29 12:49:46 10splitTasks](my_trainer.py 302): INFO Epoch:22
[2023-09-29 12:49:46 10splitTasks](my_trainer.py 308): INFO LR:0.004000471543966667
[2023-09-29 12:49:46 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:49:46 10splitTasks](trainer.py 286): INFO [0/157]	0.6791(0.6791)	0.5744(0.5744)	0.206(0.206)	90.62(90.62)
[2023-09-29 12:49:47 10splitTasks](trainer.py 286): INFO [10/157]	0.1042(0.1577)	0.0003(0.0525)	0.620(0.394)	78.12(86.93)
[2023-09-29 12:49:48 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1319)	0.0001(0.0277)	0.631(0.417)	71.88(85.57)
[2023-09-29 12:49:49 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1224)	0.0003(0.0189)	0.359(0.393)	84.38(85.89)
[2023-09-29 12:49:50 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1175)	0.0002(0.0143)	0.365(0.400)	84.38(85.90)
[2023-09-29 12:49:51 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1145)	0.0002(0.0116)	0.748(0.412)	75.00(85.78)
[2023-09-29 12:49:52 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1124)	0.0001(0.0097)	0.455(0.415)	81.25(85.60)
[2023-09-29 12:49:53 10splitTasks](trainer.py 286): INFO [70/157]	0.1121(0.1111)	0.0002(0.0084)	0.201(0.413)	90.62(85.61)
[2023-09-29 12:49:54 10splitTasks](trainer.py 286): INFO [80/157]	0.1023(0.1100)	0.0003(0.0074)	0.838(0.418)	71.88(85.46)
[2023-09-29 12:49:55 10splitTasks](trainer.py 286): INFO [90/157]	0.1065(0.1094)	0.0002(0.0066)	0.462(0.420)	84.38(85.51)
[2023-09-29 12:49:57 10splitTasks](trainer.py 286): INFO [100/157]	0.1044(0.1088)	0.0002(0.0060)	0.199(0.415)	93.75(85.52)
[2023-09-29 12:49:58 10splitTasks](trainer.py 286): INFO [110/157]	0.1055(0.1084)	0.0002(0.0055)	0.085(0.412)	96.88(85.47)
[2023-09-29 12:49:59 10splitTasks](trainer.py 286): INFO [120/157]	0.1040(0.1080)	0.0002(0.0051)	0.355(0.416)	87.50(85.38)
[2023-09-29 12:50:00 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1076)	0.0002(0.0047)	0.845(0.421)	75.00(85.19)
[2023-09-29 12:50:01 10splitTasks](trainer.py 286): INFO [140/157]	0.1044(0.1074)	0.0003(0.0044)	0.471(0.416)	84.38(85.33)
[2023-09-29 12:50:02 10splitTasks](trainer.py 286): INFO [150/157]	0.1021(0.1072)	0.0002(0.0042)	0.541(0.417)	78.12(85.31)
[2023-09-29 12:50:02 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1068)	0.0001(0.0040)	0.577(0.416)	87.50(85.40)
[2023-09-29 12:50:02 10splitTasks](trainer.py 288): INFO  * Train Acc 85.400
[2023-09-29 12:50:04 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.800, Total time 1.66
[2023-09-29 12:50:04 10splitTasks](my_trainer.py 302): INFO Epoch:23
[2023-09-29 12:50:04 10splitTasks](my_trainer.py 308): INFO LR:0.0036095517891496956
[2023-09-29 12:50:04 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:50:05 10splitTasks](trainer.py 286): INFO [0/157]	0.5948(0.5948)	0.4897(0.4897)	0.341(0.341)	81.25(81.25)
[2023-09-29 12:50:06 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1509)	0.0002(0.0448)	0.286(0.391)	90.62(86.93)
[2023-09-29 12:50:07 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1277)	0.0002(0.0236)	0.309(0.416)	87.50(86.01)
[2023-09-29 12:50:08 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1198)	0.0002(0.0161)	0.219(0.402)	93.75(86.49)
[2023-09-29 12:50:09 10splitTasks](trainer.py 286): INFO [40/157]	0.1027(0.1157)	0.0003(0.0122)	0.849(0.400)	81.25(86.81)
[2023-09-29 12:50:10 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1132)	0.0002(0.0099)	0.153(0.385)	93.75(87.13)
[2023-09-29 12:50:11 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1114)	0.0002(0.0083)	0.178(0.379)	96.88(87.50)
[2023-09-29 12:50:12 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1101)	0.0003(0.0072)	0.591(0.389)	75.00(86.97)
[2023-09-29 12:50:13 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1092)	0.0003(0.0064)	0.557(0.391)	84.38(86.92)
[2023-09-29 12:50:14 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1085)	0.0002(0.0057)	0.295(0.393)	93.75(86.74)
[2023-09-29 12:50:15 10splitTasks](trainer.py 286): INFO [100/157]	0.1025(0.1081)	0.0003(0.0052)	0.334(0.396)	93.75(86.66)
[2023-09-29 12:50:16 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1075)	0.0001(0.0047)	0.505(0.391)	84.38(86.82)
[2023-09-29 12:50:17 10splitTasks](trainer.py 286): INFO [120/157]	0.1038(0.1071)	0.0002(0.0044)	0.173(0.385)	96.88(87.04)
[2023-09-29 12:50:18 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1067)	0.0002(0.0040)	0.258(0.385)	90.62(87.09)
[2023-09-29 12:50:19 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1065)	0.0003(0.0038)	0.454(0.384)	81.25(87.12)
[2023-09-29 12:50:20 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1061)	0.0001(0.0036)	0.303(0.382)	90.62(87.17)
[2023-09-29 12:50:21 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1058)	0.0001(0.0034)	0.712(0.381)	75.00(87.24)
[2023-09-29 12:50:21 10splitTasks](trainer.py 288): INFO  * Train Acc 87.240
[2023-09-29 12:50:22 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.000, Total time 1.57
[2023-09-29 12:50:22 10splitTasks](my_trainer.py 302): INFO Epoch:24
[2023-09-29 12:50:22 10splitTasks](my_trainer.py 308): INFO LR:0.0032276528672308433
[2023-09-29 12:50:22 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:50:23 10splitTasks](trainer.py 286): INFO [0/157]	0.5948(0.5948)	0.4613(0.4613)	0.248(0.248)	90.62(90.62)
[2023-09-29 12:50:24 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1495)	0.0003(0.0422)	0.393(0.282)	84.38(88.92)
[2023-09-29 12:50:25 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1274)	0.0002(0.0222)	0.352(0.371)	87.50(86.46)
[2023-09-29 12:50:26 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1196)	0.0002(0.0152)	0.445(0.367)	81.25(87.00)
[2023-09-29 12:50:27 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1157)	0.0002(0.0115)	0.308(0.375)	84.38(86.66)
[2023-09-29 12:50:28 10splitTasks](trainer.py 286): INFO [50/157]	0.1035(0.1136)	0.0002(0.0093)	0.222(0.357)	96.88(87.50)
[2023-09-29 12:50:29 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1117)	0.0003(0.0079)	0.249(0.355)	93.75(87.04)
[2023-09-29 12:50:30 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1104)	0.0002(0.0068)	0.486(0.356)	87.50(87.06)
[2023-09-29 12:50:31 10splitTasks](trainer.py 286): INFO [80/157]	0.1418(0.1099)	0.0005(0.0060)	0.231(0.362)	90.62(86.88)
[2023-09-29 12:50:32 10splitTasks](trainer.py 286): INFO [90/157]	0.1026(0.1091)	0.0006(0.0054)	0.168(0.360)	93.75(87.02)
[2023-09-29 12:50:33 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1087)	0.0003(0.0049)	0.278(0.364)	87.50(86.79)
[2023-09-29 12:50:34 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1081)	0.0002(0.0045)	0.277(0.363)	87.50(86.80)
[2023-09-29 12:50:35 10splitTasks](trainer.py 286): INFO [120/157]	0.1087(0.1078)	0.0006(0.0041)	0.390(0.356)	90.62(87.14)
[2023-09-29 12:50:36 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1074)	0.0003(0.0038)	0.438(0.355)	84.38(87.07)
[2023-09-29 12:50:37 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1070)	0.0002(0.0036)	0.381(0.357)	93.75(87.19)
[2023-09-29 12:50:38 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1067)	0.0001(0.0034)	0.162(0.356)	96.88(87.27)
[2023-09-29 12:50:39 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1063)	0.0001(0.0033)	0.507(0.356)	75.00(87.24)
[2023-09-29 12:50:39 10splitTasks](trainer.py 288): INFO  * Train Acc 87.240
[2023-09-29 12:50:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.600, Total time 1.53
[2023-09-29 12:50:41 10splitTasks](my_trainer.py 302): INFO Epoch:25
[2023-09-29 12:50:41 10splitTasks](my_trainer.py 308): INFO LR:0.0028572515392654304
[2023-09-29 12:50:41 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:50:41 10splitTasks](trainer.py 286): INFO [0/157]	0.6279(0.6279)	0.5168(0.5168)	0.358(0.358)	87.50(87.50)
[2023-09-29 12:50:42 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1504)	0.0001(0.0473)	0.153(0.363)	93.75(87.78)
[2023-09-29 12:50:43 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1273)	0.0002(0.0249)	0.380(0.334)	87.50(88.10)
[2023-09-29 12:50:44 10splitTasks](trainer.py 286): INFO [30/157]	0.1093(0.1196)	0.0003(0.0170)	0.238(0.347)	93.75(88.10)
[2023-09-29 12:50:45 10splitTasks](trainer.py 286): INFO [40/157]	0.1036(0.1154)	0.0003(0.0129)	0.302(0.338)	87.50(88.41)
[2023-09-29 12:50:46 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1128)	0.0003(0.0104)	0.165(0.337)	93.75(88.05)
[2023-09-29 12:50:47 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1111)	0.0002(0.0088)	0.361(0.334)	90.62(88.52)
[2023-09-29 12:50:49 10splitTasks](trainer.py 286): INFO [70/157]	0.1053(0.1099)	0.0003(0.0076)	0.338(0.340)	87.50(88.25)
[2023-09-29 12:50:50 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1090)	0.0003(0.0067)	0.336(0.331)	78.12(88.43)
[2023-09-29 12:50:51 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1083)	0.0002(0.0060)	0.144(0.328)	93.75(88.56)
[2023-09-29 12:50:52 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1077)	0.0002(0.0054)	0.334(0.329)	90.62(88.71)
[2023-09-29 12:50:53 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1072)	0.0002(0.0050)	0.175(0.326)	90.62(88.74)
[2023-09-29 12:50:54 10splitTasks](trainer.py 286): INFO [120/157]	0.1045(0.1069)	0.0002(0.0046)	0.450(0.331)	87.50(88.64)
[2023-09-29 12:50:55 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1065)	0.0002(0.0043)	0.561(0.336)	84.38(88.48)
[2023-09-29 12:50:56 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1064)	0.0002(0.0040)	0.453(0.341)	84.38(88.30)
[2023-09-29 12:50:57 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1061)	0.0001(0.0037)	0.394(0.343)	84.38(88.22)
[2023-09-29 12:50:57 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1058)	0.0001(0.0036)	0.539(0.342)	87.50(88.24)
[2023-09-29 12:50:57 10splitTasks](trainer.py 288): INFO  * Train Acc 88.240
[2023-09-29 12:50:59 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.200, Total time 1.61
[2023-09-29 12:50:59 10splitTasks](my_trainer.py 302): INFO Epoch:26
[2023-09-29 12:50:59 10splitTasks](my_trainer.py 308): INFO LR:0.0025007500000000017
[2023-09-29 12:50:59 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:51:00 10splitTasks](trainer.py 286): INFO [0/157]	0.6030(0.6030)	0.4856(0.4856)	0.254(0.254)	90.62(90.62)
[2023-09-29 12:51:01 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1487)	0.0002(0.0444)	0.615(0.309)	81.25(91.19)
[2023-09-29 12:51:02 10splitTasks](trainer.py 286): INFO [20/157]	0.1042(0.1267)	0.0002(0.0234)	0.138(0.306)	96.88(89.88)
[2023-09-29 12:51:03 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1188)	0.0003(0.0160)	0.857(0.323)	62.50(88.10)
[2023-09-29 12:51:04 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1147)	0.0003(0.0121)	0.320(0.322)	87.50(88.03)
[2023-09-29 12:51:05 10splitTasks](trainer.py 286): INFO [50/157]	0.1031(0.1124)	0.0003(0.0098)	0.269(0.327)	90.62(88.11)
[2023-09-29 12:51:06 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1108)	0.0003(0.0083)	0.494(0.331)	87.50(88.17)
[2023-09-29 12:51:07 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1096)	0.0003(0.0071)	0.325(0.331)	87.50(88.29)
[2023-09-29 12:51:08 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1089)	0.0002(0.0063)	0.348(0.321)	90.62(88.73)
[2023-09-29 12:51:09 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1082)	0.0003(0.0056)	0.311(0.317)	84.38(88.91)
[2023-09-29 12:51:10 10splitTasks](trainer.py 286): INFO [100/157]	0.1028(0.1078)	0.0002(0.0051)	0.088(0.308)	96.88(89.33)
[2023-09-29 12:51:11 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1073)	0.0002(0.0047)	0.301(0.310)	93.75(89.25)
[2023-09-29 12:51:12 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1070)	0.0003(0.0043)	0.282(0.308)	87.50(89.28)
[2023-09-29 12:51:13 10splitTasks](trainer.py 286): INFO [130/157]	0.1093(0.1068)	0.0002(0.0040)	0.537(0.315)	84.38(88.96)
[2023-09-29 12:51:14 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1065)	0.0002(0.0037)	0.215(0.317)	93.75(88.96)
[2023-09-29 12:51:15 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1063)	0.0002(0.0035)	0.297(0.315)	90.62(89.03)
[2023-09-29 12:51:16 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1059)	0.0001(0.0034)	0.405(0.316)	87.50(89.04)
[2023-09-29 12:51:16 10splitTasks](trainer.py 288): INFO  * Train Acc 89.040
[2023-09-29 12:51:17 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.200, Total time 1.62
[2023-09-29 12:51:17 10splitTasks](my_trainer.py 302): INFO Epoch:27
[2023-09-29 12:51:17 10splitTasks](my_trainer.py 308): INFO LR:0.0021604602987175875
[2023-09-29 12:51:17 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:51:18 10splitTasks](trainer.py 286): INFO [0/157]	0.5935(0.5935)	0.4707(0.4707)	0.449(0.449)	81.25(81.25)
[2023-09-29 12:51:19 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1482)	0.0003(0.0431)	0.121(0.318)	96.88(88.07)
[2023-09-29 12:51:20 10splitTasks](trainer.py 286): INFO [20/157]	0.1042(0.1270)	0.0030(0.0228)	0.343(0.316)	87.50(88.10)
[2023-09-29 12:51:21 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1189)	0.0002(0.0155)	0.472(0.342)	78.12(88.10)
[2023-09-29 12:51:22 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1147)	0.0001(0.0118)	0.084(0.339)	100.00(88.57)
[2023-09-29 12:51:23 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1122)	0.0003(0.0095)	0.478(0.336)	78.12(88.73)
[2023-09-29 12:51:24 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1105)	0.0003(0.0080)	0.260(0.330)	96.88(88.73)
[2023-09-29 12:51:25 10splitTasks](trainer.py 286): INFO [70/157]	0.1024(0.1093)	0.0003(0.0069)	0.340(0.329)	84.38(89.08)
[2023-09-29 12:51:26 10splitTasks](trainer.py 286): INFO [80/157]	0.1030(0.1085)	0.0002(0.0061)	0.412(0.325)	84.38(89.16)
[2023-09-29 12:51:27 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1078)	0.0003(0.0055)	0.237(0.314)	87.50(89.63)
[2023-09-29 12:51:28 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1072)	0.0003(0.0050)	0.265(0.312)	93.75(89.63)
[2023-09-29 12:51:29 10splitTasks](trainer.py 286): INFO [110/157]	0.1051(0.1068)	0.0003(0.0045)	0.420(0.310)	81.25(89.61)
[2023-09-29 12:51:30 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1065)	0.0002(0.0042)	0.383(0.309)	87.50(89.75)
[2023-09-29 12:51:31 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1063)	0.0003(0.0039)	0.298(0.309)	84.38(89.69)
[2023-09-29 12:51:32 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1060)	0.0002(0.0037)	0.168(0.304)	87.50(89.80)
[2023-09-29 12:51:33 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1059)	0.0001(0.0034)	0.348(0.305)	84.38(89.80)
[2023-09-29 12:51:34 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1056)	0.0001(0.0033)	0.815(0.303)	62.50(89.82)
[2023-09-29 12:51:34 10splitTasks](trainer.py 288): INFO  * Train Acc 89.820
[2023-09-29 12:51:36 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.400, Total time 1.57
[2023-09-29 12:51:36 10splitTasks](my_trainer.py 302): INFO Epoch:28
[2023-09-29 12:51:36 10splitTasks](my_trainer.py 308): INFO LR:0.001838589344710912
[2023-09-29 12:51:36 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:51:36 10splitTasks](trainer.py 286): INFO [0/157]	0.5870(0.5870)	0.4723(0.4723)	0.576(0.576)	81.25(81.25)
[2023-09-29 12:51:37 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1477)	0.0001(0.0432)	0.342(0.317)	87.50(90.06)
[2023-09-29 12:51:38 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1261)	0.0003(0.0228)	0.220(0.301)	93.75(90.77)
[2023-09-29 12:51:39 10splitTasks](trainer.py 286): INFO [30/157]	0.1076(0.1187)	0.0002(0.0155)	0.216(0.306)	93.75(89.92)
[2023-09-29 12:51:40 10splitTasks](trainer.py 286): INFO [40/157]	0.1044(0.1153)	0.0002(0.0118)	0.678(0.290)	78.12(90.40)
[2023-09-29 12:51:41 10splitTasks](trainer.py 286): INFO [50/157]	0.1022(0.1131)	0.0002(0.0096)	0.229(0.290)	87.50(90.01)
[2023-09-29 12:51:42 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1114)	0.0003(0.0081)	0.306(0.297)	87.50(89.75)
[2023-09-29 12:51:43 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1101)	0.0003(0.0070)	0.124(0.294)	96.88(89.96)
[2023-09-29 12:51:45 10splitTasks](trainer.py 286): INFO [80/157]	0.1031(0.1092)	0.0003(0.0062)	0.160(0.294)	96.88(90.05)
[2023-09-29 12:51:46 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1085)	0.0003(0.0055)	0.138(0.285)	100.00(90.42)
[2023-09-29 12:51:47 10splitTasks](trainer.py 286): INFO [100/157]	0.1025(0.1081)	0.0006(0.0050)	0.569(0.286)	78.12(90.22)
[2023-09-29 12:51:48 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1076)	0.0002(0.0046)	0.178(0.284)	90.62(90.37)
[2023-09-29 12:51:49 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1072)	0.0003(0.0042)	0.226(0.285)	93.75(90.29)
[2023-09-29 12:51:50 10splitTasks](trainer.py 286): INFO [130/157]	0.1056(0.1069)	0.0002(0.0039)	0.219(0.285)	93.75(90.27)
[2023-09-29 12:51:51 10splitTasks](trainer.py 286): INFO [140/157]	0.1041(0.1067)	0.0002(0.0037)	0.352(0.284)	87.50(90.25)
[2023-09-29 12:51:52 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1064)	0.0001(0.0035)	0.188(0.286)	93.75(90.17)
[2023-09-29 12:51:52 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1060)	0.0001(0.0033)	1.004(0.286)	62.50(90.16)
[2023-09-29 12:51:52 10splitTasks](trainer.py 288): INFO  * Train Acc 90.160
[2023-09-29 12:51:54 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.800, Total time 1.56
[2023-09-29 12:51:54 10splitTasks](my_trainer.py 302): INFO Epoch:29
[2023-09-29 12:51:54 10splitTasks](my_trainer.py 308): INFO LR:0.001537224594628758
[2023-09-29 12:51:54 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:51:55 10splitTasks](trainer.py 286): INFO [0/157]	0.5915(0.5915)	0.4823(0.4823)	0.381(0.381)	87.50(87.50)
[2023-09-29 12:51:56 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1477)	0.0002(0.0441)	0.414(0.312)	87.50(90.91)
[2023-09-29 12:51:57 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1261)	0.0002(0.0233)	0.195(0.260)	93.75(91.96)
[2023-09-29 12:51:58 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1184)	0.0002(0.0159)	0.154(0.250)	96.88(91.94)
[2023-09-29 12:51:59 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1146)	0.0002(0.0121)	0.104(0.241)	96.88(92.00)
[2023-09-29 12:52:00 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1123)	0.0002(0.0098)	0.130(0.240)	96.88(91.97)
[2023-09-29 12:52:01 10splitTasks](trainer.py 286): INFO [60/157]	0.1028(0.1106)	0.0002(0.0083)	0.277(0.252)	90.62(91.70)
[2023-09-29 12:52:02 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1095)	0.0002(0.0071)	0.225(0.257)	93.75(91.73)
[2023-09-29 12:52:03 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1089)	0.0002(0.0063)	0.073(0.264)	100.00(91.47)
[2023-09-29 12:52:04 10splitTasks](trainer.py 286): INFO [90/157]	0.1037(0.1084)	0.0002(0.0056)	0.373(0.259)	90.62(91.72)
[2023-09-29 12:52:05 10splitTasks](trainer.py 286): INFO [100/157]	0.1087(0.1080)	0.0002(0.0051)	0.073(0.258)	100.00(91.80)
[2023-09-29 12:52:06 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1076)	0.0002(0.0047)	0.347(0.266)	87.50(91.58)
[2023-09-29 12:52:07 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1071)	0.0002(0.0043)	0.177(0.270)	93.75(91.53)
[2023-09-29 12:52:08 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1069)	0.0003(0.0040)	0.562(0.273)	84.38(91.39)
[2023-09-29 12:52:09 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1066)	0.0002(0.0037)	0.364(0.276)	90.62(91.40)
[2023-09-29 12:52:10 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1063)	0.0001(0.0035)	0.158(0.271)	93.75(91.54)
[2023-09-29 12:52:11 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1060)	0.0001(0.0034)	1.344(0.270)	50.00(91.52)
[2023-09-29 12:52:11 10splitTasks](trainer.py 288): INFO  * Train Acc 91.520
[2023-09-29 12:52:12 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.800, Total time 1.64
[2023-09-29 12:52:12 10splitTasks](my_trainer.py 302): INFO Epoch:30
[2023-09-29 12:52:12 10splitTasks](my_trainer.py 308): INFO LR:0.0012583205145185797
[2023-09-29 12:52:12 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:52:13 10splitTasks](trainer.py 286): INFO [0/157]	0.6036(0.6036)	0.4986(0.4986)	0.123(0.123)	96.88(96.88)
[2023-09-29 12:52:14 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1483)	0.0001(0.0456)	0.214(0.271)	90.62(90.34)
[2023-09-29 12:52:15 10splitTasks](trainer.py 286): INFO [20/157]	0.1010(0.1262)	0.0002(0.0241)	0.361(0.273)	90.62(90.62)
[2023-09-29 12:52:16 10splitTasks](trainer.py 286): INFO [30/157]	0.1047(0.1193)	0.0003(0.0164)	0.333(0.279)	87.50(90.12)
[2023-09-29 12:52:17 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1152)	0.0004(0.0125)	0.164(0.288)	93.75(89.94)
[2023-09-29 12:52:18 10splitTasks](trainer.py 286): INFO [50/157]	0.1049(0.1129)	0.0003(0.0101)	0.232(0.286)	90.62(89.77)
[2023-09-29 12:52:19 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1114)	0.0002(0.0085)	0.332(0.288)	93.75(89.91)
[2023-09-29 12:52:20 10splitTasks](trainer.py 286): INFO [70/157]	0.1029(0.1100)	0.0002(0.0074)	0.195(0.276)	96.88(90.49)
[2023-09-29 12:52:21 10splitTasks](trainer.py 286): INFO [80/157]	0.1062(0.1092)	0.0002(0.0065)	0.112(0.271)	96.88(90.90)
[2023-09-29 12:52:22 10splitTasks](trainer.py 286): INFO [90/157]	0.1025(0.1085)	0.0002(0.0058)	0.370(0.272)	90.62(90.73)
[2023-09-29 12:52:23 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1079)	0.0002(0.0053)	0.235(0.276)	90.62(90.32)
[2023-09-29 12:52:24 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1075)	0.0003(0.0048)	0.249(0.281)	90.62(90.20)
[2023-09-29 12:52:25 10splitTasks](trainer.py 286): INFO [120/157]	0.1045(0.1071)	0.0002(0.0045)	0.410(0.277)	87.50(90.39)
[2023-09-29 12:52:26 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1068)	0.0003(0.0041)	0.593(0.278)	75.00(90.22)
[2023-09-29 12:52:27 10splitTasks](trainer.py 286): INFO [140/157]	0.1125(0.1067)	0.0002(0.0039)	0.109(0.274)	96.88(90.38)
[2023-09-29 12:52:28 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1064)	0.0001(0.0036)	0.229(0.274)	93.75(90.27)
[2023-09-29 12:52:29 10splitTasks](trainer.py 286): INFO [156/157]	0.0800(0.1061)	0.0001(0.0035)	1.372(0.271)	62.50(90.40)
[2023-09-29 12:52:29 10splitTasks](trainer.py 288): INFO  * Train Acc 90.400
[2023-09-29 12:52:31 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.200, Total time 1.56
[2023-09-29 12:52:31 10splitTasks](my_trainer.py 302): INFO Epoch:31
[2023-09-29 12:52:31 10splitTasks](my_trainer.py 308): INFO LR:0.001003685904364197
[2023-09-29 12:52:31 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:52:31 10splitTasks](trainer.py 286): INFO [0/157]	0.6273(0.6273)	0.5184(0.5184)	0.025(0.025)	100.00(100.00)
[2023-09-29 12:52:32 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1540)	0.0002(0.0474)	0.360(0.206)	87.50(92.90)
[2023-09-29 12:52:33 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1298)	0.0002(0.0250)	0.639(0.220)	87.50(93.15)
[2023-09-29 12:52:34 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1208)	0.0002(0.0170)	0.149(0.212)	93.75(93.25)
[2023-09-29 12:52:36 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1165)	0.0002(0.0130)	0.383(0.219)	87.50(92.61)
[2023-09-29 12:52:37 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1138)	0.0002(0.0105)	0.395(0.220)	96.88(92.28)
[2023-09-29 12:52:38 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1119)	0.0002(0.0088)	0.289(0.225)	87.50(92.01)
[2023-09-29 12:52:39 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1105)	0.0003(0.0076)	0.216(0.232)	90.62(91.95)
[2023-09-29 12:52:40 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1097)	0.0003(0.0067)	0.334(0.238)	90.62(91.74)
[2023-09-29 12:52:41 10splitTasks](trainer.py 286): INFO [90/157]	0.1034(0.1091)	0.0002(0.0060)	0.318(0.236)	90.62(91.83)
[2023-09-29 12:52:42 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1085)	0.0002(0.0054)	0.399(0.234)	84.38(91.92)
[2023-09-29 12:52:43 10splitTasks](trainer.py 286): INFO [110/157]	0.1044(0.1082)	0.0005(0.0050)	0.080(0.231)	100.00(92.03)
[2023-09-29 12:52:44 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1078)	0.0002(0.0046)	0.163(0.228)	96.88(92.28)
[2023-09-29 12:52:45 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1074)	0.0002(0.0043)	0.173(0.229)	93.75(92.32)
[2023-09-29 12:52:46 10splitTasks](trainer.py 286): INFO [140/157]	0.1209(0.1073)	0.0006(0.0040)	0.340(0.231)	90.62(92.31)
[2023-09-29 12:52:47 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1069)	0.0001(0.0038)	0.322(0.227)	87.50(92.43)
[2023-09-29 12:52:47 10splitTasks](trainer.py 286): INFO [156/157]	0.0810(0.1066)	0.0001(0.0036)	1.085(0.229)	75.00(92.38)
[2023-09-29 12:52:48 10splitTasks](trainer.py 288): INFO  * Train Acc 92.380
[2023-09-29 12:52:49 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.200, Total time 1.66
[2023-09-29 12:52:49 10splitTasks](my_trainer.py 302): INFO Epoch:32
[2023-09-29 12:52:49 10splitTasks](my_trainer.py 308): INFO LR:0.000774972167323799
[2023-09-29 12:52:49 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:52:50 10splitTasks](trainer.py 286): INFO [0/157]	0.6570(0.6570)	0.5460(0.5460)	0.228(0.228)	93.75(93.75)
[2023-09-29 12:52:51 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1546)	0.0002(0.0501)	0.154(0.224)	93.75(91.19)
[2023-09-29 12:52:52 10splitTasks](trainer.py 286): INFO [20/157]	0.1050(0.1305)	0.0002(0.0264)	0.175(0.225)	90.62(91.82)
[2023-09-29 12:52:53 10splitTasks](trainer.py 286): INFO [30/157]	0.1071(0.1222)	0.0002(0.0180)	0.152(0.237)	93.75(91.63)
[2023-09-29 12:52:54 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1171)	0.0003(0.0137)	0.207(0.234)	96.88(91.92)
[2023-09-29 12:52:55 10splitTasks](trainer.py 286): INFO [50/157]	0.1024(0.1144)	0.0002(0.0111)	0.184(0.235)	90.62(91.79)
[2023-09-29 12:52:56 10splitTasks](trainer.py 286): INFO [60/157]	0.1042(0.1126)	0.0003(0.0093)	0.131(0.229)	96.88(92.06)
[2023-09-29 12:52:57 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1111)	0.0003(0.0080)	0.190(0.229)	96.88(92.30)
[2023-09-29 12:52:58 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1101)	0.0002(0.0071)	0.170(0.230)	93.75(92.28)
[2023-09-29 12:52:59 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1093)	0.0002(0.0063)	0.470(0.240)	87.50(91.83)
[2023-09-29 12:53:00 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1086)	0.0002(0.0057)	0.074(0.235)	96.88(91.96)
[2023-09-29 12:53:01 10splitTasks](trainer.py 286): INFO [110/157]	0.1041(0.1081)	0.0002(0.0053)	0.315(0.231)	84.38(92.06)
[2023-09-29 12:53:02 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1077)	0.0003(0.0049)	0.237(0.232)	96.88(92.12)
[2023-09-29 12:53:03 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1073)	0.0002(0.0045)	0.101(0.233)	96.88(91.94)
[2023-09-29 12:53:04 10splitTasks](trainer.py 286): INFO [140/157]	0.1050(0.1070)	0.0002(0.0042)	0.178(0.234)	93.75(91.93)
[2023-09-29 12:53:05 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1067)	0.0001(0.0040)	0.179(0.232)	96.88(92.09)
[2023-09-29 12:53:06 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1064)	0.0001(0.0038)	2.052(0.235)	62.50(92.08)
[2023-09-29 12:53:06 10splitTasks](trainer.py 288): INFO  * Train Acc 92.080
[2023-09-29 12:53:08 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.200, Total time 1.57
[2023-09-29 12:53:08 10splitTasks](my_trainer.py 302): INFO Epoch:33
[2023-09-29 12:53:08 10splitTasks](my_trainer.py 308): INFO LR:0.0005736625997467777
[2023-09-29 12:53:08 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:53:08 10splitTasks](trainer.py 286): INFO [0/157]	0.6051(0.6051)	0.4891(0.4891)	0.353(0.353)	81.25(81.25)
[2023-09-29 12:53:09 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1491)	0.0002(0.0448)	0.174(0.208)	93.75(93.47)
[2023-09-29 12:53:10 10splitTasks](trainer.py 286): INFO [20/157]	0.1040(0.1274)	0.0005(0.0236)	0.152(0.203)	96.88(93.15)
[2023-09-29 12:53:11 10splitTasks](trainer.py 286): INFO [30/157]	0.1032(0.1202)	0.0003(0.0161)	0.154(0.209)	96.88(92.84)
[2023-09-29 12:53:12 10splitTasks](trainer.py 286): INFO [40/157]	0.1031(0.1165)	0.0002(0.0123)	0.302(0.202)	87.50(93.37)
[2023-09-29 12:53:13 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1141)	0.0002(0.0099)	0.421(0.207)	84.38(93.08)
[2023-09-29 12:53:14 10splitTasks](trainer.py 286): INFO [60/157]	0.1025(0.1123)	0.0004(0.0083)	0.083(0.199)	100.00(93.44)
[2023-09-29 12:53:15 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1108)	0.0003(0.0072)	0.503(0.204)	87.50(93.35)
[2023-09-29 12:53:17 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1098)	0.0002(0.0064)	0.318(0.213)	87.50(93.02)
[2023-09-29 12:53:18 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1091)	0.0002(0.0057)	0.334(0.219)	87.50(92.75)
[2023-09-29 12:53:19 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1084)	0.0003(0.0052)	0.269(0.219)	90.62(92.73)
[2023-09-29 12:53:20 10splitTasks](trainer.py 286): INFO [110/157]	0.1027(0.1080)	0.0002(0.0048)	0.207(0.217)	93.75(92.79)
[2023-09-29 12:53:21 10splitTasks](trainer.py 286): INFO [120/157]	0.1051(0.1076)	0.0005(0.0044)	0.139(0.214)	93.75(92.79)
[2023-09-29 12:53:22 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1073)	0.0002(0.0041)	0.063(0.209)	100.00(93.03)
[2023-09-29 12:53:23 10splitTasks](trainer.py 286): INFO [140/157]	0.1011(0.1070)	0.0001(0.0038)	0.051(0.208)	100.00(93.17)
[2023-09-29 12:53:24 10splitTasks](trainer.py 286): INFO [150/157]	0.1021(0.1068)	0.0002(0.0036)	0.161(0.209)	93.75(93.05)
[2023-09-29 12:53:24 10splitTasks](trainer.py 286): INFO [156/157]	0.0822(0.1064)	0.0001(0.0035)	0.657(0.210)	75.00(92.92)
[2023-09-29 12:53:24 10splitTasks](trainer.py 288): INFO  * Train Acc 92.920
[2023-09-29 12:53:26 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.58
[2023-09-29 12:53:26 10splitTasks](my_trainer.py 302): INFO Epoch:34
[2023-09-29 12:53:26 10splitTasks](my_trainer.py 308): INFO LR:0.0004010627714277083
[2023-09-29 12:53:26 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:53:27 10splitTasks](trainer.py 286): INFO [0/157]	0.6506(0.6506)	0.5394(0.5394)	0.245(0.245)	87.50(87.50)
[2023-09-29 12:53:28 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1560)	0.0002(0.0497)	0.221(0.219)	87.50(91.19)
[2023-09-29 12:53:29 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1311)	0.0003(0.0262)	0.210(0.239)	93.75(91.07)
[2023-09-29 12:53:30 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1220)	0.0001(0.0179)	0.419(0.214)	90.62(92.64)
[2023-09-29 12:53:31 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1179)	0.0003(0.0136)	0.138(0.209)	96.88(93.06)
[2023-09-29 12:53:32 10splitTasks](trainer.py 286): INFO [50/157]	0.1082(0.1155)	0.0002(0.0110)	0.364(0.209)	87.50(92.83)
[2023-09-29 12:53:33 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1134)	0.0003(0.0093)	0.083(0.200)	100.00(93.24)
[2023-09-29 12:53:34 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1124)	0.0003(0.0080)	0.111(0.202)	96.88(93.09)
[2023-09-29 12:53:35 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1111)	0.0003(0.0071)	0.148(0.203)	93.75(93.02)
[2023-09-29 12:53:36 10splitTasks](trainer.py 286): INFO [90/157]	0.1117(0.1102)	0.0002(0.0063)	0.196(0.209)	93.75(92.93)
[2023-09-29 12:53:37 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1094)	0.0002(0.0057)	0.197(0.209)	93.75(92.95)
[2023-09-29 12:53:38 10splitTasks](trainer.py 286): INFO [110/157]	0.1055(0.1088)	0.0003(0.0052)	0.117(0.209)	96.88(92.82)
[2023-09-29 12:53:39 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1085)	0.0003(0.0048)	0.061(0.206)	100.00(92.95)
[2023-09-29 12:53:40 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1081)	0.0002(0.0045)	0.239(0.202)	90.62(93.08)
[2023-09-29 12:53:41 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1078)	0.0002(0.0042)	0.202(0.203)	93.75(93.11)
[2023-09-29 12:53:42 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1075)	0.0001(0.0039)	0.772(0.209)	71.88(92.90)
[2023-09-29 12:53:43 10splitTasks](trainer.py 286): INFO [156/157]	0.0795(0.1071)	0.0001(0.0038)	0.723(0.214)	75.00(92.76)
[2023-09-29 12:53:43 10splitTasks](trainer.py 288): INFO  * Train Acc 92.760
[2023-09-29 12:53:45 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.400, Total time 1.68
[2023-09-29 12:53:45 10splitTasks](my_trainer.py 302): INFO Epoch:35
[2023-09-29 12:53:45 10splitTasks](my_trainer.py 308): INFO LR:0.00025829205848524624
[2023-09-29 12:53:45 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:53:45 10splitTasks](trainer.py 286): INFO [0/157]	0.6324(0.6324)	0.5224(0.5224)	0.190(0.190)	90.62(90.62)
[2023-09-29 12:53:46 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1512)	0.0002(0.0477)	0.191(0.195)	93.75(93.18)
[2023-09-29 12:53:47 10splitTasks](trainer.py 286): INFO [20/157]	0.1033(0.1287)	0.0003(0.0252)	0.202(0.179)	93.75(93.45)
[2023-09-29 12:53:48 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1206)	0.0003(0.0172)	0.277(0.185)	93.75(93.55)
[2023-09-29 12:53:49 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1169)	0.0002(0.0131)	0.306(0.191)	93.75(93.45)
[2023-09-29 12:53:50 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1141)	0.0003(0.0106)	0.271(0.187)	93.75(93.69)
[2023-09-29 12:53:51 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1123)	0.0002(0.0089)	0.147(0.194)	100.00(93.34)
[2023-09-29 12:53:53 10splitTasks](trainer.py 286): INFO [70/157]	0.1046(0.1111)	0.0003(0.0077)	0.283(0.196)	84.38(93.09)
[2023-09-29 12:53:54 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1101)	0.0003(0.0068)	0.327(0.205)	93.75(92.94)
[2023-09-29 12:53:55 10splitTasks](trainer.py 286): INFO [90/157]	0.1027(0.1094)	0.0002(0.0061)	0.298(0.200)	93.75(93.27)
[2023-09-29 12:53:56 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1087)	0.0002(0.0055)	0.161(0.202)	96.88(93.19)
[2023-09-29 12:53:57 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1082)	0.0003(0.0050)	0.068(0.204)	100.00(93.16)
[2023-09-29 12:53:58 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1077)	0.0002(0.0046)	0.177(0.200)	90.62(93.16)
[2023-09-29 12:53:59 10splitTasks](trainer.py 286): INFO [130/157]	0.1050(0.1074)	0.0002(0.0043)	0.372(0.203)	84.38(93.13)
[2023-09-29 12:54:00 10splitTasks](trainer.py 286): INFO [140/157]	0.1042(0.1071)	0.0002(0.0040)	0.229(0.203)	93.75(93.13)
[2023-09-29 12:54:01 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1069)	0.0001(0.0038)	0.309(0.205)	90.62(93.07)
[2023-09-29 12:54:01 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1066)	0.0001(0.0036)	0.713(0.208)	75.00(92.92)
[2023-09-29 12:54:01 10splitTasks](trainer.py 288): INFO  * Train Acc 92.920
[2023-09-29 12:54:03 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.200, Total time 1.52
[2023-09-29 12:54:03 10splitTasks](my_trainer.py 302): INFO Epoch:36
[2023-09-29 12:54:03 10splitTasks](my_trainer.py 308): INFO LR:0.00014627638377845298
[2023-09-29 12:54:03 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:54:04 10splitTasks](trainer.py 286): INFO [0/157]	0.6869(0.6869)	0.5713(0.5713)	0.055(0.055)	100.00(100.00)
[2023-09-29 12:54:05 10splitTasks](trainer.py 286): INFO [10/157]	0.1107(0.1595)	0.0001(0.0522)	0.075(0.185)	100.00(94.60)
[2023-09-29 12:54:06 10splitTasks](trainer.py 286): INFO [20/157]	0.1042(0.1327)	0.0003(0.0275)	0.653(0.205)	68.75(92.86)
[2023-09-29 12:54:07 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1228)	0.0004(0.0188)	0.437(0.194)	84.38(93.45)
[2023-09-29 12:54:08 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1178)	0.0003(0.0143)	0.194(0.192)	87.50(93.45)
[2023-09-29 12:54:09 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1147)	0.0003(0.0115)	0.127(0.198)	93.75(93.63)
[2023-09-29 12:54:10 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1127)	0.0003(0.0097)	0.153(0.195)	96.88(93.65)
[2023-09-29 12:54:11 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1113)	0.0003(0.0084)	0.228(0.192)	87.50(93.79)
[2023-09-29 12:54:12 10splitTasks](trainer.py 286): INFO [80/157]	0.1049(0.1102)	0.0003(0.0074)	0.179(0.188)	90.62(93.98)
[2023-09-29 12:54:13 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1093)	0.0002(0.0066)	0.214(0.190)	87.50(93.85)
[2023-09-29 12:54:14 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1086)	0.0003(0.0060)	0.129(0.187)	100.00(94.06)
[2023-09-29 12:54:15 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1080)	0.0002(0.0055)	0.483(0.197)	81.25(93.75)
[2023-09-29 12:54:16 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1076)	0.0002(0.0050)	0.231(0.194)	93.75(93.80)
[2023-09-29 12:54:17 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1072)	0.0001(0.0047)	0.130(0.193)	96.88(93.85)
[2023-09-29 12:54:18 10splitTasks](trainer.py 286): INFO [140/157]	0.1060(0.1069)	0.0003(0.0044)	0.071(0.194)	100.00(93.88)
[2023-09-29 12:54:19 10splitTasks](trainer.py 286): INFO [150/157]	0.1019(0.1066)	0.0002(0.0041)	0.206(0.198)	93.75(93.65)
[2023-09-29 12:54:20 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1063)	0.0001(0.0039)	0.515(0.199)	87.50(93.66)
[2023-09-29 12:54:20 10splitTasks](trainer.py 288): INFO  * Train Acc 93.660
[2023-09-29 12:54:21 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.400, Total time 1.56
[2023-09-29 12:54:21 10splitTasks](my_trainer.py 302): INFO Epoch:37
[2023-09-29 12:54:21 10splitTasks](my_trainer.py 308): INFO LR:6.574221194175474e-05
[2023-09-29 12:54:21 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:54:22 10splitTasks](trainer.py 286): INFO [0/157]	0.5980(0.5980)	0.4719(0.4719)	0.087(0.087)	96.88(96.88)
[2023-09-29 12:54:23 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1493)	0.0002(0.0432)	0.093(0.183)	96.88(92.61)
[2023-09-29 12:54:24 10splitTasks](trainer.py 286): INFO [20/157]	0.1055(0.1271)	0.0005(0.0228)	0.074(0.162)	100.00(93.75)
[2023-09-29 12:54:25 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1204)	0.0003(0.0155)	0.059(0.179)	100.00(93.15)
[2023-09-29 12:54:26 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1159)	0.0003(0.0118)	0.259(0.184)	90.62(93.29)
[2023-09-29 12:54:27 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1134)	0.0003(0.0096)	0.218(0.188)	93.75(93.38)
[2023-09-29 12:54:28 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1119)	0.0003(0.0081)	0.080(0.191)	100.00(93.39)
[2023-09-29 12:54:29 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1107)	0.0002(0.0070)	0.414(0.192)	87.50(93.35)
[2023-09-29 12:54:30 10splitTasks](trainer.py 286): INFO [80/157]	0.1059(0.1098)	0.0002(0.0062)	0.138(0.197)	93.75(93.33)
[2023-09-29 12:54:31 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1092)	0.0002(0.0055)	0.186(0.201)	96.88(92.99)
[2023-09-29 12:54:32 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1084)	0.0002(0.0050)	0.075(0.196)	100.00(93.29)
[2023-09-29 12:54:33 10splitTasks](trainer.py 286): INFO [110/157]	0.1059(0.1079)	0.0002(0.0046)	0.069(0.194)	96.88(93.36)
[2023-09-29 12:54:34 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1075)	0.0002(0.0042)	0.175(0.196)	96.88(93.31)
[2023-09-29 12:54:35 10splitTasks](trainer.py 286): INFO [130/157]	0.1041(0.1071)	0.0003(0.0039)	0.243(0.193)	93.75(93.46)
[2023-09-29 12:54:36 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1068)	0.0002(0.0037)	0.309(0.193)	93.75(93.48)
[2023-09-29 12:54:37 10splitTasks](trainer.py 286): INFO [150/157]	0.1037(0.1066)	0.0001(0.0034)	0.259(0.199)	90.62(93.27)
[2023-09-29 12:54:38 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1062)	0.0001(0.0033)	0.679(0.198)	62.50(93.28)
[2023-09-29 12:54:38 10splitTasks](trainer.py 288): INFO  * Train Acc 93.280
[2023-09-29 12:54:40 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.000, Total time 1.63
[2023-09-29 12:54:40 10splitTasks](my_trainer.py 302): INFO Epoch:38
[2023-09-29 12:54:40 10splitTasks](my_trainer.py 308): INFO LR:1.721183798301746e-05
[2023-09-29 12:54:40 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:54:40 10splitTasks](trainer.py 286): INFO [0/157]	0.5969(0.5969)	0.4626(0.4626)	0.177(0.177)	96.88(96.88)
[2023-09-29 12:54:41 10splitTasks](trainer.py 286): INFO [10/157]	0.1039(0.1499)	0.0002(0.0423)	0.237(0.177)	93.75(94.89)
[2023-09-29 12:54:42 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1272)	0.0002(0.0223)	0.339(0.185)	90.62(94.49)
[2023-09-29 12:54:43 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1190)	0.0002(0.0152)	0.184(0.205)	93.75(93.65)
[2023-09-29 12:54:45 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1151)	0.0002(0.0116)	0.267(0.196)	90.62(94.05)
[2023-09-29 12:54:46 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1126)	0.0002(0.0094)	0.294(0.199)	90.62(93.87)
[2023-09-29 12:54:47 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1110)	0.0002(0.0079)	0.078(0.192)	100.00(93.95)
[2023-09-29 12:54:48 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1097)	0.0003(0.0069)	0.078(0.194)	100.00(93.62)
[2023-09-29 12:54:49 10splitTasks](trainer.py 286): INFO [80/157]	0.1030(0.1095)	0.0003(0.0061)	0.201(0.196)	93.75(93.52)
[2023-09-29 12:54:50 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1087)	0.0003(0.0054)	0.126(0.190)	93.75(93.82)
[2023-09-29 12:54:51 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1081)	0.0001(0.0049)	0.126(0.189)	96.88(93.97)
[2023-09-29 12:54:52 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1076)	0.0003(0.0045)	0.271(0.189)	90.62(93.89)
[2023-09-29 12:54:53 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1071)	0.0003(0.0042)	0.094(0.187)	96.88(93.88)
[2023-09-29 12:54:54 10splitTasks](trainer.py 286): INFO [130/157]	0.1033(0.1068)	0.0006(0.0039)	0.326(0.190)	84.38(93.85)
[2023-09-29 12:54:55 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1065)	0.0002(0.0036)	0.384(0.191)	90.62(93.77)
[2023-09-29 12:54:56 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1062)	0.0001(0.0034)	0.089(0.192)	100.00(93.75)
[2023-09-29 12:54:56 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1058)	0.0001(0.0033)	1.077(0.192)	75.00(93.72)
[2023-09-29 12:54:57 10splitTasks](trainer.py 288): INFO  * Train Acc 93.720
[2023-09-29 12:54:58 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.600, Total time 1.50
[2023-09-29 12:54:58 10splitTasks](my_trainer.py 302): INFO Epoch:39
[2023-09-29 12:54:58 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 12:54:58 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:54:59 10splitTasks](trainer.py 286): INFO [0/157]	0.6345(0.6345)	0.5269(0.5269)	0.318(0.318)	90.62(90.62)
[2023-09-29 12:55:00 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1512)	0.0003(0.0482)	0.194(0.190)	93.75(92.90)
[2023-09-29 12:55:01 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1284)	0.0002(0.0254)	0.252(0.173)	87.50(93.90)
[2023-09-29 12:55:02 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1203)	0.0002(0.0173)	0.257(0.174)	87.50(93.55)
[2023-09-29 12:55:03 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1160)	0.0003(0.0132)	0.266(0.175)	87.50(93.75)
[2023-09-29 12:55:04 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1134)	0.0004(0.0106)	0.114(0.180)	93.75(93.69)
[2023-09-29 12:55:05 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1117)	0.0003(0.0089)	0.109(0.192)	100.00(93.29)
[2023-09-29 12:55:06 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1104)	0.0002(0.0077)	0.192(0.191)	93.75(93.57)
[2023-09-29 12:55:07 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1095)	0.0002(0.0068)	0.073(0.192)	100.00(93.63)
[2023-09-29 12:55:08 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1086)	0.0002(0.0061)	0.214(0.195)	90.62(93.54)
[2023-09-29 12:55:09 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1080)	0.0002(0.0055)	0.060(0.189)	100.00(93.84)
[2023-09-29 12:55:10 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1076)	0.0003(0.0050)	0.248(0.192)	90.62(93.61)
[2023-09-29 12:55:11 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1072)	0.0002(0.0047)	0.337(0.194)	87.50(93.60)
[2023-09-29 12:55:12 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1069)	0.0002(0.0043)	0.200(0.193)	93.75(93.65)
[2023-09-29 12:55:13 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1066)	0.0003(0.0040)	0.185(0.192)	90.62(93.57)
[2023-09-29 12:55:14 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1063)	0.0001(0.0038)	0.170(0.194)	93.75(93.56)
[2023-09-29 12:55:15 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1059)	0.0001(0.0037)	0.218(0.193)	87.50(93.60)
[2023-09-29 12:55:15 10splitTasks](trainer.py 288): INFO  * Train Acc 93.600
[2023-09-29 12:55:16 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.63
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-0.pth
=> Save Done
[2023-09-29 12:55:17 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 12:55:18 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.71
[2023-09-29 12:55:18 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 12:55:18 10splitTasks](trainer.py 341): INFO done
[2023-09-29 12:55:18 10splitTasks](iBatchLearn.py 155): INFO Acc:79.4; BWT:0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 12:55:22 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:55:22 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 12:55:22 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 0, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-0.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-0.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_0.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:55:22 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-0.pth
[2023-09-29 12:55:23 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:55:25 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 12:55:25 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 12:55:25 10splitTasks](my_trainer.py 64): INFO tensor([[0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 12:55:25 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 12:55:25 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 12:55:25 10splitTasks](iBatchLearn.py 167): INFO test split name:0
--------------------------------Official Evaluation--------------------------------
0 79.9
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 12:55:35 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 12:55:35 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 12:55:35 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 1, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-0.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-1.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-0.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-1.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 12:55:36 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-0.pth
[2023-09-29 12:55:36 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 12:55:38 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 12:55:38 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 12:55:38 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 12:55:38 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 12:55:38 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 12:55:38 10splitTasks](iBatchLearn.py 92): INFO ====================== 1 =======================
[2023-09-29 12:55:38 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 12:55:38 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 12:55:38 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 12:55:38 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:55:42 10splitTasks](trainer.py 286): INFO [0/157]	3.5472(3.5472)	0.5111(0.5111)	2.355(2.355)	3.12(3.12)
[2023-09-29 12:55:43 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.4164)	0.0002(0.0467)	2.275(2.281)	25.00(17.90)
[2023-09-29 12:55:44 10splitTasks](trainer.py 286): INFO [20/157]	0.1033(0.2674)	0.0003(0.0246)	1.964(2.193)	37.50(23.51)
[2023-09-29 12:55:45 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.2138)	0.0002(0.0168)	1.542(2.049)	50.00(28.53)
[2023-09-29 12:55:46 10splitTasks](trainer.py 286): INFO [40/157]	0.1026(0.1867)	0.0002(0.0128)	1.427(1.947)	50.00(32.47)
[2023-09-29 12:55:47 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1701)	0.0003(0.0103)	1.446(1.858)	53.12(35.48)
[2023-09-29 12:55:48 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1592)	0.0002(0.0087)	1.332(1.815)	59.38(36.94)
[2023-09-29 12:55:49 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1512)	0.0003(0.0075)	1.420(1.774)	50.00(38.25)
[2023-09-29 12:55:50 10splitTasks](trainer.py 286): INFO [80/157]	0.1040(0.1454)	0.0004(0.0066)	1.767(1.723)	56.25(40.43)
[2023-09-29 12:55:51 10splitTasks](trainer.py 286): INFO [90/157]	0.1077(0.1408)	0.0002(0.0059)	1.479(1.695)	50.00(41.52)
[2023-09-29 12:55:52 10splitTasks](trainer.py 286): INFO [100/157]	0.1050(0.1371)	0.0002(0.0054)	1.484(1.674)	50.00(42.42)
[2023-09-29 12:55:53 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1343)	0.0002(0.0049)	1.440(1.648)	53.12(43.24)
[2023-09-29 12:55:54 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1320)	0.0002(0.0045)	1.427(1.620)	43.75(44.06)
[2023-09-29 12:55:55 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1299)	0.0003(0.0042)	1.229(1.609)	56.25(44.37)
[2023-09-29 12:55:56 10splitTasks](trainer.py 286): INFO [140/157]	0.1025(0.1280)	0.0003(0.0039)	1.841(1.594)	59.38(44.77)
[2023-09-29 12:55:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1264)	0.0002(0.0037)	1.590(1.574)	40.62(45.32)
[2023-09-29 12:55:58 10splitTasks](trainer.py 286): INFO [156/157]	0.0855(0.1253)	0.0001(0.0036)	1.205(1.565)	62.50(45.54)
[2023-09-29 12:55:58 10splitTasks](trainer.py 288): INFO  * Train Acc 45.540
[2023-09-29 12:56:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 52.400, Total time 1.75
[2023-09-29 12:56:00 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 12:56:00 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 12:56:00 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:56:01 10splitTasks](trainer.py 286): INFO [0/157]	0.6166(0.6166)	0.5020(0.5020)	1.014(1.014)	65.62(65.62)
[2023-09-29 12:56:02 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1511)	0.0002(0.0459)	0.898(1.212)	65.62(58.24)
[2023-09-29 12:56:03 10splitTasks](trainer.py 286): INFO [20/157]	0.1047(0.1287)	0.0002(0.0242)	1.030(1.321)	65.62(54.76)
[2023-09-29 12:56:04 10splitTasks](trainer.py 286): INFO [30/157]	0.1071(0.1211)	0.0003(0.0165)	1.279(1.386)	53.12(52.62)
[2023-09-29 12:56:05 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1166)	0.0002(0.0125)	1.188(1.370)	62.50(53.81)
[2023-09-29 12:56:06 10splitTasks](trainer.py 286): INFO [50/157]	0.1093(0.1143)	0.0002(0.0101)	0.947(1.368)	68.75(54.04)
[2023-09-29 12:56:07 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1127)	0.0002(0.0085)	1.271(1.373)	50.00(52.92)
[2023-09-29 12:56:08 10splitTasks](trainer.py 286): INFO [70/157]	0.1079(0.1118)	0.0006(0.0074)	1.025(1.370)	53.12(52.82)
[2023-09-29 12:56:09 10splitTasks](trainer.py 286): INFO [80/157]	0.1048(0.1109)	0.0002(0.0065)	1.158(1.361)	68.75(53.67)
[2023-09-29 12:56:10 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1101)	0.0002(0.0058)	1.234(1.341)	62.50(54.33)
[2023-09-29 12:56:11 10splitTasks](trainer.py 286): INFO [100/157]	0.1026(0.1094)	0.0002(0.0053)	1.359(1.332)	59.38(54.70)
[2023-09-29 12:56:12 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1088)	0.0003(0.0048)	1.256(1.323)	62.50(54.87)
[2023-09-29 12:56:13 10splitTasks](trainer.py 286): INFO [120/157]	0.1055(0.1085)	0.0002(0.0045)	1.225(1.315)	53.12(54.91)
[2023-09-29 12:56:14 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1081)	0.0002(0.0041)	1.212(1.302)	59.38(55.30)
[2023-09-29 12:56:15 10splitTasks](trainer.py 286): INFO [140/157]	0.1042(0.1077)	0.0002(0.0039)	1.372(1.300)	56.25(55.54)
[2023-09-29 12:56:16 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1073)	0.0001(0.0036)	1.622(1.289)	56.25(55.96)
[2023-09-29 12:56:17 10splitTasks](trainer.py 286): INFO [156/157]	0.0861(0.1070)	0.0001(0.0035)	0.755(1.279)	62.50(56.30)
[2023-09-29 12:56:17 10splitTasks](trainer.py 288): INFO  * Train Acc 56.300
[2023-09-29 12:56:19 10splitTasks](my_trainer.py 503): INFO  * Val Acc 56.000, Total time 1.67
[2023-09-29 12:56:19 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 12:56:19 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 12:56:19 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:56:19 10splitTasks](trainer.py 286): INFO [0/157]	0.5950(0.5950)	0.4823(0.4823)	1.258(1.258)	59.38(59.38)
[2023-09-29 12:56:20 10splitTasks](trainer.py 286): INFO [10/157]	0.1085(0.1501)	0.0002(0.0441)	1.284(1.417)	62.50(53.12)
[2023-09-29 12:56:21 10splitTasks](trainer.py 286): INFO [20/157]	0.1209(0.1297)	0.0006(0.0233)	0.869(1.328)	71.88(55.06)
[2023-09-29 12:56:22 10splitTasks](trainer.py 286): INFO [30/157]	0.1101(0.1212)	0.0003(0.0158)	1.238(1.316)	53.12(56.25)
[2023-09-29 12:56:23 10splitTasks](trainer.py 286): INFO [40/157]	0.1041(0.1173)	0.0002(0.0121)	1.280(1.286)	56.25(57.24)
[2023-09-29 12:56:24 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1145)	0.0002(0.0098)	1.095(1.273)	68.75(57.72)
[2023-09-29 12:56:25 10splitTasks](trainer.py 286): INFO [60/157]	0.1008(0.1128)	0.0002(0.0082)	0.933(1.250)	59.38(58.09)
[2023-09-29 12:56:26 10splitTasks](trainer.py 286): INFO [70/157]	0.1170(0.1119)	0.0006(0.0071)	1.547(1.282)	43.75(57.66)
[2023-09-29 12:56:28 10splitTasks](trainer.py 286): INFO [80/157]	0.1220(0.1111)	0.0002(0.0063)	0.920(1.295)	62.50(57.37)
[2023-09-29 12:56:29 10splitTasks](trainer.py 286): INFO [90/157]	0.1038(0.1108)	0.0004(0.0056)	1.201(1.288)	53.12(57.38)
[2023-09-29 12:56:30 10splitTasks](trainer.py 286): INFO [100/157]	0.1072(0.1100)	0.0002(0.0051)	1.753(1.284)	46.88(57.64)
[2023-09-29 12:56:31 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1096)	0.0002(0.0047)	1.596(1.293)	46.88(57.43)
[2023-09-29 12:56:32 10splitTasks](trainer.py 286): INFO [120/157]	0.1027(0.1090)	0.0002(0.0043)	0.921(1.287)	65.62(57.31)
[2023-09-29 12:56:33 10splitTasks](trainer.py 286): INFO [130/157]	0.1044(0.1086)	0.0002(0.0040)	1.390(1.278)	50.00(57.37)
[2023-09-29 12:56:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1041(0.1082)	0.0002(0.0037)	2.364(1.287)	28.12(57.20)
[2023-09-29 12:56:35 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1079)	0.0002(0.0035)	1.489(1.288)	50.00(57.12)
[2023-09-29 12:56:35 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1075)	0.0001(0.0034)	1.048(1.290)	62.50(57.08)
[2023-09-29 12:56:35 10splitTasks](trainer.py 288): INFO  * Train Acc 57.080
[2023-09-29 12:56:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 57.800, Total time 1.60
[2023-09-29 12:56:37 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 12:56:37 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 12:56:37 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:56:38 10splitTasks](trainer.py 286): INFO [0/157]	0.5939(0.5939)	0.4811(0.4811)	0.944(0.944)	65.62(65.62)
[2023-09-29 12:56:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1079(0.1511)	0.0003(0.0440)	1.443(1.177)	56.25(61.36)
[2023-09-29 12:56:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1033(0.1285)	0.0002(0.0232)	1.455(1.228)	46.88(58.48)
[2023-09-29 12:56:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1026(0.1205)	0.0002(0.0158)	0.947(1.163)	68.75(61.09)
[2023-09-29 12:56:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1163)	0.0002(0.0120)	0.892(1.145)	68.75(61.81)
[2023-09-29 12:56:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1075(0.1139)	0.0057(0.0099)	0.828(1.134)	68.75(62.01)
[2023-09-29 12:56:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1024(0.1121)	0.0003(0.0083)	1.187(1.136)	56.25(61.78)
[2023-09-29 12:56:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1061(0.1109)	0.0002(0.0072)	1.749(1.134)	34.38(61.97)
[2023-09-29 12:56:46 10splitTasks](trainer.py 286): INFO [80/157]	0.1026(0.1100)	0.0002(0.0063)	0.972(1.125)	65.62(62.27)
[2023-09-29 12:56:47 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1093)	0.0002(0.0057)	1.168(1.121)	59.38(62.26)
[2023-09-29 12:56:48 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1087)	0.0002(0.0051)	0.938(1.119)	59.38(61.97)
[2023-09-29 12:56:49 10splitTasks](trainer.py 286): INFO [110/157]	0.1059(0.1082)	0.0002(0.0047)	0.822(1.106)	68.75(62.25)
[2023-09-29 12:56:50 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1079)	0.0002(0.0043)	1.023(1.093)	65.62(62.94)
[2023-09-29 12:56:51 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1076)	0.0002(0.0040)	1.057(1.086)	71.88(63.43)
[2023-09-29 12:56:52 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1073)	0.0003(0.0038)	1.270(1.086)	71.88(63.65)
[2023-09-29 12:56:53 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1072)	0.0002(0.0036)	1.169(1.085)	65.62(63.82)
[2023-09-29 12:56:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1069)	0.0001(0.0034)	1.281(1.090)	50.00(63.54)
[2023-09-29 12:56:54 10splitTasks](trainer.py 288): INFO  * Train Acc 63.540
[2023-09-29 12:56:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 64.800, Total time 1.66
[2023-09-29 12:56:56 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 12:56:56 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 12:56:56 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:56:56 10splitTasks](trainer.py 286): INFO [0/157]	0.6649(0.6649)	0.5583(0.5583)	1.018(1.018)	59.38(59.38)
[2023-09-29 12:56:57 10splitTasks](trainer.py 286): INFO [10/157]	0.1009(0.1537)	0.0002(0.0510)	0.827(1.011)	78.12(65.62)
[2023-09-29 12:56:58 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1298)	0.0002(0.0268)	1.197(1.030)	59.38(65.18)
[2023-09-29 12:56:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1266(0.1217)	0.0006(0.0183)	0.779(0.959)	75.00(67.94)
[2023-09-29 12:57:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1172)	0.0002(0.0139)	1.180(0.938)	53.12(68.60)
[2023-09-29 12:57:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1012(0.1144)	0.0002(0.0112)	0.897(0.949)	62.50(67.77)
[2023-09-29 12:57:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1046(0.1123)	0.0003(0.0094)	0.924(0.966)	71.88(67.52)
[2023-09-29 12:57:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1054(0.1111)	0.0003(0.0081)	0.945(0.948)	62.50(68.00)
[2023-09-29 12:57:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1101)	0.0002(0.0072)	0.986(0.949)	59.38(67.75)
[2023-09-29 12:57:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1094)	0.0003(0.0064)	0.985(0.939)	62.50(68.10)
[2023-09-29 12:57:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1041(0.1088)	0.0003(0.0058)	0.785(0.943)	65.62(67.76)
[2023-09-29 12:57:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1022(0.1085)	0.0004(0.0053)	0.482(0.927)	81.25(68.13)
[2023-09-29 12:57:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1144(0.1082)	0.0006(0.0049)	0.983(0.920)	65.62(68.18)
[2023-09-29 12:57:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1079)	0.0003(0.0046)	0.810(0.920)	75.00(68.27)
[2023-09-29 12:57:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1075)	0.0002(0.0043)	0.689(0.912)	81.25(68.57)
[2023-09-29 12:57:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1074)	0.0002(0.0040)	0.888(0.914)	59.38(68.42)
[2023-09-29 12:57:12 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1070)	0.0001(0.0039)	2.347(0.916)	62.50(68.46)
[2023-09-29 12:57:13 10splitTasks](trainer.py 288): INFO  * Train Acc 68.460
[2023-09-29 12:57:14 10splitTasks](my_trainer.py 503): INFO  * Val Acc 68.800, Total time 1.66
[2023-09-29 12:57:14 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 12:57:14 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 12:57:14 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:57:15 10splitTasks](trainer.py 286): INFO [0/157]	0.6281(0.6281)	0.5208(0.5208)	0.830(0.830)	75.00(75.00)
[2023-09-29 12:57:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1050(0.1558)	0.0001(0.0490)	0.799(0.935)	75.00(68.47)
[2023-09-29 12:57:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1050(0.1310)	0.0002(0.0258)	0.903(0.870)	75.00(71.43)
[2023-09-29 12:57:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1024(0.1217)	0.0002(0.0176)	0.414(0.819)	84.38(72.98)
[2023-09-29 12:57:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1057(0.1174)	0.0002(0.0134)	1.151(0.793)	65.62(73.63)
[2023-09-29 12:57:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1144)	0.0003(0.0108)	0.614(0.801)	81.25(73.71)
[2023-09-29 12:57:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1125)	0.0001(0.0091)	0.773(0.804)	75.00(73.10)
[2023-09-29 12:57:22 10splitTasks](trainer.py 286): INFO [70/157]	0.1040(0.1113)	0.0002(0.0078)	0.885(0.802)	65.62(72.93)
[2023-09-29 12:57:23 10splitTasks](trainer.py 286): INFO [80/157]	0.1031(0.1103)	0.0003(0.0069)	0.705(0.802)	71.88(72.92)
[2023-09-29 12:57:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1097)	0.0002(0.0062)	0.851(0.798)	75.00(73.01)
[2023-09-29 12:57:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1103(0.1091)	0.0003(0.0056)	0.554(0.790)	78.12(73.42)
[2023-09-29 12:57:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1085)	0.0003(0.0051)	0.726(0.784)	84.38(73.62)
[2023-09-29 12:57:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1080)	0.0002(0.0047)	0.816(0.780)	71.88(73.55)
[2023-09-29 12:57:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1075)	0.0003(0.0044)	0.915(0.780)	71.88(73.47)
[2023-09-29 12:57:29 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1074)	0.0003(0.0041)	0.698(0.774)	78.12(73.67)
[2023-09-29 12:57:30 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1072)	0.0002(0.0039)	0.874(0.765)	78.12(73.99)
[2023-09-29 12:57:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1069)	0.0001(0.0037)	0.624(0.768)	62.50(74.10)
[2023-09-29 12:57:31 10splitTasks](trainer.py 288): INFO  * Train Acc 74.100
[2023-09-29 12:57:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.200, Total time 1.83
[2023-09-29 12:57:33 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 12:57:33 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 12:57:33 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:57:34 10splitTasks](trainer.py 286): INFO [0/157]	0.6508(0.6508)	0.5436(0.5436)	0.644(0.644)	75.00(75.00)
[2023-09-29 12:57:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1535)	0.0002(0.0497)	0.487(0.707)	84.38(77.56)
[2023-09-29 12:57:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1024(0.1292)	0.0003(0.0262)	0.858(0.695)	78.12(77.23)
[2023-09-29 12:57:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1036(0.1221)	0.0002(0.0178)	0.637(0.742)	71.88(74.70)
[2023-09-29 12:57:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1024(0.1176)	0.0005(0.0136)	0.549(0.698)	81.25(75.99)
[2023-09-29 12:57:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1037(0.1152)	0.0003(0.0110)	0.619(0.689)	78.12(76.90)
[2023-09-29 12:57:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1133)	0.0002(0.0092)	0.514(0.682)	84.38(77.25)
[2023-09-29 12:57:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1120)	0.0003(0.0080)	0.531(0.676)	81.25(77.60)
[2023-09-29 12:57:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1063(0.1113)	0.0004(0.0070)	0.431(0.669)	81.25(77.78)
[2023-09-29 12:57:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1043(0.1107)	0.0002(0.0063)	0.578(0.669)	81.25(77.82)
[2023-09-29 12:57:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1032(0.1098)	0.0002(0.0057)	0.486(0.661)	84.38(78.00)
[2023-09-29 12:57:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1064(0.1092)	0.0005(0.0052)	0.686(0.654)	75.00(78.24)
[2023-09-29 12:57:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1086)	0.0002(0.0048)	0.812(0.649)	75.00(78.33)
[2023-09-29 12:57:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1082)	0.0002(0.0045)	0.680(0.655)	68.75(77.98)
[2023-09-29 12:57:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1024(0.1078)	0.0003(0.0042)	1.081(0.655)	68.75(78.10)
[2023-09-29 12:57:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1075)	0.0002(0.0039)	0.822(0.656)	71.88(77.90)
[2023-09-29 12:57:50 10splitTasks](trainer.py 286): INFO [156/157]	0.0818(0.1071)	0.0005(0.0038)	1.301(0.656)	62.50(77.88)
[2023-09-29 12:57:50 10splitTasks](trainer.py 288): INFO  * Train Acc 77.880
[2023-09-29 12:57:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.200, Total time 1.74
[2023-09-29 12:57:52 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 12:57:52 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 12:57:52 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:57:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6242(0.6242)	0.5128(0.5128)	0.728(0.728)	78.12(78.12)
[2023-09-29 12:57:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1078(0.1521)	0.0003(0.0469)	0.516(0.557)	84.38(81.25)
[2023-09-29 12:57:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1294)	0.0002(0.0250)	0.807(0.513)	71.88(83.33)
[2023-09-29 12:57:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1055(0.1210)	0.0007(0.0171)	0.869(0.533)	68.75(82.56)
[2023-09-29 12:57:57 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1168)	0.0002(0.0130)	0.483(0.525)	81.25(82.47)
[2023-09-29 12:57:58 10splitTasks](trainer.py 286): INFO [50/157]	0.1010(0.1140)	0.0002(0.0105)	0.405(0.520)	87.50(82.35)
[2023-09-29 12:57:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1025(0.1121)	0.0003(0.0088)	0.602(0.522)	81.25(82.12)
[2023-09-29 12:58:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1028(0.1112)	0.0003(0.0076)	0.413(0.507)	81.25(82.57)
[2023-09-29 12:58:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1102)	0.0004(0.0067)	0.445(0.508)	81.25(82.52)
[2023-09-29 12:58:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1040(0.1095)	0.0002(0.0060)	0.669(0.503)	75.00(82.76)
[2023-09-29 12:58:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1078(0.1089)	0.0002(0.0055)	0.741(0.500)	78.12(83.14)
[2023-09-29 12:58:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1037(0.1086)	0.0002(0.0050)	0.452(0.500)	87.50(83.33)
[2023-09-29 12:58:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1094(0.1082)	0.0006(0.0046)	0.517(0.508)	84.38(83.06)
[2023-09-29 12:58:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1078)	0.0001(0.0043)	0.445(0.510)	87.50(83.13)
[2023-09-29 12:58:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1082(0.1075)	0.0002(0.0040)	0.719(0.517)	75.00(82.87)
[2023-09-29 12:58:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1073)	0.0001(0.0038)	0.564(0.525)	78.12(82.53)
[2023-09-29 12:58:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1069)	0.0001(0.0036)	1.011(0.523)	62.50(82.56)
[2023-09-29 12:58:09 10splitTasks](trainer.py 288): INFO  * Train Acc 82.560
[2023-09-29 12:58:10 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.600, Total time 1.71
[2023-09-29 12:58:10 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 12:58:10 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 12:58:10 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:58:11 10splitTasks](trainer.py 286): INFO [0/157]	0.6217(0.6217)	0.5041(0.5041)	0.635(0.635)	75.00(75.00)
[2023-09-29 12:58:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1514)	0.0003(0.0461)	0.409(0.495)	81.25(81.82)
[2023-09-29 12:58:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1287)	0.0001(0.0243)	0.328(0.500)	90.62(82.59)
[2023-09-29 12:58:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1201)	0.0001(0.0166)	0.443(0.469)	87.50(83.87)
[2023-09-29 12:58:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1031(0.1158)	0.0002(0.0126)	0.228(0.472)	90.62(83.54)
[2023-09-29 12:58:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1130)	0.0002(0.0102)	0.541(0.468)	81.25(83.95)
[2023-09-29 12:58:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1027(0.1120)	0.0002(0.0086)	0.291(0.461)	87.50(84.17)
[2023-09-29 12:58:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1107)	0.0003(0.0074)	0.427(0.463)	90.62(84.20)
[2023-09-29 12:58:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1099)	0.0002(0.0065)	0.439(0.456)	87.50(84.41)
[2023-09-29 12:58:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1331(0.1093)	0.0005(0.0059)	0.713(0.453)	81.25(84.44)
[2023-09-29 12:58:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1086)	0.0002(0.0053)	0.687(0.454)	71.88(84.19)
[2023-09-29 12:58:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1081)	0.0002(0.0049)	0.418(0.457)	87.50(84.01)
[2023-09-29 12:58:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1077)	0.0001(0.0045)	0.698(0.459)	75.00(84.01)
[2023-09-29 12:58:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1073)	0.0003(0.0042)	0.310(0.458)	93.75(84.21)
[2023-09-29 12:58:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1070)	0.0003(0.0039)	0.362(0.461)	90.62(84.18)
[2023-09-29 12:58:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1050(0.1068)	0.0001(0.0037)	0.371(0.465)	84.38(84.04)
[2023-09-29 12:58:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1064)	0.0001(0.0036)	0.426(0.463)	87.50(84.18)
[2023-09-29 12:58:27 10splitTasks](trainer.py 288): INFO  * Train Acc 84.180
[2023-09-29 12:58:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.66
[2023-09-29 12:58:29 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 12:58:29 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 12:58:29 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:58:30 10splitTasks](trainer.py 286): INFO [0/157]	0.6988(0.6988)	0.5937(0.5937)	0.490(0.490)	81.25(81.25)
[2023-09-29 12:58:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1568)	0.0003(0.0543)	0.117(0.348)	96.88(88.92)
[2023-09-29 12:58:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1032(0.1311)	0.0002(0.0286)	0.490(0.407)	87.50(87.80)
[2023-09-29 12:58:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1225)	0.0002(0.0195)	0.275(0.403)	90.62(87.20)
[2023-09-29 12:58:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1177)	0.0003(0.0148)	0.417(0.413)	81.25(87.12)
[2023-09-29 12:58:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1042(0.1149)	0.0003(0.0120)	0.511(0.424)	81.25(86.34)
[2023-09-29 12:58:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1042(0.1129)	0.0002(0.0101)	0.232(0.424)	93.75(86.42)
[2023-09-29 12:58:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1117)	0.0003(0.0087)	0.476(0.430)	81.25(86.14)
[2023-09-29 12:58:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1105)	0.0002(0.0077)	0.393(0.434)	87.50(85.80)
[2023-09-29 12:58:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1096)	0.0002(0.0069)	0.624(0.447)	78.12(85.20)
[2023-09-29 12:58:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1090)	0.0003(0.0062)	0.608(0.442)	84.38(85.37)
[2023-09-29 12:58:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1084)	0.0003(0.0057)	0.253(0.442)	90.62(85.16)
[2023-09-29 12:58:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1196(0.1081)	0.0006(0.0053)	0.672(0.441)	78.12(85.10)
[2023-09-29 12:58:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1078)	0.0002(0.0049)	0.309(0.440)	93.75(85.21)
[2023-09-29 12:58:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1075)	0.0003(0.0046)	0.631(0.443)	71.88(85.04)
[2023-09-29 12:58:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1019(0.1072)	0.0003(0.0043)	0.417(0.442)	81.25(85.08)
[2023-09-29 12:58:46 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1069)	0.0001(0.0041)	0.232(0.439)	100.00(85.24)
[2023-09-29 12:58:46 10splitTasks](trainer.py 288): INFO  * Train Acc 85.240
[2023-09-29 12:58:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.65
[2023-09-29 12:58:47 10splitTasks](my_trainer.py 206): INFO Pruning for task1
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 6797/7997 (84.99%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 2960/3482 (85.01%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 26634/31334 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 11837/13926 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 11837/13926 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 11837/13926 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 26634/31334 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 11837/13926 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 11837/13926 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 26634/31334 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 11837/13926 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 23675/27853 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 106537/125338 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 47350/55706 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 94699/111411 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 47350/55706 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 106537/125338 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 47350/55706 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 47350/55706 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 106537/125338 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 47350/55706 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 47350/55706 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 106537/125338 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 47350/55706 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 94699/111411 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 426148/501350 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 378798/445645 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 426148/501350 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 426148/501350 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 426148/501350 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 426148/501350 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 426148/501350 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 189399/222822 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 378798/445645 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 1704592/2005402 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 757596/891290 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 1515192/1782579 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 757596/891290 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 1704592/2005402 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 757596/891290 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 757596/891290 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 1704593/2005403 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 757596/891290 (85.00%) (Total in layer: 1048576)
[2023-09-29 12:58:48 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 12:58:48 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 12:58:48 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 12:58:48 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:58:48 10splitTasks](trainer.py 286): INFO [0/157]	0.5781(0.5781)	0.4580(0.4580)	0.763(0.763)	71.88(71.88)
[2023-09-29 12:58:49 10splitTasks](trainer.py 286): INFO [10/157]	0.1027(0.1472)	0.0003(0.0420)	0.714(0.629)	68.75(78.12)
[2023-09-29 12:58:50 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1263)	0.0003(0.0221)	0.686(0.622)	81.25(78.42)
[2023-09-29 12:58:52 10splitTasks](trainer.py 286): INFO [30/157]	0.1039(0.1197)	0.0003(0.0151)	0.494(0.624)	84.38(78.83)
[2023-09-29 12:58:53 10splitTasks](trainer.py 286): INFO [40/157]	0.1077(0.1158)	0.0006(0.0115)	0.808(0.631)	75.00(78.73)
[2023-09-29 12:58:54 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1135)	0.0001(0.0093)	0.859(0.647)	78.12(78.12)
[2023-09-29 12:58:55 10splitTasks](trainer.py 286): INFO [60/157]	0.1058(0.1122)	0.0003(0.0078)	0.491(0.645)	78.12(77.66)
[2023-09-29 12:58:56 10splitTasks](trainer.py 286): INFO [70/157]	0.1118(0.1111)	0.0005(0.0068)	1.037(0.667)	75.00(77.11)
[2023-09-29 12:58:57 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1102)	0.0002(0.0060)	0.430(0.660)	87.50(77.85)
[2023-09-29 12:58:58 10splitTasks](trainer.py 286): INFO [90/157]	0.1024(0.1098)	0.0002(0.0054)	0.344(0.650)	87.50(77.92)
[2023-09-29 12:58:59 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1092)	0.0003(0.0049)	0.721(0.656)	75.00(77.78)
[2023-09-29 12:59:00 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1087)	0.0002(0.0045)	0.578(0.658)	84.38(77.73)
[2023-09-29 12:59:01 10splitTasks](trainer.py 286): INFO [120/157]	0.1140(0.1083)	0.0005(0.0041)	0.512(0.658)	81.25(77.92)
[2023-09-29 12:59:02 10splitTasks](trainer.py 286): INFO [130/157]	0.1043(0.1078)	0.0002(0.0038)	0.588(0.666)	84.38(77.91)
[2023-09-29 12:59:03 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1075)	0.0003(0.0036)	0.702(0.672)	81.25(77.86)
[2023-09-29 12:59:04 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1072)	0.0001(0.0034)	0.587(0.674)	78.12(77.71)
[2023-09-29 12:59:05 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1068)	0.0001(0.0032)	0.931(0.679)	75.00(77.50)
[2023-09-29 12:59:05 10splitTasks](trainer.py 288): INFO  * Train Acc 77.500
[2023-09-29 12:59:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.000, Total time 1.62
[2023-09-29 12:59:06 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 12:59:06 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 12:59:06 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:59:07 10splitTasks](trainer.py 286): INFO [0/157]	0.5999(0.5999)	0.4917(0.4917)	0.421(0.421)	81.25(81.25)
[2023-09-29 12:59:08 10splitTasks](trainer.py 286): INFO [10/157]	0.1034(0.1515)	0.0004(0.0450)	0.626(0.733)	84.38(77.56)
[2023-09-29 12:59:09 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1285)	0.0002(0.0238)	0.261(0.651)	96.88(79.32)
[2023-09-29 12:59:10 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1202)	0.0003(0.0162)	0.496(0.658)	78.12(79.03)
[2023-09-29 12:59:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1248(0.1165)	0.0006(0.0123)	0.728(0.668)	68.75(78.35)
[2023-09-29 12:59:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1137)	0.0002(0.0100)	0.583(0.681)	81.25(77.33)
[2023-09-29 12:59:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1264(0.1127)	0.0007(0.0084)	0.749(0.709)	71.88(76.28)
[2023-09-29 12:59:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1116)	0.0003(0.0073)	0.519(0.692)	84.38(76.67)
[2023-09-29 12:59:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1172(0.1110)	0.0006(0.0064)	0.686(0.691)	78.12(76.39)
[2023-09-29 12:59:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1098(0.1102)	0.0002(0.0058)	0.440(0.686)	84.38(76.68)
[2023-09-29 12:59:17 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1096)	0.0003(0.0052)	0.582(0.677)	81.25(77.04)
[2023-09-29 12:59:18 10splitTasks](trainer.py 286): INFO [110/157]	0.1209(0.1091)	0.0003(0.0048)	0.473(0.668)	84.38(77.28)
[2023-09-29 12:59:19 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1089)	0.0003(0.0044)	0.974(0.671)	71.88(77.20)
[2023-09-29 12:59:21 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1084)	0.0003(0.0041)	0.217(0.658)	90.62(77.65)
[2023-09-29 12:59:22 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1084)	0.0003(0.0038)	0.531(0.657)	81.25(77.66)
[2023-09-29 12:59:23 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1080)	0.0001(0.0036)	0.669(0.655)	75.00(77.65)
[2023-09-29 12:59:23 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1076)	0.0001(0.0035)	0.905(0.665)	62.50(77.38)
[2023-09-29 12:59:23 10splitTasks](trainer.py 288): INFO  * Train Acc 77.380
[2023-09-29 12:59:25 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.400, Total time 1.74
[2023-09-29 12:59:25 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 12:59:25 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 12:59:25 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:59:26 10splitTasks](trainer.py 286): INFO [0/157]	0.7402(0.7402)	0.6352(0.6352)	0.871(0.871)	75.00(75.00)
[2023-09-29 12:59:27 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1611)	0.0003(0.0580)	0.454(0.670)	84.38(75.57)
[2023-09-29 12:59:28 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1336)	0.0002(0.0305)	0.311(0.662)	87.50(77.23)
[2023-09-29 12:59:29 10splitTasks](trainer.py 286): INFO [30/157]	0.1031(0.1238)	0.0002(0.0208)	0.839(0.649)	75.00(77.52)
[2023-09-29 12:59:30 10splitTasks](trainer.py 286): INFO [40/157]	0.1025(0.1188)	0.0003(0.0158)	0.685(0.632)	84.38(78.66)
[2023-09-29 12:59:31 10splitTasks](trainer.py 286): INFO [50/157]	0.1130(0.1160)	0.0001(0.0128)	0.554(0.607)	84.38(79.41)
[2023-09-29 12:59:32 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1140)	0.0003(0.0107)	0.463(0.601)	81.25(79.30)
[2023-09-29 12:59:33 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1127)	0.0003(0.0093)	0.415(0.590)	84.38(79.40)
[2023-09-29 12:59:34 10splitTasks](trainer.py 286): INFO [80/157]	0.1043(0.1120)	0.0005(0.0082)	0.393(0.583)	81.25(79.78)
[2023-09-29 12:59:35 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1113)	0.0003(0.0073)	0.479(0.592)	81.25(79.57)
[2023-09-29 12:59:36 10splitTasks](trainer.py 286): INFO [100/157]	0.1091(0.1105)	0.0003(0.0066)	0.657(0.597)	84.38(79.58)
[2023-09-29 12:59:37 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1098)	0.0003(0.0060)	0.977(0.601)	65.62(79.34)
[2023-09-29 12:59:38 10splitTasks](trainer.py 286): INFO [120/157]	0.1057(0.1092)	0.0002(0.0056)	0.473(0.600)	78.12(79.29)
[2023-09-29 12:59:39 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1087)	0.0002(0.0052)	0.470(0.603)	87.50(79.34)
[2023-09-29 12:59:40 10splitTasks](trainer.py 286): INFO [140/157]	0.1184(0.1085)	0.0002(0.0048)	0.496(0.601)	90.62(79.61)
[2023-09-29 12:59:41 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1082)	0.0001(0.0045)	0.676(0.606)	81.25(79.57)
[2023-09-29 12:59:42 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1078)	0.0001(0.0044)	0.657(0.602)	75.00(79.62)
[2023-09-29 12:59:42 10splitTasks](trainer.py 288): INFO  * Train Acc 79.620
[2023-09-29 12:59:44 10splitTasks](my_trainer.py 503): INFO  * Val Acc 71.800, Total time 1.61
[2023-09-29 12:59:44 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 12:59:44 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 12:59:44 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 12:59:44 10splitTasks](trainer.py 286): INFO [0/157]	0.6364(0.6364)	0.5128(0.5128)	0.509(0.509)	84.38(84.38)
[2023-09-29 12:59:45 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1538)	0.0003(0.0469)	0.681(0.658)	71.88(78.69)
[2023-09-29 12:59:46 10splitTasks](trainer.py 286): INFO [20/157]	0.1053(0.1302)	0.0004(0.0247)	0.283(0.604)	87.50(79.91)
[2023-09-29 12:59:47 10splitTasks](trainer.py 286): INFO [30/157]	0.1022(0.1220)	0.0006(0.0169)	0.546(0.570)	84.38(80.95)
[2023-09-29 12:59:49 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1177)	0.0003(0.0128)	0.436(0.578)	90.62(81.17)
[2023-09-29 12:59:50 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1154)	0.0003(0.0104)	0.675(0.588)	75.00(80.70)
[2023-09-29 12:59:51 10splitTasks](trainer.py 286): INFO [60/157]	0.1051(0.1138)	0.0002(0.0087)	0.598(0.571)	75.00(81.56)
[2023-09-29 12:59:52 10splitTasks](trainer.py 286): INFO [70/157]	0.1059(0.1123)	0.0001(0.0076)	1.281(0.588)	56.25(81.34)
[2023-09-29 12:59:53 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1111)	0.0003(0.0067)	0.611(0.592)	81.25(80.98)
[2023-09-29 12:59:54 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1101)	0.0002(0.0060)	0.408(0.601)	87.50(80.73)
[2023-09-29 12:59:55 10splitTasks](trainer.py 286): INFO [100/157]	0.1057(0.1094)	0.0002(0.0054)	0.205(0.594)	90.62(80.75)
[2023-09-29 12:59:56 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1089)	0.0003(0.0049)	0.481(0.596)	81.25(80.63)
[2023-09-29 12:59:57 10splitTasks](trainer.py 286): INFO [120/157]	0.1037(0.1085)	0.0003(0.0046)	0.325(0.582)	90.62(81.02)
[2023-09-29 12:59:58 10splitTasks](trainer.py 286): INFO [130/157]	0.1053(0.1081)	0.0003(0.0042)	0.481(0.576)	78.12(81.13)
[2023-09-29 12:59:59 10splitTasks](trainer.py 286): INFO [140/157]	0.1046(0.1078)	0.0002(0.0040)	0.387(0.573)	90.62(81.18)
[2023-09-29 13:00:00 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1074)	0.0003(0.0037)	0.540(0.568)	90.62(81.31)
[2023-09-29 13:00:00 10splitTasks](trainer.py 286): INFO [156/157]	0.0811(0.1071)	0.0001(0.0036)	1.707(0.571)	62.50(81.16)
[2023-09-29 13:00:01 10splitTasks](trainer.py 288): INFO  * Train Acc 81.160
[2023-09-29 13:00:02 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.000, Total time 1.70
[2023-09-29 13:00:02 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 13:00:02 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 13:00:02 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:00:03 10splitTasks](trainer.py 286): INFO [0/157]	0.6493(0.6493)	0.5386(0.5386)	0.982(0.982)	65.62(65.62)
[2023-09-29 13:00:04 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1533)	0.0002(0.0492)	0.676(0.613)	71.88(79.83)
[2023-09-29 13:00:05 10splitTasks](trainer.py 286): INFO [20/157]	0.1217(0.1320)	0.0002(0.0260)	0.470(0.598)	78.12(80.80)
[2023-09-29 13:00:06 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1222)	0.0003(0.0177)	1.605(0.622)	53.12(79.54)
[2023-09-29 13:00:07 10splitTasks](trainer.py 286): INFO [40/157]	0.1031(0.1173)	0.0002(0.0134)	0.682(0.613)	81.25(79.34)
[2023-09-29 13:00:08 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1147)	0.0004(0.0109)	0.470(0.601)	90.62(80.02)
[2023-09-29 13:00:09 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1128)	0.0002(0.0091)	0.391(0.579)	87.50(80.74)
[2023-09-29 13:00:10 10splitTasks](trainer.py 286): INFO [70/157]	0.1027(0.1118)	0.0002(0.0079)	0.622(0.574)	84.38(81.07)
[2023-09-29 13:00:11 10splitTasks](trainer.py 286): INFO [80/157]	0.1024(0.1111)	0.0005(0.0070)	0.503(0.558)	84.38(81.52)
[2023-09-29 13:00:12 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1102)	0.0001(0.0062)	0.542(0.567)	90.62(81.32)
[2023-09-29 13:00:13 10splitTasks](trainer.py 286): INFO [100/157]	0.1129(0.1097)	0.0002(0.0057)	0.356(0.560)	90.62(81.44)
[2023-09-29 13:00:14 10splitTasks](trainer.py 286): INFO [110/157]	0.1071(0.1092)	0.0002(0.0052)	0.747(0.554)	81.25(81.56)
[2023-09-29 13:00:15 10splitTasks](trainer.py 286): INFO [120/157]	0.1040(0.1088)	0.0002(0.0048)	0.499(0.550)	84.38(81.71)
[2023-09-29 13:00:16 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1083)	0.0003(0.0044)	0.889(0.547)	62.50(81.70)
[2023-09-29 13:00:18 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1079)	0.0003(0.0041)	0.297(0.550)	87.50(81.74)
[2023-09-29 13:00:19 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1075)	0.0001(0.0039)	0.754(0.548)	75.00(81.81)
[2023-09-29 13:00:19 10splitTasks](trainer.py 286): INFO [156/157]	0.0809(0.1072)	0.0001(0.0038)	0.318(0.545)	87.50(81.78)
[2023-09-29 13:00:19 10splitTasks](trainer.py 288): INFO  * Train Acc 81.780
[2023-09-29 13:00:21 10splitTasks](my_trainer.py 503): INFO  * Val Acc 68.600, Total time 1.66
[2023-09-29 13:00:21 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 13:00:21 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 13:00:21 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:00:22 10splitTasks](trainer.py 286): INFO [0/157]	0.6085(0.6085)	0.4666(0.4666)	0.429(0.429)	90.62(90.62)
[2023-09-29 13:00:23 10splitTasks](trainer.py 286): INFO [10/157]	0.1047(0.1501)	0.0002(0.0427)	0.164(0.467)	96.88(84.66)
[2023-09-29 13:00:24 10splitTasks](trainer.py 286): INFO [20/157]	0.1046(0.1281)	0.0002(0.0225)	0.404(0.455)	84.38(84.52)
[2023-09-29 13:00:25 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1202)	0.0002(0.0153)	0.506(0.468)	87.50(84.68)
[2023-09-29 13:00:26 10splitTasks](trainer.py 286): INFO [40/157]	0.1139(0.1165)	0.0003(0.0117)	0.447(0.489)	90.62(84.45)
[2023-09-29 13:00:27 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1142)	0.0002(0.0095)	0.305(0.491)	87.50(83.70)
[2023-09-29 13:00:28 10splitTasks](trainer.py 286): INFO [60/157]	0.1154(0.1126)	0.0002(0.0080)	0.292(0.485)	87.50(83.56)
[2023-09-29 13:00:29 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1114)	0.0003(0.0069)	0.454(0.494)	87.50(83.32)
[2023-09-29 13:00:30 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1104)	0.0003(0.0061)	0.192(0.492)	96.88(83.41)
[2023-09-29 13:00:31 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1099)	0.0002(0.0054)	0.281(0.484)	90.62(83.83)
[2023-09-29 13:00:32 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1092)	0.0002(0.0049)	0.704(0.479)	75.00(84.16)
[2023-09-29 13:00:33 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1088)	0.0002(0.0045)	0.313(0.478)	90.62(84.21)
[2023-09-29 13:00:34 10splitTasks](trainer.py 286): INFO [120/157]	0.1058(0.1085)	0.0003(0.0042)	0.398(0.487)	81.25(83.99)
[2023-09-29 13:00:35 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1080)	0.0002(0.0039)	0.434(0.492)	81.25(83.68)
[2023-09-29 13:00:36 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1078)	0.0003(0.0036)	0.426(0.489)	81.25(83.71)
[2023-09-29 13:00:37 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1076)	0.0001(0.0034)	0.504(0.491)	81.25(83.63)
[2023-09-29 13:00:38 10splitTasks](trainer.py 286): INFO [156/157]	0.0814(0.1072)	0.0001(0.0033)	0.601(0.491)	87.50(83.62)
[2023-09-29 13:00:38 10splitTasks](trainer.py 288): INFO  * Train Acc 83.620
[2023-09-29 13:00:40 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.800, Total time 1.76
[2023-09-29 13:00:40 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 13:00:40 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 13:00:40 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:00:40 10splitTasks](trainer.py 286): INFO [0/157]	0.6687(0.6687)	0.5634(0.5634)	0.249(0.249)	93.75(93.75)
[2023-09-29 13:00:41 10splitTasks](trainer.py 286): INFO [10/157]	0.1008(0.1538)	0.0001(0.0514)	0.273(0.386)	87.50(86.93)
[2023-09-29 13:00:42 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1297)	0.0003(0.0271)	0.153(0.409)	96.88(86.76)
[2023-09-29 13:00:43 10splitTasks](trainer.py 286): INFO [30/157]	0.1008(0.1206)	0.0002(0.0184)	0.437(0.448)	87.50(85.58)
[2023-09-29 13:00:44 10splitTasks](trainer.py 286): INFO [40/157]	0.1078(0.1162)	0.0002(0.0140)	0.245(0.446)	90.62(84.91)
[2023-09-29 13:00:45 10splitTasks](trainer.py 286): INFO [50/157]	0.1025(0.1139)	0.0003(0.0113)	0.593(0.438)	81.25(85.17)
[2023-09-29 13:00:46 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1124)	0.0002(0.0095)	0.245(0.435)	84.38(85.04)
[2023-09-29 13:00:47 10splitTasks](trainer.py 286): INFO [70/157]	0.1044(0.1112)	0.0002(0.0082)	0.377(0.434)	87.50(84.95)
[2023-09-29 13:00:49 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1100)	0.0003(0.0072)	0.330(0.431)	90.62(85.07)
[2023-09-29 13:00:50 10splitTasks](trainer.py 286): INFO [90/157]	0.1010(0.1093)	0.0001(0.0065)	0.844(0.432)	75.00(84.86)
[2023-09-29 13:00:51 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1085)	0.0001(0.0059)	0.386(0.434)	90.62(84.93)
[2023-09-29 13:00:52 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1079)	0.0003(0.0054)	0.299(0.440)	87.50(84.63)
[2023-09-29 13:00:53 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1075)	0.0002(0.0049)	0.452(0.447)	84.38(84.40)
[2023-09-29 13:00:54 10splitTasks](trainer.py 286): INFO [130/157]	0.1027(0.1071)	0.0003(0.0046)	0.174(0.447)	96.88(84.45)
[2023-09-29 13:00:55 10splitTasks](trainer.py 286): INFO [140/157]	0.1023(0.1069)	0.0003(0.0043)	0.321(0.449)	90.62(84.33)
[2023-09-29 13:00:56 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1066)	0.0001(0.0040)	0.663(0.450)	78.12(84.29)
[2023-09-29 13:00:56 10splitTasks](trainer.py 286): INFO [156/157]	0.0804(0.1062)	0.0001(0.0039)	0.641(0.450)	62.50(84.32)
[2023-09-29 13:00:56 10splitTasks](trainer.py 288): INFO  * Train Acc 84.320
[2023-09-29 13:00:58 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.200, Total time 1.65
[2023-09-29 13:00:58 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 13:00:58 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 13:00:58 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:00:59 10splitTasks](trainer.py 286): INFO [0/157]	0.6013(0.6013)	0.4912(0.4912)	0.625(0.625)	75.00(75.00)
[2023-09-29 13:01:00 10splitTasks](trainer.py 286): INFO [10/157]	0.1032(0.1477)	0.0001(0.0449)	0.244(0.365)	87.50(85.51)
[2023-09-29 13:01:01 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1269)	0.0002(0.0237)	0.556(0.383)	75.00(85.57)
[2023-09-29 13:01:02 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1193)	0.0002(0.0161)	0.294(0.413)	90.62(84.38)
[2023-09-29 13:01:03 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1151)	0.0002(0.0123)	0.773(0.431)	78.12(84.91)
[2023-09-29 13:01:04 10splitTasks](trainer.py 286): INFO [50/157]	0.1041(0.1125)	0.0002(0.0099)	0.292(0.434)	93.75(85.05)
[2023-09-29 13:01:05 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1108)	0.0003(0.0083)	0.463(0.430)	84.38(85.66)
[2023-09-29 13:01:06 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1096)	0.0003(0.0072)	0.444(0.435)	78.12(85.12)
[2023-09-29 13:01:07 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1090)	0.0002(0.0063)	0.329(0.429)	93.75(85.30)
[2023-09-29 13:01:08 10splitTasks](trainer.py 286): INFO [90/157]	0.1086(0.1088)	0.0006(0.0057)	0.419(0.432)	81.25(85.20)
[2023-09-29 13:01:09 10splitTasks](trainer.py 286): INFO [100/157]	0.1036(0.1084)	0.0003(0.0052)	0.494(0.424)	81.25(85.46)
[2023-09-29 13:01:10 10splitTasks](trainer.py 286): INFO [110/157]	0.1118(0.1080)	0.0006(0.0047)	0.224(0.425)	90.62(85.50)
[2023-09-29 13:01:11 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1076)	0.0002(0.0044)	0.466(0.428)	87.50(85.51)
[2023-09-29 13:01:12 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1072)	0.0001(0.0041)	0.352(0.425)	87.50(85.66)
[2023-09-29 13:01:13 10splitTasks](trainer.py 286): INFO [140/157]	0.1027(0.1069)	0.0003(0.0038)	0.387(0.423)	81.25(85.55)
[2023-09-29 13:01:14 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1066)	0.0002(0.0036)	0.596(0.420)	78.12(85.49)
[2023-09-29 13:01:15 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1063)	0.0001(0.0034)	0.671(0.426)	87.50(85.46)
[2023-09-29 13:01:15 10splitTasks](trainer.py 288): INFO  * Train Acc 85.460
[2023-09-29 13:01:17 10splitTasks](my_trainer.py 503): INFO  * Val Acc 75.400, Total time 1.76
[2023-09-29 13:01:17 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 13:01:17 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 13:01:17 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:01:17 10splitTasks](trainer.py 286): INFO [0/157]	0.6088(0.6088)	0.5000(0.5000)	0.796(0.796)	75.00(75.00)
[2023-09-29 13:01:18 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1494)	0.0002(0.0458)	0.351(0.396)	87.50(86.08)
[2023-09-29 13:01:19 10splitTasks](trainer.py 286): INFO [20/157]	0.1089(0.1274)	0.0003(0.0241)	0.623(0.408)	78.12(86.16)
[2023-09-29 13:01:20 10splitTasks](trainer.py 286): INFO [30/157]	0.1027(0.1197)	0.0004(0.0165)	0.297(0.421)	90.62(85.58)
[2023-09-29 13:01:21 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1156)	0.0003(0.0125)	0.250(0.404)	87.50(85.98)
[2023-09-29 13:01:22 10splitTasks](trainer.py 286): INFO [50/157]	0.1054(0.1136)	0.0003(0.0101)	0.893(0.393)	71.88(86.34)
[2023-09-29 13:01:23 10splitTasks](trainer.py 286): INFO [60/157]	0.1156(0.1121)	0.0002(0.0085)	0.363(0.402)	87.50(86.22)
[2023-09-29 13:01:25 10splitTasks](trainer.py 286): INFO [70/157]	0.1059(0.1112)	0.0002(0.0074)	0.675(0.394)	81.25(86.66)
[2023-09-29 13:01:26 10splitTasks](trainer.py 286): INFO [80/157]	0.1054(0.1110)	0.0002(0.0065)	0.226(0.383)	93.75(86.92)
[2023-09-29 13:01:27 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1103)	0.0003(0.0058)	0.191(0.390)	93.75(86.71)
[2023-09-29 13:01:28 10splitTasks](trainer.py 286): INFO [100/157]	0.1066(0.1099)	0.0002(0.0053)	0.343(0.387)	87.50(86.88)
[2023-09-29 13:01:29 10splitTasks](trainer.py 286): INFO [110/157]	0.1032(0.1092)	0.0003(0.0049)	0.299(0.386)	87.50(86.82)
[2023-09-29 13:01:30 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1087)	0.0002(0.0045)	0.068(0.385)	100.00(86.96)
[2023-09-29 13:01:31 10splitTasks](trainer.py 286): INFO [130/157]	0.1028(0.1084)	0.0004(0.0042)	0.225(0.378)	93.75(87.21)
[2023-09-29 13:01:32 10splitTasks](trainer.py 286): INFO [140/157]	0.1055(0.1080)	0.0002(0.0039)	0.344(0.379)	93.75(87.37)
[2023-09-29 13:01:33 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1078)	0.0001(0.0037)	0.525(0.380)	84.38(87.31)
[2023-09-29 13:01:33 10splitTasks](trainer.py 286): INFO [156/157]	0.0806(0.1074)	0.0001(0.0035)	0.543(0.381)	87.50(87.26)
[2023-09-29 13:01:34 10splitTasks](trainer.py 288): INFO  * Train Acc 87.260
[2023-09-29 13:01:35 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.63
[2023-09-29 13:01:35 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 13:01:35 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 13:01:35 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:01:36 10splitTasks](trainer.py 286): INFO [0/157]	0.6382(0.6382)	0.5254(0.5254)	0.514(0.514)	78.12(78.12)
[2023-09-29 13:01:37 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1559)	0.0002(0.0508)	0.337(0.388)	87.50(84.94)
[2023-09-29 13:01:38 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1304)	0.0002(0.0268)	0.334(0.398)	87.50(84.97)
[2023-09-29 13:01:39 10splitTasks](trainer.py 286): INFO [30/157]	0.1083(0.1216)	0.0002(0.0183)	0.421(0.377)	84.38(85.79)
[2023-09-29 13:01:40 10splitTasks](trainer.py 286): INFO [40/157]	0.1025(0.1172)	0.0003(0.0139)	0.378(0.366)	84.38(86.66)
[2023-09-29 13:01:41 10splitTasks](trainer.py 286): INFO [50/157]	0.1043(0.1146)	0.0002(0.0112)	0.318(0.360)	90.62(87.01)
[2023-09-29 13:01:42 10splitTasks](trainer.py 286): INFO [60/157]	0.1028(0.1126)	0.0003(0.0094)	0.334(0.349)	87.50(87.40)
[2023-09-29 13:01:43 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1117)	0.0002(0.0081)	0.426(0.335)	84.38(87.81)
[2023-09-29 13:01:44 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1108)	0.0003(0.0072)	0.250(0.339)	87.50(87.65)
[2023-09-29 13:01:45 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1101)	0.0002(0.0064)	0.243(0.345)	90.62(87.60)
[2023-09-29 13:01:46 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1095)	0.0002(0.0058)	0.348(0.338)	90.62(87.90)
[2023-09-29 13:01:47 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1091)	0.0003(0.0053)	0.254(0.332)	90.62(88.23)
[2023-09-29 13:01:48 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1089)	0.0003(0.0049)	0.399(0.335)	90.62(88.15)
[2023-09-29 13:01:49 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1083)	0.0002(0.0046)	0.533(0.333)	78.12(88.17)
[2023-09-29 13:01:50 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1079)	0.0002(0.0043)	0.096(0.334)	96.88(88.21)
[2023-09-29 13:01:51 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1076)	0.0001(0.0040)	0.196(0.332)	93.75(88.16)
[2023-09-29 13:01:52 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1072)	0.0001(0.0039)	1.003(0.329)	62.50(88.30)
[2023-09-29 13:01:52 10splitTasks](trainer.py 288): INFO  * Train Acc 88.300
[2023-09-29 13:01:54 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.66
[2023-09-29 13:01:54 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 13:01:54 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 13:01:54 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:01:54 10splitTasks](trainer.py 286): INFO [0/157]	0.5957(0.5957)	0.4718(0.4718)	0.192(0.192)	90.62(90.62)
[2023-09-29 13:01:55 10splitTasks](trainer.py 286): INFO [10/157]	0.1137(0.1508)	0.0003(0.0432)	0.477(0.303)	81.25(88.35)
[2023-09-29 13:01:57 10splitTasks](trainer.py 286): INFO [20/157]	0.1048(0.1281)	0.0002(0.0229)	0.291(0.312)	90.62(88.84)
[2023-09-29 13:01:58 10splitTasks](trainer.py 286): INFO [30/157]	0.1044(0.1207)	0.0005(0.0156)	0.357(0.323)	87.50(89.11)
[2023-09-29 13:01:59 10splitTasks](trainer.py 286): INFO [40/157]	0.1032(0.1163)	0.0002(0.0119)	0.185(0.326)	96.88(89.18)
[2023-09-29 13:02:00 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1135)	0.0003(0.0096)	0.185(0.317)	93.75(89.34)
[2023-09-29 13:02:01 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1123)	0.0002(0.0081)	0.201(0.325)	90.62(88.73)
[2023-09-29 13:02:02 10splitTasks](trainer.py 286): INFO [70/157]	0.1099(0.1111)	0.0003(0.0070)	0.174(0.329)	90.62(88.29)
[2023-09-29 13:02:03 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1101)	0.0002(0.0062)	0.218(0.319)	90.62(88.66)
[2023-09-29 13:02:04 10splitTasks](trainer.py 286): INFO [90/157]	0.1038(0.1095)	0.0005(0.0055)	0.261(0.318)	90.62(88.53)
[2023-09-29 13:02:05 10splitTasks](trainer.py 286): INFO [100/157]	0.1051(0.1088)	0.0002(0.0050)	0.379(0.322)	93.75(88.64)
[2023-09-29 13:02:06 10splitTasks](trainer.py 286): INFO [110/157]	0.1065(0.1086)	0.0002(0.0046)	0.273(0.319)	90.62(88.71)
[2023-09-29 13:02:07 10splitTasks](trainer.py 286): INFO [120/157]	0.1052(0.1081)	0.0002(0.0042)	0.359(0.324)	90.62(88.51)
[2023-09-29 13:02:08 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1078)	0.0002(0.0040)	0.209(0.321)	93.75(88.84)
[2023-09-29 13:02:09 10splitTasks](trainer.py 286): INFO [140/157]	0.1110(0.1078)	0.0006(0.0037)	0.173(0.316)	90.62(89.05)
[2023-09-29 13:02:10 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1074)	0.0001(0.0035)	0.071(0.318)	100.00(88.99)
[2023-09-29 13:02:11 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1070)	0.0001(0.0033)	1.434(0.321)	62.50(88.82)
[2023-09-29 13:02:11 10splitTasks](trainer.py 288): INFO  * Train Acc 88.820
[2023-09-29 13:02:12 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.65
[2023-09-29 13:02:12 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 13:02:12 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 13:02:12 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:02:13 10splitTasks](trainer.py 286): INFO [0/157]	0.6139(0.6139)	0.4933(0.4933)	0.109(0.109)	96.88(96.88)
[2023-09-29 13:02:14 10splitTasks](trainer.py 286): INFO [10/157]	0.1134(0.1516)	0.0002(0.0451)	0.513(0.312)	87.50(88.92)
[2023-09-29 13:02:15 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1295)	0.0002(0.0238)	0.166(0.323)	96.88(89.88)
[2023-09-29 13:02:16 10splitTasks](trainer.py 286): INFO [30/157]	0.1066(0.1209)	0.0005(0.0163)	0.232(0.297)	90.62(90.73)
[2023-09-29 13:02:17 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1164)	0.0002(0.0124)	0.245(0.304)	90.62(90.47)
[2023-09-29 13:02:18 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1140)	0.0003(0.0101)	0.149(0.287)	93.75(90.69)
[2023-09-29 13:02:19 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1128)	0.0003(0.0085)	0.156(0.285)	93.75(90.52)
[2023-09-29 13:02:20 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1115)	0.0003(0.0074)	0.164(0.279)	93.75(90.71)
[2023-09-29 13:02:21 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1104)	0.0003(0.0065)	0.305(0.286)	93.75(90.59)
[2023-09-29 13:02:22 10splitTasks](trainer.py 286): INFO [90/157]	0.1031(0.1098)	0.0003(0.0058)	0.343(0.289)	90.62(90.62)
[2023-09-29 13:02:23 10splitTasks](trainer.py 286): INFO [100/157]	0.1082(0.1093)	0.0002(0.0053)	0.132(0.286)	96.88(90.78)
[2023-09-29 13:02:24 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1087)	0.0002(0.0048)	0.507(0.288)	87.50(90.43)
[2023-09-29 13:02:26 10splitTasks](trainer.py 286): INFO [120/157]	0.1045(0.1082)	0.0002(0.0045)	0.137(0.284)	96.88(90.52)
[2023-09-29 13:02:27 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1078)	0.0002(0.0041)	0.448(0.288)	78.12(90.31)
[2023-09-29 13:02:28 10splitTasks](trainer.py 286): INFO [140/157]	0.1071(0.1076)	0.0002(0.0039)	0.076(0.288)	100.00(90.25)
[2023-09-29 13:02:29 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1074)	0.0002(0.0037)	0.352(0.289)	87.50(90.27)
[2023-09-29 13:02:29 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1070)	0.0001(0.0035)	0.561(0.288)	75.00(90.22)
[2023-09-29 13:02:29 10splitTasks](trainer.py 288): INFO  * Train Acc 90.220
[2023-09-29 13:02:31 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.200, Total time 1.70
[2023-09-29 13:02:31 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 13:02:31 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 13:02:31 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:02:32 10splitTasks](trainer.py 286): INFO [0/157]	0.6769(0.6769)	0.5659(0.5659)	0.224(0.224)	90.62(90.62)
[2023-09-29 13:02:33 10splitTasks](trainer.py 286): INFO [10/157]	0.1084(0.1594)	0.0002(0.0517)	0.376(0.292)	84.38(88.64)
[2023-09-29 13:02:34 10splitTasks](trainer.py 286): INFO [20/157]	0.1085(0.1339)	0.0004(0.0273)	0.141(0.237)	96.88(91.07)
[2023-09-29 13:02:35 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1250)	0.0002(0.0186)	0.260(0.278)	87.50(89.92)
[2023-09-29 13:02:36 10splitTasks](trainer.py 286): INFO [40/157]	0.1029(0.1199)	0.0002(0.0141)	0.231(0.261)	93.75(90.32)
[2023-09-29 13:02:37 10splitTasks](trainer.py 286): INFO [50/157]	0.1059(0.1167)	0.0003(0.0114)	0.247(0.251)	93.75(90.99)
[2023-09-29 13:02:38 10splitTasks](trainer.py 286): INFO [60/157]	0.1091(0.1147)	0.0006(0.0096)	0.470(0.253)	87.50(91.19)
[2023-09-29 13:02:39 10splitTasks](trainer.py 286): INFO [70/157]	0.1190(0.1140)	0.0002(0.0083)	0.066(0.238)	100.00(91.90)
[2023-09-29 13:02:40 10splitTasks](trainer.py 286): INFO [80/157]	0.1046(0.1128)	0.0003(0.0073)	0.254(0.244)	90.62(91.67)
[2023-09-29 13:02:41 10splitTasks](trainer.py 286): INFO [90/157]	0.1129(0.1122)	0.0003(0.0066)	0.354(0.243)	90.62(91.66)
[2023-09-29 13:02:42 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1113)	0.0002(0.0060)	0.091(0.242)	96.88(91.71)
[2023-09-29 13:02:43 10splitTasks](trainer.py 286): INFO [110/157]	0.1029(0.1108)	0.0002(0.0055)	0.271(0.245)	90.62(91.61)
[2023-09-29 13:02:44 10splitTasks](trainer.py 286): INFO [120/157]	0.1058(0.1101)	0.0003(0.0050)	0.489(0.246)	81.25(91.58)
[2023-09-29 13:02:45 10splitTasks](trainer.py 286): INFO [130/157]	0.1100(0.1097)	0.0006(0.0047)	0.247(0.248)	90.62(91.51)
[2023-09-29 13:02:46 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1092)	0.0003(0.0044)	0.273(0.248)	87.50(91.53)
[2023-09-29 13:02:47 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1090)	0.0001(0.0041)	0.258(0.251)	93.75(91.41)
[2023-09-29 13:02:48 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1086)	0.0001(0.0040)	1.485(0.256)	62.50(91.24)
[2023-09-29 13:02:48 10splitTasks](trainer.py 288): INFO  * Train Acc 91.240
[2023-09-29 13:02:50 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.64
[2023-09-29 13:02:50 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 13:02:50 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 13:02:50 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:02:50 10splitTasks](trainer.py 286): INFO [0/157]	0.5719(0.5719)	0.4568(0.4568)	0.067(0.067)	100.00(100.00)
[2023-09-29 13:02:51 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1475)	0.0002(0.0419)	0.119(0.219)	100.00(92.33)
[2023-09-29 13:02:52 10splitTasks](trainer.py 286): INFO [20/157]	0.1077(0.1266)	0.0004(0.0221)	0.299(0.238)	93.75(92.41)
[2023-09-29 13:02:54 10splitTasks](trainer.py 286): INFO [30/157]	0.1007(0.1201)	0.0002(0.0151)	0.362(0.247)	87.50(91.53)
[2023-09-29 13:02:55 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1160)	0.0002(0.0115)	0.364(0.245)	84.38(91.92)
[2023-09-29 13:02:56 10splitTasks](trainer.py 286): INFO [50/157]	0.1026(0.1137)	0.0003(0.0093)	0.157(0.247)	93.75(91.67)
[2023-09-29 13:02:57 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1121)	0.0004(0.0078)	0.209(0.240)	93.75(91.91)
[2023-09-29 13:02:58 10splitTasks](trainer.py 286): INFO [70/157]	0.1028(0.1110)	0.0002(0.0067)	0.323(0.239)	93.75(92.03)
[2023-09-29 13:02:59 10splitTasks](trainer.py 286): INFO [80/157]	0.1033(0.1103)	0.0003(0.0059)	0.405(0.234)	90.62(92.09)
[2023-09-29 13:03:00 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1097)	0.0002(0.0053)	0.119(0.232)	96.88(92.07)
[2023-09-29 13:03:01 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1091)	0.0003(0.0048)	0.274(0.229)	90.62(92.08)
[2023-09-29 13:03:02 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1085)	0.0003(0.0044)	0.200(0.226)	90.62(92.20)
[2023-09-29 13:03:03 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1081)	0.0003(0.0041)	0.223(0.224)	96.88(92.33)
[2023-09-29 13:03:04 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1077)	0.0003(0.0038)	0.178(0.221)	90.62(92.41)
[2023-09-29 13:03:05 10splitTasks](trainer.py 286): INFO [140/157]	0.1011(0.1073)	0.0003(0.0035)	0.242(0.222)	87.50(92.44)
[2023-09-29 13:03:06 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1070)	0.0001(0.0033)	0.382(0.221)	87.50(92.36)
[2023-09-29 13:03:07 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1066)	0.0001(0.0032)	0.892(0.225)	87.50(92.26)
[2023-09-29 13:03:07 10splitTasks](trainer.py 288): INFO  * Train Acc 92.260
[2023-09-29 13:03:08 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.63
[2023-09-29 13:03:08 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 13:03:08 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 13:03:08 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:03:09 10splitTasks](trainer.py 286): INFO [0/157]	0.6327(0.6327)	0.5267(0.5267)	0.035(0.035)	100.00(100.00)
[2023-09-29 13:03:10 10splitTasks](trainer.py 286): INFO [10/157]	0.1206(0.1603)	0.0006(0.0541)	0.170(0.190)	93.75(94.03)
[2023-09-29 13:03:11 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1347)	0.0003(0.0285)	0.138(0.189)	96.88(93.75)
[2023-09-29 13:03:12 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1245)	0.0003(0.0194)	0.054(0.198)	100.00(92.54)
[2023-09-29 13:03:13 10splitTasks](trainer.py 286): INFO [40/157]	0.1053(0.1193)	0.0002(0.0148)	0.328(0.193)	90.62(93.22)
[2023-09-29 13:03:14 10splitTasks](trainer.py 286): INFO [50/157]	0.1095(0.1168)	0.0003(0.0120)	0.192(0.192)	96.88(93.26)
[2023-09-29 13:03:15 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1146)	0.0003(0.0101)	0.053(0.197)	96.88(93.14)
[2023-09-29 13:03:16 10splitTasks](trainer.py 286): INFO [70/157]	0.1045(0.1130)	0.0003(0.0087)	0.255(0.197)	87.50(93.22)
[2023-09-29 13:03:17 10splitTasks](trainer.py 286): INFO [80/157]	0.1030(0.1118)	0.0004(0.0077)	0.078(0.195)	100.00(93.40)
[2023-09-29 13:03:18 10splitTasks](trainer.py 286): INFO [90/157]	0.1050(0.1109)	0.0006(0.0069)	0.455(0.200)	84.38(93.27)
[2023-09-29 13:03:19 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1103)	0.0003(0.0063)	0.095(0.203)	96.88(93.19)
[2023-09-29 13:03:21 10splitTasks](trainer.py 286): INFO [110/157]	0.1079(0.1098)	0.0002(0.0057)	0.170(0.201)	96.88(93.47)
[2023-09-29 13:03:22 10splitTasks](trainer.py 286): INFO [120/157]	0.1054(0.1093)	0.0002(0.0053)	0.193(0.195)	93.75(93.62)
[2023-09-29 13:03:23 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1088)	0.0002(0.0049)	0.306(0.196)	84.38(93.42)
[2023-09-29 13:03:24 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1084)	0.0003(0.0046)	0.320(0.196)	84.38(93.37)
[2023-09-29 13:03:25 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1082)	0.0001(0.0043)	0.128(0.196)	96.88(93.40)
[2023-09-29 13:03:25 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1077)	0.0001(0.0041)	0.172(0.195)	87.50(93.48)
[2023-09-29 13:03:25 10splitTasks](trainer.py 288): INFO  * Train Acc 93.480
[2023-09-29 13:03:27 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.88
[2023-09-29 13:03:27 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 13:03:27 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 13:03:27 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:03:28 10splitTasks](trainer.py 286): INFO [0/157]	0.6189(0.6189)	0.5105(0.5105)	0.060(0.060)	100.00(100.00)
[2023-09-29 13:03:29 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1512)	0.0004(0.0467)	0.375(0.178)	90.62(95.45)
[2023-09-29 13:03:30 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1285)	0.0003(0.0247)	0.065(0.186)	96.88(94.20)
[2023-09-29 13:03:31 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.1207)	0.0002(0.0168)	0.499(0.193)	87.50(94.15)
[2023-09-29 13:03:32 10splitTasks](trainer.py 286): INFO [40/157]	0.1030(0.1164)	0.0002(0.0128)	0.251(0.195)	90.62(93.67)
[2023-09-29 13:03:33 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1139)	0.0002(0.0104)	0.229(0.192)	93.75(93.69)
[2023-09-29 13:03:34 10splitTasks](trainer.py 286): INFO [60/157]	0.1063(0.1124)	0.0003(0.0087)	0.302(0.199)	90.62(93.29)
[2023-09-29 13:03:35 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1112)	0.0001(0.0075)	0.541(0.203)	84.38(93.05)
[2023-09-29 13:03:36 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1101)	0.0002(0.0066)	0.324(0.201)	93.75(93.13)
[2023-09-29 13:03:37 10splitTasks](trainer.py 286): INFO [90/157]	0.1041(0.1093)	0.0002(0.0060)	0.202(0.200)	93.75(93.13)
[2023-09-29 13:03:38 10splitTasks](trainer.py 286): INFO [100/157]	0.1026(0.1088)	0.0002(0.0054)	0.184(0.200)	96.88(93.19)
[2023-09-29 13:03:39 10splitTasks](trainer.py 286): INFO [110/157]	0.1063(0.1084)	0.0002(0.0049)	0.180(0.200)	87.50(93.19)
[2023-09-29 13:03:40 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1079)	0.0003(0.0046)	0.107(0.196)	96.88(93.34)
[2023-09-29 13:03:41 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1075)	0.0002(0.0042)	0.174(0.191)	93.75(93.46)
[2023-09-29 13:03:42 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1073)	0.0003(0.0040)	0.135(0.191)	90.62(93.42)
[2023-09-29 13:03:43 10splitTasks](trainer.py 286): INFO [150/157]	0.1045(0.1072)	0.0001(0.0037)	0.142(0.188)	93.75(93.50)
[2023-09-29 13:03:44 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1068)	0.0001(0.0036)	0.031(0.187)	100.00(93.58)
[2023-09-29 13:03:44 10splitTasks](trainer.py 288): INFO  * Train Acc 93.580
[2023-09-29 13:03:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.59
[2023-09-29 13:03:46 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 13:03:46 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 13:03:46 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:03:46 10splitTasks](trainer.py 286): INFO [0/157]	0.5977(0.5977)	0.4870(0.4870)	0.249(0.249)	93.75(93.75)
[2023-09-29 13:03:47 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1488)	0.0002(0.0446)	0.234(0.185)	93.75(93.47)
[2023-09-29 13:03:48 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1272)	0.0002(0.0235)	0.203(0.190)	96.88(93.75)
[2023-09-29 13:03:49 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1195)	0.0003(0.0160)	0.228(0.196)	90.62(93.55)
[2023-09-29 13:03:50 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1158)	0.0002(0.0122)	0.123(0.183)	93.75(93.75)
[2023-09-29 13:03:52 10splitTasks](trainer.py 286): INFO [50/157]	0.1075(0.1136)	0.0004(0.0099)	0.131(0.182)	96.88(93.69)
[2023-09-29 13:03:53 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1119)	0.0003(0.0083)	0.097(0.177)	96.88(94.01)
[2023-09-29 13:03:54 10splitTasks](trainer.py 286): INFO [70/157]	0.1025(0.1107)	0.0003(0.0072)	0.180(0.182)	87.50(93.66)
[2023-09-29 13:03:55 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1096)	0.0003(0.0063)	0.049(0.179)	100.00(93.71)
[2023-09-29 13:03:56 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1089)	0.0002(0.0057)	0.141(0.180)	96.88(93.72)
[2023-09-29 13:03:57 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1083)	0.0002(0.0052)	0.293(0.182)	90.62(93.63)
[2023-09-29 13:03:58 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1078)	0.0003(0.0047)	0.160(0.182)	96.88(93.75)
[2023-09-29 13:03:59 10splitTasks](trainer.py 286): INFO [120/157]	0.1050(0.1076)	0.0003(0.0044)	0.147(0.182)	96.88(93.78)
[2023-09-29 13:04:00 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1073)	0.0003(0.0041)	0.308(0.180)	90.62(93.89)
[2023-09-29 13:04:01 10splitTasks](trainer.py 286): INFO [140/157]	0.1048(0.1071)	0.0003(0.0038)	0.232(0.183)	87.50(93.73)
[2023-09-29 13:04:02 10splitTasks](trainer.py 286): INFO [150/157]	0.1062(0.1069)	0.0001(0.0036)	0.133(0.182)	96.88(93.81)
[2023-09-29 13:04:02 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1065)	0.0001(0.0034)	0.012(0.182)	100.00(93.80)
[2023-09-29 13:04:03 10splitTasks](trainer.py 288): INFO  * Train Acc 93.800
[2023-09-29 13:04:04 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.66
[2023-09-29 13:04:04 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 13:04:04 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 13:04:04 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:04:05 10splitTasks](trainer.py 286): INFO [0/157]	0.5858(0.5858)	0.4565(0.4565)	0.359(0.359)	84.38(84.38)
[2023-09-29 13:04:06 10splitTasks](trainer.py 286): INFO [10/157]	0.1218(0.1516)	0.0006(0.0418)	0.239(0.164)	90.62(93.18)
[2023-09-29 13:04:07 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1288)	0.0002(0.0221)	0.199(0.170)	90.62(93.30)
[2023-09-29 13:04:08 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1203)	0.0002(0.0150)	0.190(0.165)	90.62(93.75)
[2023-09-29 13:04:09 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1160)	0.0003(0.0114)	0.210(0.169)	93.75(94.05)
[2023-09-29 13:04:10 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1134)	0.0002(0.0093)	0.232(0.173)	96.88(93.93)
[2023-09-29 13:04:11 10splitTasks](trainer.py 286): INFO [60/157]	0.1031(0.1117)	0.0002(0.0078)	0.055(0.167)	100.00(94.11)
[2023-09-29 13:04:12 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1105)	0.0002(0.0067)	0.284(0.168)	87.50(93.97)
[2023-09-29 13:04:13 10splitTasks](trainer.py 286): INFO [80/157]	0.1047(0.1095)	0.0003(0.0059)	0.142(0.166)	93.75(94.06)
[2023-09-29 13:04:14 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1087)	0.0002(0.0053)	0.225(0.167)	90.62(94.02)
[2023-09-29 13:04:15 10splitTasks](trainer.py 286): INFO [100/157]	0.1040(0.1081)	0.0003(0.0048)	0.097(0.163)	96.88(94.18)
[2023-09-29 13:04:16 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1076)	0.0003(0.0044)	0.146(0.165)	96.88(94.14)
[2023-09-29 13:04:17 10splitTasks](trainer.py 286): INFO [120/157]	0.1049(0.1073)	0.0003(0.0041)	0.178(0.162)	96.88(94.29)
[2023-09-29 13:04:18 10splitTasks](trainer.py 286): INFO [130/157]	0.1084(0.1070)	0.0002(0.0038)	0.251(0.162)	87.50(94.30)
[2023-09-29 13:04:19 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1067)	0.0003(0.0035)	0.234(0.164)	90.62(94.22)
[2023-09-29 13:04:20 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1064)	0.0001(0.0033)	0.176(0.166)	93.75(94.16)
[2023-09-29 13:04:21 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1060)	0.0001(0.0032)	0.003(0.164)	100.00(94.20)
[2023-09-29 13:04:21 10splitTasks](trainer.py 288): INFO  * Train Acc 94.200
[2023-09-29 13:04:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.400, Total time 1.76
[2023-09-29 13:04:23 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 13:04:23 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 13:04:23 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:04:23 10splitTasks](trainer.py 286): INFO [0/157]	0.7053(0.7053)	0.5951(0.5951)	0.153(0.153)	93.75(93.75)
[2023-09-29 13:04:25 10splitTasks](trainer.py 286): INFO [10/157]	0.1089(0.1595)	0.0002(0.0544)	0.070(0.147)	96.88(94.32)
[2023-09-29 13:04:26 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1327)	0.0002(0.0286)	0.064(0.126)	100.00(95.68)
[2023-09-29 13:04:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1032(0.1235)	0.0006(0.0195)	0.155(0.152)	93.75(94.86)
[2023-09-29 13:04:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1046(0.1188)	0.0002(0.0148)	0.185(0.155)	93.75(94.82)
[2023-09-29 13:04:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1047(0.1159)	0.0002(0.0120)	0.161(0.145)	96.88(95.34)
[2023-09-29 13:04:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1139)	0.0002(0.0101)	0.250(0.156)	90.62(95.03)
[2023-09-29 13:04:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1029(0.1125)	0.0005(0.0087)	0.126(0.157)	93.75(94.94)
[2023-09-29 13:04:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1114)	0.0002(0.0077)	0.084(0.163)	96.88(94.71)
[2023-09-29 13:04:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1158(0.1107)	0.0002(0.0069)	0.057(0.163)	100.00(94.64)
[2023-09-29 13:04:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1058(0.1100)	0.0002(0.0062)	0.093(0.160)	96.88(94.71)
[2023-09-29 13:04:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1093)	0.0004(0.0057)	0.240(0.160)	90.62(94.71)
[2023-09-29 13:04:36 10splitTasks](trainer.py 286): INFO [120/157]	0.1054(0.1087)	0.0002(0.0053)	0.108(0.162)	93.75(94.63)
[2023-09-29 13:04:37 10splitTasks](trainer.py 286): INFO [130/157]	0.1032(0.1083)	0.0002(0.0049)	0.224(0.163)	93.75(94.51)
[2023-09-29 13:04:38 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1079)	0.0003(0.0046)	0.098(0.163)	96.88(94.61)
[2023-09-29 13:04:39 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1075)	0.0001(0.0043)	0.217(0.164)	93.75(94.64)
[2023-09-29 13:04:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1072)	0.0001(0.0041)	0.583(0.162)	62.50(94.72)
[2023-09-29 13:04:40 10splitTasks](trainer.py 288): INFO  * Train Acc 94.720
[2023-09-29 13:04:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.800, Total time 1.74
[2023-09-29 13:04:41 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 13:04:41 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 13:04:41 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:04:42 10splitTasks](trainer.py 286): INFO [0/157]	0.6241(0.6241)	0.5146(0.5146)	0.131(0.131)	96.88(96.88)
[2023-09-29 13:04:43 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1527)	0.0002(0.0470)	0.149(0.125)	93.75(95.45)
[2023-09-29 13:04:44 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1292)	0.0002(0.0247)	0.105(0.135)	96.88(94.94)
[2023-09-29 13:04:45 10splitTasks](trainer.py 286): INFO [30/157]	0.1040(0.1217)	0.0002(0.0168)	0.102(0.139)	96.88(95.16)
[2023-09-29 13:04:46 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1172)	0.0002(0.0128)	0.200(0.150)	90.62(94.74)
[2023-09-29 13:04:47 10splitTasks](trainer.py 286): INFO [50/157]	0.1171(0.1148)	0.0002(0.0104)	0.145(0.149)	100.00(94.91)
[2023-09-29 13:04:48 10splitTasks](trainer.py 286): INFO [60/157]	0.1102(0.1134)	0.0002(0.0087)	0.304(0.150)	93.75(94.98)
[2023-09-29 13:04:49 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1120)	0.0002(0.0076)	0.257(0.146)	93.75(95.20)
[2023-09-29 13:04:50 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1109)	0.0001(0.0067)	0.089(0.150)	96.88(95.06)
[2023-09-29 13:04:51 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1102)	0.0003(0.0060)	0.216(0.150)	87.50(94.99)
[2023-09-29 13:04:52 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1094)	0.0002(0.0054)	0.122(0.151)	96.88(94.86)
[2023-09-29 13:04:54 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1088)	0.0002(0.0049)	0.421(0.154)	84.38(94.82)
[2023-09-29 13:04:55 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1083)	0.0002(0.0045)	0.104(0.155)	93.75(94.65)
[2023-09-29 13:04:56 10splitTasks](trainer.py 286): INFO [130/157]	0.1047(0.1078)	0.0002(0.0042)	0.138(0.158)	93.75(94.54)
[2023-09-29 13:04:57 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1074)	0.0002(0.0039)	0.334(0.161)	93.75(94.55)
[2023-09-29 13:04:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1047(0.1072)	0.0001(0.0037)	0.178(0.158)	93.75(94.70)
[2023-09-29 13:04:58 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1068)	0.0001(0.0036)	0.584(0.159)	87.50(94.70)
[2023-09-29 13:04:58 10splitTasks](trainer.py 288): INFO  * Train Acc 94.700
[2023-09-29 13:05:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.79
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-1.pth
=> Save Done
[2023-09-29 13:05:00 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 13:05:02 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.73
[2023-09-29 13:05:02 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 13:05:04 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.76
[2023-09-29 13:05:04 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 13:05:04 10splitTasks](trainer.py 341): INFO done
[2023-09-29 13:05:04 10splitTasks](iBatchLearn.py 155): INFO Acc:81.2; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:05:08 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:05:08 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:05:08 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 1, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-1.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-1.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_1.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:05:08 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-1.pth
[2023-09-29 13:05:08 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:05:11 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:05:11 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:05:11 10splitTasks](my_trainer.py 64): INFO tensor([[0, 0, 2, 2, 0, 0, 0],
        [0, 0, 2, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 2, 2, 0],
        [0, 0, 0, 2, 2, 2, 0]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 13:05:11 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:05:11 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:05:11 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 13:05:16 10splitTasks](iBatchLearn.py 167): INFO test split name:1
--------------------------------Official Evaluation--------------------------------
1 80.55000000000001
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:05:24 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:05:24 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:05:24 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 2, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-1.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-2.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-1.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-2.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:05:25 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-1.pth
[2023-09-29 13:05:25 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:05:28 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:05:28 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:05:28 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:05:28 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:05:28 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 13:05:28 10splitTasks](iBatchLearn.py 92): INFO ====================== 2 =======================
[2023-09-29 13:05:28 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 13:05:28 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 13:05:28 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 13:05:28 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:05:31 10splitTasks](trainer.py 286): INFO [0/157]	3.6292(3.6292)	0.4832(0.4832)	2.332(2.332)	0.00(0.00)
[2023-09-29 13:05:32 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.4237)	0.0002(0.0442)	2.198(2.263)	18.75(14.20)
[2023-09-29 13:05:33 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.2705)	0.0002(0.0233)	1.965(2.180)	28.12(19.94)
[2023-09-29 13:05:34 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.2167)	0.0003(0.0159)	1.375(2.046)	65.62(27.12)
[2023-09-29 13:05:35 10splitTasks](trainer.py 286): INFO [40/157]	0.1025(0.1888)	0.0003(0.0121)	1.470(1.921)	46.88(32.16)
[2023-09-29 13:05:36 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1719)	0.0002(0.0098)	1.524(1.824)	43.75(34.87)
[2023-09-29 13:05:37 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1604)	0.0003(0.0082)	1.501(1.769)	43.75(36.94)
[2023-09-29 13:05:38 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1523)	0.0003(0.0071)	1.233(1.700)	40.62(39.44)
[2023-09-29 13:05:39 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1461)	0.0002(0.0062)	1.115(1.640)	65.62(41.51)
[2023-09-29 13:05:41 10splitTasks](trainer.py 286): INFO [90/157]	0.1046(0.1413)	0.0002(0.0056)	1.652(1.605)	40.62(43.10)
[2023-09-29 13:05:42 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1373)	0.0002(0.0051)	1.254(1.569)	56.25(44.40)
[2023-09-29 13:05:43 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1342)	0.0003(0.0046)	1.189(1.545)	53.12(45.19)
[2023-09-29 13:05:44 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1317)	0.0003(0.0043)	0.631(1.511)	75.00(46.51)
[2023-09-29 13:05:45 10splitTasks](trainer.py 286): INFO [130/157]	0.1053(0.1296)	0.0007(0.0040)	0.827(1.485)	75.00(47.35)
[2023-09-29 13:05:46 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1276)	0.0002(0.0037)	1.236(1.461)	62.50(48.23)
[2023-09-29 13:05:47 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1260)	0.0002(0.0035)	0.936(1.431)	68.75(49.32)
[2023-09-29 13:05:47 10splitTasks](trainer.py 286): INFO [156/157]	0.0801(0.1249)	0.0001(0.0034)	1.129(1.420)	75.00(49.60)
[2023-09-29 13:05:47 10splitTasks](trainer.py 288): INFO  * Train Acc 49.600
[2023-09-29 13:05:49 10splitTasks](my_trainer.py 503): INFO  * Val Acc 62.200, Total time 1.79
[2023-09-29 13:05:49 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 13:05:49 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 13:05:49 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:05:50 10splitTasks](trainer.py 286): INFO [0/157]	0.5853(0.5853)	0.4587(0.4587)	1.025(1.025)	65.62(65.62)
[2023-09-29 13:05:51 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1480)	0.0002(0.0420)	1.807(1.186)	43.75(59.09)
[2023-09-29 13:05:52 10splitTasks](trainer.py 286): INFO [20/157]	0.1060(0.1274)	0.0006(0.0222)	0.729(1.202)	75.00(58.18)
[2023-09-29 13:05:53 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1200)	0.0003(0.0151)	1.207(1.207)	53.12(58.27)
[2023-09-29 13:05:54 10splitTasks](trainer.py 286): INFO [40/157]	0.1169(0.1167)	0.0005(0.0115)	0.892(1.198)	65.62(58.84)
[2023-09-29 13:05:55 10splitTasks](trainer.py 286): INFO [50/157]	0.1037(0.1143)	0.0002(0.0093)	0.796(1.187)	78.12(59.31)
[2023-09-29 13:05:56 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1127)	0.0003(0.0078)	0.891(1.169)	71.88(59.68)
[2023-09-29 13:05:57 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1113)	0.0002(0.0068)	1.113(1.150)	68.75(60.65)
[2023-09-29 13:05:58 10splitTasks](trainer.py 286): INFO [80/157]	0.1102(0.1104)	0.0005(0.0060)	1.303(1.134)	59.38(61.03)
[2023-09-29 13:05:59 10splitTasks](trainer.py 286): INFO [90/157]	0.1048(0.1095)	0.0003(0.0054)	0.776(1.128)	78.12(60.99)
[2023-09-29 13:06:00 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1089)	0.0003(0.0049)	0.764(1.120)	71.88(61.45)
[2023-09-29 13:06:01 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1083)	0.0002(0.0045)	1.028(1.114)	65.62(61.66)
[2023-09-29 13:06:02 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1078)	0.0003(0.0041)	1.250(1.103)	53.12(61.96)
[2023-09-29 13:06:03 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1074)	0.0002(0.0038)	1.153(1.107)	53.12(61.57)
[2023-09-29 13:06:04 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1071)	0.0002(0.0036)	0.858(1.102)	62.50(61.86)
[2023-09-29 13:06:05 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1068)	0.0002(0.0034)	0.995(1.094)	65.62(62.00)
[2023-09-29 13:06:06 10splitTasks](trainer.py 286): INFO [156/157]	0.0849(0.1065)	0.0001(0.0032)	0.697(1.093)	62.50(62.06)
[2023-09-29 13:06:06 10splitTasks](trainer.py 288): INFO  * Train Acc 62.060
[2023-09-29 13:06:08 10splitTasks](my_trainer.py 503): INFO  * Val Acc 65.800, Total time 1.61
[2023-09-29 13:06:08 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 13:06:08 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 13:06:08 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:06:08 10splitTasks](trainer.py 286): INFO [0/157]	0.6198(0.6198)	0.4787(0.4787)	0.901(0.901)	65.62(65.62)
[2023-09-29 13:06:09 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1503)	0.0002(0.0438)	0.787(1.104)	71.88(62.78)
[2023-09-29 13:06:10 10splitTasks](trainer.py 286): INFO [20/157]	0.1050(0.1278)	0.0003(0.0231)	1.098(1.127)	62.50(62.80)
[2023-09-29 13:06:11 10splitTasks](trainer.py 286): INFO [30/157]	0.1038(0.1203)	0.0003(0.0158)	1.151(1.074)	65.62(63.61)
[2023-09-29 13:06:12 10splitTasks](trainer.py 286): INFO [40/157]	0.1027(0.1161)	0.0002(0.0120)	1.047(1.074)	62.50(63.26)
[2023-09-29 13:06:13 10splitTasks](trainer.py 286): INFO [50/157]	0.1060(0.1136)	0.0003(0.0097)	1.332(1.050)	53.12(64.34)
[2023-09-29 13:06:14 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1119)	0.0003(0.0082)	1.243(1.035)	56.25(64.70)
[2023-09-29 13:06:15 10splitTasks](trainer.py 286): INFO [70/157]	0.1045(0.1106)	0.0003(0.0071)	1.283(1.038)	50.00(64.70)
[2023-09-29 13:06:16 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1097)	0.0003(0.0062)	1.380(1.035)	62.50(64.93)
[2023-09-29 13:06:18 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1089)	0.0003(0.0056)	0.789(1.021)	78.12(65.32)
[2023-09-29 13:06:19 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1083)	0.0003(0.0050)	1.130(1.010)	62.50(65.56)
[2023-09-29 13:06:20 10splitTasks](trainer.py 286): INFO [110/157]	0.1032(0.1079)	0.0002(0.0046)	0.973(1.018)	62.50(65.23)
[2023-09-29 13:06:21 10splitTasks](trainer.py 286): INFO [120/157]	0.1052(0.1075)	0.0002(0.0043)	1.045(1.012)	68.75(65.52)
[2023-09-29 13:06:22 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1072)	0.0003(0.0040)	0.822(1.012)	65.62(65.43)
[2023-09-29 13:06:23 10splitTasks](trainer.py 286): INFO [140/157]	0.1025(0.1068)	0.0003(0.0037)	1.064(1.009)	62.50(65.60)
[2023-09-29 13:06:24 10splitTasks](trainer.py 286): INFO [150/157]	0.1006(0.1065)	0.0001(0.0035)	0.893(1.006)	65.62(65.65)
[2023-09-29 13:06:24 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1062)	0.0001(0.0034)	1.325(1.007)	50.00(65.60)
[2023-09-29 13:06:24 10splitTasks](trainer.py 288): INFO  * Train Acc 65.600
[2023-09-29 13:06:26 10splitTasks](my_trainer.py 503): INFO  * Val Acc 63.400, Total time 1.71
[2023-09-29 13:06:26 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 13:06:26 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 13:06:26 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:06:27 10splitTasks](trainer.py 286): INFO [0/157]	0.6052(0.6052)	0.4965(0.4965)	0.714(0.714)	68.75(68.75)
[2023-09-29 13:06:28 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1483)	0.0003(0.0454)	0.865(0.921)	65.62(69.03)
[2023-09-29 13:06:29 10splitTasks](trainer.py 286): INFO [20/157]	0.1029(0.1270)	0.0002(0.0239)	0.954(0.981)	71.88(67.26)
[2023-09-29 13:06:30 10splitTasks](trainer.py 286): INFO [30/157]	0.1028(0.1193)	0.0002(0.0163)	0.894(0.906)	68.75(69.56)
[2023-09-29 13:06:31 10splitTasks](trainer.py 286): INFO [40/157]	0.1049(0.1156)	0.0002(0.0124)	0.614(0.871)	84.38(71.19)
[2023-09-29 13:06:32 10splitTasks](trainer.py 286): INFO [50/157]	0.1043(0.1133)	0.0002(0.0101)	0.791(0.861)	65.62(71.57)
[2023-09-29 13:06:33 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1117)	0.0002(0.0085)	0.916(0.873)	68.75(71.52)
[2023-09-29 13:06:34 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1105)	0.0004(0.0073)	0.762(0.870)	75.00(71.39)
[2023-09-29 13:06:35 10splitTasks](trainer.py 286): INFO [80/157]	0.1010(0.1098)	0.0002(0.0065)	1.060(0.896)	59.38(70.64)
[2023-09-29 13:06:36 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1092)	0.0002(0.0058)	0.759(0.899)	75.00(70.64)
[2023-09-29 13:06:37 10splitTasks](trainer.py 286): INFO [100/157]	0.1041(0.1085)	0.0004(0.0053)	0.833(0.903)	68.75(70.64)
[2023-09-29 13:06:38 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1080)	0.0002(0.0048)	0.618(0.902)	78.12(70.38)
[2023-09-29 13:06:39 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1076)	0.0002(0.0045)	1.136(0.896)	62.50(70.48)
[2023-09-29 13:06:40 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1072)	0.0002(0.0042)	1.015(0.894)	71.88(70.52)
[2023-09-29 13:06:41 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1069)	0.0003(0.0039)	0.818(0.894)	81.25(70.59)
[2023-09-29 13:06:42 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1066)	0.0001(0.0037)	0.966(0.895)	62.50(70.53)
[2023-09-29 13:06:43 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1062)	0.0001(0.0035)	0.864(0.892)	75.00(70.62)
[2023-09-29 13:06:43 10splitTasks](trainer.py 288): INFO  * Train Acc 70.620
[2023-09-29 13:06:45 10splitTasks](my_trainer.py 503): INFO  * Val Acc 66.800, Total time 1.76
[2023-09-29 13:06:45 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 13:06:45 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 13:06:45 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:06:45 10splitTasks](trainer.py 286): INFO [0/157]	0.6470(0.6470)	0.5295(0.5295)	0.560(0.560)	81.25(81.25)
[2023-09-29 13:06:46 10splitTasks](trainer.py 286): INFO [10/157]	0.1052(0.1558)	0.0002(0.0485)	0.795(0.817)	71.88(73.01)
[2023-09-29 13:06:47 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1303)	0.0002(0.0255)	0.616(0.777)	75.00(73.36)
[2023-09-29 13:06:48 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1213)	0.0004(0.0174)	0.643(0.761)	81.25(74.40)
[2023-09-29 13:06:49 10splitTasks](trainer.py 286): INFO [40/157]	0.1046(0.1168)	0.0002(0.0132)	0.782(0.770)	68.75(74.24)
[2023-09-29 13:06:50 10splitTasks](trainer.py 286): INFO [50/157]	0.1053(0.1140)	0.0002(0.0107)	0.578(0.776)	81.25(73.65)
[2023-09-29 13:06:51 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1122)	0.0002(0.0090)	0.806(0.776)	68.75(73.41)
[2023-09-29 13:06:52 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1109)	0.0004(0.0078)	0.774(0.768)	75.00(73.59)
[2023-09-29 13:06:54 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1100)	0.0003(0.0069)	0.637(0.770)	81.25(73.53)
[2023-09-29 13:06:55 10splitTasks](trainer.py 286): INFO [90/157]	0.1058(0.1093)	0.0002(0.0061)	1.234(0.794)	56.25(72.73)
[2023-09-29 13:06:56 10splitTasks](trainer.py 286): INFO [100/157]	0.1171(0.1090)	0.0003(0.0056)	0.746(0.808)	68.75(71.97)
[2023-09-29 13:06:57 10splitTasks](trainer.py 286): INFO [110/157]	0.1029(0.1086)	0.0003(0.0052)	0.909(0.806)	71.88(72.38)
[2023-09-29 13:06:58 10splitTasks](trainer.py 286): INFO [120/157]	0.1045(0.1084)	0.0006(0.0048)	0.692(0.803)	71.88(72.52)
[2023-09-29 13:06:59 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1082)	0.0003(0.0044)	0.861(0.799)	71.88(72.57)
[2023-09-29 13:07:00 10splitTasks](trainer.py 286): INFO [140/157]	0.1115(0.1080)	0.0002(0.0042)	0.778(0.802)	81.25(72.52)
[2023-09-29 13:07:01 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1076)	0.0001(0.0039)	0.790(0.804)	71.88(72.50)
[2023-09-29 13:07:01 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1072)	0.0001(0.0038)	1.821(0.802)	50.00(72.58)
[2023-09-29 13:07:02 10splitTasks](trainer.py 288): INFO  * Train Acc 72.580
[2023-09-29 13:07:03 10splitTasks](my_trainer.py 503): INFO  * Val Acc 70.600, Total time 1.59
[2023-09-29 13:07:03 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 13:07:03 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 13:07:03 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:07:04 10splitTasks](trainer.py 286): INFO [0/157]	0.6170(0.6170)	0.4975(0.4975)	0.516(0.516)	81.25(81.25)
[2023-09-29 13:07:05 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1494)	0.0002(0.0455)	0.688(0.706)	68.75(73.58)
[2023-09-29 13:07:06 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1275)	0.0002(0.0240)	0.883(0.722)	71.88(74.85)
[2023-09-29 13:07:07 10splitTasks](trainer.py 286): INFO [30/157]	0.1049(0.1202)	0.0002(0.0164)	0.669(0.708)	81.25(75.91)
[2023-09-29 13:07:08 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1159)	0.0002(0.0125)	0.515(0.656)	84.38(77.74)
[2023-09-29 13:07:09 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1136)	0.0002(0.0101)	0.776(0.639)	75.00(78.55)
[2023-09-29 13:07:10 10splitTasks](trainer.py 286): INFO [60/157]	0.1054(0.1119)	0.0003(0.0085)	0.501(0.635)	84.38(78.74)
[2023-09-29 13:07:11 10splitTasks](trainer.py 286): INFO [70/157]	0.1029(0.1105)	0.0003(0.0073)	0.535(0.629)	78.12(78.52)
[2023-09-29 13:07:12 10splitTasks](trainer.py 286): INFO [80/157]	0.1039(0.1097)	0.0002(0.0065)	0.931(0.623)	65.62(78.67)
[2023-09-29 13:07:13 10splitTasks](trainer.py 286): INFO [90/157]	0.1032(0.1089)	0.0002(0.0058)	0.677(0.638)	75.00(78.33)
[2023-09-29 13:07:14 10splitTasks](trainer.py 286): INFO [100/157]	0.1035(0.1083)	0.0002(0.0052)	0.575(0.631)	81.25(78.77)
[2023-09-29 13:07:15 10splitTasks](trainer.py 286): INFO [110/157]	0.1025(0.1078)	0.0002(0.0048)	0.853(0.633)	81.25(78.63)
[2023-09-29 13:07:16 10splitTasks](trainer.py 286): INFO [120/157]	0.1118(0.1075)	0.0002(0.0044)	0.965(0.633)	65.62(78.64)
[2023-09-29 13:07:17 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1073)	0.0002(0.0041)	0.655(0.626)	78.12(78.98)
[2023-09-29 13:07:18 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1070)	0.0002(0.0038)	1.073(0.627)	65.62(78.88)
[2023-09-29 13:07:19 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1068)	0.0001(0.0036)	0.819(0.623)	71.88(79.01)
[2023-09-29 13:07:20 10splitTasks](trainer.py 286): INFO [156/157]	0.0828(0.1065)	0.0001(0.0035)	0.240(0.629)	87.50(78.88)
[2023-09-29 13:07:20 10splitTasks](trainer.py 288): INFO  * Train Acc 78.880
[2023-09-29 13:07:22 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.200, Total time 1.67
[2023-09-29 13:07:22 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 13:07:22 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 13:07:22 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:07:22 10splitTasks](trainer.py 286): INFO [0/157]	0.6334(0.6334)	0.5131(0.5131)	0.590(0.590)	81.25(81.25)
[2023-09-29 13:07:23 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1516)	0.0003(0.0470)	0.557(0.559)	81.25(80.40)
[2023-09-29 13:07:24 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1281)	0.0002(0.0247)	0.600(0.534)	78.12(80.06)
[2023-09-29 13:07:25 10splitTasks](trainer.py 286): INFO [30/157]	0.1027(0.1205)	0.0002(0.0169)	0.428(0.533)	84.38(80.85)
[2023-09-29 13:07:26 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1165)	0.0005(0.0128)	0.856(0.544)	75.00(81.17)
[2023-09-29 13:07:28 10splitTasks](trainer.py 286): INFO [50/157]	0.1022(0.1139)	0.0003(0.0104)	0.403(0.538)	84.38(81.31)
[2023-09-29 13:07:29 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1122)	0.0001(0.0087)	0.612(0.521)	71.88(81.71)
[2023-09-29 13:07:30 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1110)	0.0002(0.0075)	0.535(0.516)	78.12(81.73)
[2023-09-29 13:07:31 10splitTasks](trainer.py 286): INFO [80/157]	0.1057(0.1101)	0.0002(0.0067)	0.360(0.511)	84.38(81.71)
[2023-09-29 13:07:32 10splitTasks](trainer.py 286): INFO [90/157]	0.1056(0.1094)	0.0002(0.0060)	0.688(0.516)	87.50(81.66)
[2023-09-29 13:07:33 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1087)	0.0002(0.0054)	0.416(0.518)	93.75(81.65)
[2023-09-29 13:07:34 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1083)	0.0003(0.0049)	0.361(0.511)	84.38(81.90)
[2023-09-29 13:07:35 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1078)	0.0003(0.0045)	0.402(0.511)	87.50(81.95)
[2023-09-29 13:07:36 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1075)	0.0003(0.0042)	0.473(0.510)	81.25(81.97)
[2023-09-29 13:07:37 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1072)	0.0002(0.0040)	0.590(0.514)	87.50(81.80)
[2023-09-29 13:07:38 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1070)	0.0002(0.0037)	0.346(0.512)	84.38(81.77)
[2023-09-29 13:07:38 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1066)	0.0001(0.0036)	0.880(0.516)	87.50(81.72)
[2023-09-29 13:07:39 10splitTasks](trainer.py 288): INFO  * Train Acc 81.720
[2023-09-29 13:07:40 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.200, Total time 1.62
[2023-09-29 13:07:40 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 13:07:40 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 13:07:40 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:07:41 10splitTasks](trainer.py 286): INFO [0/157]	0.5981(0.5981)	0.4640(0.4640)	0.279(0.279)	93.75(93.75)
[2023-09-29 13:07:42 10splitTasks](trainer.py 286): INFO [10/157]	0.1026(0.1490)	0.0002(0.0425)	0.298(0.493)	93.75(84.38)
[2023-09-29 13:07:43 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1275)	0.0003(0.0224)	0.563(0.443)	81.25(85.71)
[2023-09-29 13:07:44 10splitTasks](trainer.py 286): INFO [30/157]	0.1167(0.1201)	0.0002(0.0153)	0.220(0.460)	93.75(85.18)
[2023-09-29 13:07:45 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1157)	0.0002(0.0116)	0.554(0.459)	78.12(84.83)
[2023-09-29 13:07:46 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1132)	0.0003(0.0095)	0.308(0.448)	90.62(85.17)
[2023-09-29 13:07:47 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1115)	0.0003(0.0080)	0.467(0.451)	81.25(84.99)
[2023-09-29 13:07:48 10splitTasks](trainer.py 286): INFO [70/157]	0.1050(0.1104)	0.0002(0.0069)	0.414(0.442)	84.38(85.12)
[2023-09-29 13:07:49 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1096)	0.0003(0.0061)	0.340(0.438)	87.50(85.19)
[2023-09-29 13:07:50 10splitTasks](trainer.py 286): INFO [90/157]	0.1046(0.1089)	0.0003(0.0054)	0.389(0.431)	84.38(85.30)
[2023-09-29 13:07:51 10splitTasks](trainer.py 286): INFO [100/157]	0.1080(0.1084)	0.0004(0.0049)	0.659(0.436)	78.12(85.33)
[2023-09-29 13:07:52 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1080)	0.0003(0.0045)	0.359(0.438)	90.62(85.33)
[2023-09-29 13:07:53 10splitTasks](trainer.py 286): INFO [120/157]	0.1011(0.1075)	0.0001(0.0042)	0.286(0.436)	90.62(85.46)
[2023-09-29 13:07:54 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1071)	0.0003(0.0039)	0.296(0.435)	90.62(85.42)
[2023-09-29 13:07:55 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1068)	0.0003(0.0036)	0.216(0.430)	87.50(85.62)
[2023-09-29 13:07:56 10splitTasks](trainer.py 286): INFO [150/157]	0.1044(0.1065)	0.0002(0.0034)	0.581(0.430)	84.38(85.64)
[2023-09-29 13:07:57 10splitTasks](trainer.py 286): INFO [156/157]	0.0798(0.1062)	0.0001(0.0033)	1.131(0.430)	75.00(85.68)
[2023-09-29 13:07:57 10splitTasks](trainer.py 288): INFO  * Train Acc 85.680
[2023-09-29 13:07:59 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.200, Total time 1.79
[2023-09-29 13:07:59 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 13:07:59 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 13:07:59 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:07:59 10splitTasks](trainer.py 286): INFO [0/157]	0.6191(0.6191)	0.4987(0.4987)	0.312(0.312)	87.50(87.50)
[2023-09-29 13:08:00 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1502)	0.0003(0.0457)	0.133(0.346)	100.00(87.50)
[2023-09-29 13:08:01 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1278)	0.0003(0.0241)	0.353(0.363)	87.50(86.90)
[2023-09-29 13:08:03 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1199)	0.0002(0.0164)	0.190(0.362)	93.75(86.79)
[2023-09-29 13:08:04 10splitTasks](trainer.py 286): INFO [40/157]	0.1244(0.1164)	0.0006(0.0125)	0.177(0.363)	93.75(86.89)
[2023-09-29 13:08:05 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1137)	0.0002(0.0101)	0.396(0.377)	87.50(86.40)
[2023-09-29 13:08:06 10splitTasks](trainer.py 286): INFO [60/157]	0.1032(0.1119)	0.0003(0.0085)	0.354(0.384)	90.62(86.58)
[2023-09-29 13:08:07 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1106)	0.0002(0.0074)	0.718(0.373)	81.25(87.15)
[2023-09-29 13:08:08 10splitTasks](trainer.py 286): INFO [80/157]	0.1046(0.1099)	0.0001(0.0065)	0.214(0.364)	93.75(87.54)
[2023-09-29 13:08:09 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1091)	0.0003(0.0058)	0.233(0.373)	90.62(87.19)
[2023-09-29 13:08:10 10splitTasks](trainer.py 286): INFO [100/157]	0.1080(0.1087)	0.0003(0.0053)	0.356(0.374)	87.50(87.10)
[2023-09-29 13:08:11 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1082)	0.0002(0.0048)	0.366(0.374)	81.25(86.99)
[2023-09-29 13:08:12 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1078)	0.0003(0.0044)	0.442(0.374)	84.38(87.09)
[2023-09-29 13:08:13 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1075)	0.0004(0.0041)	0.252(0.375)	93.75(87.07)
[2023-09-29 13:08:14 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1072)	0.0003(0.0039)	0.419(0.383)	84.38(86.81)
[2023-09-29 13:08:15 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1070)	0.0002(0.0036)	0.555(0.384)	78.12(86.88)
[2023-09-29 13:08:16 10splitTasks](trainer.py 286): INFO [156/157]	0.0806(0.1067)	0.0001(0.0035)	0.225(0.382)	100.00(86.96)
[2023-09-29 13:08:16 10splitTasks](trainer.py 288): INFO  * Train Acc 86.960
[2023-09-29 13:08:17 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.200, Total time 1.74
[2023-09-29 13:08:17 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 13:08:17 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 13:08:17 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:08:18 10splitTasks](trainer.py 286): INFO [0/157]	0.5681(0.5681)	0.4473(0.4473)	0.328(0.328)	90.62(90.62)
[2023-09-29 13:08:19 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1495)	0.0002(0.0414)	0.182(0.349)	96.88(89.77)
[2023-09-29 13:08:20 10splitTasks](trainer.py 286): INFO [20/157]	0.1054(0.1287)	0.0002(0.0218)	0.424(0.345)	81.25(88.24)
[2023-09-29 13:08:21 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1206)	0.0002(0.0149)	0.236(0.330)	93.75(88.51)
[2023-09-29 13:08:22 10splitTasks](trainer.py 286): INFO [40/157]	0.1042(0.1170)	0.0002(0.0114)	0.222(0.329)	90.62(88.72)
[2023-09-29 13:08:23 10splitTasks](trainer.py 286): INFO [50/157]	0.1154(0.1148)	0.0002(0.0092)	0.711(0.348)	78.12(88.48)
[2023-09-29 13:08:24 10splitTasks](trainer.py 286): INFO [60/157]	0.1075(0.1132)	0.0003(0.0078)	0.523(0.349)	87.50(88.47)
[2023-09-29 13:08:25 10splitTasks](trainer.py 286): INFO [70/157]	0.1032(0.1117)	0.0002(0.0067)	0.364(0.341)	93.75(88.78)
[2023-09-29 13:08:26 10splitTasks](trainer.py 286): INFO [80/157]	0.1145(0.1108)	0.0005(0.0059)	0.340(0.348)	87.50(88.73)
[2023-09-29 13:08:27 10splitTasks](trainer.py 286): INFO [90/157]	0.1026(0.1100)	0.0003(0.0053)	0.270(0.348)	90.62(88.60)
[2023-09-29 13:08:28 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1092)	0.0002(0.0048)	0.183(0.343)	93.75(88.68)
[2023-09-29 13:08:29 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1087)	0.0003(0.0044)	0.379(0.342)	87.50(88.71)
[2023-09-29 13:08:30 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1081)	0.0002(0.0041)	0.195(0.337)	96.88(88.84)
[2023-09-29 13:08:32 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1077)	0.0002(0.0038)	0.418(0.340)	84.38(88.69)
[2023-09-29 13:08:33 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1074)	0.0003(0.0035)	0.150(0.339)	96.88(88.67)
[2023-09-29 13:08:34 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1072)	0.0002(0.0033)	0.261(0.337)	93.75(88.76)
[2023-09-29 13:08:34 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1068)	0.0001(0.0032)	0.776(0.337)	87.50(88.78)
[2023-09-29 13:08:34 10splitTasks](trainer.py 288): INFO  * Train Acc 88.780
[2023-09-29 13:08:36 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.61
[2023-09-29 13:08:36 10splitTasks](my_trainer.py 206): INFO Pruning for task2
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 5777/6797 (84.99%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 2516/2960 (85.00%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 22639/26634 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 10061/11837 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 10061/11837 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 10061/11837 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 22639/26634 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 10061/11837 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 10061/11837 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 22639/26634 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 10061/11837 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 20124/23675 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 90556/106537 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 40248/47350 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 80494/94699 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 40248/47350 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 90556/106537 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 40248/47350 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 40248/47350 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 90556/106537 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 40248/47350 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 40248/47350 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 90556/106537 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 40248/47350 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 80494/94699 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 362226/426148 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 321978/378798 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 362226/426148 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 362226/426148 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 362226/426148 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 362226/426148 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 362226/426148 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 160989/189399 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 321978/378798 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 1448903/1704592 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 643957/757596 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 1287913/1515192 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 643957/757596 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 1448903/1704592 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 643957/757596 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 643957/757596 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 1448904/1704593 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 643957/757596 (85.00%) (Total in layer: 1048576)
[2023-09-29 13:08:36 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 13:08:36 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 13:08:36 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 13:08:36 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:08:37 10splitTasks](trainer.py 286): INFO [0/157]	0.8090(0.8090)	0.7026(0.7026)	0.607(0.607)	75.00(75.00)
[2023-09-29 13:08:38 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1693)	0.0003(0.0642)	0.570(0.426)	84.38(86.65)
[2023-09-29 13:08:39 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1379)	0.0002(0.0338)	0.290(0.487)	87.50(83.93)
[2023-09-29 13:08:40 10splitTasks](trainer.py 286): INFO [30/157]	0.1061(0.1270)	0.0002(0.0230)	0.730(0.530)	78.12(82.46)
[2023-09-29 13:08:41 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1215)	0.0002(0.0175)	0.597(0.504)	78.12(82.93)
[2023-09-29 13:08:42 10splitTasks](trainer.py 286): INFO [50/157]	0.1035(0.1179)	0.0003(0.0141)	0.821(0.511)	68.75(82.66)
[2023-09-29 13:08:43 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1155)	0.0002(0.0119)	0.815(0.523)	75.00(82.17)
[2023-09-29 13:08:44 10splitTasks](trainer.py 286): INFO [70/157]	0.1110(0.1138)	0.0002(0.0102)	0.686(0.523)	78.12(82.17)
[2023-09-29 13:08:45 10splitTasks](trainer.py 286): INFO [80/157]	0.1024(0.1126)	0.0002(0.0090)	0.701(0.531)	65.62(81.94)
[2023-09-29 13:08:47 10splitTasks](trainer.py 286): INFO [90/157]	0.1024(0.1117)	0.0002(0.0081)	0.626(0.543)	84.38(81.46)
[2023-09-29 13:08:48 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1109)	0.0003(0.0073)	0.301(0.546)	90.62(81.19)
[2023-09-29 13:08:49 10splitTasks](trainer.py 286): INFO [110/157]	0.1040(0.1103)	0.0003(0.0067)	0.676(0.547)	78.12(81.00)
[2023-09-29 13:08:50 10splitTasks](trainer.py 286): INFO [120/157]	0.1040(0.1097)	0.0003(0.0061)	0.462(0.545)	87.50(81.17)
[2023-09-29 13:08:51 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1094)	0.0003(0.0057)	0.538(0.546)	81.25(81.18)
[2023-09-29 13:08:52 10splitTasks](trainer.py 286): INFO [140/157]	0.1047(0.1089)	0.0002(0.0053)	0.550(0.546)	87.50(81.41)
[2023-09-29 13:08:53 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1086)	0.0001(0.0050)	0.490(0.553)	78.12(81.13)
[2023-09-29 13:08:53 10splitTasks](trainer.py 286): INFO [156/157]	0.0854(0.1083)	0.0001(0.0048)	1.407(0.556)	75.00(81.00)
[2023-09-29 13:08:53 10splitTasks](trainer.py 288): INFO  * Train Acc 81.000
[2023-09-29 13:08:55 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.600, Total time 1.64
[2023-09-29 13:08:55 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 13:08:55 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 13:08:55 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:08:56 10splitTasks](trainer.py 286): INFO [0/157]	0.6046(0.6046)	0.4863(0.4863)	0.460(0.460)	87.50(87.50)
[2023-09-29 13:08:57 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1521)	0.0002(0.0445)	0.741(0.627)	75.00(79.55)
[2023-09-29 13:08:58 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1300)	0.0002(0.0234)	0.449(0.580)	78.12(80.65)
[2023-09-29 13:08:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1213)	0.0002(0.0160)	0.557(0.562)	87.50(80.75)
[2023-09-29 13:09:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1058(0.1169)	0.0002(0.0122)	0.407(0.534)	87.50(82.16)
[2023-09-29 13:09:01 10splitTasks](trainer.py 286): INFO [50/157]	0.1073(0.1145)	0.0003(0.0099)	0.363(0.527)	78.12(82.17)
[2023-09-29 13:09:02 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1126)	0.0002(0.0083)	0.585(0.529)	81.25(82.27)
[2023-09-29 13:09:03 10splitTasks](trainer.py 286): INFO [70/157]	0.1055(0.1113)	0.0002(0.0072)	0.239(0.524)	90.62(82.35)
[2023-09-29 13:09:04 10splitTasks](trainer.py 286): INFO [80/157]	0.1031(0.1102)	0.0003(0.0063)	0.527(0.523)	81.25(82.41)
[2023-09-29 13:09:05 10splitTasks](trainer.py 286): INFO [90/157]	0.1027(0.1094)	0.0002(0.0057)	0.480(0.520)	90.62(82.52)
[2023-09-29 13:09:06 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1092)	0.0002(0.0051)	0.520(0.508)	78.12(82.86)
[2023-09-29 13:09:07 10splitTasks](trainer.py 286): INFO [110/157]	0.1022(0.1086)	0.0003(0.0047)	0.603(0.513)	84.38(82.69)
[2023-09-29 13:09:08 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1082)	0.0003(0.0043)	0.711(0.512)	71.88(82.75)
[2023-09-29 13:09:09 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1078)	0.0002(0.0040)	0.320(0.502)	84.38(83.11)
[2023-09-29 13:09:10 10splitTasks](trainer.py 286): INFO [140/157]	0.1160(0.1076)	0.0003(0.0038)	0.540(0.511)	75.00(82.78)
[2023-09-29 13:09:11 10splitTasks](trainer.py 286): INFO [150/157]	0.1035(0.1073)	0.0001(0.0035)	0.546(0.508)	75.00(82.86)
[2023-09-29 13:09:12 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1070)	0.0001(0.0034)	2.173(0.510)	50.00(82.70)
[2023-09-29 13:09:12 10splitTasks](trainer.py 288): INFO  * Train Acc 82.700
[2023-09-29 13:09:14 10splitTasks](my_trainer.py 503): INFO  * Val Acc 43.400, Total time 1.65
[2023-09-29 13:09:14 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 13:09:14 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 13:09:14 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:09:14 10splitTasks](trainer.py 286): INFO [0/157]	0.7603(0.7603)	0.6449(0.6449)	0.799(0.799)	71.88(71.88)
[2023-09-29 13:09:15 10splitTasks](trainer.py 286): INFO [10/157]	0.1025(0.1628)	0.0002(0.0589)	1.017(0.997)	65.62(67.90)
[2023-09-29 13:09:16 10splitTasks](trainer.py 286): INFO [20/157]	0.1027(0.1344)	0.0003(0.0310)	1.131(1.075)	59.38(65.62)
[2023-09-29 13:09:17 10splitTasks](trainer.py 286): INFO [30/157]	0.1022(0.1241)	0.0003(0.0211)	1.393(1.069)	50.00(65.32)
[2023-09-29 13:09:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1030(0.1188)	0.0002(0.0160)	0.663(1.043)	81.25(66.39)
[2023-09-29 13:09:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1024(0.1156)	0.0004(0.0129)	0.898(0.989)	78.12(68.08)
[2023-09-29 13:09:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1134)	0.0002(0.0109)	0.717(0.951)	71.88(69.11)
[2023-09-29 13:09:22 10splitTasks](trainer.py 286): INFO [70/157]	0.1040(0.1119)	0.0002(0.0094)	0.902(0.930)	68.75(69.54)
[2023-09-29 13:09:23 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1107)	0.0002(0.0083)	0.821(0.886)	75.00(71.03)
[2023-09-29 13:09:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1061(0.1100)	0.0006(0.0074)	0.651(0.875)	81.25(71.15)
[2023-09-29 13:09:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1092)	0.0001(0.0067)	0.612(0.848)	75.00(72.00)
[2023-09-29 13:09:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1086)	0.0002(0.0061)	1.050(0.836)	62.50(72.21)
[2023-09-29 13:09:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1081)	0.0002(0.0056)	0.713(0.816)	78.12(72.93)
[2023-09-29 13:09:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1077)	0.0002(0.0052)	0.531(0.798)	78.12(73.47)
[2023-09-29 13:09:29 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1074)	0.0002(0.0049)	0.499(0.789)	81.25(73.80)
[2023-09-29 13:09:30 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1070)	0.0001(0.0046)	0.519(0.774)	87.50(74.38)
[2023-09-29 13:09:30 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1067)	0.0001(0.0044)	0.534(0.769)	62.50(74.58)
[2023-09-29 13:09:30 10splitTasks](trainer.py 288): INFO  * Train Acc 74.580
[2023-09-29 13:09:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.800, Total time 1.66
[2023-09-29 13:09:32 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 13:09:32 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 13:09:32 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:09:33 10splitTasks](trainer.py 286): INFO [0/157]	0.5879(0.5879)	0.4563(0.4563)	0.603(0.603)	84.38(84.38)
[2023-09-29 13:09:34 10splitTasks](trainer.py 286): INFO [10/157]	0.1054(0.1491)	0.0003(0.0421)	0.489(0.499)	90.62(86.36)
[2023-09-29 13:09:35 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1272)	0.0003(0.0222)	0.550(0.537)	81.25(84.23)
[2023-09-29 13:09:36 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1192)	0.0002(0.0151)	0.498(0.551)	81.25(83.67)
[2023-09-29 13:09:37 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1152)	0.0003(0.0115)	0.615(0.560)	68.75(82.24)
[2023-09-29 13:09:38 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1127)	0.0002(0.0093)	0.713(0.577)	78.12(81.31)
[2023-09-29 13:09:39 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1119)	0.0002(0.0078)	1.142(0.580)	62.50(80.94)
[2023-09-29 13:09:40 10splitTasks](trainer.py 286): INFO [70/157]	0.1039(0.1106)	0.0002(0.0068)	0.645(0.586)	75.00(80.90)
[2023-09-29 13:09:41 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1100)	0.0003(0.0060)	0.417(0.587)	78.12(80.48)
[2023-09-29 13:09:42 10splitTasks](trainer.py 286): INFO [90/157]	0.1225(0.1096)	0.0006(0.0054)	0.262(0.579)	90.62(80.77)
[2023-09-29 13:09:43 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1090)	0.0002(0.0049)	0.385(0.575)	90.62(80.75)
[2023-09-29 13:09:44 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1085)	0.0002(0.0045)	0.743(0.569)	75.00(80.94)
[2023-09-29 13:09:45 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1080)	0.0002(0.0041)	0.565(0.565)	75.00(80.84)
[2023-09-29 13:09:46 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1076)	0.0003(0.0038)	0.376(0.561)	87.50(80.89)
[2023-09-29 13:09:47 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1072)	0.0002(0.0036)	0.255(0.556)	90.62(81.25)
[2023-09-29 13:09:48 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1070)	0.0001(0.0034)	0.706(0.557)	75.00(81.02)
[2023-09-29 13:09:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1066)	0.0001(0.0032)	0.733(0.558)	75.00(81.04)
[2023-09-29 13:09:49 10splitTasks](trainer.py 288): INFO  * Train Acc 81.040
[2023-09-29 13:09:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.400, Total time 1.58
[2023-09-29 13:09:51 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 13:09:51 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 13:09:51 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:09:51 10splitTasks](trainer.py 286): INFO [0/157]	0.6538(0.6538)	0.5366(0.5366)	0.793(0.793)	75.00(75.00)
[2023-09-29 13:09:52 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1548)	0.0002(0.0491)	0.483(0.512)	84.38(81.82)
[2023-09-29 13:09:53 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1303)	0.0003(0.0259)	0.306(0.487)	84.38(83.48)
[2023-09-29 13:09:54 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1219)	0.0004(0.0176)	0.527(0.474)	78.12(83.67)
[2023-09-29 13:09:55 10splitTasks](trainer.py 286): INFO [40/157]	0.1023(0.1175)	0.0004(0.0135)	0.525(0.475)	84.38(83.54)
[2023-09-29 13:09:56 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1145)	0.0003(0.0109)	0.689(0.477)	81.25(83.52)
[2023-09-29 13:09:57 10splitTasks](trainer.py 286): INFO [60/157]	0.1054(0.1126)	0.0004(0.0092)	0.467(0.485)	87.50(83.04)
[2023-09-29 13:09:58 10splitTasks](trainer.py 286): INFO [70/157]	0.1031(0.1112)	0.0002(0.0079)	0.344(0.468)	87.50(83.85)
[2023-09-29 13:10:00 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1102)	0.0003(0.0070)	0.660(0.470)	78.12(83.83)
[2023-09-29 13:10:01 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1094)	0.0003(0.0062)	0.636(0.472)	81.25(83.79)
[2023-09-29 13:10:02 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1087)	0.0003(0.0057)	0.374(0.470)	84.38(83.79)
[2023-09-29 13:10:03 10splitTasks](trainer.py 286): INFO [110/157]	0.1045(0.1083)	0.0003(0.0052)	0.608(0.471)	78.12(83.92)
[2023-09-29 13:10:04 10splitTasks](trainer.py 286): INFO [120/157]	0.1026(0.1079)	0.0003(0.0048)	0.345(0.470)	87.50(83.88)
[2023-09-29 13:10:05 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1075)	0.0002(0.0044)	0.488(0.467)	87.50(83.92)
[2023-09-29 13:10:06 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1071)	0.0004(0.0041)	0.564(0.469)	81.25(83.87)
[2023-09-29 13:10:07 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1069)	0.0001(0.0039)	0.604(0.475)	81.25(83.71)
[2023-09-29 13:10:07 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1065)	0.0001(0.0037)	1.099(0.471)	62.50(83.86)
[2023-09-29 13:10:07 10splitTasks](trainer.py 288): INFO  * Train Acc 83.860
[2023-09-29 13:10:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.600, Total time 1.69
[2023-09-29 13:10:09 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 13:10:09 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 13:10:09 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:10:10 10splitTasks](trainer.py 286): INFO [0/157]	0.6906(0.6906)	0.5847(0.5847)	0.264(0.264)	90.62(90.62)
[2023-09-29 13:10:11 10splitTasks](trainer.py 286): INFO [10/157]	0.1062(0.1579)	0.0002(0.0534)	0.484(0.486)	87.50(82.10)
[2023-09-29 13:10:12 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1319)	0.0002(0.0281)	0.565(0.479)	84.38(83.18)
[2023-09-29 13:10:13 10splitTasks](trainer.py 286): INFO [30/157]	0.1114(0.1234)	0.0007(0.0195)	0.346(0.456)	87.50(84.58)
[2023-09-29 13:10:14 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1184)	0.0003(0.0148)	0.689(0.460)	84.38(84.38)
[2023-09-29 13:10:15 10splitTasks](trainer.py 286): INFO [50/157]	0.1034(0.1154)	0.0003(0.0120)	0.307(0.466)	93.75(84.74)
[2023-09-29 13:10:16 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1132)	0.0003(0.0101)	0.452(0.442)	87.50(85.30)
[2023-09-29 13:10:17 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1119)	0.0003(0.0087)	0.403(0.452)	78.12(84.46)
[2023-09-29 13:10:18 10splitTasks](trainer.py 286): INFO [80/157]	0.1022(0.1107)	0.0003(0.0076)	0.305(0.444)	90.62(84.61)
[2023-09-29 13:10:19 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1098)	0.0002(0.0068)	0.302(0.441)	90.62(84.82)
[2023-09-29 13:10:20 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1094)	0.0002(0.0062)	0.489(0.443)	78.12(84.72)
[2023-09-29 13:10:21 10splitTasks](trainer.py 286): INFO [110/157]	0.1033(0.1090)	0.0003(0.0057)	0.739(0.445)	71.88(84.63)
[2023-09-29 13:10:22 10splitTasks](trainer.py 286): INFO [120/157]	0.1076(0.1085)	0.0002(0.0052)	0.329(0.444)	90.62(84.71)
[2023-09-29 13:10:23 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1082)	0.0002(0.0049)	0.506(0.445)	78.12(84.66)
[2023-09-29 13:10:24 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1079)	0.0002(0.0045)	0.586(0.447)	75.00(84.57)
[2023-09-29 13:10:25 10splitTasks](trainer.py 286): INFO [150/157]	0.1060(0.1076)	0.0001(0.0043)	0.340(0.445)	90.62(84.81)
[2023-09-29 13:10:26 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1072)	0.0001(0.0041)	0.837(0.448)	87.50(84.74)
[2023-09-29 13:10:26 10splitTasks](trainer.py 288): INFO  * Train Acc 84.740
[2023-09-29 13:10:28 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.800, Total time 1.70
[2023-09-29 13:10:28 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 13:10:28 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 13:10:28 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:10:28 10splitTasks](trainer.py 286): INFO [0/157]	0.6271(0.6271)	0.5076(0.5076)	0.324(0.324)	93.75(93.75)
[2023-09-29 13:10:29 10splitTasks](trainer.py 286): INFO [10/157]	0.1034(0.1527)	0.0003(0.0465)	0.200(0.370)	90.62(86.65)
[2023-09-29 13:10:31 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1295)	0.0003(0.0245)	0.492(0.364)	78.12(87.35)
[2023-09-29 13:10:32 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1209)	0.0002(0.0167)	0.368(0.378)	84.38(87.20)
[2023-09-29 13:10:33 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1164)	0.0003(0.0127)	0.422(0.384)	81.25(86.36)
[2023-09-29 13:10:34 10splitTasks](trainer.py 286): INFO [50/157]	0.1125(0.1143)	0.0003(0.0103)	0.169(0.379)	96.88(86.83)
[2023-09-29 13:10:35 10splitTasks](trainer.py 286): INFO [60/157]	0.1039(0.1125)	0.0003(0.0086)	0.471(0.389)	81.25(86.53)
[2023-09-29 13:10:36 10splitTasks](trainer.py 286): INFO [70/157]	0.1055(0.1112)	0.0003(0.0075)	0.307(0.386)	87.50(86.84)
[2023-09-29 13:10:37 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1101)	0.0003(0.0066)	0.191(0.386)	90.62(86.81)
[2023-09-29 13:10:38 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1093)	0.0003(0.0059)	0.360(0.382)	81.25(86.98)
[2023-09-29 13:10:39 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1086)	0.0002(0.0053)	0.259(0.381)	87.50(87.00)
[2023-09-29 13:10:40 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1080)	0.0003(0.0049)	0.358(0.383)	84.38(86.94)
[2023-09-29 13:10:41 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1076)	0.0003(0.0045)	0.296(0.384)	93.75(86.91)
[2023-09-29 13:10:42 10splitTasks](trainer.py 286): INFO [130/157]	0.1024(0.1074)	0.0003(0.0042)	0.413(0.387)	81.25(86.76)
[2023-09-29 13:10:43 10splitTasks](trainer.py 286): INFO [140/157]	0.1075(0.1071)	0.0009(0.0039)	0.375(0.387)	90.62(86.79)
[2023-09-29 13:10:44 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1069)	0.0003(0.0037)	0.288(0.390)	90.62(86.78)
[2023-09-29 13:10:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0804(0.1065)	0.0001(0.0035)	0.249(0.388)	87.50(86.76)
[2023-09-29 13:10:45 10splitTasks](trainer.py 288): INFO  * Train Acc 86.760
[2023-09-29 13:10:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 75.400, Total time 1.69
[2023-09-29 13:10:46 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 13:10:46 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 13:10:46 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:10:47 10splitTasks](trainer.py 286): INFO [0/157]	0.6300(0.6300)	0.5158(0.5158)	0.214(0.214)	90.62(90.62)
[2023-09-29 13:10:48 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1516)	0.0002(0.0472)	0.209(0.283)	93.75(90.06)
[2023-09-29 13:10:49 10splitTasks](trainer.py 286): INFO [20/157]	0.1062(0.1285)	0.0002(0.0249)	0.275(0.327)	87.50(88.84)
[2023-09-29 13:10:50 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1206)	0.0002(0.0170)	0.464(0.347)	84.38(88.00)
[2023-09-29 13:10:51 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1162)	0.0002(0.0129)	0.408(0.352)	87.50(87.58)
[2023-09-29 13:10:52 10splitTasks](trainer.py 286): INFO [50/157]	0.1150(0.1138)	0.0006(0.0105)	0.247(0.356)	93.75(87.25)
[2023-09-29 13:10:53 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1121)	0.0003(0.0089)	0.383(0.361)	87.50(87.40)
[2023-09-29 13:10:54 10splitTasks](trainer.py 286): INFO [70/157]	0.1033(0.1109)	0.0003(0.0077)	0.228(0.361)	90.62(87.46)
[2023-09-29 13:10:55 10splitTasks](trainer.py 286): INFO [80/157]	0.1025(0.1099)	0.0003(0.0067)	0.322(0.364)	84.38(87.31)
[2023-09-29 13:10:56 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1091)	0.0002(0.0060)	0.213(0.367)	90.62(87.23)
[2023-09-29 13:10:57 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1084)	0.0003(0.0055)	0.289(0.364)	87.50(87.31)
[2023-09-29 13:10:58 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1078)	0.0003(0.0050)	0.211(0.372)	93.75(86.94)
[2023-09-29 13:10:59 10splitTasks](trainer.py 286): INFO [120/157]	0.1033(0.1075)	0.0002(0.0046)	0.597(0.378)	78.12(86.57)
[2023-09-29 13:11:00 10splitTasks](trainer.py 286): INFO [130/157]	0.1046(0.1071)	0.0003(0.0043)	0.308(0.381)	87.50(86.50)
[2023-09-29 13:11:01 10splitTasks](trainer.py 286): INFO [140/157]	0.1055(0.1069)	0.0002(0.0040)	0.401(0.378)	87.50(86.59)
[2023-09-29 13:11:02 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1066)	0.0001(0.0038)	0.237(0.373)	90.62(86.78)
[2023-09-29 13:11:03 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1062)	0.0001(0.0036)	1.234(0.380)	75.00(86.62)
[2023-09-29 13:11:03 10splitTasks](trainer.py 288): INFO  * Train Acc 86.620
[2023-09-29 13:11:05 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.000, Total time 1.69
[2023-09-29 13:11:05 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 13:11:05 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 13:11:05 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:11:06 10splitTasks](trainer.py 286): INFO [0/157]	0.7256(0.7256)	0.6190(0.6190)	0.417(0.417)	84.38(84.38)
[2023-09-29 13:11:07 10splitTasks](trainer.py 286): INFO [10/157]	0.1040(0.1616)	0.0003(0.0565)	0.579(0.357)	75.00(87.78)
[2023-09-29 13:11:08 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1342)	0.0003(0.0298)	0.338(0.380)	87.50(87.35)
[2023-09-29 13:11:09 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1243)	0.0002(0.0203)	0.138(0.368)	96.88(87.90)
[2023-09-29 13:11:10 10splitTasks](trainer.py 286): INFO [40/157]	0.1045(0.1193)	0.0002(0.0154)	0.276(0.362)	90.62(87.80)
[2023-09-29 13:11:11 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1161)	0.0002(0.0125)	0.531(0.346)	81.25(88.30)
[2023-09-29 13:11:12 10splitTasks](trainer.py 286): INFO [60/157]	0.1036(0.1139)	0.0003(0.0105)	0.232(0.335)	90.62(88.47)
[2023-09-29 13:11:13 10splitTasks](trainer.py 286): INFO [70/157]	0.1058(0.1124)	0.0003(0.0090)	0.335(0.331)	84.38(88.64)
[2023-09-29 13:11:14 10splitTasks](trainer.py 286): INFO [80/157]	0.1058(0.1112)	0.0003(0.0080)	0.216(0.324)	90.62(88.93)
[2023-09-29 13:11:15 10splitTasks](trainer.py 286): INFO [90/157]	0.1058(0.1103)	0.0004(0.0071)	0.358(0.332)	87.50(88.74)
[2023-09-29 13:11:16 10splitTasks](trainer.py 286): INFO [100/157]	0.1052(0.1097)	0.0002(0.0065)	0.099(0.324)	100.00(89.05)
[2023-09-29 13:11:17 10splitTasks](trainer.py 286): INFO [110/157]	0.1047(0.1091)	0.0002(0.0059)	0.399(0.330)	90.62(88.85)
[2023-09-29 13:11:18 10splitTasks](trainer.py 286): INFO [120/157]	0.1026(0.1087)	0.0003(0.0054)	0.178(0.335)	90.62(88.71)
[2023-09-29 13:11:19 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1084)	0.0002(0.0051)	0.170(0.331)	96.88(88.84)
[2023-09-29 13:11:20 10splitTasks](trainer.py 286): INFO [140/157]	0.1030(0.1080)	0.0003(0.0047)	0.464(0.334)	87.50(88.76)
[2023-09-29 13:11:21 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1076)	0.0001(0.0044)	0.188(0.331)	96.88(88.78)
[2023-09-29 13:11:22 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1072)	0.0001(0.0043)	0.445(0.329)	87.50(88.84)
[2023-09-29 13:11:22 10splitTasks](trainer.py 288): INFO  * Train Acc 88.840
[2023-09-29 13:11:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.600, Total time 1.66
[2023-09-29 13:11:23 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 13:11:23 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 13:11:23 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:11:24 10splitTasks](trainer.py 286): INFO [0/157]	0.6540(0.6540)	0.5341(0.5341)	0.407(0.407)	93.75(93.75)
[2023-09-29 13:11:25 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1546)	0.0002(0.0488)	0.254(0.327)	90.62(87.78)
[2023-09-29 13:11:26 10splitTasks](trainer.py 286): INFO [20/157]	0.1027(0.1302)	0.0003(0.0257)	0.189(0.338)	96.88(87.95)
[2023-09-29 13:11:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.1216)	0.0002(0.0175)	0.252(0.321)	93.75(88.10)
[2023-09-29 13:11:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1027(0.1170)	0.0002(0.0133)	0.786(0.331)	81.25(88.41)
[2023-09-29 13:11:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1141)	0.0002(0.0108)	0.259(0.324)	93.75(88.91)
[2023-09-29 13:11:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1122)	0.0002(0.0090)	0.622(0.319)	81.25(89.45)
[2023-09-29 13:11:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1098(0.1109)	0.0003(0.0078)	0.238(0.310)	90.62(89.70)
[2023-09-29 13:11:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1040(0.1100)	0.0003(0.0069)	0.434(0.311)	87.50(89.70)
[2023-09-29 13:11:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1092)	0.0003(0.0062)	0.147(0.299)	96.88(89.87)
[2023-09-29 13:11:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1086)	0.0002(0.0056)	0.300(0.301)	90.62(89.88)
[2023-09-29 13:11:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1057(0.1082)	0.0002(0.0051)	0.179(0.303)	93.75(89.61)
[2023-09-29 13:11:37 10splitTasks](trainer.py 286): INFO [120/157]	0.1057(0.1079)	0.0002(0.0047)	0.189(0.306)	90.62(89.49)
[2023-09-29 13:11:38 10splitTasks](trainer.py 286): INFO [130/157]	0.1042(0.1077)	0.0002(0.0044)	0.292(0.303)	90.62(89.60)
[2023-09-29 13:11:39 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1074)	0.0002(0.0041)	0.178(0.303)	93.75(89.54)
[2023-09-29 13:11:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1071)	0.0001(0.0038)	0.258(0.299)	93.75(89.65)
[2023-09-29 13:11:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0806(0.1067)	0.0001(0.0037)	1.554(0.302)	50.00(89.56)
[2023-09-29 13:11:40 10splitTasks](trainer.py 288): INFO  * Train Acc 89.560
[2023-09-29 13:11:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.400, Total time 1.67
[2023-09-29 13:11:42 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 13:11:42 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 13:11:42 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:11:43 10splitTasks](trainer.py 286): INFO [0/157]	0.5853(0.5853)	0.4683(0.4683)	0.627(0.627)	84.38(84.38)
[2023-09-29 13:11:44 10splitTasks](trainer.py 286): INFO [10/157]	0.1044(0.1482)	0.0002(0.0429)	0.817(0.365)	71.88(88.35)
[2023-09-29 13:11:45 10splitTasks](trainer.py 286): INFO [20/157]	0.1044(0.1266)	0.0003(0.0226)	0.216(0.294)	93.75(90.33)
[2023-09-29 13:11:46 10splitTasks](trainer.py 286): INFO [30/157]	0.1045(0.1193)	0.0002(0.0154)	0.136(0.282)	93.75(90.22)
[2023-09-29 13:11:47 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1151)	0.0002(0.0117)	0.192(0.281)	93.75(90.02)
[2023-09-29 13:11:48 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1126)	0.0002(0.0095)	0.244(0.281)	87.50(90.13)
[2023-09-29 13:11:49 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1110)	0.0003(0.0080)	0.653(0.288)	78.12(89.91)
[2023-09-29 13:11:50 10splitTasks](trainer.py 286): INFO [70/157]	0.1054(0.1099)	0.0002(0.0069)	0.243(0.287)	90.62(89.92)
[2023-09-29 13:11:51 10splitTasks](trainer.py 286): INFO [80/157]	0.1023(0.1090)	0.0003(0.0061)	0.095(0.285)	96.88(89.66)
[2023-09-29 13:11:52 10splitTasks](trainer.py 286): INFO [90/157]	0.1024(0.1083)	0.0002(0.0054)	0.315(0.295)	87.50(89.32)
[2023-09-29 13:11:53 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1079)	0.0003(0.0050)	0.414(0.293)	87.50(89.36)
[2023-09-29 13:11:54 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1074)	0.0002(0.0045)	0.160(0.300)	93.75(89.30)
[2023-09-29 13:11:55 10splitTasks](trainer.py 286): INFO [120/157]	0.1060(0.1071)	0.0002(0.0042)	0.366(0.297)	87.50(89.39)
[2023-09-29 13:11:56 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1068)	0.0002(0.0039)	0.309(0.296)	93.75(89.55)
[2023-09-29 13:11:57 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1066)	0.0003(0.0036)	0.373(0.294)	93.75(89.74)
[2023-09-29 13:11:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1064)	0.0001(0.0034)	0.276(0.293)	87.50(89.80)
[2023-09-29 13:11:59 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1061)	0.0001(0.0033)	0.459(0.296)	87.50(89.76)
[2023-09-29 13:11:59 10splitTasks](trainer.py 288): INFO  * Train Acc 89.760
[2023-09-29 13:12:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.600, Total time 1.68
[2023-09-29 13:12:00 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 13:12:00 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 13:12:00 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:12:01 10splitTasks](trainer.py 286): INFO [0/157]	0.5945(0.5945)	0.4896(0.4896)	0.180(0.180)	93.75(93.75)
[2023-09-29 13:12:02 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1483)	0.0002(0.0448)	0.235(0.276)	90.62(89.49)
[2023-09-29 13:12:03 10splitTasks](trainer.py 286): INFO [20/157]	0.1033(0.1266)	0.0002(0.0236)	0.551(0.269)	84.38(90.03)
[2023-09-29 13:12:04 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1190)	0.0003(0.0161)	0.363(0.253)	84.38(90.52)
[2023-09-29 13:12:05 10splitTasks](trainer.py 286): INFO [40/157]	0.1043(0.1157)	0.0002(0.0122)	0.166(0.244)	96.88(91.16)
[2023-09-29 13:12:06 10splitTasks](trainer.py 286): INFO [50/157]	0.1052(0.1134)	0.0002(0.0099)	0.365(0.241)	90.62(91.36)
[2023-09-29 13:12:07 10splitTasks](trainer.py 286): INFO [60/157]	0.1045(0.1118)	0.0003(0.0084)	0.342(0.235)	87.50(91.75)
[2023-09-29 13:12:08 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1107)	0.0002(0.0072)	0.210(0.234)	93.75(91.86)
[2023-09-29 13:12:09 10splitTasks](trainer.py 286): INFO [80/157]	0.1031(0.1097)	0.0003(0.0064)	0.345(0.236)	87.50(91.78)
[2023-09-29 13:12:10 10splitTasks](trainer.py 286): INFO [90/157]	0.1028(0.1090)	0.0002(0.0057)	0.183(0.231)	96.88(92.07)
[2023-09-29 13:12:11 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1084)	0.0002(0.0052)	0.170(0.229)	96.88(92.08)
[2023-09-29 13:12:12 10splitTasks](trainer.py 286): INFO [110/157]	0.1062(0.1080)	0.0003(0.0047)	0.128(0.224)	100.00(92.29)
[2023-09-29 13:12:13 10splitTasks](trainer.py 286): INFO [120/157]	0.1034(0.1075)	0.0003(0.0044)	0.197(0.229)	90.62(92.10)
[2023-09-29 13:12:14 10splitTasks](trainer.py 286): INFO [130/157]	0.1036(0.1073)	0.0003(0.0041)	0.315(0.239)	84.38(91.65)
[2023-09-29 13:12:16 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1072)	0.0002(0.0038)	0.111(0.238)	96.88(91.76)
[2023-09-29 13:12:17 10splitTasks](trainer.py 286): INFO [150/157]	0.1047(0.1069)	0.0002(0.0036)	0.304(0.241)	93.75(91.64)
[2023-09-29 13:12:17 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1066)	0.0001(0.0034)	0.136(0.243)	87.50(91.58)
[2023-09-29 13:12:17 10splitTasks](trainer.py 288): INFO  * Train Acc 91.580
[2023-09-29 13:12:19 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.200, Total time 1.69
[2023-09-29 13:12:19 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 13:12:19 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 13:12:19 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:12:20 10splitTasks](trainer.py 286): INFO [0/157]	0.6230(0.6230)	0.5174(0.5174)	0.230(0.230)	90.62(90.62)
[2023-09-29 13:12:21 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1529)	0.0002(0.0473)	0.213(0.195)	90.62(93.47)
[2023-09-29 13:12:22 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1295)	0.0002(0.0250)	0.130(0.204)	93.75(93.30)
[2023-09-29 13:12:23 10splitTasks](trainer.py 286): INFO [30/157]	0.1032(0.1218)	0.0002(0.0170)	0.411(0.223)	84.38(91.73)
[2023-09-29 13:12:24 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1176)	0.0003(0.0130)	0.185(0.231)	93.75(91.62)
[2023-09-29 13:12:25 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1148)	0.0002(0.0105)	0.105(0.222)	96.88(92.10)
[2023-09-29 13:12:26 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1136)	0.0003(0.0088)	0.119(0.222)	93.75(92.26)
[2023-09-29 13:12:27 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1123)	0.0003(0.0076)	0.340(0.216)	90.62(92.61)
[2023-09-29 13:12:28 10splitTasks](trainer.py 286): INFO [80/157]	0.1119(0.1113)	0.0002(0.0067)	0.381(0.222)	90.62(92.40)
[2023-09-29 13:12:29 10splitTasks](trainer.py 286): INFO [90/157]	0.1035(0.1106)	0.0002(0.0060)	0.371(0.227)	84.38(92.24)
[2023-09-29 13:12:30 10splitTasks](trainer.py 286): INFO [100/157]	0.1051(0.1099)	0.0002(0.0054)	0.242(0.227)	93.75(92.20)
[2023-09-29 13:12:31 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1092)	0.0003(0.0050)	0.343(0.223)	84.38(92.26)
[2023-09-29 13:12:32 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1087)	0.0002(0.0046)	0.286(0.222)	90.62(92.30)
[2023-09-29 13:12:33 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1083)	0.0002(0.0043)	0.186(0.220)	93.75(92.34)
[2023-09-29 13:12:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1027(0.1079)	0.0002(0.0040)	0.160(0.221)	93.75(92.13)
[2023-09-29 13:12:35 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1076)	0.0002(0.0037)	0.215(0.219)	93.75(92.22)
[2023-09-29 13:12:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1073)	0.0001(0.0036)	0.208(0.218)	87.50(92.24)
[2023-09-29 13:12:36 10splitTasks](trainer.py 288): INFO  * Train Acc 92.240
[2023-09-29 13:12:38 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.800, Total time 1.72
[2023-09-29 13:12:38 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 13:12:38 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 13:12:38 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:12:38 10splitTasks](trainer.py 286): INFO [0/157]	0.5916(0.5916)	0.4805(0.4805)	0.103(0.103)	96.88(96.88)
[2023-09-29 13:12:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1486)	0.0002(0.0439)	0.389(0.218)	87.50(94.03)
[2023-09-29 13:12:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1035(0.1266)	0.0003(0.0231)	0.225(0.190)	90.62(94.20)
[2023-09-29 13:12:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1188)	0.0002(0.0158)	0.281(0.197)	90.62(93.65)
[2023-09-29 13:12:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1152)	0.0003(0.0120)	0.286(0.197)	84.38(93.37)
[2023-09-29 13:12:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1035(0.1130)	0.0003(0.0097)	0.166(0.190)	96.88(93.69)
[2023-09-29 13:12:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1115)	0.0002(0.0082)	0.141(0.194)	90.62(93.34)
[2023-09-29 13:12:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1104)	0.0003(0.0071)	0.152(0.191)	96.88(93.44)
[2023-09-29 13:12:47 10splitTasks](trainer.py 286): INFO [80/157]	0.1054(0.1098)	0.0002(0.0062)	0.064(0.190)	100.00(93.29)
[2023-09-29 13:12:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1154(0.1091)	0.0005(0.0056)	0.234(0.187)	90.62(93.34)
[2023-09-29 13:12:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1087)	0.0002(0.0051)	0.175(0.184)	96.88(93.50)
[2023-09-29 13:12:50 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1082)	0.0002(0.0046)	0.231(0.186)	96.88(93.41)
[2023-09-29 13:12:51 10splitTasks](trainer.py 286): INFO [120/157]	0.1025(0.1077)	0.0003(0.0043)	0.064(0.192)	100.00(92.98)
[2023-09-29 13:12:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1074)	0.0003(0.0040)	0.261(0.192)	90.62(92.82)
[2023-09-29 13:12:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1071)	0.0002(0.0037)	0.143(0.193)	93.75(92.86)
[2023-09-29 13:12:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1067)	0.0001(0.0035)	0.262(0.194)	87.50(92.88)
[2023-09-29 13:12:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1064)	0.0001(0.0034)	0.924(0.195)	75.00(92.86)
[2023-09-29 13:12:54 10splitTasks](trainer.py 288): INFO  * Train Acc 92.860
[2023-09-29 13:12:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.200, Total time 1.66
[2023-09-29 13:12:56 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 13:12:56 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 13:12:56 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:12:57 10splitTasks](trainer.py 286): INFO [0/157]	0.6199(0.6199)	0.5096(0.5096)	0.100(0.100)	100.00(100.00)
[2023-09-29 13:12:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1503)	0.0002(0.0466)	0.188(0.152)	93.75(94.32)
[2023-09-29 13:12:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1038(0.1281)	0.0002(0.0246)	0.034(0.166)	100.00(94.05)
[2023-09-29 13:13:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1119(0.1202)	0.0002(0.0167)	0.170(0.164)	93.75(94.25)
[2023-09-29 13:13:01 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1159)	0.0002(0.0127)	0.279(0.168)	93.75(94.44)
[2023-09-29 13:13:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1022(0.1138)	0.0002(0.0103)	0.220(0.174)	96.88(94.24)
[2023-09-29 13:13:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1028(0.1121)	0.0002(0.0087)	0.131(0.174)	100.00(94.11)
[2023-09-29 13:13:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1060(0.1110)	0.0002(0.0075)	0.165(0.171)	93.75(94.23)
[2023-09-29 13:13:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1099)	0.0003(0.0066)	0.051(0.179)	100.00(93.90)
[2023-09-29 13:13:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1037(0.1092)	0.0004(0.0059)	0.150(0.176)	93.75(93.92)
[2023-09-29 13:13:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1063(0.1086)	0.0002(0.0054)	0.274(0.182)	87.50(93.66)
[2023-09-29 13:13:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1073(0.1082)	0.0004(0.0049)	0.100(0.182)	96.88(93.72)
[2023-09-29 13:13:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1086(0.1080)	0.0002(0.0045)	0.072(0.183)	96.88(93.62)
[2023-09-29 13:13:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1077)	0.0003(0.0042)	0.049(0.178)	100.00(93.92)
[2023-09-29 13:13:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1074)	0.0002(0.0039)	0.101(0.176)	96.88(93.99)
[2023-09-29 13:13:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1071)	0.0001(0.0037)	0.226(0.177)	84.38(94.04)
[2023-09-29 13:13:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1067)	0.0001(0.0036)	0.058(0.177)	100.00(94.10)
[2023-09-29 13:13:13 10splitTasks](trainer.py 288): INFO  * Train Acc 94.100
[2023-09-29 13:13:15 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.400, Total time 1.81
[2023-09-29 13:13:15 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 13:13:15 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 13:13:15 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:13:15 10splitTasks](trainer.py 286): INFO [0/157]	0.6025(0.6025)	0.4953(0.4953)	0.167(0.167)	96.88(96.88)
[2023-09-29 13:13:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1046(0.1493)	0.0002(0.0453)	0.143(0.199)	93.75(94.03)
[2023-09-29 13:13:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1025(0.1280)	0.0003(0.0239)	0.161(0.176)	96.88(94.79)
[2023-09-29 13:13:19 10splitTasks](trainer.py 286): INFO [30/157]	0.1024(0.1199)	0.0003(0.0164)	0.088(0.182)	100.00(94.66)
[2023-09-29 13:13:20 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1158)	0.0002(0.0125)	0.219(0.178)	93.75(94.66)
[2023-09-29 13:13:21 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1133)	0.0002(0.0101)	0.282(0.173)	87.50(94.61)
[2023-09-29 13:13:22 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1117)	0.0003(0.0085)	0.424(0.181)	90.62(94.26)
[2023-09-29 13:13:23 10splitTasks](trainer.py 286): INFO [70/157]	0.1040(0.1104)	0.0002(0.0073)	0.065(0.177)	100.00(94.54)
[2023-09-29 13:13:24 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1093)	0.0003(0.0065)	0.303(0.175)	87.50(94.48)
[2023-09-29 13:13:25 10splitTasks](trainer.py 286): INFO [90/157]	0.1033(0.1086)	0.0003(0.0058)	0.118(0.177)	93.75(94.30)
[2023-09-29 13:13:26 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1080)	0.0003(0.0052)	0.254(0.177)	90.62(94.25)
[2023-09-29 13:13:27 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1075)	0.0002(0.0048)	0.106(0.176)	96.88(94.28)
[2023-09-29 13:13:28 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1073)	0.0004(0.0044)	0.094(0.173)	96.88(94.34)
[2023-09-29 13:13:29 10splitTasks](trainer.py 286): INFO [130/157]	0.1098(0.1070)	0.0004(0.0041)	0.270(0.168)	87.50(94.47)
[2023-09-29 13:13:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1067)	0.0002(0.0039)	0.143(0.169)	93.75(94.39)
[2023-09-29 13:13:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1044(0.1064)	0.0001(0.0036)	0.231(0.168)	96.88(94.43)
[2023-09-29 13:13:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0797(0.1061)	0.0001(0.0035)	0.009(0.166)	100.00(94.54)
[2023-09-29 13:13:32 10splitTasks](trainer.py 288): INFO  * Train Acc 94.540
[2023-09-29 13:13:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.600, Total time 1.66
[2023-09-29 13:13:33 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 13:13:33 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 13:13:33 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:13:34 10splitTasks](trainer.py 286): INFO [0/157]	0.6304(0.6304)	0.5185(0.5185)	0.167(0.167)	93.75(93.75)
[2023-09-29 13:13:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1077(0.1532)	0.0002(0.0475)	0.352(0.167)	87.50(94.32)
[2023-09-29 13:13:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1026(0.1296)	0.0005(0.0250)	0.060(0.157)	100.00(94.49)
[2023-09-29 13:13:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1040(0.1212)	0.0004(0.0171)	0.092(0.153)	100.00(94.86)
[2023-09-29 13:13:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1122(0.1170)	0.0003(0.0130)	0.089(0.162)	93.75(94.44)
[2023-09-29 13:13:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1143)	0.0002(0.0105)	0.118(0.155)	93.75(94.73)
[2023-09-29 13:13:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1026(0.1124)	0.0002(0.0088)	0.102(0.149)	96.88(94.77)
[2023-09-29 13:13:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1112)	0.0002(0.0076)	0.137(0.143)	96.88(95.07)
[2023-09-29 13:13:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1033(0.1102)	0.0002(0.0067)	0.217(0.147)	93.75(94.83)
[2023-09-29 13:13:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1053(0.1095)	0.0036(0.0061)	0.178(0.148)	93.75(94.88)
[2023-09-29 13:13:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1088)	0.0002(0.0055)	0.257(0.148)	87.50(94.89)
[2023-09-29 13:13:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1040(0.1083)	0.0003(0.0050)	0.118(0.150)	96.88(94.82)
[2023-09-29 13:13:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1078)	0.0002(0.0046)	0.034(0.153)	100.00(94.78)
[2023-09-29 13:13:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1075)	0.0002(0.0043)	0.115(0.149)	96.88(94.97)
[2023-09-29 13:13:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1072)	0.0003(0.0040)	0.121(0.151)	96.88(94.95)
[2023-09-29 13:13:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1068)	0.0001(0.0038)	0.035(0.149)	100.00(94.99)
[2023-09-29 13:13:50 10splitTasks](trainer.py 286): INFO [156/157]	0.0804(0.1065)	0.0001(0.0036)	0.057(0.151)	100.00(94.96)
[2023-09-29 13:13:50 10splitTasks](trainer.py 288): INFO  * Train Acc 94.960
[2023-09-29 13:13:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.600, Total time 1.62
[2023-09-29 13:13:52 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 13:13:52 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 13:13:52 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:13:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6677(0.6677)	0.5601(0.5601)	0.079(0.079)	96.88(96.88)
[2023-09-29 13:13:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1551)	0.0002(0.0512)	0.092(0.124)	100.00(95.45)
[2023-09-29 13:13:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1029(0.1309)	0.0003(0.0270)	0.036(0.148)	100.00(94.79)
[2023-09-29 13:13:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1216)	0.0003(0.0184)	0.185(0.139)	90.62(95.16)
[2023-09-29 13:13:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1168)	0.0002(0.0139)	0.089(0.129)	96.88(95.43)
[2023-09-29 13:13:57 10splitTasks](trainer.py 286): INFO [50/157]	0.1045(0.1144)	0.0004(0.0113)	0.166(0.139)	93.75(95.34)
[2023-09-29 13:13:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1042(0.1127)	0.0002(0.0095)	0.171(0.143)	96.88(95.24)
[2023-09-29 13:14:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1113)	0.0003(0.0082)	0.185(0.140)	96.88(95.47)
[2023-09-29 13:14:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1104)	0.0002(0.0072)	0.283(0.139)	90.62(95.45)
[2023-09-29 13:14:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1096)	0.0003(0.0065)	0.068(0.138)	100.00(95.57)
[2023-09-29 13:14:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1032(0.1089)	0.0003(0.0058)	0.090(0.141)	96.88(95.30)
[2023-09-29 13:14:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1041(0.1083)	0.0002(0.0053)	0.083(0.147)	100.00(95.19)
[2023-09-29 13:14:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1078)	0.0002(0.0049)	0.167(0.146)	90.62(95.17)
[2023-09-29 13:14:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1074)	0.0002(0.0046)	0.258(0.150)	90.62(95.06)
[2023-09-29 13:14:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1070(0.1071)	0.0003(0.0043)	0.161(0.150)	93.75(95.04)
[2023-09-29 13:14:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1044(0.1068)	0.0001(0.0040)	0.118(0.148)	93.75(95.05)
[2023-09-29 13:14:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1064)	0.0001(0.0039)	0.008(0.149)	100.00(95.06)
[2023-09-29 13:14:08 10splitTasks](trainer.py 288): INFO  * Train Acc 95.060
[2023-09-29 13:14:10 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.600, Total time 1.62
[2023-09-29 13:14:10 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 13:14:10 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 13:14:10 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:14:11 10splitTasks](trainer.py 286): INFO [0/157]	0.6282(0.6282)	0.5235(0.5235)	0.067(0.067)	100.00(100.00)
[2023-09-29 13:14:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1596)	0.0002(0.0574)	0.137(0.131)	93.75(96.31)
[2023-09-29 13:14:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1327)	0.0003(0.0302)	0.165(0.119)	90.62(96.73)
[2023-09-29 13:14:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1228)	0.0002(0.0206)	0.047(0.123)	100.00(96.57)
[2023-09-29 13:14:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1025(0.1177)	0.0003(0.0156)	0.309(0.128)	90.62(96.65)
[2023-09-29 13:14:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1147)	0.0003(0.0126)	0.207(0.135)	93.75(96.14)
[2023-09-29 13:14:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1127)	0.0003(0.0106)	0.162(0.136)	96.88(96.16)
[2023-09-29 13:14:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1113)	0.0002(0.0092)	0.040(0.138)	100.00(96.08)
[2023-09-29 13:14:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1103)	0.0002(0.0081)	0.264(0.141)	90.62(95.83)
[2023-09-29 13:14:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1051(0.1095)	0.0002(0.0072)	0.294(0.139)	90.62(95.78)
[2023-09-29 13:14:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1030(0.1090)	0.0002(0.0065)	0.263(0.141)	90.62(95.67)
[2023-09-29 13:14:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1074(0.1084)	0.0003(0.0060)	0.211(0.140)	90.62(95.69)
[2023-09-29 13:14:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1025(0.1080)	0.0004(0.0055)	0.308(0.144)	90.62(95.56)
[2023-09-29 13:14:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1077)	0.0002(0.0051)	0.130(0.147)	93.75(95.30)
[2023-09-29 13:14:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1074)	0.0003(0.0048)	0.158(0.143)	93.75(95.43)
[2023-09-29 13:14:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1071)	0.0001(0.0045)	0.119(0.144)	96.88(95.30)
[2023-09-29 13:14:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1067)	0.0001(0.0043)	0.119(0.145)	87.50(95.24)
[2023-09-29 13:14:27 10splitTasks](trainer.py 288): INFO  * Train Acc 95.240
[2023-09-29 13:14:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.200, Total time 1.61
[2023-09-29 13:14:29 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 13:14:29 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 13:14:29 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:14:29 10splitTasks](trainer.py 286): INFO [0/157]	0.6051(0.6051)	0.4903(0.4903)	0.104(0.104)	96.88(96.88)
[2023-09-29 13:14:30 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1505)	0.0002(0.0448)	0.106(0.165)	93.75(94.60)
[2023-09-29 13:14:31 10splitTasks](trainer.py 286): INFO [20/157]	0.1165(0.1289)	0.0002(0.0236)	0.321(0.171)	90.62(94.64)
[2023-09-29 13:14:32 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1208)	0.0003(0.0161)	0.157(0.165)	96.88(94.86)
[2023-09-29 13:14:33 10splitTasks](trainer.py 286): INFO [40/157]	0.1212(0.1172)	0.0003(0.0123)	0.178(0.152)	90.62(95.12)
[2023-09-29 13:14:34 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1145)	0.0002(0.0099)	0.100(0.146)	96.88(95.40)
[2023-09-29 13:14:35 10splitTasks](trainer.py 286): INFO [60/157]	0.1063(0.1128)	0.0002(0.0083)	0.391(0.157)	84.38(95.03)
[2023-09-29 13:14:36 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1114)	0.0004(0.0072)	0.194(0.152)	90.62(95.25)
[2023-09-29 13:14:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1088(0.1104)	0.0004(0.0064)	0.120(0.158)	93.75(94.95)
[2023-09-29 13:14:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1167(0.1098)	0.0007(0.0057)	0.107(0.161)	96.88(94.68)
[2023-09-29 13:14:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1094)	0.0003(0.0052)	0.042(0.155)	100.00(94.89)
[2023-09-29 13:14:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1059(0.1089)	0.0002(0.0047)	0.244(0.152)	87.50(94.96)
[2023-09-29 13:14:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1049(0.1083)	0.0003(0.0044)	0.134(0.154)	93.75(94.91)
[2023-09-29 13:14:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1046(0.1079)	0.0003(0.0040)	0.193(0.150)	90.62(95.04)
[2023-09-29 13:14:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1076)	0.0002(0.0038)	0.137(0.150)	96.88(95.06)
[2023-09-29 13:14:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1073)	0.0001(0.0036)	0.191(0.149)	90.62(95.18)
[2023-09-29 13:14:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1069)	0.0001(0.0034)	0.027(0.148)	100.00(95.14)
[2023-09-29 13:14:45 10splitTasks](trainer.py 288): INFO  * Train Acc 95.140
[2023-09-29 13:14:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.63
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-2.pth
=> Save Done
[2023-09-29 13:14:47 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 13:14:49 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.81
[2023-09-29 13:14:49 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 13:14:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.84
[2023-09-29 13:14:51 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 13:14:53 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.83
[2023-09-29 13:14:53 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 13:14:53 10splitTasks](trainer.py 341): INFO done
[2023-09-29 13:14:53 10splitTasks](iBatchLearn.py 155): INFO Acc:80.8; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:14:57 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:14:57 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:14:57 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 2, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-2.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-2.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_2.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:14:57 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-2.pth
[2023-09-29 13:14:57 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:15:00 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:15:00 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:15:00 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 0, 0, 0],
        [3, 3, 2, 0, 0, 0, 0],
        [0, 3, 3, 0, 0, 0, 0],
        [0, 3, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 2, 2, 0],
        [0, 3, 0, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 13:15:00 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:15:00 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:15:00 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 13:15:05 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 13:15:08 10splitTasks](iBatchLearn.py 167): INFO test split name:2
--------------------------------Official Evaluation--------------------------------
2 80.9
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:15:17 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:15:17 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:15:17 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 3, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-2.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-3.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-2.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-3.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:15:18 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-2.pth
[2023-09-29 13:15:18 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:15:20 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:15:20 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:15:20 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:15:20 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:15:20 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 13:15:20 10splitTasks](iBatchLearn.py 92): INFO ====================== 3 =======================
[2023-09-29 13:15:20 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 13:15:20 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 13:15:20 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 13:15:20 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:15:24 10splitTasks](trainer.py 286): INFO [0/157]	3.5853(3.5853)	0.5466(0.5466)	2.284(2.284)	15.62(15.62)
[2023-09-29 13:15:25 10splitTasks](trainer.py 286): INFO [10/157]	0.1028(0.4192)	0.0002(0.0499)	2.166(2.242)	18.75(18.75)
[2023-09-29 13:15:26 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.2685)	0.0003(0.0263)	2.014(2.146)	34.38(23.66)
[2023-09-29 13:15:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.2149)	0.0002(0.0179)	1.512(1.995)	46.88(30.54)
[2023-09-29 13:15:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1881)	0.0003(0.0136)	1.435(1.867)	56.25(34.76)
[2023-09-29 13:15:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1714)	0.0003(0.0110)	1.263(1.758)	56.25(38.36)
[2023-09-29 13:15:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1040(0.1607)	0.0002(0.0094)	1.400(1.695)	56.25(40.32)
[2023-09-29 13:15:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1048(0.1529)	0.0008(0.0081)	1.317(1.636)	50.00(42.08)
[2023-09-29 13:15:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1466)	0.0002(0.0071)	1.276(1.596)	53.12(43.52)
[2023-09-29 13:15:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1177(0.1419)	0.0003(0.0064)	1.427(1.552)	50.00(45.12)
[2023-09-29 13:15:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1379)	0.0003(0.0058)	1.033(1.528)	65.62(46.44)
[2023-09-29 13:15:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1347)	0.0002(0.0053)	1.445(1.491)	59.38(48.03)
[2023-09-29 13:15:36 10splitTasks](trainer.py 286): INFO [120/157]	0.1012(0.1321)	0.0002(0.0049)	0.872(1.453)	68.75(49.43)
[2023-09-29 13:15:37 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1300)	0.0002(0.0045)	0.978(1.422)	68.75(50.69)
[2023-09-29 13:15:38 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1282)	0.0003(0.0042)	0.570(1.397)	84.38(51.64)
[2023-09-29 13:15:39 10splitTasks](trainer.py 286): INFO [150/157]	0.1040(0.1265)	0.0001(0.0040)	0.692(1.367)	75.00(52.63)
[2023-09-29 13:15:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0824(0.1254)	0.0001(0.0038)	1.295(1.357)	50.00(53.00)
[2023-09-29 13:15:40 10splitTasks](trainer.py 288): INFO  * Train Acc 53.000
[2023-09-29 13:15:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 63.000, Total time 1.71
[2023-09-29 13:15:42 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 13:15:42 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 13:15:42 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:15:42 10splitTasks](trainer.py 286): INFO [0/157]	0.5959(0.5959)	0.4899(0.4899)	0.845(0.845)	71.88(71.88)
[2023-09-29 13:15:43 10splitTasks](trainer.py 286): INFO [10/157]	0.1056(0.1516)	0.0006(0.0449)	1.484(0.979)	50.00(69.03)
[2023-09-29 13:15:44 10splitTasks](trainer.py 286): INFO [20/157]	0.1081(0.1289)	0.0003(0.0237)	0.883(0.968)	68.75(68.75)
[2023-09-29 13:15:45 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1204)	0.0003(0.0162)	0.791(0.968)	71.88(67.74)
[2023-09-29 13:15:46 10splitTasks](trainer.py 286): INFO [40/157]	0.1025(0.1167)	0.0003(0.0123)	0.847(1.025)	71.88(66.08)
[2023-09-29 13:15:47 10splitTasks](trainer.py 286): INFO [50/157]	0.1105(0.1142)	0.0005(0.0100)	1.011(1.033)	65.62(65.26)
[2023-09-29 13:15:49 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1125)	0.0001(0.0084)	0.807(1.038)	81.25(64.96)
[2023-09-29 13:15:50 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1113)	0.0002(0.0073)	1.273(1.039)	62.50(64.70)
[2023-09-29 13:15:51 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1104)	0.0002(0.0064)	0.851(1.021)	81.25(65.47)
[2023-09-29 13:15:52 10splitTasks](trainer.py 286): INFO [90/157]	0.1055(0.1099)	0.0003(0.0057)	0.815(1.014)	81.25(65.93)
[2023-09-29 13:15:53 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1093)	0.0003(0.0052)	0.742(1.023)	81.25(65.72)
[2023-09-29 13:15:54 10splitTasks](trainer.py 286): INFO [110/157]	0.1178(0.1089)	0.0005(0.0048)	0.934(1.016)	62.50(65.82)
[2023-09-29 13:15:55 10splitTasks](trainer.py 286): INFO [120/157]	0.1047(0.1085)	0.0002(0.0044)	0.821(1.009)	71.88(65.91)
[2023-09-29 13:15:56 10splitTasks](trainer.py 286): INFO [130/157]	0.1025(0.1082)	0.0003(0.0041)	1.261(1.002)	59.38(66.17)
[2023-09-29 13:15:57 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1078)	0.0003(0.0038)	1.318(1.004)	65.62(66.18)
[2023-09-29 13:15:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1075)	0.0001(0.0036)	1.125(1.008)	59.38(65.87)
[2023-09-29 13:15:58 10splitTasks](trainer.py 286): INFO [156/157]	0.0839(0.1071)	0.0001(0.0035)	0.577(1.006)	87.50(65.86)
[2023-09-29 13:15:59 10splitTasks](trainer.py 288): INFO  * Train Acc 65.860
[2023-09-29 13:16:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 64.400, Total time 1.68
[2023-09-29 13:16:00 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 13:16:00 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 13:16:00 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:16:01 10splitTasks](trainer.py 286): INFO [0/157]	0.6653(0.6653)	0.5564(0.5564)	0.916(0.916)	71.88(71.88)
[2023-09-29 13:16:02 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1557)	0.0001(0.0510)	1.353(1.119)	56.25(61.65)
[2023-09-29 13:16:03 10splitTasks](trainer.py 286): INFO [20/157]	0.1022(0.1322)	0.0003(0.0286)	1.036(0.997)	62.50(65.77)
[2023-09-29 13:16:04 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1225)	0.0003(0.0194)	0.690(0.954)	81.25(68.35)
[2023-09-29 13:16:05 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1179)	0.0003(0.0148)	0.904(0.945)	62.50(68.60)
[2023-09-29 13:16:06 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1148)	0.0002(0.0119)	1.006(0.945)	59.38(69.06)
[2023-09-29 13:16:07 10splitTasks](trainer.py 286): INFO [60/157]	0.1049(0.1132)	0.0002(0.0100)	0.710(0.923)	75.00(69.72)
[2023-09-29 13:16:08 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1117)	0.0003(0.0087)	1.028(0.924)	71.88(69.54)
[2023-09-29 13:16:09 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1107)	0.0003(0.0076)	1.164(0.948)	68.75(68.98)
[2023-09-29 13:16:10 10splitTasks](trainer.py 286): INFO [90/157]	0.1033(0.1099)	0.0003(0.0068)	1.348(0.954)	62.50(69.02)
[2023-09-29 13:16:11 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1091)	0.0003(0.0062)	0.929(0.958)	71.88(68.78)
[2023-09-29 13:16:12 10splitTasks](trainer.py 286): INFO [110/157]	0.1050(0.1086)	0.0003(0.0057)	0.722(0.953)	81.25(68.64)
[2023-09-29 13:16:13 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1082)	0.0002(0.0052)	1.181(0.952)	62.50(68.54)
[2023-09-29 13:16:14 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1078)	0.0002(0.0048)	1.216(0.942)	50.00(68.80)
[2023-09-29 13:16:15 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1075)	0.0002(0.0045)	1.251(0.935)	71.88(69.26)
[2023-09-29 13:16:16 10splitTasks](trainer.py 286): INFO [150/157]	0.1005(0.1072)	0.0001(0.0043)	0.780(0.942)	71.88(68.94)
[2023-09-29 13:16:17 10splitTasks](trainer.py 286): INFO [156/157]	0.0778(0.1068)	0.0001(0.0041)	0.951(0.945)	50.00(68.68)
[2023-09-29 13:16:17 10splitTasks](trainer.py 288): INFO  * Train Acc 68.680
[2023-09-29 13:16:19 10splitTasks](my_trainer.py 503): INFO  * Val Acc 65.800, Total time 1.62
[2023-09-29 13:16:19 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 13:16:19 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 13:16:19 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:16:19 10splitTasks](trainer.py 286): INFO [0/157]	0.5843(0.5843)	0.4594(0.4594)	0.892(0.892)	71.88(71.88)
[2023-09-29 13:16:21 10splitTasks](trainer.py 286): INFO [10/157]	0.1053(0.1583)	0.0002(0.0527)	1.054(1.015)	75.00(67.33)
[2023-09-29 13:16:22 10splitTasks](trainer.py 286): INFO [20/157]	0.1024(0.1320)	0.0002(0.0277)	0.758(0.996)	71.88(67.26)
[2023-09-29 13:16:23 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1233)	0.0004(0.0189)	1.030(0.930)	59.38(69.46)
[2023-09-29 13:16:24 10splitTasks](trainer.py 286): INFO [40/157]	0.1007(0.1182)	0.0002(0.0144)	0.847(0.911)	68.75(69.97)
[2023-09-29 13:16:25 10splitTasks](trainer.py 286): INFO [50/157]	0.1040(0.1150)	0.0003(0.0116)	0.654(0.890)	65.62(70.65)
[2023-09-29 13:16:26 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1132)	0.0003(0.0098)	0.569(0.869)	81.25(71.11)
[2023-09-29 13:16:27 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1117)	0.0004(0.0084)	0.850(0.860)	71.88(71.52)
[2023-09-29 13:16:28 10splitTasks](trainer.py 286): INFO [80/157]	0.1010(0.1109)	0.0002(0.0074)	0.660(0.854)	81.25(71.53)
[2023-09-29 13:16:29 10splitTasks](trainer.py 286): INFO [90/157]	0.1026(0.1100)	0.0003(0.0067)	1.036(0.848)	65.62(71.74)
[2023-09-29 13:16:30 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1096)	0.0003(0.0060)	1.011(0.844)	65.62(71.84)
[2023-09-29 13:16:31 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1092)	0.0003(0.0055)	0.691(0.832)	75.00(72.24)
[2023-09-29 13:16:32 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1086)	0.0002(0.0051)	0.798(0.827)	68.75(72.44)
[2023-09-29 13:16:33 10splitTasks](trainer.py 286): INFO [130/157]	0.1108(0.1085)	0.0005(0.0047)	0.697(0.823)	78.12(72.38)
[2023-09-29 13:16:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1158(0.1082)	0.0005(0.0044)	0.707(0.823)	84.38(72.45)
[2023-09-29 13:16:35 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1080)	0.0001(0.0042)	0.734(0.822)	78.12(72.39)
[2023-09-29 13:16:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1075)	0.0001(0.0040)	1.550(0.816)	62.50(72.54)
[2023-09-29 13:16:36 10splitTasks](trainer.py 288): INFO  * Train Acc 72.540
[2023-09-29 13:16:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.800, Total time 1.64
[2023-09-29 13:16:37 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 13:16:37 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 13:16:37 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:16:38 10splitTasks](trainer.py 286): INFO [0/157]	0.6427(0.6427)	0.5326(0.5326)	0.530(0.530)	81.25(81.25)
[2023-09-29 13:16:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1657)	0.0002(0.0501)	0.630(0.618)	78.12(78.98)
[2023-09-29 13:16:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1353)	0.0003(0.0264)	0.717(0.668)	71.88(76.79)
[2023-09-29 13:16:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1043(0.1247)	0.0003(0.0180)	0.504(0.713)	81.25(75.91)
[2023-09-29 13:16:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1197)	0.0002(0.0136)	0.744(0.681)	75.00(76.22)
[2023-09-29 13:16:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1167)	0.0002(0.0110)	0.518(0.690)	81.25(76.16)
[2023-09-29 13:16:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1145)	0.0003(0.0093)	0.706(0.700)	65.62(75.77)
[2023-09-29 13:16:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1062(0.1129)	0.0002(0.0080)	0.940(0.699)	71.88(75.88)
[2023-09-29 13:16:46 10splitTasks](trainer.py 286): INFO [80/157]	0.1049(0.1119)	0.0003(0.0071)	0.610(0.712)	68.75(75.46)
[2023-09-29 13:16:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1110)	0.0001(0.0063)	0.660(0.721)	75.00(75.21)
[2023-09-29 13:16:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1102)	0.0003(0.0057)	0.708(0.715)	71.88(75.25)
[2023-09-29 13:16:50 10splitTasks](trainer.py 286): INFO [110/157]	0.1026(0.1096)	0.0003(0.0052)	0.354(0.705)	90.62(75.42)
[2023-09-29 13:16:51 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1090)	0.0003(0.0048)	0.470(0.704)	84.38(75.41)
[2023-09-29 13:16:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1035(0.1086)	0.0002(0.0045)	0.706(0.703)	71.88(75.55)
[2023-09-29 13:16:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1083)	0.0002(0.0042)	0.409(0.699)	87.50(75.69)
[2023-09-29 13:16:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1061(0.1080)	0.0002(0.0040)	0.705(0.698)	75.00(75.85)
[2023-09-29 13:16:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1076)	0.0001(0.0038)	1.533(0.697)	75.00(76.12)
[2023-09-29 13:16:54 10splitTasks](trainer.py 288): INFO  * Train Acc 76.120
[2023-09-29 13:16:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.400, Total time 1.63
[2023-09-29 13:16:56 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 13:16:56 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 13:16:56 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:16:57 10splitTasks](trainer.py 286): INFO [0/157]	0.5962(0.5962)	0.4741(0.4741)	0.750(0.750)	78.12(78.12)
[2023-09-29 13:16:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1487)	0.0002(0.0434)	0.395(0.671)	84.38(77.27)
[2023-09-29 13:16:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1275)	0.0003(0.0229)	0.511(0.613)	81.25(79.76)
[2023-09-29 13:17:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1024(0.1199)	0.0002(0.0156)	0.164(0.604)	90.62(80.14)
[2023-09-29 13:17:01 10splitTasks](trainer.py 286): INFO [40/157]	0.1024(0.1157)	0.0003(0.0119)	0.662(0.585)	68.75(80.87)
[2023-09-29 13:17:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1133)	0.0003(0.0096)	0.498(0.582)	87.50(80.82)
[2023-09-29 13:17:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1117)	0.0002(0.0081)	0.784(0.577)	81.25(81.20)
[2023-09-29 13:17:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1104)	0.0002(0.0070)	1.072(0.593)	62.50(80.72)
[2023-09-29 13:17:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1031(0.1095)	0.0002(0.0062)	0.601(0.593)	81.25(80.52)
[2023-09-29 13:17:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1030(0.1087)	0.0002(0.0055)	0.508(0.589)	84.38(80.53)
[2023-09-29 13:17:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1028(0.1082)	0.0004(0.0050)	0.527(0.589)	87.50(80.38)
[2023-09-29 13:17:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1070(0.1079)	0.0002(0.0046)	0.406(0.591)	87.50(80.32)
[2023-09-29 13:17:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1073(0.1076)	0.0003(0.0043)	1.297(0.595)	59.38(80.04)
[2023-09-29 13:17:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1071)	0.0003(0.0040)	0.362(0.590)	87.50(80.18)
[2023-09-29 13:17:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1068)	0.0002(0.0037)	0.657(0.591)	78.12(80.25)
[2023-09-29 13:17:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1019(0.1065)	0.0002(0.0035)	0.971(0.589)	68.75(80.32)
[2023-09-29 13:17:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0810(0.1062)	0.0001(0.0034)	0.684(0.590)	75.00(80.24)
[2023-09-29 13:17:13 10splitTasks](trainer.py 288): INFO  * Train Acc 80.240
[2023-09-29 13:17:15 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.600, Total time 1.75
[2023-09-29 13:17:15 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 13:17:15 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 13:17:15 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:17:15 10splitTasks](trainer.py 286): INFO [0/157]	0.6046(0.6046)	0.4831(0.4831)	0.386(0.386)	84.38(84.38)
[2023-09-29 13:17:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1047(0.1508)	0.0002(0.0442)	0.313(0.423)	87.50(85.51)
[2023-09-29 13:17:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1051(0.1286)	0.0003(0.0233)	0.474(0.433)	78.12(84.97)
[2023-09-29 13:17:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.1202)	0.0002(0.0159)	0.514(0.463)	87.50(84.98)
[2023-09-29 13:17:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1100(0.1164)	0.0002(0.0121)	0.936(0.490)	65.62(83.99)
[2023-09-29 13:17:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1225(0.1145)	0.0003(0.0098)	0.483(0.499)	84.38(83.64)
[2023-09-29 13:17:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1127)	0.0003(0.0082)	0.893(0.497)	71.88(83.97)
[2023-09-29 13:17:23 10splitTasks](trainer.py 286): INFO [70/157]	0.1043(0.1116)	0.0003(0.0071)	0.598(0.491)	84.38(84.11)
[2023-09-29 13:17:24 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1105)	0.0003(0.0063)	0.404(0.486)	87.50(84.22)
[2023-09-29 13:17:25 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1096)	0.0002(0.0056)	0.653(0.479)	81.25(84.55)
[2023-09-29 13:17:26 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1091)	0.0002(0.0051)	0.341(0.476)	87.50(84.68)
[2023-09-29 13:17:27 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1086)	0.0002(0.0047)	0.313(0.478)	87.50(84.57)
[2023-09-29 13:17:28 10splitTasks](trainer.py 286): INFO [120/157]	0.1047(0.1083)	0.0002(0.0043)	0.569(0.478)	81.25(84.66)
[2023-09-29 13:17:29 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1080)	0.0002(0.0040)	0.404(0.471)	84.38(84.88)
[2023-09-29 13:17:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1036(0.1077)	0.0002(0.0038)	0.620(0.470)	78.12(84.88)
[2023-09-29 13:17:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1073)	0.0001(0.0035)	0.119(0.467)	100.00(85.06)
[2023-09-29 13:17:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0806(0.1069)	0.0001(0.0034)	0.117(0.466)	100.00(85.14)
[2023-09-29 13:17:31 10splitTasks](trainer.py 288): INFO  * Train Acc 85.140
[2023-09-29 13:17:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.200, Total time 1.67
[2023-09-29 13:17:33 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 13:17:33 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 13:17:33 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:17:34 10splitTasks](trainer.py 286): INFO [0/157]	0.7611(0.7611)	0.6483(0.6483)	0.325(0.325)	87.50(87.50)
[2023-09-29 13:17:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1630)	0.0002(0.0592)	0.383(0.373)	81.25(86.36)
[2023-09-29 13:17:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1343)	0.0002(0.0312)	0.407(0.348)	90.62(87.35)
[2023-09-29 13:17:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1242)	0.0002(0.0212)	0.504(0.358)	81.25(86.69)
[2023-09-29 13:17:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1189)	0.0003(0.0161)	0.298(0.371)	90.62(87.04)
[2023-09-29 13:17:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1160)	0.0003(0.0130)	0.312(0.373)	90.62(86.89)
[2023-09-29 13:17:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1094(0.1138)	0.0003(0.0109)	0.308(0.379)	90.62(86.68)
[2023-09-29 13:17:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1125)	0.0003(0.0094)	0.279(0.377)	87.50(86.80)
[2023-09-29 13:17:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1114)	0.0003(0.0083)	0.160(0.371)	93.75(87.00)
[2023-09-29 13:17:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1097(0.1106)	0.0002(0.0074)	0.317(0.373)	90.62(87.09)
[2023-09-29 13:17:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1054(0.1099)	0.0003(0.0068)	0.225(0.367)	90.62(87.25)
[2023-09-29 13:17:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1095)	0.0003(0.0062)	0.368(0.366)	90.62(87.30)
[2023-09-29 13:17:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1055(0.1090)	0.0002(0.0057)	0.433(0.368)	87.50(87.27)
[2023-09-29 13:17:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1028(0.1086)	0.0004(0.0053)	0.419(0.374)	90.62(87.26)
[2023-09-29 13:17:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1082)	0.0003(0.0049)	0.355(0.376)	87.50(87.28)
[2023-09-29 13:17:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1080)	0.0001(0.0046)	0.333(0.377)	90.62(87.15)
[2023-09-29 13:17:50 10splitTasks](trainer.py 286): INFO [156/157]	0.0809(0.1075)	0.0001(0.0045)	0.266(0.377)	87.50(87.14)
[2023-09-29 13:17:50 10splitTasks](trainer.py 288): INFO  * Train Acc 87.140
[2023-09-29 13:17:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.68
[2023-09-29 13:17:52 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 13:17:52 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 13:17:52 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:17:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6302(0.6302)	0.5142(0.5142)	0.195(0.195)	96.88(96.88)
[2023-09-29 13:17:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1062(0.1519)	0.0002(0.0470)	0.328(0.307)	90.62(88.92)
[2023-09-29 13:17:55 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1287)	0.0003(0.0248)	0.240(0.306)	90.62(89.58)
[2023-09-29 13:17:56 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1212)	0.0004(0.0169)	0.324(0.305)	90.62(89.42)
[2023-09-29 13:17:57 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1168)	0.0003(0.0129)	0.196(0.311)	93.75(89.25)
[2023-09-29 13:17:58 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1146)	0.0003(0.0104)	0.358(0.313)	84.38(89.34)
[2023-09-29 13:17:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1126)	0.0003(0.0088)	0.324(0.318)	84.38(89.19)
[2023-09-29 13:18:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1115)	0.0003(0.0076)	0.319(0.317)	90.62(89.22)
[2023-09-29 13:18:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1022(0.1107)	0.0002(0.0067)	0.249(0.319)	93.75(89.27)
[2023-09-29 13:18:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1100)	0.0003(0.0060)	0.220(0.318)	90.62(89.15)
[2023-09-29 13:18:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1043(0.1093)	0.0003(0.0054)	0.244(0.325)	93.75(89.05)
[2023-09-29 13:18:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1049(0.1088)	0.0003(0.0050)	0.353(0.326)	84.38(89.16)
[2023-09-29 13:18:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1135(0.1086)	0.0007(0.0046)	0.805(0.331)	75.00(89.02)
[2023-09-29 13:18:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1146(0.1083)	0.0005(0.0043)	0.109(0.324)	100.00(89.38)
[2023-09-29 13:18:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1079)	0.0002(0.0040)	0.121(0.318)	100.00(89.63)
[2023-09-29 13:18:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1076)	0.0001(0.0037)	0.397(0.323)	84.38(89.49)
[2023-09-29 13:18:09 10splitTasks](trainer.py 286): INFO [156/157]	0.0780(0.1073)	0.0001(0.0036)	0.334(0.323)	87.50(89.50)
[2023-09-29 13:18:09 10splitTasks](trainer.py 288): INFO  * Train Acc 89.500
[2023-09-29 13:18:10 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.800, Total time 1.66
[2023-09-29 13:18:10 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 13:18:10 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 13:18:10 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:18:11 10splitTasks](trainer.py 286): INFO [0/157]	0.6208(0.6208)	0.4969(0.4969)	0.218(0.218)	93.75(93.75)
[2023-09-29 13:18:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1026(0.1509)	0.0002(0.0455)	0.227(0.270)	90.62(90.06)
[2023-09-29 13:18:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1079(0.1302)	0.0004(0.0240)	0.490(0.300)	81.25(89.73)
[2023-09-29 13:18:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1216)	0.0003(0.0164)	0.235(0.290)	96.88(90.62)
[2023-09-29 13:18:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1171)	0.0002(0.0124)	0.298(0.301)	87.50(90.40)
[2023-09-29 13:18:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1038(0.1144)	0.0002(0.0101)	0.420(0.310)	84.38(90.01)
[2023-09-29 13:18:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1124)	0.0003(0.0085)	0.180(0.311)	93.75(89.70)
[2023-09-29 13:18:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1113)	0.0003(0.0074)	0.367(0.304)	87.50(89.74)
[2023-09-29 13:18:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1058(0.1104)	0.0002(0.0065)	0.355(0.308)	90.62(89.78)
[2023-09-29 13:18:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1052(0.1096)	0.0003(0.0058)	0.171(0.302)	93.75(89.90)
[2023-09-29 13:18:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1089)	0.0003(0.0053)	0.166(0.297)	96.88(90.16)
[2023-09-29 13:18:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1026(0.1085)	0.0002(0.0048)	0.168(0.292)	93.75(90.40)
[2023-09-29 13:18:24 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1081)	0.0002(0.0045)	0.258(0.293)	90.62(90.32)
[2023-09-29 13:18:25 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1077)	0.0002(0.0042)	0.495(0.297)	81.25(90.08)
[2023-09-29 13:18:26 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1073)	0.0003(0.0039)	0.352(0.296)	84.38(90.20)
[2023-09-29 13:18:27 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1072)	0.0001(0.0037)	0.390(0.294)	87.50(90.27)
[2023-09-29 13:18:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0807(0.1068)	0.0001(0.0035)	1.161(0.296)	75.00(90.22)
[2023-09-29 13:18:27 10splitTasks](trainer.py 288): INFO  * Train Acc 90.220
[2023-09-29 13:18:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.800, Total time 1.68
[2023-09-29 13:18:29 10splitTasks](my_trainer.py 206): INFO Pruning for task3
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 4910/5777 (84.99%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 2139/2516 (85.02%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 19243/22639 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 8552/10061 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 8552/10061 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 8552/10061 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 19243/22639 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 8552/10061 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 8552/10061 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 19243/22639 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 8552/10061 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 17105/20124 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 76973/90556 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 34211/40248 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 68420/80494 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 34211/40248 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 76973/90556 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 34211/40248 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 34211/40248 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 76973/90556 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 34211/40248 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 34211/40248 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 76973/90556 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 34211/40248 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 68420/80494 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 307892/362226 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 273681/321978 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 307892/362226 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 307892/362226 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 307892/362226 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 307892/362226 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 307892/362226 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 136841/160989 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 273681/321978 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 1231568/1448903 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 547363/643957 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 1094726/1287913 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 547363/643957 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 1231568/1448903 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 547363/643957 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 547363/643957 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 1231568/1448904 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 547363/643957 (85.00%) (Total in layer: 1048576)
[2023-09-29 13:18:29 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 13:18:29 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 13:18:29 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 13:18:29 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:18:30 10splitTasks](trainer.py 286): INFO [0/157]	0.5941(0.5941)	0.4662(0.4662)	0.171(0.171)	93.75(93.75)
[2023-09-29 13:18:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1023(0.1502)	0.0002(0.0427)	0.132(0.412)	100.00(87.22)
[2023-09-29 13:18:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1279)	0.0003(0.0227)	0.502(0.453)	81.25(84.08)
[2023-09-29 13:18:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1201)	0.0002(0.0154)	0.448(0.437)	87.50(84.48)
[2023-09-29 13:18:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1024(0.1163)	0.0003(0.0118)	0.428(0.480)	87.50(83.16)
[2023-09-29 13:18:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1137)	0.0003(0.0095)	0.430(0.502)	84.38(81.86)
[2023-09-29 13:18:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1027(0.1120)	0.0004(0.0080)	0.178(0.506)	93.75(82.07)
[2023-09-29 13:18:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1107)	0.0002(0.0069)	0.448(0.499)	84.38(82.26)
[2023-09-29 13:18:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1128(0.1099)	0.0005(0.0061)	0.662(0.501)	75.00(82.52)
[2023-09-29 13:18:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1091)	0.0002(0.0055)	0.241(0.491)	90.62(82.66)
[2023-09-29 13:18:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1086)	0.0003(0.0050)	0.814(0.500)	65.62(82.33)
[2023-09-29 13:18:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1082)	0.0002(0.0046)	0.653(0.506)	75.00(81.98)
[2023-09-29 13:18:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1077)	0.0002(0.0042)	0.233(0.507)	90.62(82.10)
[2023-09-29 13:18:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1038(0.1074)	0.0003(0.0039)	0.540(0.508)	75.00(81.97)
[2023-09-29 13:18:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1072)	0.0002(0.0037)	1.192(0.506)	71.88(82.16)
[2023-09-29 13:18:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1070)	0.0001(0.0034)	0.637(0.512)	81.25(82.06)
[2023-09-29 13:18:46 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1066)	0.0001(0.0033)	1.383(0.517)	50.00(81.92)
[2023-09-29 13:18:46 10splitTasks](trainer.py 288): INFO  * Train Acc 81.920
[2023-09-29 13:18:48 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.400, Total time 1.72
[2023-09-29 13:18:48 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 13:18:48 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 13:18:48 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:18:48 10splitTasks](trainer.py 286): INFO [0/157]	0.6022(0.6022)	0.4913(0.4913)	0.438(0.438)	84.38(84.38)
[2023-09-29 13:18:50 10splitTasks](trainer.py 286): INFO [10/157]	0.1053(0.1509)	0.0003(0.0449)	0.269(0.565)	93.75(80.11)
[2023-09-29 13:18:51 10splitTasks](trainer.py 286): INFO [20/157]	0.1062(0.1288)	0.0003(0.0238)	0.692(0.522)	78.12(81.70)
[2023-09-29 13:18:52 10splitTasks](trainer.py 286): INFO [30/157]	0.1051(0.1204)	0.0002(0.0162)	0.666(0.529)	78.12(81.75)
[2023-09-29 13:18:53 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1161)	0.0002(0.0124)	0.443(0.522)	87.50(82.01)
[2023-09-29 13:18:54 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1140)	0.0003(0.0100)	0.386(0.515)	87.50(82.41)
[2023-09-29 13:18:55 10splitTasks](trainer.py 286): INFO [60/157]	0.1045(0.1125)	0.0002(0.0084)	0.590(0.505)	75.00(82.74)
[2023-09-29 13:18:56 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1111)	0.0002(0.0073)	0.436(0.503)	87.50(82.61)
[2023-09-29 13:18:57 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1102)	0.0002(0.0064)	0.315(0.493)	87.50(82.95)
[2023-09-29 13:18:58 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1096)	0.0002(0.0058)	0.407(0.480)	87.50(83.28)
[2023-09-29 13:18:59 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1091)	0.0003(0.0052)	0.365(0.475)	93.75(83.45)
[2023-09-29 13:19:00 10splitTasks](trainer.py 286): INFO [110/157]	0.1047(0.1086)	0.0003(0.0048)	0.294(0.479)	84.38(83.25)
[2023-09-29 13:19:01 10splitTasks](trainer.py 286): INFO [120/157]	0.1042(0.1082)	0.0004(0.0044)	0.402(0.476)	87.50(83.52)
[2023-09-29 13:19:02 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1080)	0.0003(0.0041)	0.352(0.473)	87.50(83.59)
[2023-09-29 13:19:03 10splitTasks](trainer.py 286): INFO [140/157]	0.1039(0.1077)	0.0002(0.0038)	0.820(0.476)	75.00(83.60)
[2023-09-29 13:19:04 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1074)	0.0001(0.0036)	0.525(0.476)	84.38(83.53)
[2023-09-29 13:19:05 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1070)	0.0001(0.0035)	0.955(0.476)	62.50(83.56)
[2023-09-29 13:19:05 10splitTasks](trainer.py 288): INFO  * Train Acc 83.560
[2023-09-29 13:19:07 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.800, Total time 1.73
[2023-09-29 13:19:07 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 13:19:07 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 13:19:07 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:19:07 10splitTasks](trainer.py 286): INFO [0/157]	0.6146(0.6146)	0.4857(0.4857)	0.538(0.538)	81.25(81.25)
[2023-09-29 13:19:08 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1513)	0.0003(0.0445)	0.840(0.445)	81.25(84.66)
[2023-09-29 13:19:09 10splitTasks](trainer.py 286): INFO [20/157]	0.1032(0.1284)	0.0002(0.0235)	0.458(0.423)	81.25(85.71)
[2023-09-29 13:19:10 10splitTasks](trainer.py 286): INFO [30/157]	0.1042(0.1200)	0.0003(0.0160)	0.400(0.390)	81.25(86.09)
[2023-09-29 13:19:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1031(0.1161)	0.0004(0.0122)	0.566(0.409)	81.25(86.05)
[2023-09-29 13:19:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1052(0.1134)	0.0003(0.0098)	0.722(0.424)	71.88(85.78)
[2023-09-29 13:19:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1029(0.1120)	0.0002(0.0083)	0.237(0.415)	90.62(86.01)
[2023-09-29 13:19:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1027(0.1108)	0.0002(0.0072)	0.661(0.404)	68.75(86.05)
[2023-09-29 13:19:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1100)	0.0003(0.0063)	0.267(0.401)	87.50(86.19)
[2023-09-29 13:19:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1094)	0.0002(0.0057)	0.375(0.399)	90.62(86.30)
[2023-09-29 13:19:18 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1090)	0.0003(0.0051)	0.587(0.398)	84.38(86.42)
[2023-09-29 13:19:19 10splitTasks](trainer.py 286): INFO [110/157]	0.1068(0.1086)	0.0002(0.0047)	0.380(0.401)	93.75(86.29)
[2023-09-29 13:19:20 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1081)	0.0002(0.0043)	0.712(0.408)	71.88(85.92)
[2023-09-29 13:19:21 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1077)	0.0002(0.0040)	0.224(0.410)	93.75(85.83)
[2023-09-29 13:19:22 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1073)	0.0002(0.0038)	0.380(0.419)	93.75(85.75)
[2023-09-29 13:19:23 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1071)	0.0001(0.0035)	0.560(0.422)	78.12(85.58)
[2023-09-29 13:19:23 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1068)	0.0001(0.0034)	2.360(0.428)	62.50(85.42)
[2023-09-29 13:19:23 10splitTasks](trainer.py 288): INFO  * Train Acc 85.420
[2023-09-29 13:19:25 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.800, Total time 1.66
[2023-09-29 13:19:25 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 13:19:25 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 13:19:25 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:19:26 10splitTasks](trainer.py 286): INFO [0/157]	0.6135(0.6135)	0.5041(0.5041)	0.373(0.373)	93.75(93.75)
[2023-09-29 13:19:27 10splitTasks](trainer.py 286): INFO [10/157]	0.1031(0.1499)	0.0003(0.0462)	0.565(0.465)	81.25(85.23)
[2023-09-29 13:19:28 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1277)	0.0003(0.0243)	0.294(0.473)	87.50(84.52)
[2023-09-29 13:19:29 10splitTasks](trainer.py 286): INFO [30/157]	0.1059(0.1196)	0.0002(0.0166)	0.441(0.459)	81.25(83.67)
[2023-09-29 13:19:30 10splitTasks](trainer.py 286): INFO [40/157]	0.1033(0.1156)	0.0004(0.0126)	0.325(0.481)	87.50(82.93)
[2023-09-29 13:19:31 10splitTasks](trainer.py 286): INFO [50/157]	0.1027(0.1132)	0.0003(0.0102)	0.735(0.478)	81.25(83.03)
[2023-09-29 13:19:32 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1120)	0.0002(0.0086)	0.501(0.475)	87.50(83.30)
[2023-09-29 13:19:33 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1106)	0.0003(0.0074)	0.312(0.465)	90.62(83.80)
[2023-09-29 13:19:34 10splitTasks](trainer.py 286): INFO [80/157]	0.1024(0.1097)	0.0002(0.0065)	0.300(0.460)	90.62(84.10)
[2023-09-29 13:19:35 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1089)	0.0002(0.0059)	0.342(0.454)	90.62(84.34)
[2023-09-29 13:19:36 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1083)	0.0002(0.0053)	0.506(0.453)	90.62(84.34)
[2023-09-29 13:19:37 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1077)	0.0004(0.0049)	0.344(0.450)	87.50(84.40)
[2023-09-29 13:19:38 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1073)	0.0003(0.0045)	0.208(0.450)	93.75(84.27)
[2023-09-29 13:19:39 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1070)	0.0003(0.0042)	0.401(0.443)	87.50(84.59)
[2023-09-29 13:19:40 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1067)	0.0003(0.0039)	0.349(0.438)	87.50(84.64)
[2023-09-29 13:19:41 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1064)	0.0001(0.0037)	0.371(0.438)	84.38(84.62)
[2023-09-29 13:19:42 10splitTasks](trainer.py 286): INFO [156/157]	0.0801(0.1060)	0.0001(0.0035)	0.715(0.435)	75.00(84.66)
[2023-09-29 13:19:42 10splitTasks](trainer.py 288): INFO  * Train Acc 84.660
[2023-09-29 13:19:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.600, Total time 1.65
[2023-09-29 13:19:43 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 13:19:43 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 13:19:43 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:19:44 10splitTasks](trainer.py 286): INFO [0/157]	0.7653(0.7653)	0.6555(0.6555)	0.231(0.231)	93.75(93.75)
[2023-09-29 13:19:45 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1630)	0.0002(0.0598)	0.552(0.359)	84.38(88.07)
[2023-09-29 13:19:46 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1346)	0.0003(0.0315)	0.213(0.349)	93.75(87.50)
[2023-09-29 13:19:47 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1244)	0.0003(0.0214)	0.425(0.373)	87.50(86.90)
[2023-09-29 13:19:48 10splitTasks](trainer.py 286): INFO [40/157]	0.1041(0.1191)	0.0002(0.0163)	0.468(0.369)	81.25(86.89)
[2023-09-29 13:19:49 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1159)	0.0002(0.0131)	0.328(0.370)	87.50(87.13)
[2023-09-29 13:19:50 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1137)	0.0002(0.0110)	0.175(0.368)	90.62(86.94)
[2023-09-29 13:19:51 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1123)	0.0003(0.0095)	0.363(0.364)	84.38(86.93)
[2023-09-29 13:19:52 10splitTasks](trainer.py 286): INFO [80/157]	0.1034(0.1112)	0.0003(0.0084)	0.372(0.374)	81.25(86.77)
[2023-09-29 13:19:54 10splitTasks](trainer.py 286): INFO [90/157]	0.1035(0.1103)	0.0005(0.0075)	0.325(0.379)	90.62(86.64)
[2023-09-29 13:19:55 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1095)	0.0002(0.0068)	0.358(0.376)	84.38(86.70)
[2023-09-29 13:19:56 10splitTasks](trainer.py 286): INFO [110/157]	0.1031(0.1091)	0.0002(0.0062)	0.376(0.371)	90.62(86.97)
[2023-09-29 13:19:57 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1086)	0.0002(0.0057)	0.466(0.372)	87.50(87.06)
[2023-09-29 13:19:58 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1083)	0.0003(0.0053)	0.713(0.374)	78.12(87.07)
[2023-09-29 13:19:59 10splitTasks](trainer.py 286): INFO [140/157]	0.1065(0.1079)	0.0003(0.0050)	0.294(0.376)	90.62(87.01)
[2023-09-29 13:20:00 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1076)	0.0001(0.0046)	0.367(0.374)	87.50(86.98)
[2023-09-29 13:20:00 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1072)	0.0001(0.0045)	0.545(0.377)	75.00(86.84)
[2023-09-29 13:20:00 10splitTasks](trainer.py 288): INFO  * Train Acc 86.840
[2023-09-29 13:20:02 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.400, Total time 1.62
[2023-09-29 13:20:02 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 13:20:02 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 13:20:02 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:20:03 10splitTasks](trainer.py 286): INFO [0/157]	0.6308(0.6308)	0.5243(0.5243)	0.234(0.234)	90.62(90.62)
[2023-09-29 13:20:04 10splitTasks](trainer.py 286): INFO [10/157]	0.1066(0.1525)	0.0003(0.0480)	0.419(0.445)	87.50(80.97)
[2023-09-29 13:20:05 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1294)	0.0002(0.0253)	0.474(0.397)	75.00(83.78)
[2023-09-29 13:20:06 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1213)	0.0002(0.0172)	0.276(0.402)	87.50(84.88)
[2023-09-29 13:20:07 10splitTasks](trainer.py 286): INFO [40/157]	0.1073(0.1170)	0.0002(0.0131)	0.414(0.376)	87.50(86.05)
[2023-09-29 13:20:08 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1144)	0.0002(0.0106)	0.250(0.368)	90.62(86.76)
[2023-09-29 13:20:09 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1124)	0.0002(0.0089)	0.281(0.357)	90.62(86.99)
[2023-09-29 13:20:10 10splitTasks](trainer.py 286): INFO [70/157]	0.1024(0.1111)	0.0002(0.0077)	0.189(0.345)	93.75(87.59)
[2023-09-29 13:20:11 10splitTasks](trainer.py 286): INFO [80/157]	0.1148(0.1102)	0.0005(0.0068)	0.161(0.342)	93.75(87.81)
[2023-09-29 13:20:12 10splitTasks](trainer.py 286): INFO [90/157]	0.1103(0.1097)	0.0002(0.0061)	0.417(0.337)	90.62(88.19)
[2023-09-29 13:20:13 10splitTasks](trainer.py 286): INFO [100/157]	0.1135(0.1091)	0.0002(0.0055)	0.225(0.330)	93.75(88.64)
[2023-09-29 13:20:14 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1087)	0.0002(0.0050)	0.758(0.337)	75.00(88.54)
[2023-09-29 13:20:15 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1082)	0.0002(0.0047)	0.108(0.340)	100.00(88.46)
[2023-09-29 13:20:16 10splitTasks](trainer.py 286): INFO [130/157]	0.1043(0.1080)	0.0002(0.0043)	0.348(0.345)	87.50(88.29)
[2023-09-29 13:20:17 10splitTasks](trainer.py 286): INFO [140/157]	0.1048(0.1076)	0.0003(0.0040)	0.296(0.340)	93.75(88.54)
[2023-09-29 13:20:18 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1074)	0.0002(0.0038)	0.273(0.336)	87.50(88.66)
[2023-09-29 13:20:19 10splitTasks](trainer.py 286): INFO [156/157]	0.0816(0.1070)	0.0001(0.0037)	0.316(0.337)	75.00(88.56)
[2023-09-29 13:20:19 10splitTasks](trainer.py 288): INFO  * Train Acc 88.560
[2023-09-29 13:20:21 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.59
[2023-09-29 13:20:21 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 13:20:21 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 13:20:21 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:20:21 10splitTasks](trainer.py 286): INFO [0/157]	0.6437(0.6437)	0.5331(0.5331)	0.101(0.101)	96.88(96.88)
[2023-09-29 13:20:22 10splitTasks](trainer.py 286): INFO [10/157]	0.1024(0.1525)	0.0002(0.0488)	0.384(0.270)	90.62(90.91)
[2023-09-29 13:20:23 10splitTasks](trainer.py 286): INFO [20/157]	0.1069(0.1298)	0.0004(0.0257)	0.322(0.286)	87.50(89.58)
[2023-09-29 13:20:24 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1215)	0.0003(0.0176)	0.247(0.313)	90.62(89.11)
[2023-09-29 13:20:25 10splitTasks](trainer.py 286): INFO [40/157]	0.1024(0.1172)	0.0002(0.0134)	0.151(0.300)	96.88(89.48)
[2023-09-29 13:20:26 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1144)	0.0003(0.0108)	0.384(0.306)	90.62(89.28)
[2023-09-29 13:20:27 10splitTasks](trainer.py 286): INFO [60/157]	0.1077(0.1128)	0.0002(0.0091)	0.425(0.304)	81.25(89.50)
[2023-09-29 13:20:28 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1113)	0.0002(0.0079)	0.132(0.298)	96.88(89.39)
[2023-09-29 13:20:29 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1102)	0.0003(0.0069)	0.193(0.293)	96.88(89.47)
[2023-09-29 13:20:30 10splitTasks](trainer.py 286): INFO [90/157]	0.1031(0.1093)	0.0002(0.0062)	0.296(0.295)	90.62(89.29)
[2023-09-29 13:20:32 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1087)	0.0002(0.0056)	0.361(0.292)	90.62(89.45)
[2023-09-29 13:20:33 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1084)	0.0002(0.0051)	0.425(0.289)	84.38(89.61)
[2023-09-29 13:20:34 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1079)	0.0002(0.0047)	0.273(0.288)	87.50(89.70)
[2023-09-29 13:20:35 10splitTasks](trainer.py 286): INFO [130/157]	0.1118(0.1076)	0.0006(0.0044)	0.097(0.288)	96.88(89.79)
[2023-09-29 13:20:36 10splitTasks](trainer.py 286): INFO [140/157]	0.1040(0.1073)	0.0002(0.0041)	0.316(0.295)	87.50(89.63)
[2023-09-29 13:20:37 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1070)	0.0001(0.0039)	0.232(0.294)	90.62(89.63)
[2023-09-29 13:20:37 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1066)	0.0001(0.0037)	0.706(0.294)	87.50(89.72)
[2023-09-29 13:20:37 10splitTasks](trainer.py 288): INFO  * Train Acc 89.720
[2023-09-29 13:20:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.600, Total time 1.68
[2023-09-29 13:20:39 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 13:20:39 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 13:20:39 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:20:40 10splitTasks](trainer.py 286): INFO [0/157]	0.6405(0.6405)	0.4935(0.4935)	0.391(0.391)	84.38(84.38)
[2023-09-29 13:20:41 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1530)	0.0002(0.0452)	0.270(0.300)	90.62(89.20)
[2023-09-29 13:20:42 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1298)	0.0003(0.0238)	0.330(0.310)	87.50(89.58)
[2023-09-29 13:20:43 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1211)	0.0003(0.0163)	0.253(0.317)	90.62(89.92)
[2023-09-29 13:20:44 10splitTasks](trainer.py 286): INFO [40/157]	0.1059(0.1167)	0.0002(0.0124)	0.317(0.304)	87.50(90.47)
[2023-09-29 13:20:45 10splitTasks](trainer.py 286): INFO [50/157]	0.1069(0.1141)	0.0002(0.0100)	0.274(0.295)	93.75(90.93)
[2023-09-29 13:20:46 10splitTasks](trainer.py 286): INFO [60/157]	0.1009(0.1123)	0.0002(0.0084)	0.250(0.299)	93.75(90.68)
[2023-09-29 13:20:47 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1109)	0.0002(0.0073)	0.471(0.305)	84.38(90.62)
[2023-09-29 13:20:48 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1099)	0.0002(0.0064)	0.109(0.295)	96.88(90.82)
[2023-09-29 13:20:49 10splitTasks](trainer.py 286): INFO [90/157]	0.1033(0.1092)	0.0003(0.0058)	0.258(0.303)	96.88(90.73)
[2023-09-29 13:20:50 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1087)	0.0002(0.0052)	0.322(0.301)	90.62(90.78)
[2023-09-29 13:20:51 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1083)	0.0002(0.0048)	0.433(0.302)	87.50(90.65)
[2023-09-29 13:20:52 10splitTasks](trainer.py 286): INFO [120/157]	0.1069(0.1078)	0.0006(0.0044)	0.725(0.308)	68.75(90.29)
[2023-09-29 13:20:53 10splitTasks](trainer.py 286): INFO [130/157]	0.1040(0.1076)	0.0002(0.0042)	0.201(0.304)	96.88(90.39)
[2023-09-29 13:20:54 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1072)	0.0003(0.0039)	0.460(0.305)	87.50(90.36)
[2023-09-29 13:20:55 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1069)	0.0001(0.0036)	0.156(0.303)	93.75(90.40)
[2023-09-29 13:20:56 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1066)	0.0001(0.0035)	0.176(0.301)	100.00(90.42)
[2023-09-29 13:20:56 10splitTasks](trainer.py 288): INFO  * Train Acc 90.420
[2023-09-29 13:20:58 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.77
[2023-09-29 13:20:58 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 13:20:58 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 13:20:58 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:20:58 10splitTasks](trainer.py 286): INFO [0/157]	0.6218(0.6218)	0.5114(0.5114)	0.109(0.109)	96.88(96.88)
[2023-09-29 13:20:59 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1513)	0.0002(0.0471)	0.377(0.195)	87.50(92.61)
[2023-09-29 13:21:00 10splitTasks](trainer.py 286): INFO [20/157]	0.1025(0.1291)	0.0002(0.0248)	0.247(0.223)	93.75(92.26)
[2023-09-29 13:21:01 10splitTasks](trainer.py 286): INFO [30/157]	0.1053(0.1206)	0.0003(0.0169)	0.159(0.220)	93.75(91.94)
[2023-09-29 13:21:02 10splitTasks](trainer.py 286): INFO [40/157]	0.1026(0.1163)	0.0002(0.0129)	0.345(0.227)	87.50(91.84)
[2023-09-29 13:21:04 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1142)	0.0003(0.0104)	0.196(0.225)	93.75(91.85)
[2023-09-29 13:21:05 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1122)	0.0003(0.0088)	0.679(0.239)	78.12(91.65)
[2023-09-29 13:21:06 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1110)	0.0003(0.0076)	0.353(0.243)	87.50(91.68)
[2023-09-29 13:21:07 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1100)	0.0004(0.0067)	0.202(0.236)	90.62(91.98)
[2023-09-29 13:21:08 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1091)	0.0002(0.0060)	0.127(0.235)	96.88(92.00)
[2023-09-29 13:21:09 10splitTasks](trainer.py 286): INFO [100/157]	0.1056(0.1085)	0.0003(0.0054)	0.231(0.240)	90.62(91.80)
[2023-09-29 13:21:10 10splitTasks](trainer.py 286): INFO [110/157]	0.1269(0.1082)	0.0005(0.0050)	0.243(0.239)	90.62(91.86)
[2023-09-29 13:21:11 10splitTasks](trainer.py 286): INFO [120/157]	0.1028(0.1080)	0.0003(0.0046)	0.360(0.242)	90.62(91.68)
[2023-09-29 13:21:12 10splitTasks](trainer.py 286): INFO [130/157]	0.1023(0.1076)	0.0004(0.0043)	0.229(0.240)	87.50(91.84)
[2023-09-29 13:21:13 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1073)	0.0004(0.0040)	0.138(0.241)	93.75(91.71)
[2023-09-29 13:21:14 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1070)	0.0001(0.0037)	0.202(0.244)	93.75(91.68)
[2023-09-29 13:21:14 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1066)	0.0001(0.0036)	1.015(0.245)	75.00(91.72)
[2023-09-29 13:21:14 10splitTasks](trainer.py 288): INFO  * Train Acc 91.720
[2023-09-29 13:21:16 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.400, Total time 1.66
[2023-09-29 13:21:16 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 13:21:16 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 13:21:16 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:21:17 10splitTasks](trainer.py 286): INFO [0/157]	0.6063(0.6063)	0.4917(0.4917)	0.445(0.445)	87.50(87.50)
[2023-09-29 13:21:18 10splitTasks](trainer.py 286): INFO [10/157]	0.1035(0.1650)	0.0003(0.0597)	0.117(0.239)	96.88(93.18)
[2023-09-29 13:21:19 10splitTasks](trainer.py 286): INFO [20/157]	0.1009(0.1351)	0.0002(0.0314)	0.112(0.245)	93.75(92.26)
[2023-09-29 13:21:20 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1244)	0.0002(0.0214)	0.375(0.251)	84.38(91.53)
[2023-09-29 13:21:21 10splitTasks](trainer.py 286): INFO [40/157]	0.1050(0.1192)	0.0004(0.0162)	0.205(0.238)	87.50(91.77)
[2023-09-29 13:21:22 10splitTasks](trainer.py 286): INFO [50/157]	0.1009(0.1157)	0.0001(0.0131)	0.249(0.232)	93.75(91.91)
[2023-09-29 13:21:23 10splitTasks](trainer.py 286): INFO [60/157]	0.1036(0.1136)	0.0002(0.0110)	0.143(0.229)	96.88(92.11)
[2023-09-29 13:21:24 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1122)	0.0001(0.0095)	0.078(0.221)	100.00(92.43)
[2023-09-29 13:21:25 10splitTasks](trainer.py 286): INFO [80/157]	0.1027(0.1109)	0.0002(0.0084)	0.282(0.229)	84.38(92.01)
[2023-09-29 13:21:26 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1099)	0.0002(0.0075)	0.447(0.229)	90.62(92.17)
[2023-09-29 13:21:27 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1093)	0.0003(0.0068)	0.354(0.226)	87.50(92.30)
[2023-09-29 13:21:28 10splitTasks](trainer.py 286): INFO [110/157]	0.1048(0.1088)	0.0002(0.0062)	0.368(0.230)	90.62(92.12)
[2023-09-29 13:21:29 10splitTasks](trainer.py 286): INFO [120/157]	0.1010(0.1083)	0.0002(0.0057)	0.162(0.231)	96.88(92.12)
[2023-09-29 13:21:30 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1079)	0.0002(0.0053)	0.223(0.233)	90.62(92.10)
[2023-09-29 13:21:31 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1075)	0.0003(0.0049)	0.296(0.237)	87.50(92.02)
[2023-09-29 13:21:32 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1072)	0.0001(0.0046)	0.424(0.236)	84.38(92.03)
[2023-09-29 13:21:33 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1068)	0.0001(0.0044)	0.238(0.236)	87.50(92.00)
[2023-09-29 13:21:33 10splitTasks](trainer.py 288): INFO  * Train Acc 92.000
[2023-09-29 13:21:35 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.400, Total time 1.64
[2023-09-29 13:21:35 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 13:21:35 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 13:21:35 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:21:35 10splitTasks](trainer.py 286): INFO [0/157]	0.5909(0.5909)	0.4792(0.4792)	0.208(0.208)	90.62(90.62)
[2023-09-29 13:21:36 10splitTasks](trainer.py 286): INFO [10/157]	0.1069(0.1476)	0.0002(0.0438)	0.372(0.198)	87.50(92.90)
[2023-09-29 13:21:37 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1258)	0.0002(0.0231)	0.171(0.188)	90.62(93.15)
[2023-09-29 13:21:38 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1188)	0.0003(0.0158)	0.159(0.187)	93.75(93.15)
[2023-09-29 13:21:39 10splitTasks](trainer.py 286): INFO [40/157]	0.1032(0.1148)	0.0005(0.0120)	0.181(0.181)	93.75(93.45)
[2023-09-29 13:21:40 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1126)	0.0003(0.0097)	0.226(0.191)	90.62(93.38)
[2023-09-29 13:21:42 10splitTasks](trainer.py 286): INFO [60/157]	0.1026(0.1113)	0.0003(0.0083)	0.172(0.205)	93.75(92.98)
[2023-09-29 13:21:43 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1102)	0.0002(0.0072)	0.032(0.201)	100.00(93.22)
[2023-09-29 13:21:44 10splitTasks](trainer.py 286): INFO [80/157]	0.1042(0.1095)	0.0006(0.0063)	0.221(0.199)	90.62(93.17)
[2023-09-29 13:21:45 10splitTasks](trainer.py 286): INFO [90/157]	0.1088(0.1091)	0.0007(0.0057)	0.084(0.202)	100.00(92.82)
[2023-09-29 13:21:46 10splitTasks](trainer.py 286): INFO [100/157]	0.1144(0.1087)	0.0006(0.0051)	0.119(0.199)	96.88(92.85)
[2023-09-29 13:21:47 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1083)	0.0003(0.0047)	0.150(0.203)	90.62(92.74)
[2023-09-29 13:21:48 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1079)	0.0002(0.0044)	0.338(0.206)	87.50(92.79)
[2023-09-29 13:21:49 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1074)	0.0001(0.0040)	0.222(0.207)	87.50(92.89)
[2023-09-29 13:21:50 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1070)	0.0002(0.0038)	0.130(0.206)	96.88(92.93)
[2023-09-29 13:21:51 10splitTasks](trainer.py 286): INFO [150/157]	0.1068(0.1068)	0.0001(0.0035)	0.468(0.206)	84.38(92.94)
[2023-09-29 13:21:51 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1064)	0.0001(0.0034)	0.114(0.207)	100.00(92.94)
[2023-09-29 13:21:52 10splitTasks](trainer.py 288): INFO  * Train Acc 92.940
[2023-09-29 13:21:53 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.200, Total time 1.71
[2023-09-29 13:21:53 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 13:21:53 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 13:21:53 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:21:54 10splitTasks](trainer.py 286): INFO [0/157]	0.5966(0.5966)	0.4896(0.4896)	0.260(0.260)	87.50(87.50)
[2023-09-29 13:21:55 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1484)	0.0002(0.0447)	0.301(0.197)	93.75(94.32)
[2023-09-29 13:21:56 10splitTasks](trainer.py 286): INFO [20/157]	0.1062(0.1270)	0.0006(0.0236)	0.093(0.178)	100.00(94.49)
[2023-09-29 13:21:57 10splitTasks](trainer.py 286): INFO [30/157]	0.1040(0.1198)	0.0002(0.0161)	0.366(0.187)	93.75(93.95)
[2023-09-29 13:21:58 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1155)	0.0002(0.0123)	0.073(0.184)	96.88(93.90)
[2023-09-29 13:21:59 10splitTasks](trainer.py 286): INFO [50/157]	0.1040(0.1132)	0.0004(0.0100)	0.454(0.190)	87.50(93.75)
[2023-09-29 13:22:00 10splitTasks](trainer.py 286): INFO [60/157]	0.1030(0.1113)	0.0002(0.0084)	0.175(0.181)	87.50(93.85)
[2023-09-29 13:22:01 10splitTasks](trainer.py 286): INFO [70/157]	0.1038(0.1101)	0.0003(0.0072)	0.131(0.179)	96.88(93.75)
[2023-09-29 13:22:02 10splitTasks](trainer.py 286): INFO [80/157]	0.1042(0.1095)	0.0002(0.0064)	0.157(0.177)	96.88(93.79)
[2023-09-29 13:22:03 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1086)	0.0002(0.0057)	0.193(0.175)	90.62(93.89)
[2023-09-29 13:22:04 10splitTasks](trainer.py 286): INFO [100/157]	0.1043(0.1080)	0.0003(0.0052)	0.468(0.177)	90.62(93.87)
[2023-09-29 13:22:05 10splitTasks](trainer.py 286): INFO [110/157]	0.1037(0.1074)	0.0003(0.0048)	0.295(0.180)	84.38(93.72)
[2023-09-29 13:22:06 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1070)	0.0003(0.0044)	0.149(0.182)	93.75(93.67)
[2023-09-29 13:22:07 10splitTasks](trainer.py 286): INFO [130/157]	0.1042(0.1068)	0.0003(0.0041)	0.229(0.182)	93.75(93.61)
[2023-09-29 13:22:08 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1065)	0.0003(0.0038)	0.069(0.180)	96.88(93.68)
[2023-09-29 13:22:09 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1063)	0.0001(0.0036)	0.116(0.182)	100.00(93.54)
[2023-09-29 13:22:10 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1059)	0.0001(0.0035)	0.228(0.182)	87.50(93.54)
[2023-09-29 13:22:10 10splitTasks](trainer.py 288): INFO  * Train Acc 93.540
[2023-09-29 13:22:12 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.200, Total time 1.66
[2023-09-29 13:22:12 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 13:22:12 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 13:22:12 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:22:12 10splitTasks](trainer.py 286): INFO [0/157]	0.6744(0.6744)	0.5571(0.5571)	0.029(0.029)	100.00(100.00)
[2023-09-29 13:22:13 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1555)	0.0003(0.0509)	0.135(0.108)	93.75(96.88)
[2023-09-29 13:22:14 10splitTasks](trainer.py 286): INFO [20/157]	0.1041(0.1314)	0.0002(0.0268)	0.072(0.130)	96.88(95.68)
[2023-09-29 13:22:15 10splitTasks](trainer.py 286): INFO [30/157]	0.1052(0.1227)	0.0002(0.0183)	0.174(0.138)	96.88(95.36)
[2023-09-29 13:22:16 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1178)	0.0002(0.0139)	0.119(0.150)	93.75(94.97)
[2023-09-29 13:22:18 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1148)	0.0002(0.0112)	0.049(0.157)	100.00(94.79)
[2023-09-29 13:22:19 10splitTasks](trainer.py 286): INFO [60/157]	0.1147(0.1130)	0.0008(0.0094)	0.070(0.154)	96.88(94.77)
[2023-09-29 13:22:20 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1116)	0.0003(0.0082)	0.171(0.150)	93.75(94.85)
[2023-09-29 13:22:21 10splitTasks](trainer.py 286): INFO [80/157]	0.1024(0.1105)	0.0002(0.0072)	0.257(0.162)	90.62(94.60)
[2023-09-29 13:22:22 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1097)	0.0003(0.0064)	0.122(0.166)	96.88(94.47)
[2023-09-29 13:22:23 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1091)	0.0002(0.0058)	0.116(0.172)	93.75(94.28)
[2023-09-29 13:22:24 10splitTasks](trainer.py 286): INFO [110/157]	0.1033(0.1085)	0.0002(0.0053)	0.108(0.170)	96.88(94.26)
[2023-09-29 13:22:25 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1080)	0.0003(0.0049)	0.065(0.165)	96.88(94.45)
[2023-09-29 13:22:26 10splitTasks](trainer.py 286): INFO [130/157]	0.1030(0.1076)	0.0002(0.0046)	0.139(0.168)	93.75(94.32)
[2023-09-29 13:22:27 10splitTasks](trainer.py 286): INFO [140/157]	0.1044(0.1072)	0.0003(0.0043)	0.396(0.172)	90.62(94.13)
[2023-09-29 13:22:28 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1070)	0.0001(0.0041)	0.166(0.172)	90.62(94.12)
[2023-09-29 13:22:28 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1066)	0.0001(0.0039)	0.799(0.173)	87.50(94.16)
[2023-09-29 13:22:28 10splitTasks](trainer.py 288): INFO  * Train Acc 94.160
[2023-09-29 13:22:30 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.70
[2023-09-29 13:22:30 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 13:22:30 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 13:22:30 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:22:31 10splitTasks](trainer.py 286): INFO [0/157]	0.6542(0.6542)	0.5394(0.5394)	0.271(0.271)	90.62(90.62)
[2023-09-29 13:22:32 10splitTasks](trainer.py 286): INFO [10/157]	0.1053(0.1545)	0.0001(0.0493)	0.188(0.159)	93.75(94.60)
[2023-09-29 13:22:33 10splitTasks](trainer.py 286): INFO [20/157]	0.1026(0.1303)	0.0002(0.0259)	0.112(0.157)	96.88(94.64)
[2023-09-29 13:22:34 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1216)	0.0002(0.0176)	0.044(0.159)	100.00(94.35)
[2023-09-29 13:22:35 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1168)	0.0002(0.0134)	0.230(0.158)	93.75(94.51)
[2023-09-29 13:22:36 10splitTasks](trainer.py 286): INFO [50/157]	0.1030(0.1141)	0.0002(0.0108)	0.126(0.164)	93.75(94.18)
[2023-09-29 13:22:37 10splitTasks](trainer.py 286): INFO [60/157]	0.1025(0.1124)	0.0003(0.0091)	0.105(0.167)	93.75(93.90)
[2023-09-29 13:22:38 10splitTasks](trainer.py 286): INFO [70/157]	0.1063(0.1110)	0.0003(0.0079)	0.074(0.160)	96.88(94.23)
[2023-09-29 13:22:39 10splitTasks](trainer.py 286): INFO [80/157]	0.1028(0.1102)	0.0003(0.0069)	0.165(0.158)	93.75(94.44)
[2023-09-29 13:22:40 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1094)	0.0002(0.0062)	0.169(0.157)	93.75(94.51)
[2023-09-29 13:22:41 10splitTasks](trainer.py 286): INFO [100/157]	0.1082(0.1088)	0.0002(0.0056)	0.077(0.151)	96.88(94.77)
[2023-09-29 13:22:42 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1083)	0.0003(0.0051)	0.163(0.151)	93.75(94.68)
[2023-09-29 13:22:43 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1078)	0.0003(0.0047)	0.267(0.154)	87.50(94.55)
[2023-09-29 13:22:44 10splitTasks](trainer.py 286): INFO [130/157]	0.1023(0.1074)	0.0002(0.0044)	0.142(0.152)	96.88(94.66)
[2023-09-29 13:22:45 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1070)	0.0002(0.0041)	0.118(0.152)	93.75(94.66)
[2023-09-29 13:22:46 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1067)	0.0002(0.0039)	0.128(0.151)	96.88(94.72)
[2023-09-29 13:22:47 10splitTasks](trainer.py 286): INFO [156/157]	0.0800(0.1063)	0.0001(0.0037)	0.148(0.151)	87.50(94.74)
[2023-09-29 13:22:47 10splitTasks](trainer.py 288): INFO  * Train Acc 94.740
[2023-09-29 13:22:49 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.800, Total time 1.71
[2023-09-29 13:22:49 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 13:22:49 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 13:22:49 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:22:49 10splitTasks](trainer.py 286): INFO [0/157]	0.6102(0.6102)	0.5048(0.5048)	0.023(0.023)	100.00(100.00)
[2023-09-29 13:22:50 10splitTasks](trainer.py 286): INFO [10/157]	0.1048(0.1511)	0.0002(0.0477)	0.232(0.107)	90.62(96.31)
[2023-09-29 13:22:51 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1278)	0.0001(0.0251)	0.043(0.119)	100.00(95.98)
[2023-09-29 13:22:52 10splitTasks](trainer.py 286): INFO [30/157]	0.1060(0.1204)	0.0003(0.0171)	0.089(0.126)	96.88(95.46)
[2023-09-29 13:22:54 10splitTasks](trainer.py 286): INFO [40/157]	0.1064(0.1169)	0.0002(0.0130)	0.035(0.121)	100.00(95.73)
[2023-09-29 13:22:55 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1141)	0.0002(0.0105)	0.219(0.129)	93.75(95.28)
[2023-09-29 13:22:56 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1122)	0.0004(0.0089)	0.050(0.133)	100.00(95.24)
[2023-09-29 13:22:57 10splitTasks](trainer.py 286): INFO [70/157]	0.1025(0.1109)	0.0003(0.0077)	0.183(0.134)	93.75(95.16)
[2023-09-29 13:22:58 10splitTasks](trainer.py 286): INFO [80/157]	0.1052(0.1099)	0.0002(0.0067)	0.086(0.134)	100.00(95.33)
[2023-09-29 13:22:59 10splitTasks](trainer.py 286): INFO [90/157]	0.1046(0.1094)	0.0002(0.0060)	0.170(0.133)	93.75(95.26)
[2023-09-29 13:23:00 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1089)	0.0002(0.0055)	0.014(0.136)	100.00(95.14)
[2023-09-29 13:23:01 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1083)	0.0003(0.0050)	0.094(0.135)	93.75(95.16)
[2023-09-29 13:23:02 10splitTasks](trainer.py 286): INFO [120/157]	0.1034(0.1079)	0.0002(0.0046)	0.345(0.136)	93.75(95.20)
[2023-09-29 13:23:03 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1075)	0.0002(0.0043)	0.274(0.135)	90.62(95.18)
[2023-09-29 13:23:04 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1073)	0.0002(0.0040)	0.138(0.135)	96.88(95.19)
[2023-09-29 13:23:05 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1071)	0.0002(0.0038)	0.257(0.136)	90.62(95.12)
[2023-09-29 13:23:05 10splitTasks](trainer.py 286): INFO [156/157]	0.0805(0.1067)	0.0001(0.0036)	0.746(0.139)	62.50(95.02)
[2023-09-29 13:23:06 10splitTasks](trainer.py 288): INFO  * Train Acc 95.020
[2023-09-29 13:23:07 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.70
[2023-09-29 13:23:07 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 13:23:07 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 13:23:07 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:23:08 10splitTasks](trainer.py 286): INFO [0/157]	0.6323(0.6323)	0.5184(0.5184)	0.284(0.284)	90.62(90.62)
[2023-09-29 13:23:09 10splitTasks](trainer.py 286): INFO [10/157]	0.1025(0.1546)	0.0003(0.0474)	0.290(0.180)	90.62(94.32)
[2023-09-29 13:23:10 10splitTasks](trainer.py 286): INFO [20/157]	0.1046(0.1317)	0.0002(0.0250)	0.194(0.155)	96.88(95.09)
[2023-09-29 13:23:11 10splitTasks](trainer.py 286): INFO [30/157]	0.1032(0.1222)	0.0002(0.0170)	0.272(0.154)	93.75(95.06)
[2023-09-29 13:23:12 10splitTasks](trainer.py 286): INFO [40/157]	0.1057(0.1174)	0.0003(0.0129)	0.172(0.162)	93.75(94.66)
[2023-09-29 13:23:13 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1146)	0.0002(0.0105)	0.203(0.157)	90.62(95.04)
[2023-09-29 13:23:14 10splitTasks](trainer.py 286): INFO [60/157]	0.1030(0.1132)	0.0004(0.0088)	0.144(0.153)	90.62(94.93)
[2023-09-29 13:23:15 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1117)	0.0002(0.0076)	0.066(0.152)	100.00(94.81)
[2023-09-29 13:23:16 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1105)	0.0002(0.0067)	0.221(0.150)	93.75(94.83)
[2023-09-29 13:23:17 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1095)	0.0002(0.0060)	0.157(0.148)	96.88(94.85)
[2023-09-29 13:23:18 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1089)	0.0002(0.0054)	0.153(0.147)	93.75(94.86)
[2023-09-29 13:23:19 10splitTasks](trainer.py 286): INFO [110/157]	0.1028(0.1085)	0.0004(0.0050)	0.141(0.145)	96.88(94.99)
[2023-09-29 13:23:20 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1082)	0.0002(0.0046)	0.037(0.141)	100.00(95.17)
[2023-09-29 13:23:21 10splitTasks](trainer.py 286): INFO [130/157]	0.1049(0.1079)	0.0002(0.0043)	0.147(0.138)	93.75(95.30)
[2023-09-29 13:23:22 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1075)	0.0002(0.0040)	0.030(0.140)	100.00(95.30)
[2023-09-29 13:23:24 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1073)	0.0001(0.0037)	0.030(0.138)	100.00(95.43)
[2023-09-29 13:23:24 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1069)	0.0001(0.0036)	0.173(0.138)	100.00(95.38)
[2023-09-29 13:23:24 10splitTasks](trainer.py 288): INFO  * Train Acc 95.380
[2023-09-29 13:23:26 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.400, Total time 1.72
[2023-09-29 13:23:26 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 13:23:26 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 13:23:26 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:23:27 10splitTasks](trainer.py 286): INFO [0/157]	0.6035(0.6035)	0.4787(0.4787)	0.353(0.353)	84.38(84.38)
[2023-09-29 13:23:28 10splitTasks](trainer.py 286): INFO [10/157]	0.1090(0.1485)	0.0002(0.0438)	0.306(0.151)	90.62(95.17)
[2023-09-29 13:23:29 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1270)	0.0002(0.0231)	0.048(0.130)	100.00(95.54)
[2023-09-29 13:23:30 10splitTasks](trainer.py 286): INFO [30/157]	0.1055(0.1196)	0.0002(0.0158)	0.218(0.140)	90.62(95.46)
[2023-09-29 13:23:31 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1153)	0.0003(0.0121)	0.244(0.136)	93.75(95.58)
[2023-09-29 13:23:32 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1126)	0.0003(0.0098)	0.186(0.138)	90.62(95.40)
[2023-09-29 13:23:33 10splitTasks](trainer.py 286): INFO [60/157]	0.1061(0.1112)	0.0002(0.0082)	0.240(0.135)	93.75(95.54)
[2023-09-29 13:23:34 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1103)	0.0002(0.0071)	0.066(0.131)	100.00(95.69)
[2023-09-29 13:23:35 10splitTasks](trainer.py 286): INFO [80/157]	0.1057(0.1100)	0.0006(0.0063)	0.275(0.131)	90.62(95.64)
[2023-09-29 13:23:36 10splitTasks](trainer.py 286): INFO [90/157]	0.1050(0.1092)	0.0035(0.0057)	0.055(0.127)	100.00(95.74)
[2023-09-29 13:23:37 10splitTasks](trainer.py 286): INFO [100/157]	0.1028(0.1085)	0.0003(0.0051)	0.163(0.125)	93.75(95.76)
[2023-09-29 13:23:38 10splitTasks](trainer.py 286): INFO [110/157]	0.1022(0.1079)	0.0003(0.0047)	0.214(0.126)	93.75(95.75)
[2023-09-29 13:23:39 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1076)	0.0003(0.0043)	0.095(0.126)	96.88(95.84)
[2023-09-29 13:23:40 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1072)	0.0003(0.0040)	0.082(0.127)	96.88(95.80)
[2023-09-29 13:23:41 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1069)	0.0002(0.0038)	0.186(0.126)	93.75(95.79)
[2023-09-29 13:23:42 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1067)	0.0001(0.0035)	0.195(0.125)	90.62(95.84)
[2023-09-29 13:23:43 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1063)	0.0001(0.0034)	0.683(0.128)	75.00(95.70)
[2023-09-29 13:23:43 10splitTasks](trainer.py 288): INFO  * Train Acc 95.700
[2023-09-29 13:23:44 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.800, Total time 1.68
[2023-09-29 13:23:44 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 13:23:44 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 13:23:44 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:23:45 10splitTasks](trainer.py 286): INFO [0/157]	0.6260(0.6260)	0.5018(0.5018)	0.109(0.109)	93.75(93.75)
[2023-09-29 13:23:46 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1599)	0.0003(0.0559)	0.173(0.077)	90.62(96.88)
[2023-09-29 13:23:47 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1327)	0.0002(0.0294)	0.051(0.076)	100.00(97.32)
[2023-09-29 13:23:48 10splitTasks](trainer.py 286): INFO [30/157]	0.1025(0.1231)	0.0002(0.0200)	0.125(0.087)	93.75(96.77)
[2023-09-29 13:23:49 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1180)	0.0001(0.0152)	0.160(0.086)	93.75(96.80)
[2023-09-29 13:23:50 10splitTasks](trainer.py 286): INFO [50/157]	0.1009(0.1147)	0.0002(0.0123)	0.205(0.095)	90.62(96.51)
[2023-09-29 13:23:51 10splitTasks](trainer.py 286): INFO [60/157]	0.1124(0.1132)	0.0006(0.0103)	0.198(0.110)	96.88(95.85)
[2023-09-29 13:23:52 10splitTasks](trainer.py 286): INFO [70/157]	0.1009(0.1117)	0.0001(0.0089)	0.020(0.104)	100.00(96.13)
[2023-09-29 13:23:53 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1106)	0.0002(0.0078)	0.381(0.115)	87.50(95.60)
[2023-09-29 13:23:54 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1096)	0.0004(0.0070)	0.054(0.112)	96.88(95.74)
[2023-09-29 13:23:55 10splitTasks](trainer.py 286): INFO [100/157]	0.1010(0.1087)	0.0001(0.0063)	0.065(0.111)	100.00(95.76)
[2023-09-29 13:23:56 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1081)	0.0002(0.0058)	0.073(0.113)	100.00(95.78)
[2023-09-29 13:23:57 10splitTasks](trainer.py 286): INFO [120/157]	0.1011(0.1075)	0.0003(0.0053)	0.197(0.114)	93.75(95.69)
[2023-09-29 13:23:58 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1072)	0.0002(0.0049)	0.107(0.116)	93.75(95.54)
[2023-09-29 13:23:59 10splitTasks](trainer.py 286): INFO [140/157]	0.1029(0.1069)	0.0002(0.0046)	0.070(0.118)	96.88(95.52)
[2023-09-29 13:24:01 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1066)	0.0001(0.0043)	0.148(0.117)	96.88(95.59)
[2023-09-29 13:24:01 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1062)	0.0001(0.0042)	0.262(0.118)	75.00(95.52)
[2023-09-29 13:24:01 10splitTasks](trainer.py 288): INFO  * Train Acc 95.520
[2023-09-29 13:24:03 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.800, Total time 1.68
[2023-09-29 13:24:03 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 13:24:03 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 13:24:03 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:24:04 10splitTasks](trainer.py 286): INFO [0/157]	0.6269(0.6269)	0.5081(0.5081)	0.143(0.143)	96.88(96.88)
[2023-09-29 13:24:05 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1510)	0.0002(0.0465)	0.144(0.111)	93.75(97.16)
[2023-09-29 13:24:06 10splitTasks](trainer.py 286): INFO [20/157]	0.1085(0.1300)	0.0002(0.0245)	0.212(0.111)	96.88(96.73)
[2023-09-29 13:24:07 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1211)	0.0001(0.0167)	0.124(0.122)	93.75(95.97)
[2023-09-29 13:24:08 10splitTasks](trainer.py 286): INFO [40/157]	0.1051(0.1168)	0.0002(0.0127)	0.112(0.118)	96.88(96.19)
[2023-09-29 13:24:09 10splitTasks](trainer.py 286): INFO [50/157]	0.1029(0.1143)	0.0004(0.0103)	0.029(0.115)	100.00(95.96)
[2023-09-29 13:24:10 10splitTasks](trainer.py 286): INFO [60/157]	0.1057(0.1126)	0.0002(0.0086)	0.037(0.110)	100.00(96.21)
[2023-09-29 13:24:11 10splitTasks](trainer.py 286): INFO [70/157]	0.1048(0.1114)	0.0002(0.0075)	0.037(0.113)	100.00(96.08)
[2023-09-29 13:24:12 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1106)	0.0002(0.0066)	0.129(0.115)	96.88(96.14)
[2023-09-29 13:24:13 10splitTasks](trainer.py 286): INFO [90/157]	0.1036(0.1100)	0.0003(0.0059)	0.113(0.117)	96.88(96.12)
[2023-09-29 13:24:14 10splitTasks](trainer.py 286): INFO [100/157]	0.1119(0.1094)	0.0003(0.0054)	0.213(0.119)	93.75(96.01)
[2023-09-29 13:24:15 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1088)	0.0002(0.0049)	0.059(0.117)	96.88(96.03)
[2023-09-29 13:24:16 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1085)	0.0002(0.0045)	0.040(0.116)	100.00(96.10)
[2023-09-29 13:24:17 10splitTasks](trainer.py 286): INFO [130/157]	0.1044(0.1081)	0.0002(0.0042)	0.018(0.115)	100.00(96.18)
[2023-09-29 13:24:18 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1077)	0.0002(0.0039)	0.027(0.112)	100.00(96.37)
[2023-09-29 13:24:19 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1074)	0.0001(0.0037)	0.162(0.113)	93.75(96.34)
[2023-09-29 13:24:20 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1070)	0.0001(0.0035)	0.088(0.115)	100.00(96.18)
[2023-09-29 13:24:20 10splitTasks](trainer.py 288): INFO  * Train Acc 96.180
[2023-09-29 13:24:22 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.70
[2023-09-29 13:24:22 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 13:24:22 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 13:24:22 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:24:22 10splitTasks](trainer.py 286): INFO [0/157]	0.6386(0.6386)	0.5199(0.5199)	0.295(0.295)	93.75(93.75)
[2023-09-29 13:24:23 10splitTasks](trainer.py 286): INFO [10/157]	0.1023(0.1528)	0.0002(0.0476)	0.033(0.129)	100.00(96.02)
[2023-09-29 13:24:24 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1283)	0.0002(0.0250)	0.054(0.136)	96.88(95.39)
[2023-09-29 13:24:25 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1207)	0.0002(0.0171)	0.130(0.126)	96.88(95.97)
[2023-09-29 13:24:26 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1166)	0.0003(0.0130)	0.212(0.123)	90.62(95.88)
[2023-09-29 13:24:27 10splitTasks](trainer.py 286): INFO [50/157]	0.1034(0.1142)	0.0004(0.0105)	0.046(0.125)	96.88(95.71)
[2023-09-29 13:24:28 10splitTasks](trainer.py 286): INFO [60/157]	0.1151(0.1127)	0.0003(0.0088)	0.103(0.120)	96.88(96.11)
[2023-09-29 13:24:29 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1114)	0.0002(0.0076)	0.103(0.118)	96.88(96.04)
[2023-09-29 13:24:30 10splitTasks](trainer.py 286): INFO [80/157]	0.1032(0.1103)	0.0002(0.0067)	0.224(0.119)	90.62(96.03)
[2023-09-29 13:24:31 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1094)	0.0002(0.0060)	0.111(0.115)	93.75(96.15)
[2023-09-29 13:24:33 10splitTasks](trainer.py 286): INFO [100/157]	0.1048(0.1089)	0.0003(0.0054)	0.030(0.118)	100.00(96.19)
[2023-09-29 13:24:34 10splitTasks](trainer.py 286): INFO [110/157]	0.1038(0.1084)	0.0006(0.0050)	0.154(0.117)	93.75(96.14)
[2023-09-29 13:24:35 10splitTasks](trainer.py 286): INFO [120/157]	0.1011(0.1081)	0.0002(0.0046)	0.139(0.116)	96.88(96.13)
[2023-09-29 13:24:36 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1076)	0.0002(0.0043)	0.088(0.114)	100.00(96.18)
[2023-09-29 13:24:37 10splitTasks](trainer.py 286): INFO [140/157]	0.1046(0.1074)	0.0002(0.0040)	0.041(0.113)	100.00(96.14)
[2023-09-29 13:24:38 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1070)	0.0002(0.0038)	0.095(0.113)	100.00(96.21)
[2023-09-29 13:24:38 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1067)	0.0001(0.0037)	0.057(0.113)	100.00(96.22)
[2023-09-29 13:24:38 10splitTasks](trainer.py 288): INFO  * Train Acc 96.220
[2023-09-29 13:24:40 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.73
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-3.pth
=> Save Done
[2023-09-29 13:24:40 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 13:24:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.74
[2023-09-29 13:24:42 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 13:24:44 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.77
[2023-09-29 13:24:44 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 13:24:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.81
[2023-09-29 13:24:46 10splitTasks](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 13:24:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.79
[2023-09-29 13:24:47 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 13:24:48 10splitTasks](trainer.py 341): INFO done
[2023-09-29 13:24:48 10splitTasks](iBatchLearn.py 155): INFO Acc:81.65; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:24:51 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:24:51 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:24:51 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 3, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-3.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-3.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_3.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:24:52 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-3.pth
[2023-09-29 13:24:52 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:24:54 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:24:55 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:24:55 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 4, 4, 4],
        [3, 3, 2, 4, 0, 4, 4],
        [4, 3, 3, 0, 0, 4, 4],
        [4, 3, 0, 0, 4, 4, 4],
        [4, 0, 0, 0, 4, 0, 0],
        [0, 0, 0, 4, 2, 2, 0],
        [0, 3, 0, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 13:24:55 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:24:55 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:24:55 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 13:25:00 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 13:25:03 10splitTasks](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 13:25:06 10splitTasks](iBatchLearn.py 167): INFO test split name:3
--------------------------------Official Evaluation--------------------------------
3 81.025
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:25:14 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:25:14 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:25:14 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 4, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-3.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-4.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-3.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-4.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:25:15 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-3.pth
[2023-09-29 13:25:15 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:25:17 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:25:17 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:25:17 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:25:18 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:25:18 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 13:25:18 10splitTasks](iBatchLearn.py 92): INFO ====================== 4 =======================
[2023-09-29 13:25:18 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 13:25:18 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 13:25:18 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 13:25:18 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:25:21 10splitTasks](trainer.py 286): INFO [0/157]	3.7314(3.7314)	0.5525(0.5525)	2.371(2.371)	6.25(6.25)
[2023-09-29 13:25:22 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.4317)	0.0003(0.0505)	2.149(2.253)	21.88(14.49)
[2023-09-29 13:25:23 10splitTasks](trainer.py 286): INFO [20/157]	0.1027(0.2752)	0.0002(0.0266)	1.797(2.141)	34.38(20.98)
[2023-09-29 13:25:24 10splitTasks](trainer.py 286): INFO [30/157]	0.1063(0.2194)	0.0003(0.0181)	1.596(1.997)	34.38(27.42)
[2023-09-29 13:25:25 10splitTasks](trainer.py 286): INFO [40/157]	0.1004(0.1906)	0.0002(0.0137)	1.613(1.884)	56.25(32.85)
[2023-09-29 13:25:26 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1732)	0.0002(0.0111)	1.383(1.804)	59.38(35.29)
[2023-09-29 13:25:27 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1616)	0.0002(0.0093)	1.413(1.741)	46.88(37.55)
[2023-09-29 13:25:28 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1532)	0.0002(0.0081)	1.307(1.681)	43.75(39.39)
[2023-09-29 13:25:29 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1469)	0.0002(0.0071)	0.942(1.624)	71.88(41.98)
[2023-09-29 13:25:30 10splitTasks](trainer.py 286): INFO [90/157]	0.1010(0.1420)	0.0002(0.0064)	1.468(1.588)	46.88(43.44)
[2023-09-29 13:25:31 10splitTasks](trainer.py 286): INFO [100/157]	0.1007(0.1379)	0.0002(0.0058)	1.093(1.551)	62.50(44.55)
[2023-09-29 13:25:32 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1346)	0.0003(0.0053)	1.178(1.512)	53.12(45.64)
[2023-09-29 13:25:34 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1319)	0.0004(0.0049)	0.748(1.467)	71.88(47.47)
[2023-09-29 13:25:35 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1301)	0.0002(0.0045)	0.828(1.430)	68.75(48.50)
[2023-09-29 13:25:36 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1281)	0.0002(0.0042)	0.752(1.399)	81.25(49.87)
[2023-09-29 13:25:37 10splitTasks](trainer.py 286): INFO [150/157]	0.1005(0.1263)	0.0001(0.0040)	0.940(1.368)	71.88(51.12)
[2023-09-29 13:25:37 10splitTasks](trainer.py 286): INFO [156/157]	0.0863(0.1252)	0.0001(0.0038)	1.348(1.356)	50.00(51.62)
[2023-09-29 13:25:37 10splitTasks](trainer.py 288): INFO  * Train Acc 51.620
[2023-09-29 13:25:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 62.400, Total time 1.70
[2023-09-29 13:25:39 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 13:25:39 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 13:25:39 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:25:40 10splitTasks](trainer.py 286): INFO [0/157]	0.5968(0.5968)	0.4888(0.4888)	0.794(0.794)	75.00(75.00)
[2023-09-29 13:25:41 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1497)	0.0002(0.0463)	1.125(1.073)	62.50(64.77)
[2023-09-29 13:25:42 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1272)	0.0003(0.0244)	0.550(1.032)	81.25(63.84)
[2023-09-29 13:25:43 10splitTasks](trainer.py 286): INFO [30/157]	0.1010(0.1188)	0.0001(0.0166)	1.258(1.055)	59.38(63.91)
[2023-09-29 13:25:44 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1147)	0.0003(0.0126)	0.859(1.077)	75.00(63.03)
[2023-09-29 13:25:45 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1122)	0.0002(0.0102)	0.898(1.062)	71.88(63.30)
[2023-09-29 13:25:46 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1105)	0.0003(0.0086)	0.726(1.061)	75.00(63.73)
[2023-09-29 13:25:47 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1095)	0.0002(0.0074)	1.059(1.039)	62.50(64.61)
[2023-09-29 13:25:48 10splitTasks](trainer.py 286): INFO [80/157]	0.1103(0.1087)	0.0085(0.0066)	0.976(1.030)	65.62(64.51)
[2023-09-29 13:25:49 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1080)	0.0002(0.0059)	0.680(1.018)	81.25(65.18)
[2023-09-29 13:25:50 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1074)	0.0003(0.0054)	0.953(1.013)	65.62(64.73)
[2023-09-29 13:25:51 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1070)	0.0003(0.0049)	0.470(1.004)	78.12(64.98)
[2023-09-29 13:25:52 10splitTasks](trainer.py 286): INFO [120/157]	0.1011(0.1066)	0.0002(0.0045)	1.183(0.997)	59.38(65.37)
[2023-09-29 13:25:53 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1063)	0.0003(0.0042)	1.129(0.989)	65.62(65.70)
[2023-09-29 13:25:54 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1061)	0.0003(0.0040)	0.977(0.981)	65.62(66.00)
[2023-09-29 13:25:55 10splitTasks](trainer.py 286): INFO [150/157]	0.1022(0.1059)	0.0001(0.0037)	1.099(0.981)	68.75(66.02)
[2023-09-29 13:25:56 10splitTasks](trainer.py 286): INFO [156/157]	0.0813(0.1056)	0.0001(0.0036)	0.463(0.980)	75.00(66.00)
[2023-09-29 13:25:56 10splitTasks](trainer.py 288): INFO  * Train Acc 66.000
[2023-09-29 13:25:57 10splitTasks](my_trainer.py 503): INFO  * Val Acc 64.000, Total time 1.68
[2023-09-29 13:25:57 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 13:25:57 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 13:25:57 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:25:58 10splitTasks](trainer.py 286): INFO [0/157]	0.6275(0.6275)	0.4952(0.4952)	0.974(0.974)	53.12(53.12)
[2023-09-29 13:25:59 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1512)	0.0004(0.0453)	1.003(0.904)	75.00(70.45)
[2023-09-29 13:26:00 10splitTasks](trainer.py 286): INFO [20/157]	0.1178(0.1284)	0.0006(0.0239)	0.712(0.920)	78.12(69.49)
[2023-09-29 13:26:01 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1203)	0.0002(0.0163)	1.088(0.951)	68.75(69.56)
[2023-09-29 13:26:02 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1161)	0.0002(0.0124)	0.881(0.943)	71.88(68.75)
[2023-09-29 13:26:03 10splitTasks](trainer.py 286): INFO [50/157]	0.1012(0.1134)	0.0003(0.0100)	0.777(0.936)	65.62(68.57)
[2023-09-29 13:26:04 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1115)	0.0002(0.0084)	0.972(0.915)	62.50(69.21)
[2023-09-29 13:26:05 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1101)	0.0003(0.0073)	1.089(0.931)	53.12(68.27)
[2023-09-29 13:26:06 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1090)	0.0001(0.0064)	1.242(0.932)	53.12(68.29)
[2023-09-29 13:26:07 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1082)	0.0002(0.0058)	0.968(0.929)	59.38(68.48)
[2023-09-29 13:26:08 10splitTasks](trainer.py 286): INFO [100/157]	0.1024(0.1075)	0.0002(0.0052)	0.723(0.931)	75.00(68.19)
[2023-09-29 13:26:09 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1070)	0.0003(0.0048)	1.139(0.923)	59.38(68.52)
[2023-09-29 13:26:10 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1068)	0.0002(0.0044)	0.765(0.909)	65.62(68.98)
[2023-09-29 13:26:11 10splitTasks](trainer.py 286): INFO [130/157]	0.1032(0.1064)	0.0003(0.0041)	0.924(0.904)	62.50(69.20)
[2023-09-29 13:26:12 10splitTasks](trainer.py 286): INFO [140/157]	0.1035(0.1061)	0.0002(0.0038)	0.770(0.897)	71.88(69.33)
[2023-09-29 13:26:13 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1058)	0.0001(0.0036)	0.982(0.907)	71.88(68.96)
[2023-09-29 13:26:14 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1055)	0.0001(0.0035)	1.073(0.907)	87.50(68.98)
[2023-09-29 13:26:14 10splitTasks](trainer.py 288): INFO  * Train Acc 68.980
[2023-09-29 13:26:16 10splitTasks](my_trainer.py 503): INFO  * Val Acc 69.800, Total time 1.72
[2023-09-29 13:26:16 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 13:26:16 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 13:26:16 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:26:16 10splitTasks](trainer.py 286): INFO [0/157]	0.7187(0.7187)	0.5940(0.5940)	0.803(0.803)	75.00(75.00)
[2023-09-29 13:26:18 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1639)	0.0002(0.0543)	1.103(0.844)	46.88(69.60)
[2023-09-29 13:26:19 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1341)	0.0004(0.0286)	1.321(0.947)	59.38(66.67)
[2023-09-29 13:26:20 10splitTasks](trainer.py 286): INFO [30/157]	0.1035(0.1238)	0.0002(0.0195)	0.604(0.891)	78.12(68.35)
[2023-09-29 13:26:21 10splitTasks](trainer.py 286): INFO [40/157]	0.1053(0.1184)	0.0003(0.0148)	0.627(0.848)	75.00(69.97)
[2023-09-29 13:26:22 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1153)	0.0002(0.0120)	0.863(0.826)	75.00(70.59)
[2023-09-29 13:26:23 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1133)	0.0003(0.0101)	0.882(0.824)	65.62(70.70)
[2023-09-29 13:26:24 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1118)	0.0003(0.0087)	0.613(0.812)	71.88(71.08)
[2023-09-29 13:26:25 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1105)	0.0002(0.0077)	0.866(0.801)	68.75(71.53)
[2023-09-29 13:26:26 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1095)	0.0001(0.0069)	0.738(0.800)	87.50(71.84)
[2023-09-29 13:26:27 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1088)	0.0002(0.0062)	0.562(0.793)	81.25(72.31)
[2023-09-29 13:26:28 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1083)	0.0002(0.0057)	0.762(0.788)	71.88(72.35)
[2023-09-29 13:26:29 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1080)	0.0002(0.0052)	0.644(0.781)	75.00(72.65)
[2023-09-29 13:26:30 10splitTasks](trainer.py 286): INFO [130/157]	0.1062(0.1077)	0.0005(0.0049)	0.913(0.785)	71.88(72.88)
[2023-09-29 13:26:31 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1073)	0.0003(0.0045)	0.471(0.784)	81.25(72.89)
[2023-09-29 13:26:32 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1070)	0.0001(0.0043)	0.844(0.785)	68.75(72.85)
[2023-09-29 13:26:32 10splitTasks](trainer.py 286): INFO [156/157]	0.0807(0.1066)	0.0001(0.0041)	0.657(0.787)	62.50(72.82)
[2023-09-29 13:26:33 10splitTasks](trainer.py 288): INFO  * Train Acc 72.820
[2023-09-29 13:26:34 10splitTasks](my_trainer.py 503): INFO  * Val Acc 70.800, Total time 1.62
[2023-09-29 13:26:34 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 13:26:34 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 13:26:34 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:26:35 10splitTasks](trainer.py 286): INFO [0/157]	0.5986(0.5986)	0.4807(0.4807)	0.501(0.501)	75.00(75.00)
[2023-09-29 13:26:36 10splitTasks](trainer.py 286): INFO [10/157]	0.1010(0.1474)	0.0002(0.0440)	0.582(0.643)	78.12(79.26)
[2023-09-29 13:26:37 10splitTasks](trainer.py 286): INFO [20/157]	0.1009(0.1257)	0.0002(0.0233)	0.483(0.634)	81.25(78.72)
[2023-09-29 13:26:38 10splitTasks](trainer.py 286): INFO [30/157]	0.1043(0.1180)	0.0003(0.0159)	0.440(0.644)	87.50(78.43)
[2023-09-29 13:26:39 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1140)	0.0002(0.0121)	0.557(0.618)	87.50(79.50)
[2023-09-29 13:26:40 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1118)	0.0002(0.0097)	0.507(0.642)	84.38(78.86)
[2023-09-29 13:26:41 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1102)	0.0003(0.0082)	1.034(0.637)	65.62(78.94)
[2023-09-29 13:26:42 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1090)	0.0002(0.0071)	0.715(0.642)	78.12(78.48)
[2023-09-29 13:26:43 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1082)	0.0002(0.0063)	0.593(0.639)	75.00(78.67)
[2023-09-29 13:26:44 10splitTasks](trainer.py 286): INFO [90/157]	0.1011(0.1074)	0.0001(0.0056)	1.217(0.655)	56.25(77.82)
[2023-09-29 13:26:45 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1069)	0.0002(0.0051)	0.809(0.658)	68.75(77.69)
[2023-09-29 13:26:46 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1064)	0.0002(0.0046)	0.824(0.661)	62.50(77.42)
[2023-09-29 13:26:47 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1061)	0.0003(0.0043)	0.707(0.661)	68.75(77.40)
[2023-09-29 13:26:48 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1058)	0.0002(0.0040)	1.108(0.662)	68.75(77.27)
[2023-09-29 13:26:49 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1055)	0.0001(0.0037)	0.563(0.653)	75.00(77.50)
[2023-09-29 13:26:50 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1052)	0.0001(0.0035)	0.997(0.652)	71.88(77.57)
[2023-09-29 13:26:51 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1049)	0.0001(0.0034)	0.920(0.651)	75.00(77.60)
[2023-09-29 13:26:51 10splitTasks](trainer.py 288): INFO  * Train Acc 77.600
[2023-09-29 13:26:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 70.800, Total time 1.58
[2023-09-29 13:26:52 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 13:26:52 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 13:26:52 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:26:53 10splitTasks](trainer.py 286): INFO [0/157]	0.5990(0.5990)	0.4788(0.4788)	0.613(0.613)	75.00(75.00)
[2023-09-29 13:26:54 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1494)	0.0003(0.0438)	0.427(0.691)	84.38(76.42)
[2023-09-29 13:26:55 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1272)	0.0003(0.0231)	0.942(0.635)	68.75(79.17)
[2023-09-29 13:26:56 10splitTasks](trainer.py 286): INFO [30/157]	0.1022(0.1192)	0.0003(0.0157)	0.544(0.627)	81.25(78.53)
[2023-09-29 13:26:57 10splitTasks](trainer.py 286): INFO [40/157]	0.1025(0.1152)	0.0006(0.0120)	0.423(0.601)	93.75(79.34)
[2023-09-29 13:26:58 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1131)	0.0003(0.0097)	0.765(0.582)	75.00(79.90)
[2023-09-29 13:26:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1065(0.1117)	0.0003(0.0082)	0.610(0.572)	75.00(80.38)
[2023-09-29 13:27:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1047(0.1105)	0.0005(0.0071)	0.445(0.563)	87.50(80.81)
[2023-09-29 13:27:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1095)	0.0003(0.0062)	0.545(0.556)	87.50(80.86)
[2023-09-29 13:27:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1047(0.1088)	0.0001(0.0056)	0.567(0.558)	75.00(80.67)
[2023-09-29 13:27:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1061(0.1081)	0.0002(0.0051)	0.399(0.553)	84.38(80.69)
[2023-09-29 13:27:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1041(0.1076)	0.0005(0.0046)	0.633(0.557)	84.38(80.86)
[2023-09-29 13:27:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1073)	0.0003(0.0043)	1.075(0.563)	71.88(80.81)
[2023-09-29 13:27:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1034(0.1069)	0.0002(0.0040)	0.312(0.558)	90.62(80.92)
[2023-09-29 13:27:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1067)	0.0003(0.0037)	0.348(0.550)	87.50(81.18)
[2023-09-29 13:27:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1064)	0.0001(0.0035)	0.633(0.547)	75.00(81.19)
[2023-09-29 13:27:09 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1060)	0.0001(0.0034)	0.304(0.550)	87.50(81.26)
[2023-09-29 13:27:09 10splitTasks](trainer.py 288): INFO  * Train Acc 81.260
[2023-09-29 13:27:11 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.400, Total time 1.64
[2023-09-29 13:27:11 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 13:27:11 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 13:27:11 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:27:12 10splitTasks](trainer.py 286): INFO [0/157]	0.7732(0.7732)	0.6688(0.6688)	0.231(0.231)	93.75(93.75)
[2023-09-29 13:27:13 10splitTasks](trainer.py 286): INFO [10/157]	0.1029(0.1646)	0.0003(0.0611)	0.459(0.362)	84.38(86.65)
[2023-09-29 13:27:14 10splitTasks](trainer.py 286): INFO [20/157]	0.1061(0.1350)	0.0003(0.0321)	0.280(0.407)	87.50(85.57)
[2023-09-29 13:27:15 10splitTasks](trainer.py 286): INFO [30/157]	0.1044(0.1247)	0.0002(0.0219)	0.728(0.411)	75.00(84.98)
[2023-09-29 13:27:16 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1194)	0.0002(0.0166)	0.556(0.425)	81.25(84.60)
[2023-09-29 13:27:17 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1159)	0.0003(0.0134)	0.325(0.422)	93.75(84.93)
[2023-09-29 13:27:18 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1137)	0.0003(0.0113)	0.439(0.422)	90.62(85.09)
[2023-09-29 13:27:19 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1125)	0.0003(0.0097)	0.297(0.425)	84.38(84.86)
[2023-09-29 13:27:20 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1112)	0.0003(0.0086)	0.211(0.415)	90.62(85.15)
[2023-09-29 13:27:21 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1107)	0.0003(0.0077)	0.628(0.412)	71.88(84.99)
[2023-09-29 13:27:22 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1098)	0.0002(0.0069)	0.422(0.411)	87.50(85.06)
[2023-09-29 13:27:23 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1091)	0.0003(0.0063)	0.191(0.418)	93.75(84.80)
[2023-09-29 13:27:24 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1085)	0.0003(0.0058)	0.388(0.426)	93.75(84.43)
[2023-09-29 13:27:25 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1081)	0.0002(0.0054)	0.362(0.425)	84.38(84.42)
[2023-09-29 13:27:26 10splitTasks](trainer.py 286): INFO [140/157]	0.1023(0.1078)	0.0002(0.0050)	0.587(0.435)	84.38(84.22)
[2023-09-29 13:27:27 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1074)	0.0001(0.0047)	0.422(0.433)	84.38(84.38)
[2023-09-29 13:27:28 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1070)	0.0001(0.0046)	0.559(0.433)	87.50(84.34)
[2023-09-29 13:27:28 10splitTasks](trainer.py 288): INFO  * Train Acc 84.340
[2023-09-29 13:27:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.000, Total time 1.59
[2023-09-29 13:27:29 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 13:27:29 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 13:27:29 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:27:30 10splitTasks](trainer.py 286): INFO [0/157]	0.6001(0.6001)	0.4732(0.4732)	0.454(0.454)	87.50(87.50)
[2023-09-29 13:27:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1501)	0.0003(0.0433)	0.423(0.345)	84.38(89.49)
[2023-09-29 13:27:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1009(0.1280)	0.0001(0.0228)	0.238(0.343)	93.75(88.39)
[2023-09-29 13:27:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1195)	0.0003(0.0155)	0.396(0.333)	93.75(88.81)
[2023-09-29 13:27:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1155)	0.0002(0.0118)	0.490(0.335)	81.25(88.49)
[2023-09-29 13:27:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1130)	0.0003(0.0096)	0.390(0.345)	87.50(88.30)
[2023-09-29 13:27:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1039(0.1115)	0.0002(0.0080)	0.334(0.341)	87.50(88.37)
[2023-09-29 13:27:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1103)	0.0003(0.0070)	0.241(0.333)	93.75(88.82)
[2023-09-29 13:27:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1093)	0.0002(0.0061)	0.339(0.325)	84.38(88.97)
[2023-09-29 13:27:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1086)	0.0002(0.0055)	0.285(0.330)	90.62(88.67)
[2023-09-29 13:27:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1080)	0.0003(0.0050)	0.209(0.336)	93.75(88.46)
[2023-09-29 13:27:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1078)	0.0002(0.0046)	0.473(0.335)	75.00(88.49)
[2023-09-29 13:27:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1073)	0.0001(0.0042)	0.224(0.331)	90.62(88.66)
[2023-09-29 13:27:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1010(0.1069)	0.0002(0.0039)	0.273(0.330)	93.75(88.74)
[2023-09-29 13:27:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1065)	0.0002(0.0037)	0.150(0.329)	96.88(88.87)
[2023-09-29 13:27:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1063)	0.0001(0.0035)	0.331(0.329)	87.50(88.89)
[2023-09-29 13:27:46 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1059)	0.0001(0.0033)	0.565(0.330)	87.50(88.78)
[2023-09-29 13:27:46 10splitTasks](trainer.py 288): INFO  * Train Acc 88.780
[2023-09-29 13:27:48 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.800, Total time 1.57
[2023-09-29 13:27:48 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 13:27:48 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 13:27:48 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:27:48 10splitTasks](trainer.py 286): INFO [0/157]	0.5902(0.5902)	0.4845(0.4845)	0.264(0.264)	90.62(90.62)
[2023-09-29 13:27:49 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1476)	0.0003(0.0444)	0.409(0.341)	90.62(91.48)
[2023-09-29 13:27:50 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1258)	0.0003(0.0234)	0.228(0.293)	87.50(91.37)
[2023-09-29 13:27:51 10splitTasks](trainer.py 286): INFO [30/157]	0.1029(0.1185)	0.0002(0.0159)	0.198(0.288)	93.75(90.73)
[2023-09-29 13:27:52 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1148)	0.0002(0.0121)	0.317(0.297)	87.50(90.02)
[2023-09-29 13:27:53 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1122)	0.0003(0.0098)	0.521(0.313)	81.25(89.40)
[2023-09-29 13:27:54 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1105)	0.0001(0.0082)	0.287(0.318)	90.62(89.14)
[2023-09-29 13:27:55 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1094)	0.0002(0.0071)	0.408(0.319)	84.38(88.95)
[2023-09-29 13:27:56 10splitTasks](trainer.py 286): INFO [80/157]	0.1010(0.1085)	0.0002(0.0063)	0.244(0.309)	93.75(89.35)
[2023-09-29 13:27:57 10splitTasks](trainer.py 286): INFO [90/157]	0.1128(0.1079)	0.0002(0.0056)	0.355(0.311)	84.38(89.42)
[2023-09-29 13:27:59 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1075)	0.0002(0.0051)	0.324(0.317)	90.62(89.33)
[2023-09-29 13:28:00 10splitTasks](trainer.py 286): INFO [110/157]	0.1031(0.1070)	0.0002(0.0047)	0.310(0.313)	90.62(89.36)
[2023-09-29 13:28:01 10splitTasks](trainer.py 286): INFO [120/157]	0.1012(0.1065)	0.0002(0.0043)	0.408(0.311)	84.38(89.59)
[2023-09-29 13:28:02 10splitTasks](trainer.py 286): INFO [130/157]	0.1163(0.1063)	0.0006(0.0040)	0.356(0.309)	84.38(89.60)
[2023-09-29 13:28:03 10splitTasks](trainer.py 286): INFO [140/157]	0.1010(0.1061)	0.0002(0.0037)	0.137(0.303)	93.75(89.76)
[2023-09-29 13:28:04 10splitTasks](trainer.py 286): INFO [150/157]	0.1049(0.1059)	0.0002(0.0035)	0.332(0.301)	81.25(89.78)
[2023-09-29 13:28:04 10splitTasks](trainer.py 286): INFO [156/157]	0.0801(0.1056)	0.0001(0.0034)	0.030(0.300)	100.00(89.82)
[2023-09-29 13:28:04 10splitTasks](trainer.py 288): INFO  * Train Acc 89.820
[2023-09-29 13:28:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.800, Total time 1.60
[2023-09-29 13:28:06 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 13:28:06 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 13:28:06 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:28:07 10splitTasks](trainer.py 286): INFO [0/157]	0.6277(0.6277)	0.5230(0.5230)	0.259(0.259)	90.62(90.62)
[2023-09-29 13:28:08 10splitTasks](trainer.py 286): INFO [10/157]	0.1009(0.1501)	0.0001(0.0478)	0.287(0.315)	93.75(89.20)
[2023-09-29 13:28:09 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1271)	0.0002(0.0252)	0.193(0.314)	90.62(89.43)
[2023-09-29 13:28:10 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1191)	0.0002(0.0171)	0.133(0.284)	96.88(90.22)
[2023-09-29 13:28:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1153)	0.0002(0.0130)	0.322(0.272)	93.75(90.78)
[2023-09-29 13:28:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1025(0.1127)	0.0002(0.0105)	0.311(0.262)	90.62(91.24)
[2023-09-29 13:28:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1112)	0.0003(0.0089)	0.350(0.265)	90.62(91.34)
[2023-09-29 13:28:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1100)	0.0003(0.0076)	0.583(0.271)	81.25(90.89)
[2023-09-29 13:28:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1090)	0.0003(0.0067)	0.274(0.269)	96.88(91.05)
[2023-09-29 13:28:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1011(0.1082)	0.0002(0.0060)	0.194(0.271)	93.75(90.90)
[2023-09-29 13:28:17 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1076)	0.0003(0.0055)	0.107(0.267)	100.00(91.09)
[2023-09-29 13:28:18 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1071)	0.0002(0.0050)	0.230(0.263)	93.75(91.24)
[2023-09-29 13:28:19 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1067)	0.0003(0.0046)	0.327(0.261)	84.38(91.40)
[2023-09-29 13:28:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1123(0.1065)	0.0005(0.0043)	0.238(0.265)	90.62(91.22)
[2023-09-29 13:28:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1080(0.1063)	0.0004(0.0040)	0.297(0.264)	87.50(91.29)
[2023-09-29 13:28:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1061)	0.0001(0.0038)	0.222(0.267)	90.62(91.16)
[2023-09-29 13:28:23 10splitTasks](trainer.py 286): INFO [156/157]	0.0797(0.1058)	0.0001(0.0036)	1.451(0.266)	62.50(91.26)
[2023-09-29 13:28:23 10splitTasks](trainer.py 288): INFO  * Train Acc 91.260
[2023-09-29 13:28:24 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.200, Total time 1.66
[2023-09-29 13:28:24 10splitTasks](my_trainer.py 206): INFO Pruning for task4
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 4174/4910 (85.01%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 1818/2139 (84.99%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 16357/19243 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 7269/8552 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 7269/8552 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 7269/8552 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 16357/19243 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 7269/8552 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 7269/8552 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 16357/19243 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 7269/8552 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 14539/17105 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 65427/76973 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 29079/34211 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 58157/68420 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 29079/34211 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 65427/76973 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 29079/34211 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 29079/34211 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 65427/76973 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 29079/34211 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 29079/34211 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 65427/76973 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 29079/34211 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 58157/68420 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 261708/307892 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 232629/273681 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 261708/307892 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 261708/307892 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 261708/307892 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 261708/307892 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 261708/307892 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 116315/136841 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 232629/273681 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 1046833/1231568 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 465259/547363 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 930517/1094726 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 465259/547363 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 1046833/1231568 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 465259/547363 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 465259/547363 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 1046833/1231568 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 465259/547363 (85.00%) (Total in layer: 1048576)
[2023-09-29 13:28:25 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 13:28:25 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 13:28:25 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 13:28:25 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:28:25 10splitTasks](trainer.py 286): INFO [0/157]	0.6290(0.6290)	0.5242(0.5242)	0.275(0.275)	90.62(90.62)
[2023-09-29 13:28:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1517)	0.0003(0.0480)	0.560(0.403)	84.38(85.51)
[2023-09-29 13:28:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1278)	0.0002(0.0252)	0.413(0.431)	87.50(85.12)
[2023-09-29 13:28:28 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1194)	0.0003(0.0172)	0.227(0.455)	87.50(83.97)
[2023-09-29 13:28:29 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1151)	0.0002(0.0131)	0.791(0.491)	68.75(82.16)
[2023-09-29 13:28:30 10splitTasks](trainer.py 286): INFO [50/157]	0.1026(0.1126)	0.0002(0.0106)	0.291(0.498)	87.50(81.80)
[2023-09-29 13:28:31 10splitTasks](trainer.py 286): INFO [60/157]	0.1011(0.1109)	0.0001(0.0089)	0.402(0.484)	84.38(82.58)
[2023-09-29 13:28:32 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1098)	0.0003(0.0077)	0.268(0.487)	93.75(82.53)
[2023-09-29 13:28:33 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1088)	0.0002(0.0068)	0.705(0.497)	71.88(82.21)
[2023-09-29 13:28:34 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1081)	0.0003(0.0061)	0.296(0.484)	87.50(82.69)
[2023-09-29 13:28:36 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1075)	0.0001(0.0055)	0.653(0.490)	75.00(82.36)
[2023-09-29 13:28:37 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1070)	0.0001(0.0050)	0.802(0.483)	75.00(82.71)
[2023-09-29 13:28:38 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1066)	0.0002(0.0047)	0.337(0.483)	90.62(82.85)
[2023-09-29 13:28:39 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1064)	0.0002(0.0043)	0.265(0.480)	93.75(83.04)
[2023-09-29 13:28:40 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1062)	0.0004(0.0040)	1.013(0.476)	65.62(83.16)
[2023-09-29 13:28:41 10splitTasks](trainer.py 286): INFO [150/157]	0.1021(0.1059)	0.0001(0.0038)	0.631(0.470)	78.12(83.40)
[2023-09-29 13:28:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0795(0.1056)	0.0001(0.0037)	1.838(0.473)	50.00(83.30)
[2023-09-29 13:28:41 10splitTasks](trainer.py 288): INFO  * Train Acc 83.300
[2023-09-29 13:28:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.000, Total time 1.66
[2023-09-29 13:28:43 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 13:28:43 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 13:28:43 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:28:44 10splitTasks](trainer.py 286): INFO [0/157]	0.5978(0.5978)	0.4830(0.4830)	0.436(0.436)	75.00(75.00)
[2023-09-29 13:28:45 10splitTasks](trainer.py 286): INFO [10/157]	0.1018(0.1482)	0.0004(0.0442)	0.864(0.797)	71.88(76.99)
[2023-09-29 13:28:46 10splitTasks](trainer.py 286): INFO [20/157]	0.1123(0.1272)	0.0007(0.0233)	0.632(0.671)	75.00(80.51)
[2023-09-29 13:28:47 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1196)	0.0001(0.0159)	0.443(0.600)	84.38(81.85)
[2023-09-29 13:28:48 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1155)	0.0003(0.0121)	0.643(0.552)	84.38(83.16)
[2023-09-29 13:28:49 10splitTasks](trainer.py 286): INFO [50/157]	0.1010(0.1132)	0.0002(0.0098)	0.386(0.543)	90.62(82.84)
[2023-09-29 13:28:50 10splitTasks](trainer.py 286): INFO [60/157]	0.1130(0.1117)	0.0006(0.0082)	0.387(0.537)	87.50(82.94)
[2023-09-29 13:28:51 10splitTasks](trainer.py 286): INFO [70/157]	0.1038(0.1105)	0.0002(0.0071)	0.458(0.532)	87.50(83.05)
[2023-09-29 13:28:52 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1094)	0.0003(0.0063)	0.558(0.520)	81.25(83.33)
[2023-09-29 13:28:53 10splitTasks](trainer.py 286): INFO [90/157]	0.1087(0.1087)	0.0004(0.0056)	0.340(0.514)	90.62(83.55)
[2023-09-29 13:28:54 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1080)	0.0002(0.0051)	0.359(0.496)	87.50(83.91)
[2023-09-29 13:28:55 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1075)	0.0001(0.0047)	0.399(0.492)	84.38(84.07)
[2023-09-29 13:28:56 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1070)	0.0003(0.0043)	0.491(0.485)	81.25(84.40)
[2023-09-29 13:28:57 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1067)	0.0003(0.0040)	0.527(0.475)	84.38(84.64)
[2023-09-29 13:28:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1063)	0.0002(0.0037)	0.563(0.477)	78.12(84.55)
[2023-09-29 13:28:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1060)	0.0001(0.0035)	0.255(0.472)	90.62(84.69)
[2023-09-29 13:29:00 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1057)	0.0001(0.0034)	1.667(0.469)	62.50(84.82)
[2023-09-29 13:29:00 10splitTasks](trainer.py 288): INFO  * Train Acc 84.820
[2023-09-29 13:29:01 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.800, Total time 1.53
[2023-09-29 13:29:01 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 13:29:01 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 13:29:01 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:29:02 10splitTasks](trainer.py 286): INFO [0/157]	0.6242(0.6242)	0.5024(0.5024)	0.805(0.805)	68.75(68.75)
[2023-09-29 13:29:03 10splitTasks](trainer.py 286): INFO [10/157]	0.1034(0.1521)	0.0002(0.0460)	0.401(0.357)	84.38(85.80)
[2023-09-29 13:29:04 10splitTasks](trainer.py 286): INFO [20/157]	0.1024(0.1296)	0.0002(0.0242)	0.178(0.369)	90.62(86.46)
[2023-09-29 13:29:05 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1206)	0.0002(0.0165)	0.345(0.386)	90.62(86.29)
[2023-09-29 13:29:06 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1160)	0.0002(0.0125)	0.674(0.401)	71.88(84.98)
[2023-09-29 13:29:07 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1134)	0.0002(0.0101)	0.330(0.379)	87.50(86.09)
[2023-09-29 13:29:08 10splitTasks](trainer.py 286): INFO [60/157]	0.1062(0.1116)	0.0003(0.0085)	0.360(0.393)	84.38(85.81)
[2023-09-29 13:29:09 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1102)	0.0002(0.0074)	0.327(0.397)	87.50(85.65)
[2023-09-29 13:29:10 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1092)	0.0002(0.0065)	0.389(0.388)	87.50(86.27)
[2023-09-29 13:29:11 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1084)	0.0003(0.0058)	0.216(0.388)	93.75(86.30)
[2023-09-29 13:29:12 10splitTasks](trainer.py 286): INFO [100/157]	0.1042(0.1078)	0.0002(0.0053)	0.187(0.392)	90.62(86.11)
[2023-09-29 13:29:13 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1075)	0.0004(0.0048)	0.419(0.385)	87.50(86.46)
[2023-09-29 13:29:14 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1072)	0.0002(0.0044)	0.257(0.385)	93.75(86.44)
[2023-09-29 13:29:15 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1068)	0.0002(0.0041)	0.167(0.388)	93.75(86.35)
[2023-09-29 13:29:16 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1066)	0.0002(0.0038)	0.212(0.388)	93.75(86.33)
[2023-09-29 13:29:17 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1062)	0.0001(0.0036)	0.220(0.393)	90.62(86.15)
[2023-09-29 13:29:18 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1059)	0.0001(0.0035)	0.849(0.398)	75.00(85.96)
[2023-09-29 13:29:18 10splitTasks](trainer.py 288): INFO  * Train Acc 85.960
[2023-09-29 13:29:20 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.600, Total time 1.59
[2023-09-29 13:29:20 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 13:29:20 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 13:29:20 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:29:20 10splitTasks](trainer.py 286): INFO [0/157]	0.6136(0.6136)	0.5101(0.5101)	0.302(0.302)	87.50(87.50)
[2023-09-29 13:29:21 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1488)	0.0003(0.0466)	0.426(0.340)	87.50(86.93)
[2023-09-29 13:29:22 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1277)	0.0003(0.0246)	0.292(0.367)	87.50(87.05)
[2023-09-29 13:29:23 10splitTasks](trainer.py 286): INFO [30/157]	0.1010(0.1192)	0.0001(0.0167)	0.299(0.350)	90.62(87.60)
[2023-09-29 13:29:24 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1151)	0.0002(0.0127)	0.345(0.344)	93.75(87.96)
[2023-09-29 13:29:25 10splitTasks](trainer.py 286): INFO [50/157]	0.1022(0.1125)	0.0003(0.0103)	0.326(0.370)	93.75(87.56)
[2023-09-29 13:29:26 10splitTasks](trainer.py 286): INFO [60/157]	0.1038(0.1110)	0.0003(0.0087)	0.417(0.366)	90.62(87.76)
[2023-09-29 13:29:27 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1097)	0.0001(0.0075)	0.244(0.361)	96.88(87.98)
[2023-09-29 13:29:28 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1088)	0.0002(0.0066)	0.250(0.366)	96.88(87.89)
[2023-09-29 13:29:29 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1080)	0.0003(0.0059)	0.139(0.365)	93.75(87.77)
[2023-09-29 13:29:30 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1074)	0.0003(0.0053)	0.148(0.375)	96.88(87.53)
[2023-09-29 13:29:31 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1069)	0.0002(0.0049)	0.766(0.368)	78.12(87.84)
[2023-09-29 13:29:32 10splitTasks](trainer.py 286): INFO [120/157]	0.1053(0.1066)	0.0002(0.0045)	0.502(0.362)	84.38(87.86)
[2023-09-29 13:29:34 10splitTasks](trainer.py 286): INFO [130/157]	0.1009(0.1063)	0.0002(0.0042)	0.225(0.364)	93.75(87.76)
[2023-09-29 13:29:35 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1061)	0.0002(0.0039)	0.260(0.356)	90.62(87.94)
[2023-09-29 13:29:36 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1058)	0.0001(0.0037)	0.325(0.358)	87.50(87.79)
[2023-09-29 13:29:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0807(0.1054)	0.0001(0.0035)	0.770(0.356)	87.50(87.82)
[2023-09-29 13:29:36 10splitTasks](trainer.py 288): INFO  * Train Acc 87.820
[2023-09-29 13:29:38 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.200, Total time 1.54
[2023-09-29 13:29:38 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 13:29:38 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 13:29:38 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:29:38 10splitTasks](trainer.py 286): INFO [0/157]	0.6273(0.6273)	0.5194(0.5194)	0.451(0.451)	87.50(87.50)
[2023-09-29 13:29:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1034(0.1532)	0.0003(0.0476)	0.238(0.277)	96.88(90.91)
[2023-09-29 13:29:41 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1292)	0.0003(0.0251)	0.330(0.252)	87.50(90.92)
[2023-09-29 13:29:42 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1203)	0.0002(0.0171)	0.733(0.271)	65.62(90.32)
[2023-09-29 13:29:43 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1161)	0.0002(0.0130)	0.227(0.282)	93.75(89.86)
[2023-09-29 13:29:44 10splitTasks](trainer.py 286): INFO [50/157]	0.1031(0.1133)	0.0003(0.0105)	0.203(0.290)	96.88(90.13)
[2023-09-29 13:29:45 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1121)	0.0003(0.0088)	0.572(0.302)	84.38(89.91)
[2023-09-29 13:29:46 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1107)	0.0002(0.0076)	0.250(0.298)	96.88(90.10)
[2023-09-29 13:29:47 10splitTasks](trainer.py 286): INFO [80/157]	0.1045(0.1096)	0.0003(0.0067)	0.244(0.304)	90.62(89.70)
[2023-09-29 13:29:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1087)	0.0004(0.0060)	0.390(0.305)	90.62(89.46)
[2023-09-29 13:29:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1034(0.1081)	0.0003(0.0054)	0.152(0.299)	93.75(89.67)
[2023-09-29 13:29:50 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1076)	0.0002(0.0050)	0.502(0.309)	81.25(89.36)
[2023-09-29 13:29:51 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1073)	0.0003(0.0046)	0.339(0.315)	87.50(89.18)
[2023-09-29 13:29:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1079(0.1069)	0.0002(0.0043)	0.436(0.319)	84.38(89.15)
[2023-09-29 13:29:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1066)	0.0001(0.0040)	0.154(0.324)	96.88(88.98)
[2023-09-29 13:29:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1064)	0.0001(0.0037)	0.408(0.324)	84.38(88.85)
[2023-09-29 13:29:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1060)	0.0001(0.0036)	0.421(0.328)	75.00(88.66)
[2023-09-29 13:29:55 10splitTasks](trainer.py 288): INFO  * Train Acc 88.660
[2023-09-29 13:29:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.200, Total time 1.57
[2023-09-29 13:29:56 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 13:29:56 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 13:29:56 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:29:57 10splitTasks](trainer.py 286): INFO [0/157]	0.5996(0.5996)	0.4749(0.4749)	0.389(0.389)	81.25(81.25)
[2023-09-29 13:29:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1504)	0.0002(0.0434)	0.201(0.322)	90.62(88.92)
[2023-09-29 13:29:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1025(0.1277)	0.0001(0.0229)	0.281(0.321)	84.38(88.69)
[2023-09-29 13:30:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1194)	0.0003(0.0156)	0.328(0.321)	87.50(89.21)
[2023-09-29 13:30:01 10splitTasks](trainer.py 286): INFO [40/157]	0.1038(0.1157)	0.0003(0.0119)	0.290(0.303)	90.62(89.86)
[2023-09-29 13:30:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1129)	0.0001(0.0096)	0.241(0.315)	93.75(89.64)
[2023-09-29 13:30:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1111)	0.0003(0.0081)	0.234(0.309)	93.75(89.91)
[2023-09-29 13:30:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1099)	0.0002(0.0070)	0.302(0.321)	90.62(89.17)
[2023-09-29 13:30:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1023(0.1090)	0.0002(0.0062)	0.221(0.307)	90.62(89.74)
[2023-09-29 13:30:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1041(0.1086)	0.0002(0.0055)	0.263(0.314)	93.75(89.35)
[2023-09-29 13:30:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1025(0.1080)	0.0003(0.0050)	0.392(0.306)	87.50(89.63)
[2023-09-29 13:30:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1074)	0.0003(0.0046)	0.387(0.311)	87.50(89.30)
[2023-09-29 13:30:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1010(0.1070)	0.0002(0.0042)	0.151(0.313)	96.88(89.26)
[2023-09-29 13:30:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1067)	0.0003(0.0039)	0.361(0.315)	87.50(89.22)
[2023-09-29 13:30:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1066)	0.0005(0.0037)	0.119(0.311)	96.88(89.30)
[2023-09-29 13:30:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1064)	0.0001(0.0034)	0.291(0.311)	90.62(89.40)
[2023-09-29 13:30:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0812(0.1061)	0.0001(0.0033)	1.027(0.314)	75.00(89.34)
[2023-09-29 13:30:13 10splitTasks](trainer.py 288): INFO  * Train Acc 89.340
[2023-09-29 13:30:14 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.600, Total time 1.58
[2023-09-29 13:30:14 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 13:30:14 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 13:30:14 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:30:15 10splitTasks](trainer.py 286): INFO [0/157]	0.6071(0.6071)	0.4941(0.4941)	0.090(0.090)	96.88(96.88)
[2023-09-29 13:30:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1026(0.1496)	0.0002(0.0461)	0.665(0.322)	84.38(90.06)
[2023-09-29 13:30:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1022(0.1268)	0.0003(0.0243)	0.260(0.291)	93.75(91.07)
[2023-09-29 13:30:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1024(0.1191)	0.0003(0.0166)	0.298(0.279)	90.62(91.33)
[2023-09-29 13:30:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1148)	0.0003(0.0126)	0.162(0.267)	93.75(91.39)
[2023-09-29 13:30:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1128)	0.0002(0.0102)	0.388(0.265)	87.50(91.54)
[2023-09-29 13:30:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1040(0.1116)	0.0023(0.0086)	0.096(0.263)	96.88(91.60)
[2023-09-29 13:30:22 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1104)	0.0002(0.0074)	0.293(0.275)	87.50(91.15)
[2023-09-29 13:30:23 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1095)	0.0002(0.0065)	0.113(0.272)	96.88(90.97)
[2023-09-29 13:30:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1087)	0.0002(0.0058)	0.325(0.269)	87.50(91.00)
[2023-09-29 13:30:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1082)	0.0002(0.0053)	0.175(0.265)	96.88(91.06)
[2023-09-29 13:30:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1175(0.1080)	0.0006(0.0049)	0.234(0.261)	90.62(91.22)
[2023-09-29 13:30:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1047(0.1075)	0.0003(0.0045)	0.268(0.266)	84.38(91.06)
[2023-09-29 13:30:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1071)	0.0003(0.0042)	0.183(0.268)	93.75(90.98)
[2023-09-29 13:30:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1024(0.1067)	0.0002(0.0039)	0.087(0.270)	96.88(90.94)
[2023-09-29 13:30:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1064)	0.0001(0.0037)	0.154(0.265)	93.75(91.12)
[2023-09-29 13:30:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0805(0.1060)	0.0001(0.0035)	1.154(0.262)	62.50(91.22)
[2023-09-29 13:30:31 10splitTasks](trainer.py 288): INFO  * Train Acc 91.220
[2023-09-29 13:30:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.800, Total time 1.63
[2023-09-29 13:30:33 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 13:30:33 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 13:30:33 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:30:34 10splitTasks](trainer.py 286): INFO [0/157]	0.6243(0.6243)	0.4987(0.4987)	0.472(0.472)	87.50(87.50)
[2023-09-29 13:30:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1512)	0.0002(0.0456)	0.080(0.255)	96.88(90.34)
[2023-09-29 13:30:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1282)	0.0001(0.0240)	0.189(0.225)	93.75(92.26)
[2023-09-29 13:30:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1010(0.1200)	0.0002(0.0165)	0.156(0.249)	93.75(90.93)
[2023-09-29 13:30:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1155)	0.0002(0.0125)	0.488(0.264)	81.25(90.70)
[2023-09-29 13:30:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1012(0.1128)	0.0001(0.0101)	0.356(0.256)	87.50(90.87)
[2023-09-29 13:30:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1111)	0.0002(0.0085)	0.314(0.257)	84.38(90.78)
[2023-09-29 13:30:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1097)	0.0002(0.0073)	0.336(0.255)	87.50(91.02)
[2023-09-29 13:30:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1086)	0.0002(0.0064)	0.159(0.266)	93.75(90.90)
[2023-09-29 13:30:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1010(0.1079)	0.0002(0.0058)	0.339(0.267)	87.50(90.90)
[2023-09-29 13:30:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1073)	0.0002(0.0052)	0.222(0.263)	96.88(91.15)
[2023-09-29 13:30:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1067)	0.0001(0.0048)	0.306(0.262)	87.50(91.02)
[2023-09-29 13:30:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1009(0.1062)	0.0001(0.0044)	0.098(0.259)	96.88(91.19)
[2023-09-29 13:30:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1059)	0.0001(0.0041)	0.146(0.257)	96.88(91.34)
[2023-09-29 13:30:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1011(0.1056)	0.0002(0.0038)	0.361(0.258)	84.38(91.40)
[2023-09-29 13:30:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1023(0.1053)	0.0001(0.0036)	0.300(0.255)	84.38(91.49)
[2023-09-29 13:30:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1050)	0.0001(0.0034)	0.284(0.256)	87.50(91.46)
[2023-09-29 13:30:49 10splitTasks](trainer.py 288): INFO  * Train Acc 91.460
[2023-09-29 13:30:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.600, Total time 1.64
[2023-09-29 13:30:51 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 13:30:51 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 13:30:51 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:30:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6419(0.6419)	0.5294(0.5294)	0.096(0.096)	96.88(96.88)
[2023-09-29 13:30:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1539)	0.0003(0.0502)	0.186(0.248)	93.75(91.19)
[2023-09-29 13:30:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1295)	0.0002(0.0264)	0.119(0.234)	93.75(91.07)
[2023-09-29 13:30:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1022(0.1206)	0.0002(0.0180)	0.132(0.230)	96.88(91.73)
[2023-09-29 13:30:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1049(0.1162)	0.0002(0.0137)	0.129(0.219)	93.75(92.00)
[2023-09-29 13:30:57 10splitTasks](trainer.py 286): INFO [50/157]	0.1026(0.1137)	0.0002(0.0111)	0.212(0.215)	93.75(91.97)
[2023-09-29 13:30:58 10splitTasks](trainer.py 286): INFO [60/157]	0.1036(0.1119)	0.0002(0.0093)	0.231(0.219)	90.62(91.80)
[2023-09-29 13:30:59 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1107)	0.0002(0.0080)	0.463(0.221)	87.50(91.81)
[2023-09-29 13:31:00 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1097)	0.0003(0.0071)	0.036(0.225)	100.00(91.78)
[2023-09-29 13:31:01 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1088)	0.0003(0.0063)	0.118(0.222)	96.88(91.93)
[2023-09-29 13:31:02 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1083)	0.0003(0.0057)	0.361(0.224)	87.50(91.89)
[2023-09-29 13:31:03 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1079)	0.0003(0.0053)	0.108(0.219)	96.88(92.06)
[2023-09-29 13:31:04 10splitTasks](trainer.py 286): INFO [120/157]	0.1011(0.1076)	0.0002(0.0049)	0.175(0.217)	93.75(92.05)
[2023-09-29 13:31:05 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1071)	0.0003(0.0045)	0.618(0.222)	81.25(91.89)
[2023-09-29 13:31:06 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1068)	0.0003(0.0042)	0.247(0.221)	93.75(91.84)
[2023-09-29 13:31:07 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1065)	0.0002(0.0040)	0.087(0.227)	96.88(91.64)
[2023-09-29 13:31:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0805(0.1062)	0.0001(0.0038)	1.147(0.227)	62.50(91.60)
[2023-09-29 13:31:08 10splitTasks](trainer.py 288): INFO  * Train Acc 91.600
[2023-09-29 13:31:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.200, Total time 1.58
[2023-09-29 13:31:09 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 13:31:09 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 13:31:09 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:31:10 10splitTasks](trainer.py 286): INFO [0/157]	0.6276(0.6276)	0.5197(0.5197)	0.051(0.051)	100.00(100.00)
[2023-09-29 13:31:11 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1524)	0.0002(0.0475)	0.346(0.167)	84.38(94.03)
[2023-09-29 13:31:12 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1292)	0.0002(0.0250)	0.415(0.234)	87.50(92.71)
[2023-09-29 13:31:13 10splitTasks](trainer.py 286): INFO [30/157]	0.1038(0.1203)	0.0002(0.0170)	0.184(0.205)	93.75(93.55)
[2023-09-29 13:31:14 10splitTasks](trainer.py 286): INFO [40/157]	0.1026(0.1160)	0.0002(0.0130)	0.339(0.206)	84.38(93.22)
[2023-09-29 13:31:15 10splitTasks](trainer.py 286): INFO [50/157]	0.1108(0.1135)	0.0005(0.0105)	0.098(0.201)	96.88(93.26)
[2023-09-29 13:31:16 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1119)	0.0002(0.0088)	0.113(0.196)	96.88(93.39)
[2023-09-29 13:31:17 10splitTasks](trainer.py 286): INFO [70/157]	0.1132(0.1113)	0.0002(0.0076)	0.135(0.200)	96.88(93.27)
[2023-09-29 13:31:18 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1107)	0.0001(0.0067)	0.196(0.194)	93.75(93.44)
[2023-09-29 13:31:19 10splitTasks](trainer.py 286): INFO [90/157]	0.1024(0.1097)	0.0004(0.0060)	0.193(0.200)	93.75(93.20)
[2023-09-29 13:31:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1092)	0.0003(0.0055)	0.379(0.206)	93.75(93.16)
[2023-09-29 13:31:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1029(0.1086)	0.0005(0.0050)	0.323(0.209)	87.50(92.96)
[2023-09-29 13:31:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1081)	0.0003(0.0046)	0.255(0.209)	87.50(92.98)
[2023-09-29 13:31:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1077(0.1076)	0.0005(0.0043)	0.421(0.215)	81.25(92.72)
[2023-09-29 13:31:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1073)	0.0003(0.0040)	0.232(0.219)	87.50(92.58)
[2023-09-29 13:31:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1039(0.1071)	0.0002(0.0038)	0.234(0.218)	90.62(92.57)
[2023-09-29 13:31:26 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1068)	0.0001(0.0036)	0.715(0.219)	75.00(92.46)
[2023-09-29 13:31:26 10splitTasks](trainer.py 288): INFO  * Train Acc 92.460
[2023-09-29 13:31:28 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.600, Total time 1.61
[2023-09-29 13:31:28 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 13:31:28 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 13:31:28 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:31:29 10splitTasks](trainer.py 286): INFO [0/157]	0.6042(0.6042)	0.4947(0.4947)	0.168(0.168)	90.62(90.62)
[2023-09-29 13:31:30 10splitTasks](trainer.py 286): INFO [10/157]	0.1086(0.1506)	0.0005(0.0453)	0.135(0.194)	93.75(91.76)
[2023-09-29 13:31:31 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1275)	0.0002(0.0238)	0.133(0.185)	93.75(92.41)
[2023-09-29 13:31:32 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1196)	0.0003(0.0162)	0.535(0.189)	87.50(92.94)
[2023-09-29 13:31:33 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1155)	0.0003(0.0123)	0.219(0.187)	93.75(93.22)
[2023-09-29 13:31:34 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1128)	0.0002(0.0100)	0.127(0.178)	93.75(93.69)
[2023-09-29 13:31:35 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1110)	0.0002(0.0084)	0.124(0.179)	96.88(93.65)
[2023-09-29 13:31:36 10splitTasks](trainer.py 286): INFO [70/157]	0.1040(0.1098)	0.0003(0.0072)	0.300(0.184)	87.50(93.53)
[2023-09-29 13:31:37 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1089)	0.0001(0.0064)	0.295(0.186)	87.50(93.52)
[2023-09-29 13:31:38 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1082)	0.0002(0.0057)	0.171(0.187)	96.88(93.72)
[2023-09-29 13:31:39 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1077)	0.0002(0.0052)	0.266(0.186)	84.38(93.53)
[2023-09-29 13:31:40 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1074)	0.0001(0.0047)	0.101(0.193)	96.88(93.38)
[2023-09-29 13:31:41 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1069)	0.0001(0.0044)	0.184(0.196)	93.75(93.21)
[2023-09-29 13:31:42 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1066)	0.0002(0.0041)	0.216(0.195)	90.62(93.27)
[2023-09-29 13:31:43 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1064)	0.0002(0.0038)	0.256(0.195)	90.62(93.26)
[2023-09-29 13:31:44 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1061)	0.0001(0.0036)	0.125(0.195)	96.88(93.25)
[2023-09-29 13:31:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1058)	0.0001(0.0034)	0.389(0.194)	87.50(93.32)
[2023-09-29 13:31:45 10splitTasks](trainer.py 288): INFO  * Train Acc 93.320
[2023-09-29 13:31:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.600, Total time 1.60
[2023-09-29 13:31:46 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 13:31:46 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 13:31:46 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:31:47 10splitTasks](trainer.py 286): INFO [0/157]	0.6101(0.6101)	0.4970(0.4970)	0.126(0.126)	93.75(93.75)
[2023-09-29 13:31:48 10splitTasks](trainer.py 286): INFO [10/157]	0.1009(0.1495)	0.0001(0.0455)	0.266(0.208)	90.62(93.75)
[2023-09-29 13:31:49 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1270)	0.0001(0.0239)	0.231(0.180)	90.62(94.64)
[2023-09-29 13:31:50 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1200)	0.0002(0.0163)	0.325(0.171)	84.38(94.66)
[2023-09-29 13:31:51 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1155)	0.0003(0.0124)	0.128(0.163)	93.75(94.89)
[2023-09-29 13:31:52 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1129)	0.0003(0.0100)	0.336(0.170)	90.62(94.61)
[2023-09-29 13:31:53 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1111)	0.0001(0.0084)	0.134(0.162)	93.75(94.98)
[2023-09-29 13:31:54 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1099)	0.0003(0.0073)	0.251(0.167)	93.75(94.67)
[2023-09-29 13:31:55 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1089)	0.0001(0.0064)	0.288(0.176)	87.50(94.14)
[2023-09-29 13:31:56 10splitTasks](trainer.py 286): INFO [90/157]	0.1041(0.1082)	0.0006(0.0058)	0.120(0.173)	96.88(94.23)
[2023-09-29 13:31:57 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1076)	0.0002(0.0052)	0.193(0.169)	93.75(94.43)
[2023-09-29 13:31:58 10splitTasks](trainer.py 286): INFO [110/157]	0.1081(0.1071)	0.0003(0.0048)	0.117(0.169)	93.75(94.26)
[2023-09-29 13:31:59 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1069)	0.0001(0.0044)	0.109(0.170)	96.88(94.21)
[2023-09-29 13:32:00 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1065)	0.0001(0.0041)	0.116(0.166)	96.88(94.39)
[2023-09-29 13:32:01 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1062)	0.0002(0.0038)	0.045(0.166)	100.00(94.39)
[2023-09-29 13:32:02 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1059)	0.0001(0.0036)	0.160(0.165)	90.62(94.39)
[2023-09-29 13:32:03 10splitTasks](trainer.py 286): INFO [156/157]	0.0807(0.1055)	0.0001(0.0035)	0.361(0.166)	75.00(94.30)
[2023-09-29 13:32:03 10splitTasks](trainer.py 288): INFO  * Train Acc 94.300
[2023-09-29 13:32:05 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.200, Total time 1.64
[2023-09-29 13:32:05 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 13:32:05 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 13:32:05 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:32:05 10splitTasks](trainer.py 286): INFO [0/157]	0.6279(0.6279)	0.5207(0.5207)	0.259(0.259)	87.50(87.50)
[2023-09-29 13:32:06 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1506)	0.0002(0.0476)	0.205(0.167)	90.62(93.47)
[2023-09-29 13:32:07 10splitTasks](trainer.py 286): INFO [20/157]	0.1047(0.1291)	0.0003(0.0251)	0.214(0.160)	93.75(94.35)
[2023-09-29 13:32:08 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1211)	0.0002(0.0171)	0.211(0.182)	90.62(93.85)
[2023-09-29 13:32:09 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1165)	0.0002(0.0130)	0.066(0.167)	96.88(94.28)
[2023-09-29 13:32:10 10splitTasks](trainer.py 286): INFO [50/157]	0.1024(0.1138)	0.0003(0.0105)	0.094(0.159)	96.88(94.67)
[2023-09-29 13:32:11 10splitTasks](trainer.py 286): INFO [60/157]	0.1025(0.1124)	0.0002(0.0089)	0.058(0.151)	100.00(95.03)
[2023-09-29 13:32:12 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1109)	0.0002(0.0077)	0.102(0.147)	96.88(95.11)
[2023-09-29 13:32:13 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1098)	0.0002(0.0067)	0.583(0.156)	78.12(94.91)
[2023-09-29 13:32:15 10splitTasks](trainer.py 286): INFO [90/157]	0.1009(0.1089)	0.0001(0.0060)	0.148(0.153)	96.88(95.12)
[2023-09-29 13:32:16 10splitTasks](trainer.py 286): INFO [100/157]	0.1130(0.1083)	0.0005(0.0055)	0.124(0.148)	96.88(95.20)
[2023-09-29 13:32:17 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1077)	0.0001(0.0050)	0.092(0.155)	96.88(95.07)
[2023-09-29 13:32:18 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1072)	0.0004(0.0046)	0.299(0.158)	93.75(95.02)
[2023-09-29 13:32:19 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1069)	0.0002(0.0043)	0.079(0.155)	96.88(95.04)
[2023-09-29 13:32:20 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1065)	0.0002(0.0040)	0.315(0.155)	96.88(95.17)
[2023-09-29 13:32:21 10splitTasks](trainer.py 286): INFO [150/157]	0.1018(0.1063)	0.0001(0.0037)	0.066(0.153)	96.88(95.22)
[2023-09-29 13:32:21 10splitTasks](trainer.py 286): INFO [156/157]	0.0810(0.1059)	0.0001(0.0036)	0.383(0.154)	87.50(95.12)
[2023-09-29 13:32:21 10splitTasks](trainer.py 288): INFO  * Train Acc 95.120
[2023-09-29 13:32:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.400, Total time 1.55
[2023-09-29 13:32:23 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 13:32:23 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 13:32:23 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:32:24 10splitTasks](trainer.py 286): INFO [0/157]	0.6248(0.6248)	0.5180(0.5180)	0.042(0.042)	100.00(100.00)
[2023-09-29 13:32:25 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1534)	0.0004(0.0475)	0.146(0.159)	90.62(94.03)
[2023-09-29 13:32:26 10splitTasks](trainer.py 286): INFO [20/157]	0.1025(0.1301)	0.0003(0.0250)	0.135(0.133)	96.88(95.24)
[2023-09-29 13:32:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1210)	0.0002(0.0170)	0.146(0.142)	96.88(94.66)
[2023-09-29 13:32:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1155(0.1166)	0.0006(0.0130)	0.341(0.145)	84.38(94.36)
[2023-09-29 13:32:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1137)	0.0002(0.0105)	0.020(0.138)	100.00(94.91)
[2023-09-29 13:32:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1123)	0.0002(0.0088)	0.325(0.140)	84.38(94.93)
[2023-09-29 13:32:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1069(0.1114)	0.0002(0.0076)	0.133(0.141)	96.88(94.81)
[2023-09-29 13:32:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1041(0.1102)	0.0002(0.0067)	0.104(0.139)	93.75(94.98)
[2023-09-29 13:32:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1094)	0.0003(0.0060)	0.179(0.137)	96.88(95.12)
[2023-09-29 13:32:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1087)	0.0002(0.0054)	0.311(0.141)	90.62(95.02)
[2023-09-29 13:32:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1080)	0.0001(0.0050)	0.034(0.144)	100.00(94.99)
[2023-09-29 13:32:36 10splitTasks](trainer.py 286): INFO [120/157]	0.1096(0.1076)	0.0005(0.0046)	0.074(0.146)	96.88(94.99)
[2023-09-29 13:32:37 10splitTasks](trainer.py 286): INFO [130/157]	0.1030(0.1072)	0.0003(0.0043)	0.082(0.143)	96.88(95.09)
[2023-09-29 13:32:38 10splitTasks](trainer.py 286): INFO [140/157]	0.1009(0.1069)	0.0002(0.0040)	0.300(0.144)	84.38(95.08)
[2023-09-29 13:32:39 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1065)	0.0002(0.0037)	0.034(0.142)	100.00(95.10)
[2023-09-29 13:32:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1062)	0.0001(0.0036)	0.104(0.143)	100.00(95.12)
[2023-09-29 13:32:40 10splitTasks](trainer.py 288): INFO  * Train Acc 95.120
[2023-09-29 13:32:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.600, Total time 1.59
[2023-09-29 13:32:41 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 13:32:41 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 13:32:41 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:32:42 10splitTasks](trainer.py 286): INFO [0/157]	0.6057(0.6057)	0.4900(0.4900)	0.048(0.048)	100.00(100.00)
[2023-09-29 13:32:43 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1481)	0.0002(0.0448)	0.092(0.094)	96.88(97.16)
[2023-09-29 13:32:44 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1264)	0.0003(0.0238)	0.090(0.109)	96.88(96.73)
[2023-09-29 13:32:45 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1193)	0.0002(0.0164)	0.025(0.124)	100.00(95.67)
[2023-09-29 13:32:46 10splitTasks](trainer.py 286): INFO [40/157]	0.1028(0.1155)	0.0003(0.0124)	0.138(0.117)	96.88(96.04)
[2023-09-29 13:32:47 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1133)	0.0002(0.0101)	0.115(0.113)	93.75(96.20)
[2023-09-29 13:32:48 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1115)	0.0003(0.0085)	0.041(0.121)	100.00(96.00)
[2023-09-29 13:32:49 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1104)	0.0003(0.0073)	0.338(0.129)	93.75(95.82)
[2023-09-29 13:32:50 10splitTasks](trainer.py 286): INFO [80/157]	0.1032(0.1098)	0.0003(0.0064)	0.056(0.128)	100.00(95.91)
[2023-09-29 13:32:51 10splitTasks](trainer.py 286): INFO [90/157]	0.1010(0.1091)	0.0001(0.0058)	0.161(0.126)	93.75(95.91)
[2023-09-29 13:32:52 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1085)	0.0003(0.0052)	0.334(0.129)	87.50(95.85)
[2023-09-29 13:32:53 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1079)	0.0002(0.0048)	0.053(0.128)	100.00(95.89)
[2023-09-29 13:32:54 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1075)	0.0003(0.0044)	0.010(0.126)	100.00(95.97)
[2023-09-29 13:32:55 10splitTasks](trainer.py 286): INFO [130/157]	0.1010(0.1071)	0.0002(0.0041)	0.121(0.122)	93.75(96.02)
[2023-09-29 13:32:56 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1067)	0.0003(0.0038)	0.154(0.125)	93.75(95.94)
[2023-09-29 13:32:57 10splitTasks](trainer.py 286): INFO [150/157]	0.1036(0.1064)	0.0002(0.0036)	0.059(0.124)	96.88(96.01)
[2023-09-29 13:32:58 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1061)	0.0001(0.0035)	0.720(0.124)	75.00(96.06)
[2023-09-29 13:32:58 10splitTasks](trainer.py 288): INFO  * Train Acc 96.060
[2023-09-29 13:33:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.55
[2023-09-29 13:33:00 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 13:33:00 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 13:33:00 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:33:00 10splitTasks](trainer.py 286): INFO [0/157]	0.5928(0.5928)	0.4860(0.4860)	0.097(0.097)	96.88(96.88)
[2023-09-29 13:33:01 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1504)	0.0003(0.0470)	0.148(0.122)	96.88(96.31)
[2023-09-29 13:33:02 10splitTasks](trainer.py 286): INFO [20/157]	0.1046(0.1275)	0.0002(0.0247)	0.087(0.113)	93.75(96.28)
[2023-09-29 13:33:03 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1194)	0.0002(0.0168)	0.064(0.106)	96.88(96.57)
[2023-09-29 13:33:04 10splitTasks](trainer.py 286): INFO [40/157]	0.1041(0.1151)	0.0003(0.0128)	0.217(0.113)	90.62(96.11)
[2023-09-29 13:33:05 10splitTasks](trainer.py 286): INFO [50/157]	0.1052(0.1126)	0.0003(0.0104)	0.126(0.112)	93.75(96.08)
[2023-09-29 13:33:06 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1113)	0.0002(0.0087)	0.175(0.120)	93.75(96.00)
[2023-09-29 13:33:07 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1100)	0.0002(0.0076)	0.273(0.126)	87.50(95.77)
[2023-09-29 13:33:08 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1090)	0.0002(0.0067)	0.010(0.122)	100.00(95.91)
[2023-09-29 13:33:09 10splitTasks](trainer.py 286): INFO [90/157]	0.1353(0.1086)	0.0005(0.0060)	0.080(0.120)	93.75(95.95)
[2023-09-29 13:33:11 10splitTasks](trainer.py 286): INFO [100/157]	0.1072(0.1082)	0.0002(0.0055)	0.217(0.125)	90.62(95.79)
[2023-09-29 13:33:12 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1078)	0.0002(0.0050)	0.134(0.124)	96.88(95.83)
[2023-09-29 13:33:13 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1073)	0.0002(0.0046)	0.128(0.123)	96.88(95.84)
[2023-09-29 13:33:14 10splitTasks](trainer.py 286): INFO [130/157]	0.1053(0.1069)	0.0003(0.0043)	0.216(0.124)	96.88(95.94)
[2023-09-29 13:33:15 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1066)	0.0002(0.0040)	0.036(0.125)	100.00(96.01)
[2023-09-29 13:33:16 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1063)	0.0001(0.0038)	0.055(0.125)	100.00(96.01)
[2023-09-29 13:33:16 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1059)	0.0001(0.0036)	0.293(0.123)	87.50(96.06)
[2023-09-29 13:33:16 10splitTasks](trainer.py 288): INFO  * Train Acc 96.060
[2023-09-29 13:33:18 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.800, Total time 1.56
[2023-09-29 13:33:18 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 13:33:18 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 13:33:18 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:33:19 10splitTasks](trainer.py 286): INFO [0/157]	0.6419(0.6419)	0.5367(0.5367)	0.258(0.258)	93.75(93.75)
[2023-09-29 13:33:20 10splitTasks](trainer.py 286): INFO [10/157]	0.1010(0.1538)	0.0002(0.0494)	0.323(0.119)	84.38(96.31)
[2023-09-29 13:33:21 10splitTasks](trainer.py 286): INFO [20/157]	0.1079(0.1300)	0.0002(0.0260)	0.033(0.101)	100.00(96.88)
[2023-09-29 13:33:22 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1210)	0.0002(0.0177)	0.121(0.112)	96.88(96.47)
[2023-09-29 13:33:23 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1166)	0.0003(0.0135)	0.095(0.108)	93.75(96.57)
[2023-09-29 13:33:24 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1137)	0.0003(0.0109)	0.056(0.104)	100.00(96.88)
[2023-09-29 13:33:25 10splitTasks](trainer.py 286): INFO [60/157]	0.1031(0.1118)	0.0003(0.0091)	0.127(0.108)	96.88(96.82)
[2023-09-29 13:33:26 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1105)	0.0004(0.0079)	0.085(0.107)	96.88(96.83)
[2023-09-29 13:33:27 10splitTasks](trainer.py 286): INFO [80/157]	0.1023(0.1097)	0.0002(0.0070)	0.121(0.110)	93.75(96.57)
[2023-09-29 13:33:28 10splitTasks](trainer.py 286): INFO [90/157]	0.1050(0.1091)	0.0003(0.0062)	0.111(0.106)	96.88(96.67)
[2023-09-29 13:33:29 10splitTasks](trainer.py 286): INFO [100/157]	0.1030(0.1085)	0.0002(0.0057)	0.031(0.108)	100.00(96.57)
[2023-09-29 13:33:30 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1079)	0.0002(0.0052)	0.309(0.109)	93.75(96.59)
[2023-09-29 13:33:31 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1075)	0.0002(0.0048)	0.035(0.112)	100.00(96.44)
[2023-09-29 13:33:32 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1071)	0.0002(0.0044)	0.062(0.112)	100.00(96.40)
[2023-09-29 13:33:33 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1067)	0.0002(0.0041)	0.091(0.110)	96.88(96.45)
[2023-09-29 13:33:34 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1065)	0.0001(0.0039)	0.134(0.111)	93.75(96.38)
[2023-09-29 13:33:35 10splitTasks](trainer.py 286): INFO [156/157]	0.0799(0.1062)	0.0001(0.0037)	0.685(0.113)	87.50(96.36)
[2023-09-29 13:33:35 10splitTasks](trainer.py 288): INFO  * Train Acc 96.360
[2023-09-29 13:33:36 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.54
[2023-09-29 13:33:36 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 13:33:36 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 13:33:36 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:33:37 10splitTasks](trainer.py 286): INFO [0/157]	0.6339(0.6339)	0.5177(0.5177)	0.295(0.295)	84.38(84.38)
[2023-09-29 13:33:38 10splitTasks](trainer.py 286): INFO [10/157]	0.1047(0.1547)	0.0002(0.0474)	0.252(0.128)	93.75(95.45)
[2023-09-29 13:33:39 10splitTasks](trainer.py 286): INFO [20/157]	0.1040(0.1302)	0.0003(0.0250)	0.129(0.114)	93.75(96.43)
[2023-09-29 13:33:40 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1224)	0.0002(0.0171)	0.130(0.117)	93.75(96.27)
[2023-09-29 13:33:41 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1177)	0.0002(0.0130)	0.129(0.110)	93.75(96.27)
[2023-09-29 13:33:42 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1151)	0.0003(0.0105)	0.157(0.111)	90.62(96.20)
[2023-09-29 13:33:43 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1130)	0.0003(0.0088)	0.250(0.107)	93.75(96.41)
[2023-09-29 13:33:44 10splitTasks](trainer.py 286): INFO [70/157]	0.1088(0.1116)	0.0003(0.0076)	0.059(0.106)	100.00(96.52)
[2023-09-29 13:33:45 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1104)	0.0002(0.0067)	0.180(0.106)	93.75(96.53)
[2023-09-29 13:33:46 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1096)	0.0002(0.0060)	0.137(0.105)	96.88(96.63)
[2023-09-29 13:33:47 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1089)	0.0003(0.0055)	0.044(0.101)	100.00(96.72)
[2023-09-29 13:33:48 10splitTasks](trainer.py 286): INFO [110/157]	0.1025(0.1085)	0.0004(0.0050)	0.111(0.103)	93.75(96.62)
[2023-09-29 13:33:49 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1081)	0.0002(0.0046)	0.037(0.104)	100.00(96.69)
[2023-09-29 13:33:50 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1077)	0.0002(0.0043)	0.037(0.104)	100.00(96.66)
[2023-09-29 13:33:51 10splitTasks](trainer.py 286): INFO [140/157]	0.1025(0.1076)	0.0002(0.0041)	0.052(0.104)	96.88(96.68)
[2023-09-29 13:33:52 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1072)	0.0001(0.0038)	0.060(0.103)	100.00(96.71)
[2023-09-29 13:33:53 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1069)	0.0001(0.0037)	0.106(0.104)	100.00(96.68)
[2023-09-29 13:33:53 10splitTasks](trainer.py 288): INFO  * Train Acc 96.680
[2023-09-29 13:33:55 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.58
[2023-09-29 13:33:55 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 13:33:55 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 13:33:55 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:33:55 10splitTasks](trainer.py 286): INFO [0/157]	0.6937(0.6937)	0.5795(0.5795)	0.096(0.096)	96.88(96.88)
[2023-09-29 13:33:56 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1586)	0.0003(0.0530)	0.155(0.116)	93.75(95.45)
[2023-09-29 13:33:57 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1321)	0.0003(0.0279)	0.063(0.101)	100.00(96.28)
[2023-09-29 13:33:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1197(0.1230)	0.0006(0.0190)	0.151(0.106)	96.88(96.47)
[2023-09-29 13:34:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1183)	0.0002(0.0145)	0.089(0.095)	100.00(96.80)
[2023-09-29 13:34:01 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1151)	0.0002(0.0117)	0.037(0.098)	100.00(96.69)
[2023-09-29 13:34:02 10splitTasks](trainer.py 286): INFO [60/157]	0.1057(0.1131)	0.0002(0.0098)	0.188(0.103)	90.62(96.67)
[2023-09-29 13:34:03 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1119)	0.0003(0.0085)	0.054(0.097)	100.00(96.96)
[2023-09-29 13:34:04 10splitTasks](trainer.py 286): INFO [80/157]	0.1022(0.1109)	0.0002(0.0075)	0.059(0.105)	100.00(96.91)
[2023-09-29 13:34:05 10splitTasks](trainer.py 286): INFO [90/157]	0.1038(0.1100)	0.0002(0.0067)	0.119(0.110)	96.88(96.70)
[2023-09-29 13:34:06 10splitTasks](trainer.py 286): INFO [100/157]	0.1077(0.1093)	0.0004(0.0061)	0.242(0.110)	93.75(96.81)
[2023-09-29 13:34:07 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1088)	0.0002(0.0055)	0.181(0.111)	96.88(96.85)
[2023-09-29 13:34:08 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1082)	0.0003(0.0051)	0.179(0.114)	96.88(96.72)
[2023-09-29 13:34:09 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1078)	0.0003(0.0047)	0.149(0.120)	93.75(96.54)
[2023-09-29 13:34:10 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1076)	0.0002(0.0044)	0.012(0.117)	100.00(96.63)
[2023-09-29 13:34:11 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1073)	0.0001(0.0042)	0.071(0.116)	100.00(96.63)
[2023-09-29 13:34:11 10splitTasks](trainer.py 286): INFO [156/157]	0.0823(0.1069)	0.0001(0.0040)	0.240(0.117)	87.50(96.62)
[2023-09-29 13:34:12 10splitTasks](trainer.py 288): INFO  * Train Acc 96.620
[2023-09-29 13:34:13 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.57
[2023-09-29 13:34:13 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 13:34:13 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 13:34:13 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:34:14 10splitTasks](trainer.py 286): INFO [0/157]	0.6006(0.6006)	0.4917(0.4917)	0.077(0.077)	96.88(96.88)
[2023-09-29 13:34:15 10splitTasks](trainer.py 286): INFO [10/157]	0.1024(0.1496)	0.0005(0.0450)	0.187(0.098)	90.62(96.88)
[2023-09-29 13:34:16 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1270)	0.0002(0.0237)	0.231(0.110)	90.62(96.28)
[2023-09-29 13:34:17 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1190)	0.0002(0.0162)	0.258(0.106)	87.50(96.17)
[2023-09-29 13:34:18 10splitTasks](trainer.py 286): INFO [40/157]	0.1051(0.1150)	0.0003(0.0123)	0.077(0.109)	96.88(96.42)
[2023-09-29 13:34:19 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1126)	0.0003(0.0099)	0.030(0.109)	100.00(96.32)
[2023-09-29 13:34:20 10splitTasks](trainer.py 286): INFO [60/157]	0.1030(0.1109)	0.0002(0.0084)	0.094(0.108)	96.88(96.36)
[2023-09-29 13:34:21 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1097)	0.0002(0.0072)	0.072(0.111)	96.88(96.17)
[2023-09-29 13:34:22 10splitTasks](trainer.py 286): INFO [80/157]	0.1065(0.1089)	0.0003(0.0064)	0.091(0.111)	96.88(96.18)
[2023-09-29 13:34:23 10splitTasks](trainer.py 286): INFO [90/157]	0.1027(0.1083)	0.0003(0.0057)	0.089(0.110)	100.00(96.22)
[2023-09-29 13:34:24 10splitTasks](trainer.py 286): INFO [100/157]	0.1048(0.1078)	0.0004(0.0052)	0.039(0.109)	100.00(96.26)
[2023-09-29 13:34:25 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1074)	0.0003(0.0047)	0.045(0.104)	100.00(96.45)
[2023-09-29 13:34:26 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1069)	0.0002(0.0044)	0.079(0.103)	96.88(96.41)
[2023-09-29 13:34:27 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1067)	0.0002(0.0041)	0.279(0.105)	87.50(96.30)
[2023-09-29 13:34:28 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1067)	0.0002(0.0038)	0.041(0.103)	96.88(96.39)
[2023-09-29 13:34:29 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1066)	0.0001(0.0036)	0.074(0.103)	96.88(96.40)
[2023-09-29 13:34:30 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1063)	0.0001(0.0034)	0.021(0.103)	100.00(96.36)
[2023-09-29 13:34:30 10splitTasks](trainer.py 288): INFO  * Train Acc 96.360
[2023-09-29 13:34:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.56
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-4.pth
=> Save Done
[2023-09-29 13:34:32 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 13:34:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.75
[2023-09-29 13:34:33 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 13:34:35 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.76
[2023-09-29 13:34:35 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 13:34:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.76
[2023-09-29 13:34:37 10splitTasks](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 13:34:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.83
[2023-09-29 13:34:39 10splitTasks](iBatchLearn.py 131): INFO validation split name:4
[2023-09-29 13:34:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.76
[2023-09-29 13:34:41 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 13:34:41 10splitTasks](trainer.py 341): INFO done
[2023-09-29 13:34:41 10splitTasks](iBatchLearn.py 155): INFO Acc:81.68; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:34:44 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:34:44 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:34:44 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 4, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-4.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-4.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_4.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:34:45 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-4.pth
[2023-09-29 13:34:45 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:34:47 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:34:48 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:34:48 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 4, 4, 4],
        [3, 3, 2, 4, 0, 4, 4],
        [4, 3, 3, 0, 0, 4, 4],
        [4, 3, 0, 5, 4, 4, 4],
        [4, 0, 0, 5, 4, 5, 5],
        [0, 0, 5, 4, 2, 2, 5],
        [0, 3, 5, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 13:34:48 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:34:48 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:34:48 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 13:34:53 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 13:34:56 10splitTasks](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 13:34:59 10splitTasks](iBatchLearn.py 167): INFO test split name:3
[2023-09-29 13:35:02 10splitTasks](iBatchLearn.py 167): INFO test split name:4
--------------------------------Official Evaluation--------------------------------
4 81.58000000000001
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:35:10 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:35:10 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:35:10 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 5, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-4.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-5.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-4.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-5.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:35:10 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-4.pth
[2023-09-29 13:35:11 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:35:13 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:35:13 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:35:13 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:35:13 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:35:13 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 13:35:13 10splitTasks](iBatchLearn.py 92): INFO ====================== 5 =======================
[2023-09-29 13:35:13 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 13:35:13 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 13:35:13 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 13:35:13 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:35:17 10splitTasks](trainer.py 286): INFO [0/157]	3.7586(3.7586)	0.6039(0.6039)	2.321(2.321)	9.38(9.38)
[2023-09-29 13:35:18 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.4351)	0.0002(0.0552)	2.059(2.234)	50.00(23.30)
[2023-09-29 13:35:19 10splitTasks](trainer.py 286): INFO [20/157]	0.1046(0.2765)	0.0003(0.0291)	1.872(2.142)	43.75(26.93)
[2023-09-29 13:35:20 10splitTasks](trainer.py 286): INFO [30/157]	0.1007(0.2209)	0.0002(0.0198)	1.667(2.028)	50.00(30.75)
[2023-09-29 13:35:21 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1918)	0.0002(0.0150)	1.442(1.928)	46.88(33.38)
[2023-09-29 13:35:22 10splitTasks](trainer.py 286): INFO [50/157]	0.1010(0.1742)	0.0002(0.0121)	1.244(1.837)	53.12(35.78)
[2023-09-29 13:35:23 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1623)	0.0002(0.0102)	1.068(1.769)	68.75(38.47)
[2023-09-29 13:35:24 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1541)	0.0003(0.0088)	1.446(1.713)	37.50(39.74)
[2023-09-29 13:35:25 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1476)	0.0002(0.0078)	1.053(1.667)	59.38(41.32)
[2023-09-29 13:35:26 10splitTasks](trainer.py 286): INFO [90/157]	0.1009(0.1426)	0.0002(0.0070)	1.314(1.634)	46.88(42.51)
[2023-09-29 13:35:27 10splitTasks](trainer.py 286): INFO [100/157]	0.1005(0.1385)	0.0002(0.0063)	0.962(1.605)	62.50(43.38)
[2023-09-29 13:35:28 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1351)	0.0003(0.0058)	1.322(1.569)	53.12(44.48)
[2023-09-29 13:35:29 10splitTasks](trainer.py 286): INFO [120/157]	0.1048(0.1324)	0.0003(0.0053)	0.983(1.530)	65.62(45.64)
[2023-09-29 13:35:30 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1300)	0.0003(0.0049)	1.143(1.504)	65.62(46.71)
[2023-09-29 13:35:31 10splitTasks](trainer.py 286): INFO [140/157]	0.1009(0.1280)	0.0003(0.0046)	1.082(1.485)	53.12(47.23)
[2023-09-29 13:35:32 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1263)	0.0001(0.0043)	1.130(1.460)	59.38(48.14)
[2023-09-29 13:35:33 10splitTasks](trainer.py 286): INFO [156/157]	0.0803(0.1252)	0.0001(0.0042)	1.921(1.453)	25.00(48.34)
[2023-09-29 13:35:33 10splitTasks](trainer.py 288): INFO  * Train Acc 48.340
[2023-09-29 13:35:34 10splitTasks](my_trainer.py 503): INFO  * Val Acc 63.800, Total time 1.56
[2023-09-29 13:35:34 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 13:35:34 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 13:35:34 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:35:35 10splitTasks](trainer.py 286): INFO [0/157]	0.5950(0.5950)	0.4860(0.4860)	0.838(0.838)	68.75(68.75)
[2023-09-29 13:35:36 10splitTasks](trainer.py 286): INFO [10/157]	0.1117(0.1508)	0.0002(0.0445)	1.005(1.060)	62.50(62.50)
[2023-09-29 13:35:37 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1280)	0.0002(0.0234)	0.992(1.103)	65.62(61.01)
[2023-09-29 13:35:38 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1205)	0.0002(0.0160)	1.037(1.166)	62.50(58.67)
[2023-09-29 13:35:39 10splitTasks](trainer.py 286): INFO [40/157]	0.1058(0.1165)	0.0003(0.0122)	0.944(1.163)	62.50(58.31)
[2023-09-29 13:35:40 10splitTasks](trainer.py 286): INFO [50/157]	0.1055(0.1139)	0.0003(0.0099)	1.127(1.145)	65.62(59.31)
[2023-09-29 13:35:41 10splitTasks](trainer.py 286): INFO [60/157]	0.1046(0.1124)	0.0002(0.0083)	1.298(1.144)	50.00(59.27)
[2023-09-29 13:35:42 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1109)	0.0002(0.0072)	1.127(1.133)	53.12(59.42)
[2023-09-29 13:35:43 10splitTasks](trainer.py 286): INFO [80/157]	0.1023(0.1099)	0.0002(0.0063)	0.965(1.122)	71.88(59.57)
[2023-09-29 13:35:44 10splitTasks](trainer.py 286): INFO [90/157]	0.1040(0.1091)	0.0002(0.0057)	0.954(1.116)	62.50(59.72)
[2023-09-29 13:35:45 10splitTasks](trainer.py 286): INFO [100/157]	0.1076(0.1086)	0.0004(0.0051)	0.890(1.103)	78.12(60.24)
[2023-09-29 13:35:46 10splitTasks](trainer.py 286): INFO [110/157]	0.1027(0.1080)	0.0002(0.0047)	0.865(1.104)	68.75(59.94)
[2023-09-29 13:35:47 10splitTasks](trainer.py 286): INFO [120/157]	0.1071(0.1077)	0.0005(0.0043)	1.130(1.098)	59.38(60.15)
[2023-09-29 13:35:48 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1073)	0.0002(0.0040)	1.330(1.109)	50.00(59.76)
[2023-09-29 13:35:49 10splitTasks](trainer.py 286): INFO [140/157]	0.1044(0.1070)	0.0002(0.0038)	1.697(1.108)	56.25(59.95)
[2023-09-29 13:35:51 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1068)	0.0001(0.0035)	0.843(1.095)	62.50(60.55)
[2023-09-29 13:35:51 10splitTasks](trainer.py 286): INFO [156/157]	0.0845(0.1065)	0.0001(0.0034)	1.542(1.087)	50.00(60.86)
[2023-09-29 13:35:51 10splitTasks](trainer.py 288): INFO  * Train Acc 60.860
[2023-09-29 13:35:53 10splitTasks](my_trainer.py 503): INFO  * Val Acc 68.000, Total time 1.67
[2023-09-29 13:35:53 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 13:35:53 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 13:35:53 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:35:54 10splitTasks](trainer.py 286): INFO [0/157]	0.6520(0.6520)	0.5439(0.5439)	0.936(0.936)	75.00(75.00)
[2023-09-29 13:35:55 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1527)	0.0002(0.0497)	0.824(0.908)	78.12(65.06)
[2023-09-29 13:35:56 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1291)	0.0003(0.0262)	1.159(0.964)	59.38(64.29)
[2023-09-29 13:35:57 10splitTasks](trainer.py 286): INFO [30/157]	0.1117(0.1206)	0.0006(0.0179)	1.004(0.951)	59.38(64.62)
[2023-09-29 13:35:58 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1163)	0.0003(0.0136)	0.720(0.946)	68.75(64.79)
[2023-09-29 13:35:59 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1135)	0.0003(0.0110)	0.805(0.966)	81.25(64.15)
[2023-09-29 13:36:00 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1117)	0.0002(0.0092)	0.840(0.979)	65.62(63.78)
[2023-09-29 13:36:01 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1104)	0.0003(0.0080)	0.880(0.983)	71.88(63.82)
[2023-09-29 13:36:02 10splitTasks](trainer.py 286): INFO [80/157]	0.1042(0.1096)	0.0003(0.0070)	1.084(0.990)	56.25(63.66)
[2023-09-29 13:36:03 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1091)	0.0002(0.0063)	1.084(0.997)	71.88(63.84)
[2023-09-29 13:36:04 10splitTasks](trainer.py 286): INFO [100/157]	0.1130(0.1087)	0.0005(0.0057)	0.999(0.992)	65.62(64.05)
[2023-09-29 13:36:05 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1082)	0.0003(0.0052)	0.847(0.992)	75.00(64.05)
[2023-09-29 13:36:06 10splitTasks](trainer.py 286): INFO [120/157]	0.1008(0.1076)	0.0003(0.0048)	1.119(0.988)	62.50(64.33)
[2023-09-29 13:36:07 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1073)	0.0002(0.0045)	0.997(0.975)	56.25(64.62)
[2023-09-29 13:36:08 10splitTasks](trainer.py 286): INFO [140/157]	0.1007(0.1069)	0.0002(0.0042)	1.182(0.977)	68.75(64.83)
[2023-09-29 13:36:09 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1065)	0.0001(0.0039)	0.936(0.971)	68.75(65.17)
[2023-09-29 13:36:10 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1062)	0.0001(0.0038)	0.509(0.973)	75.00(65.14)
[2023-09-29 13:36:10 10splitTasks](trainer.py 288): INFO  * Train Acc 65.140
[2023-09-29 13:36:11 10splitTasks](my_trainer.py 503): INFO  * Val Acc 67.000, Total time 1.63
[2023-09-29 13:36:11 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 13:36:11 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 13:36:11 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:36:12 10splitTasks](trainer.py 286): INFO [0/157]	0.6082(0.6082)	0.5034(0.5034)	1.009(1.009)	65.62(65.62)
[2023-09-29 13:36:13 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1485)	0.0001(0.0460)	0.979(0.906)	68.75(68.47)
[2023-09-29 13:36:14 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1268)	0.0003(0.0242)	1.116(0.952)	65.62(65.92)
[2023-09-29 13:36:15 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1191)	0.0003(0.0165)	0.982(0.931)	59.38(66.63)
[2023-09-29 13:36:16 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1154)	0.0003(0.0126)	0.923(0.907)	75.00(67.53)
[2023-09-29 13:36:17 10splitTasks](trainer.py 286): INFO [50/157]	0.1041(0.1129)	0.0002(0.0102)	0.947(0.887)	62.50(68.32)
[2023-09-29 13:36:18 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1112)	0.0002(0.0086)	0.837(0.873)	81.25(69.01)
[2023-09-29 13:36:19 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1099)	0.0003(0.0074)	0.829(0.871)	68.75(69.01)
[2023-09-29 13:36:20 10splitTasks](trainer.py 286): INFO [80/157]	0.1070(0.1092)	0.0002(0.0065)	1.446(0.870)	59.38(69.29)
[2023-09-29 13:36:21 10splitTasks](trainer.py 286): INFO [90/157]	0.1050(0.1086)	0.0004(0.0058)	1.233(0.874)	56.25(68.89)
[2023-09-29 13:36:22 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1082)	0.0003(0.0053)	0.833(0.885)	75.00(68.69)
[2023-09-29 13:36:23 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1077)	0.0002(0.0049)	0.550(0.882)	81.25(69.09)
[2023-09-29 13:36:24 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1072)	0.0003(0.0045)	0.845(0.872)	71.88(69.42)
[2023-09-29 13:36:25 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1068)	0.0001(0.0042)	0.900(0.868)	71.88(69.56)
[2023-09-29 13:36:26 10splitTasks](trainer.py 286): INFO [140/157]	0.1033(0.1065)	0.0003(0.0039)	0.933(0.866)	68.75(69.59)
[2023-09-29 13:36:27 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1063)	0.0001(0.0037)	1.216(0.864)	65.62(69.56)
[2023-09-29 13:36:28 10splitTasks](trainer.py 286): INFO [156/157]	0.0820(0.1060)	0.0001(0.0035)	2.086(0.864)	25.00(69.54)
[2023-09-29 13:36:28 10splitTasks](trainer.py 288): INFO  * Train Acc 69.540
[2023-09-29 13:36:30 10splitTasks](my_trainer.py 503): INFO  * Val Acc 66.000, Total time 1.58
[2023-09-29 13:36:30 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 13:36:30 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 13:36:30 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:36:30 10splitTasks](trainer.py 286): INFO [0/157]	0.5812(0.5812)	0.4768(0.4768)	0.708(0.708)	65.62(65.62)
[2023-09-29 13:36:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1471)	0.0002(0.0436)	0.836(0.828)	56.25(69.03)
[2023-09-29 13:36:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1258)	0.0002(0.0230)	0.528(0.807)	81.25(70.68)
[2023-09-29 13:36:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1193)	0.0003(0.0157)	0.695(0.777)	78.12(71.77)
[2023-09-29 13:36:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1159)	0.0003(0.0119)	0.765(0.766)	62.50(71.80)
[2023-09-29 13:36:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1010(0.1133)	0.0002(0.0097)	0.813(0.748)	68.75(72.37)
[2023-09-29 13:36:37 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1117)	0.0003(0.0081)	0.801(0.746)	75.00(72.49)
[2023-09-29 13:36:38 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1104)	0.0003(0.0070)	0.679(0.751)	68.75(72.32)
[2023-09-29 13:36:39 10splitTasks](trainer.py 286): INFO [80/157]	0.1028(0.1094)	0.0003(0.0062)	0.472(0.744)	84.38(72.65)
[2023-09-29 13:36:40 10splitTasks](trainer.py 286): INFO [90/157]	0.1053(0.1086)	0.0002(0.0055)	0.630(0.743)	78.12(72.77)
[2023-09-29 13:36:41 10splitTasks](trainer.py 286): INFO [100/157]	0.1038(0.1080)	0.0003(0.0050)	0.830(0.741)	68.75(72.80)
[2023-09-29 13:36:42 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1076)	0.0002(0.0046)	0.792(0.744)	65.62(72.89)
[2023-09-29 13:36:43 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1072)	0.0002(0.0042)	0.915(0.750)	71.88(72.93)
[2023-09-29 13:36:44 10splitTasks](trainer.py 286): INFO [130/157]	0.1076(0.1072)	0.0003(0.0039)	1.120(0.754)	71.88(72.97)
[2023-09-29 13:36:45 10splitTasks](trainer.py 286): INFO [140/157]	0.1036(0.1070)	0.0002(0.0037)	0.411(0.751)	87.50(73.23)
[2023-09-29 13:36:46 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1067)	0.0001(0.0035)	0.793(0.758)	71.88(72.97)
[2023-09-29 13:36:46 10splitTasks](trainer.py 286): INFO [156/157]	0.0788(0.1063)	0.0001(0.0033)	0.808(0.754)	87.50(73.12)
[2023-09-29 13:36:46 10splitTasks](trainer.py 288): INFO  * Train Acc 73.120
[2023-09-29 13:36:48 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.000, Total time 1.58
[2023-09-29 13:36:48 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 13:36:48 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 13:36:48 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:36:49 10splitTasks](trainer.py 286): INFO [0/157]	0.5732(0.5732)	0.4613(0.4613)	0.409(0.409)	90.62(90.62)
[2023-09-29 13:36:50 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1491)	0.0002(0.0422)	0.625(0.660)	75.00(76.70)
[2023-09-29 13:36:51 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1276)	0.0002(0.0223)	1.296(0.732)	62.50(73.66)
[2023-09-29 13:36:52 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1195)	0.0003(0.0152)	0.545(0.688)	78.12(75.40)
[2023-09-29 13:36:53 10splitTasks](trainer.py 286): INFO [40/157]	0.1042(0.1152)	0.0003(0.0115)	0.486(0.667)	84.38(75.76)
[2023-09-29 13:36:54 10splitTasks](trainer.py 286): INFO [50/157]	0.1026(0.1127)	0.0002(0.0093)	0.468(0.639)	87.50(77.02)
[2023-09-29 13:36:55 10splitTasks](trainer.py 286): INFO [60/157]	0.1098(0.1111)	0.0005(0.0079)	0.252(0.623)	93.75(77.56)
[2023-09-29 13:36:56 10splitTasks](trainer.py 286): INFO [70/157]	0.1030(0.1098)	0.0001(0.0068)	0.726(0.621)	75.00(77.73)
[2023-09-29 13:36:57 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1091)	0.0002(0.0060)	0.515(0.615)	84.38(78.24)
[2023-09-29 13:36:58 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1084)	0.0002(0.0054)	0.605(0.617)	84.38(78.47)
[2023-09-29 13:36:59 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1078)	0.0002(0.0049)	0.746(0.617)	75.00(78.40)
[2023-09-29 13:37:00 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1073)	0.0002(0.0045)	0.614(0.619)	75.00(78.29)
[2023-09-29 13:37:01 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1069)	0.0003(0.0041)	0.665(0.616)	84.38(78.49)
[2023-09-29 13:37:02 10splitTasks](trainer.py 286): INFO [130/157]	0.1044(0.1067)	0.0001(0.0038)	0.349(0.617)	87.50(78.36)
[2023-09-29 13:37:03 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1063)	0.0003(0.0036)	0.780(0.623)	68.75(78.19)
[2023-09-29 13:37:04 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1061)	0.0002(0.0033)	0.693(0.625)	78.12(78.08)
[2023-09-29 13:37:05 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1058)	0.0001(0.0032)	1.023(0.628)	75.00(77.94)
[2023-09-29 13:37:05 10splitTasks](trainer.py 288): INFO  * Train Acc 77.940
[2023-09-29 13:37:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.800, Total time 1.62
[2023-09-29 13:37:06 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 13:37:06 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 13:37:06 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:37:07 10splitTasks](trainer.py 286): INFO [0/157]	0.6029(0.6029)	0.4814(0.4814)	0.348(0.348)	87.50(87.50)
[2023-09-29 13:37:08 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1486)	0.0002(0.0440)	0.869(0.519)	65.62(79.83)
[2023-09-29 13:37:09 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1264)	0.0002(0.0232)	0.349(0.552)	84.38(79.76)
[2023-09-29 13:37:10 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1184)	0.0003(0.0158)	0.248(0.532)	93.75(81.25)
[2023-09-29 13:37:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1146)	0.0003(0.0120)	0.534(0.523)	78.12(81.33)
[2023-09-29 13:37:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1122)	0.0003(0.0097)	0.371(0.522)	90.62(81.25)
[2023-09-29 13:37:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1106)	0.0001(0.0082)	0.664(0.527)	65.62(80.89)
[2023-09-29 13:37:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1094)	0.0003(0.0070)	0.232(0.515)	90.62(81.38)
[2023-09-29 13:37:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1085)	0.0002(0.0062)	0.382(0.507)	84.38(81.71)
[2023-09-29 13:37:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1011(0.1077)	0.0002(0.0055)	0.611(0.511)	81.25(81.49)
[2023-09-29 13:37:17 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1074)	0.0002(0.0050)	0.338(0.507)	87.50(81.47)
[2023-09-29 13:37:18 10splitTasks](trainer.py 286): INFO [110/157]	0.1046(0.1069)	0.0003(0.0046)	0.371(0.502)	84.38(81.93)
[2023-09-29 13:37:19 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1066)	0.0002(0.0042)	0.635(0.504)	78.12(81.92)
[2023-09-29 13:37:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1064)	0.0003(0.0039)	0.560(0.507)	75.00(81.70)
[2023-09-29 13:37:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1062)	0.0003(0.0037)	0.401(0.507)	90.62(81.72)
[2023-09-29 13:37:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1059)	0.0001(0.0035)	0.672(0.513)	75.00(81.48)
[2023-09-29 13:37:23 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1055)	0.0001(0.0033)	1.498(0.512)	62.50(81.46)
[2023-09-29 13:37:23 10splitTasks](trainer.py 288): INFO  * Train Acc 81.460
[2023-09-29 13:37:25 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.000, Total time 1.59
[2023-09-29 13:37:25 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 13:37:25 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 13:37:25 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:37:25 10splitTasks](trainer.py 286): INFO [0/157]	0.6250(0.6250)	0.5198(0.5198)	0.494(0.494)	78.12(78.12)
[2023-09-29 13:37:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1503)	0.0002(0.0475)	0.425(0.424)	90.62(84.38)
[2023-09-29 13:37:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1275)	0.0002(0.0250)	0.384(0.440)	84.38(83.78)
[2023-09-29 13:37:28 10splitTasks](trainer.py 286): INFO [30/157]	0.1073(0.1196)	0.0002(0.0171)	0.502(0.433)	81.25(84.48)
[2023-09-29 13:37:29 10splitTasks](trainer.py 286): INFO [40/157]	0.1055(0.1154)	0.0003(0.0130)	0.327(0.418)	87.50(84.91)
[2023-09-29 13:37:30 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1127)	0.0002(0.0105)	0.611(0.425)	71.88(84.44)
[2023-09-29 13:37:31 10splitTasks](trainer.py 286): INFO [60/157]	0.1047(0.1110)	0.0003(0.0088)	0.590(0.433)	81.25(84.43)
[2023-09-29 13:37:32 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1098)	0.0001(0.0076)	0.154(0.429)	96.88(84.38)
[2023-09-29 13:37:34 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1089)	0.0003(0.0067)	0.524(0.435)	78.12(84.22)
[2023-09-29 13:37:35 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1081)	0.0002(0.0060)	0.336(0.435)	90.62(84.38)
[2023-09-29 13:37:36 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1075)	0.0002(0.0054)	0.637(0.432)	81.25(84.38)
[2023-09-29 13:37:37 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1070)	0.0002(0.0050)	0.420(0.437)	87.50(84.29)
[2023-09-29 13:37:38 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1066)	0.0003(0.0046)	0.427(0.441)	87.50(84.38)
[2023-09-29 13:37:39 10splitTasks](trainer.py 286): INFO [130/157]	0.1023(0.1064)	0.0002(0.0043)	0.307(0.436)	93.75(84.52)
[2023-09-29 13:37:40 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1061)	0.0002(0.0040)	0.275(0.438)	93.75(84.33)
[2023-09-29 13:37:41 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1061)	0.0001(0.0037)	0.430(0.439)	81.25(84.11)
[2023-09-29 13:37:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1057)	0.0001(0.0036)	0.799(0.442)	75.00(84.00)
[2023-09-29 13:37:41 10splitTasks](trainer.py 288): INFO  * Train Acc 84.000
[2023-09-29 13:37:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.400, Total time 1.64
[2023-09-29 13:37:43 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 13:37:43 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 13:37:43 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:37:44 10splitTasks](trainer.py 286): INFO [0/157]	0.5977(0.5977)	0.4930(0.4930)	0.306(0.306)	87.50(87.50)
[2023-09-29 13:37:45 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1489)	0.0003(0.0451)	0.405(0.344)	84.38(86.65)
[2023-09-29 13:37:46 10splitTasks](trainer.py 286): INFO [20/157]	0.1028(0.1268)	0.0001(0.0237)	0.280(0.369)	93.75(86.76)
[2023-09-29 13:37:47 10splitTasks](trainer.py 286): INFO [30/157]	0.1025(0.1189)	0.0003(0.0162)	0.198(0.378)	100.00(86.69)
[2023-09-29 13:37:48 10splitTasks](trainer.py 286): INFO [40/157]	0.1035(0.1149)	0.0002(0.0123)	0.551(0.390)	78.12(86.05)
[2023-09-29 13:37:49 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1124)	0.0002(0.0099)	0.380(0.390)	84.38(85.36)
[2023-09-29 13:37:50 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1110)	0.0002(0.0084)	0.303(0.380)	90.62(86.07)
[2023-09-29 13:37:51 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1096)	0.0003(0.0072)	0.488(0.385)	81.25(86.09)
[2023-09-29 13:37:52 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1090)	0.0002(0.0064)	0.467(0.378)	81.25(86.38)
[2023-09-29 13:37:53 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1086)	0.0002(0.0057)	0.864(0.374)	71.88(86.47)
[2023-09-29 13:37:54 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1079)	0.0003(0.0052)	0.361(0.375)	87.50(86.51)
[2023-09-29 13:37:55 10splitTasks](trainer.py 286): INFO [110/157]	0.1052(0.1074)	0.0002(0.0047)	0.382(0.381)	87.50(86.29)
[2023-09-29 13:37:56 10splitTasks](trainer.py 286): INFO [120/157]	0.1012(0.1069)	0.0002(0.0044)	0.254(0.380)	87.50(86.47)
[2023-09-29 13:37:57 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1066)	0.0003(0.0041)	0.194(0.376)	93.75(86.62)
[2023-09-29 13:37:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1062)	0.0002(0.0038)	0.408(0.375)	81.25(86.55)
[2023-09-29 13:37:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1061)	0.0001(0.0036)	0.771(0.375)	71.88(86.57)
[2023-09-29 13:38:00 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1057)	0.0001(0.0035)	1.411(0.375)	75.00(86.76)
[2023-09-29 13:38:00 10splitTasks](trainer.py 288): INFO  * Train Acc 86.760
[2023-09-29 13:38:01 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.200, Total time 1.62
[2023-09-29 13:38:01 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 13:38:01 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 13:38:01 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:38:02 10splitTasks](trainer.py 286): INFO [0/157]	0.5735(0.5735)	0.4602(0.4602)	0.855(0.855)	75.00(75.00)
[2023-09-29 13:38:03 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1458)	0.0002(0.0421)	0.264(0.431)	90.62(86.08)
[2023-09-29 13:38:04 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1250)	0.0002(0.0222)	0.260(0.395)	93.75(85.57)
[2023-09-29 13:38:05 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1182)	0.0002(0.0151)	0.346(0.367)	90.62(86.79)
[2023-09-29 13:38:06 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1141)	0.0002(0.0115)	0.553(0.366)	78.12(87.04)
[2023-09-29 13:38:07 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1117)	0.0003(0.0093)	0.186(0.351)	93.75(87.93)
[2023-09-29 13:38:08 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1100)	0.0003(0.0078)	0.302(0.347)	90.62(87.65)
[2023-09-29 13:38:09 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1094)	0.0002(0.0068)	0.619(0.350)	81.25(87.54)
[2023-09-29 13:38:10 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1085)	0.0002(0.0060)	0.308(0.346)	90.62(87.42)
[2023-09-29 13:38:11 10splitTasks](trainer.py 286): INFO [90/157]	0.1107(0.1080)	0.0004(0.0053)	0.485(0.352)	81.25(87.05)
[2023-09-29 13:38:12 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1074)	0.0003(0.0048)	0.179(0.350)	96.88(87.22)
[2023-09-29 13:38:13 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1069)	0.0002(0.0044)	0.350(0.346)	90.62(87.50)
[2023-09-29 13:38:14 10splitTasks](trainer.py 286): INFO [120/157]	0.1084(0.1065)	0.0002(0.0041)	0.781(0.351)	75.00(87.32)
[2023-09-29 13:38:15 10splitTasks](trainer.py 286): INFO [130/157]	0.1009(0.1063)	0.0002(0.0038)	0.475(0.354)	84.38(87.17)
[2023-09-29 13:38:16 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1061)	0.0002(0.0035)	0.349(0.352)	90.62(87.43)
[2023-09-29 13:38:17 10splitTasks](trainer.py 286): INFO [150/157]	0.1027(0.1059)	0.0001(0.0033)	0.340(0.355)	87.50(87.23)
[2023-09-29 13:38:18 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1056)	0.0001(0.0032)	1.166(0.355)	62.50(87.26)
[2023-09-29 13:38:18 10splitTasks](trainer.py 288): INFO  * Train Acc 87.260
[2023-09-29 13:38:20 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.000, Total time 1.60
[2023-09-29 13:38:20 10splitTasks](my_trainer.py 206): INFO Pruning for task5
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 3548/4174 (85.00%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 1545/1818 (84.98%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 13903/16357 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 6179/7269 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 6179/7269 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 6179/7269 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 13903/16357 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 6179/7269 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 6179/7269 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 13903/16357 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 6179/7269 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 12358/14539 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 55613/65427 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 24717/29079 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 49433/58157 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 24717/29079 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 55613/65427 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 24717/29079 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 24717/29079 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 55613/65427 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 24717/29079 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 24717/29079 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 55613/65427 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 24717/29079 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 49433/58157 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 222452/261708 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 197735/232629 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 222452/261708 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 222452/261708 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 222452/261708 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 222452/261708 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 222452/261708 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 98868/116315 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 197735/232629 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 889808/1046833 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 395470/465259 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 790939/930517 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 395470/465259 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 889808/1046833 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 395470/465259 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 395470/465259 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 889808/1046833 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 395470/465259 (85.00%) (Total in layer: 1048576)
[2023-09-29 13:38:20 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 13:38:20 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 13:38:20 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 13:38:20 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:38:21 10splitTasks](trainer.py 286): INFO [0/157]	0.5709(0.5709)	0.4546(0.4546)	0.327(0.327)	81.25(81.25)
[2023-09-29 13:38:22 10splitTasks](trainer.py 286): INFO [10/157]	0.1008(0.1451)	0.0002(0.0416)	0.518(0.493)	75.00(80.97)
[2023-09-29 13:38:23 10splitTasks](trainer.py 286): INFO [20/157]	0.1049(0.1244)	0.0003(0.0219)	0.388(0.504)	81.25(79.61)
[2023-09-29 13:38:24 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1171)	0.0002(0.0149)	0.654(0.515)	81.25(79.33)
[2023-09-29 13:38:25 10splitTasks](trainer.py 286): INFO [40/157]	0.1033(0.1136)	0.0003(0.0114)	0.437(0.514)	81.25(79.57)
[2023-09-29 13:38:26 10splitTasks](trainer.py 286): INFO [50/157]	0.1009(0.1115)	0.0002(0.0092)	0.773(0.535)	65.62(79.47)
[2023-09-29 13:38:27 10splitTasks](trainer.py 286): INFO [60/157]	0.1081(0.1100)	0.0002(0.0077)	0.401(0.549)	87.50(79.25)
[2023-09-29 13:38:28 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1089)	0.0002(0.0067)	0.530(0.549)	78.12(79.58)
[2023-09-29 13:38:29 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1082)	0.0002(0.0059)	0.853(0.555)	71.88(79.13)
[2023-09-29 13:38:30 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1075)	0.0002(0.0053)	0.450(0.558)	78.12(79.16)
[2023-09-29 13:38:31 10splitTasks](trainer.py 286): INFO [100/157]	0.1158(0.1073)	0.0005(0.0048)	0.720(0.569)	71.88(78.65)
[2023-09-29 13:38:32 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1068)	0.0002(0.0044)	0.666(0.566)	71.88(78.72)
[2023-09-29 13:38:33 10splitTasks](trainer.py 286): INFO [120/157]	0.1082(0.1065)	0.0003(0.0040)	0.617(0.572)	81.25(78.67)
[2023-09-29 13:38:34 10splitTasks](trainer.py 286): INFO [130/157]	0.1009(0.1062)	0.0001(0.0038)	0.293(0.571)	90.62(78.70)
[2023-09-29 13:38:35 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1059)	0.0003(0.0035)	1.324(0.574)	53.12(78.63)
[2023-09-29 13:38:36 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1057)	0.0001(0.0033)	0.700(0.571)	75.00(78.81)
[2023-09-29 13:38:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1054)	0.0001(0.0032)	1.078(0.574)	50.00(78.74)
[2023-09-29 13:38:37 10splitTasks](trainer.py 288): INFO  * Train Acc 78.740
[2023-09-29 13:38:38 10splitTasks](my_trainer.py 503): INFO  * Val Acc 75.000, Total time 1.67
[2023-09-29 13:38:38 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 13:38:38 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 13:38:38 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:38:39 10splitTasks](trainer.py 286): INFO [0/157]	0.6010(0.6010)	0.4961(0.4961)	0.280(0.280)	93.75(93.75)
[2023-09-29 13:38:40 10splitTasks](trainer.py 286): INFO [10/157]	0.1038(0.1487)	0.0004(0.0454)	0.573(0.513)	78.12(80.97)
[2023-09-29 13:38:41 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1271)	0.0002(0.0239)	0.316(0.501)	87.50(81.70)
[2023-09-29 13:38:42 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1192)	0.0003(0.0163)	0.355(0.518)	87.50(81.25)
[2023-09-29 13:38:43 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1152)	0.0002(0.0124)	0.281(0.503)	90.62(81.40)
[2023-09-29 13:38:44 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1126)	0.0003(0.0100)	0.377(0.499)	90.62(81.92)
[2023-09-29 13:38:45 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1108)	0.0002(0.0084)	0.231(0.494)	93.75(82.27)
[2023-09-29 13:38:46 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1100)	0.0002(0.0073)	0.316(0.490)	90.62(82.31)
[2023-09-29 13:38:47 10splitTasks](trainer.py 286): INFO [80/157]	0.1045(0.1098)	0.0003(0.0064)	0.423(0.491)	81.25(82.37)
[2023-09-29 13:38:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1090)	0.0004(0.0058)	0.576(0.499)	78.12(82.11)
[2023-09-29 13:38:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1086)	0.0002(0.0052)	0.569(0.503)	78.12(82.09)
[2023-09-29 13:38:50 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1085)	0.0002(0.0048)	0.470(0.495)	81.25(82.35)
[2023-09-29 13:38:51 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1080)	0.0004(0.0044)	0.471(0.495)	81.25(82.21)
[2023-09-29 13:38:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1075)	0.0002(0.0041)	0.579(0.505)	75.00(81.92)
[2023-09-29 13:38:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1011(0.1072)	0.0002(0.0038)	0.405(0.505)	84.38(81.78)
[2023-09-29 13:38:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1069)	0.0001(0.0036)	0.490(0.508)	75.00(81.66)
[2023-09-29 13:38:55 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1065)	0.0001(0.0035)	0.543(0.505)	62.50(81.66)
[2023-09-29 13:38:55 10splitTasks](trainer.py 288): INFO  * Train Acc 81.660
[2023-09-29 13:38:57 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.800, Total time 1.59
[2023-09-29 13:38:57 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 13:38:57 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 13:38:57 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:38:57 10splitTasks](trainer.py 286): INFO [0/157]	0.5669(0.5669)	0.4489(0.4489)	0.667(0.667)	75.00(75.00)
[2023-09-29 13:38:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1037(0.1454)	0.0002(0.0411)	0.486(0.455)	84.38(82.95)
[2023-09-29 13:38:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1251)	0.0002(0.0216)	0.169(0.490)	90.62(81.10)
[2023-09-29 13:39:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1177)	0.0002(0.0147)	0.632(0.500)	78.12(81.35)
[2023-09-29 13:39:01 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1144)	0.0002(0.0112)	0.438(0.485)	93.75(82.24)
[2023-09-29 13:39:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1122)	0.0003(0.0091)	0.300(0.459)	90.62(83.03)
[2023-09-29 13:39:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1045(0.1109)	0.0002(0.0077)	0.659(0.467)	81.25(82.63)
[2023-09-29 13:39:05 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1097)	0.0002(0.0066)	0.415(0.465)	78.12(82.66)
[2023-09-29 13:39:06 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1088)	0.0002(0.0058)	0.332(0.454)	87.50(83.14)
[2023-09-29 13:39:07 10splitTasks](trainer.py 286): INFO [90/157]	0.1010(0.1081)	0.0001(0.0052)	0.534(0.451)	81.25(83.38)
[2023-09-29 13:39:08 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1075)	0.0003(0.0047)	0.567(0.463)	81.25(82.86)
[2023-09-29 13:39:09 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1071)	0.0003(0.0043)	0.513(0.468)	75.00(82.71)
[2023-09-29 13:39:10 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1067)	0.0003(0.0040)	0.215(0.461)	90.62(82.95)
[2023-09-29 13:39:11 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1064)	0.0003(0.0037)	0.799(0.465)	71.88(82.90)
[2023-09-29 13:39:12 10splitTasks](trainer.py 286): INFO [140/157]	0.1028(0.1064)	0.0002(0.0035)	0.413(0.466)	81.25(82.85)
[2023-09-29 13:39:13 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1061)	0.0001(0.0033)	0.422(0.467)	84.38(82.72)
[2023-09-29 13:39:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0798(0.1058)	0.0001(0.0031)	1.024(0.470)	75.00(82.64)
[2023-09-29 13:39:13 10splitTasks](trainer.py 288): INFO  * Train Acc 82.640
[2023-09-29 13:39:15 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.600, Total time 1.71
[2023-09-29 13:39:15 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 13:39:15 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 13:39:15 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:39:16 10splitTasks](trainer.py 286): INFO [0/157]	0.6254(0.6254)	0.4977(0.4977)	0.253(0.253)	81.25(81.25)
[2023-09-29 13:39:17 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1513)	0.0002(0.0455)	0.616(0.450)	75.00(82.39)
[2023-09-29 13:39:18 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1278)	0.0004(0.0240)	0.378(0.475)	81.25(82.59)
[2023-09-29 13:39:19 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1199)	0.0001(0.0164)	0.455(0.460)	81.25(83.57)
[2023-09-29 13:39:20 10splitTasks](trainer.py 286): INFO [40/157]	0.1143(0.1158)	0.0005(0.0125)	0.499(0.468)	78.12(83.00)
[2023-09-29 13:39:21 10splitTasks](trainer.py 286): INFO [50/157]	0.1228(0.1137)	0.0002(0.0101)	0.318(0.461)	87.50(83.52)
[2023-09-29 13:39:22 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1121)	0.0003(0.0085)	0.251(0.460)	93.75(83.86)
[2023-09-29 13:39:23 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1107)	0.0002(0.0073)	0.370(0.458)	84.38(83.89)
[2023-09-29 13:39:24 10splitTasks](trainer.py 286): INFO [80/157]	0.1040(0.1097)	0.0002(0.0065)	0.263(0.460)	96.88(83.76)
[2023-09-29 13:39:25 10splitTasks](trainer.py 286): INFO [90/157]	0.1052(0.1089)	0.0002(0.0058)	0.251(0.457)	87.50(83.83)
[2023-09-29 13:39:26 10splitTasks](trainer.py 286): INFO [100/157]	0.1048(0.1083)	0.0002(0.0052)	0.264(0.451)	87.50(84.19)
[2023-09-29 13:39:27 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1077)	0.0002(0.0048)	0.475(0.446)	87.50(84.38)
[2023-09-29 13:39:28 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1074)	0.0002(0.0044)	0.316(0.442)	87.50(84.45)
[2023-09-29 13:39:29 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1070)	0.0003(0.0041)	0.225(0.443)	87.50(84.21)
[2023-09-29 13:39:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1067)	0.0002(0.0038)	0.344(0.439)	90.62(84.35)
[2023-09-29 13:39:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1064)	0.0001(0.0036)	0.379(0.438)	84.38(84.27)
[2023-09-29 13:39:32 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1061)	0.0001(0.0035)	1.161(0.440)	75.00(84.34)
[2023-09-29 13:39:32 10splitTasks](trainer.py 288): INFO  * Train Acc 84.340
[2023-09-29 13:39:34 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.600, Total time 1.64
[2023-09-29 13:39:34 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 13:39:34 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 13:39:34 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:39:34 10splitTasks](trainer.py 286): INFO [0/157]	0.5902(0.5902)	0.4814(0.4814)	0.485(0.485)	87.50(87.50)
[2023-09-29 13:39:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1038(0.1485)	0.0005(0.0440)	0.590(0.371)	81.25(86.93)
[2023-09-29 13:39:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1024(0.1265)	0.0003(0.0232)	0.293(0.353)	90.62(87.35)
[2023-09-29 13:39:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1186)	0.0001(0.0158)	0.460(0.359)	87.50(86.59)
[2023-09-29 13:39:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1149)	0.0002(0.0120)	0.164(0.350)	96.88(86.89)
[2023-09-29 13:39:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1059(0.1125)	0.0001(0.0097)	0.325(0.347)	87.50(86.95)
[2023-09-29 13:39:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1143(0.1111)	0.0006(0.0082)	0.320(0.352)	90.62(87.09)
[2023-09-29 13:39:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1031(0.1100)	0.0003(0.0071)	0.454(0.367)	87.50(87.02)
[2023-09-29 13:39:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1060(0.1092)	0.0003(0.0063)	0.438(0.372)	81.25(86.81)
[2023-09-29 13:39:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1097(0.1084)	0.0003(0.0056)	0.365(0.371)	87.50(86.81)
[2023-09-29 13:39:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1078)	0.0003(0.0051)	0.317(0.364)	87.50(87.00)
[2023-09-29 13:39:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1073)	0.0003(0.0047)	0.356(0.368)	87.50(86.88)
[2023-09-29 13:39:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1069)	0.0003(0.0043)	0.487(0.373)	78.12(86.57)
[2023-09-29 13:39:48 10splitTasks](trainer.py 286): INFO [130/157]	0.1026(0.1067)	0.0003(0.0040)	0.275(0.378)	87.50(86.43)
[2023-09-29 13:39:49 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1063)	0.0003(0.0037)	0.187(0.380)	93.75(86.41)
[2023-09-29 13:39:50 10splitTasks](trainer.py 286): INFO [150/157]	0.1018(0.1060)	0.0002(0.0035)	0.385(0.383)	90.62(86.42)
[2023-09-29 13:39:50 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1057)	0.0001(0.0034)	0.236(0.383)	87.50(86.46)
[2023-09-29 13:39:50 10splitTasks](trainer.py 288): INFO  * Train Acc 86.460
[2023-09-29 13:39:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.200, Total time 1.71
[2023-09-29 13:39:52 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 13:39:52 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 13:39:52 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:39:53 10splitTasks](trainer.py 286): INFO [0/157]	0.6068(0.6068)	0.4953(0.4953)	0.285(0.285)	87.50(87.50)
[2023-09-29 13:39:54 10splitTasks](trainer.py 286): INFO [10/157]	0.1030(0.1503)	0.0002(0.0453)	0.248(0.379)	90.62(84.38)
[2023-09-29 13:39:55 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1280)	0.0002(0.0239)	0.521(0.354)	84.38(86.31)
[2023-09-29 13:39:56 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1203)	0.0003(0.0163)	0.249(0.367)	90.62(86.39)
[2023-09-29 13:39:57 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1158)	0.0001(0.0124)	0.316(0.372)	87.50(85.98)
[2023-09-29 13:39:58 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1135)	0.0002(0.0100)	0.070(0.357)	100.00(86.70)
[2023-09-29 13:39:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1118)	0.0004(0.0084)	0.215(0.357)	93.75(86.42)
[2023-09-29 13:40:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1030(0.1105)	0.0002(0.0073)	0.242(0.360)	87.50(86.31)
[2023-09-29 13:40:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1045(0.1095)	0.0003(0.0064)	0.392(0.358)	84.38(86.54)
[2023-09-29 13:40:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1088)	0.0003(0.0057)	0.391(0.355)	87.50(86.61)
[2023-09-29 13:40:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1079(0.1084)	0.0002(0.0052)	0.385(0.345)	84.38(86.94)
[2023-09-29 13:40:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1029(0.1078)	0.0003(0.0047)	0.252(0.343)	90.62(86.99)
[2023-09-29 13:40:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1026(0.1075)	0.0003(0.0044)	0.424(0.348)	81.25(86.80)
[2023-09-29 13:40:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1070)	0.0002(0.0041)	0.500(0.352)	87.50(86.67)
[2023-09-29 13:40:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1010(0.1066)	0.0001(0.0038)	0.117(0.355)	100.00(86.72)
[2023-09-29 13:40:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1063)	0.0001(0.0036)	0.309(0.355)	90.62(86.67)
[2023-09-29 13:40:09 10splitTasks](trainer.py 286): INFO [156/157]	0.0803(0.1060)	0.0001(0.0034)	0.688(0.357)	75.00(86.58)
[2023-09-29 13:40:09 10splitTasks](trainer.py 288): INFO  * Train Acc 86.580
[2023-09-29 13:40:10 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.000, Total time 1.64
[2023-09-29 13:40:10 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 13:40:10 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 13:40:10 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:40:11 10splitTasks](trainer.py 286): INFO [0/157]	0.6231(0.6231)	0.4975(0.4975)	0.201(0.201)	93.75(93.75)
[2023-09-29 13:40:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1075(0.1536)	0.0002(0.0455)	0.280(0.374)	87.50(86.36)
[2023-09-29 13:40:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1022(0.1305)	0.0003(0.0240)	0.179(0.344)	96.88(87.50)
[2023-09-29 13:40:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1026(0.1216)	0.0003(0.0164)	0.446(0.348)	87.50(87.80)
[2023-09-29 13:40:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1170)	0.0002(0.0125)	0.158(0.347)	96.88(87.65)
[2023-09-29 13:40:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1143)	0.0003(0.0101)	0.377(0.339)	90.62(87.81)
[2023-09-29 13:40:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1011(0.1125)	0.0002(0.0085)	0.293(0.343)	87.50(87.65)
[2023-09-29 13:40:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1112)	0.0002(0.0073)	0.098(0.343)	96.88(87.76)
[2023-09-29 13:40:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1102)	0.0001(0.0065)	0.356(0.343)	87.50(87.69)
[2023-09-29 13:40:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1094)	0.0003(0.0058)	0.412(0.339)	90.62(87.88)
[2023-09-29 13:40:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1149(0.1088)	0.0003(0.0052)	0.159(0.336)	90.62(87.78)
[2023-09-29 13:40:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1083)	0.0002(0.0048)	0.489(0.336)	78.12(87.75)
[2023-09-29 13:40:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1009(0.1079)	0.0001(0.0044)	0.130(0.328)	96.88(88.04)
[2023-09-29 13:40:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1074)	0.0003(0.0041)	0.295(0.330)	93.75(87.98)
[2023-09-29 13:40:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1070)	0.0002(0.0038)	0.354(0.331)	87.50(87.94)
[2023-09-29 13:40:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1067)	0.0001(0.0036)	0.155(0.332)	90.62(87.89)
[2023-09-29 13:40:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1063)	0.0001(0.0035)	1.079(0.329)	50.00(88.00)
[2023-09-29 13:40:27 10splitTasks](trainer.py 288): INFO  * Train Acc 88.000
[2023-09-29 13:40:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.72
[2023-09-29 13:40:29 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 13:40:29 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 13:40:29 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:40:30 10splitTasks](trainer.py 286): INFO [0/157]	0.6306(0.6306)	0.5251(0.5251)	0.513(0.513)	78.12(78.12)
[2023-09-29 13:40:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1518)	0.0002(0.0480)	0.166(0.255)	93.75(91.76)
[2023-09-29 13:40:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1026(0.1283)	0.0002(0.0253)	0.193(0.291)	90.62(89.88)
[2023-09-29 13:40:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1200)	0.0002(0.0172)	0.269(0.296)	93.75(89.42)
[2023-09-29 13:40:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1036(0.1159)	0.0002(0.0131)	0.313(0.303)	90.62(89.02)
[2023-09-29 13:40:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1054(0.1137)	0.0002(0.0106)	0.193(0.305)	96.88(88.85)
[2023-09-29 13:40:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1118)	0.0002(0.0089)	0.321(0.317)	90.62(88.78)
[2023-09-29 13:40:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1057(0.1105)	0.0002(0.0077)	0.231(0.319)	90.62(88.86)
[2023-09-29 13:40:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1048(0.1097)	0.0003(0.0068)	0.110(0.314)	100.00(88.93)
[2023-09-29 13:40:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1090)	0.0003(0.0061)	0.466(0.313)	87.50(89.01)
[2023-09-29 13:40:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1084)	0.0003(0.0055)	0.475(0.309)	84.38(89.11)
[2023-09-29 13:40:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1080)	0.0002(0.0050)	0.197(0.309)	90.62(89.16)
[2023-09-29 13:40:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1063(0.1079)	0.0003(0.0047)	0.184(0.310)	93.75(89.20)
[2023-09-29 13:40:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1075)	0.0002(0.0043)	0.125(0.308)	96.88(89.22)
[2023-09-29 13:40:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1049(0.1072)	0.0002(0.0040)	0.408(0.307)	84.38(89.18)
[2023-09-29 13:40:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1075(0.1071)	0.0002(0.0038)	0.406(0.313)	84.38(88.99)
[2023-09-29 13:40:46 10splitTasks](trainer.py 286): INFO [156/157]	0.0800(0.1067)	0.0001(0.0037)	0.476(0.317)	75.00(88.78)
[2023-09-29 13:40:46 10splitTasks](trainer.py 288): INFO  * Train Acc 88.780
[2023-09-29 13:40:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.59
[2023-09-29 13:40:47 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 13:40:47 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 13:40:47 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:40:48 10splitTasks](trainer.py 286): INFO [0/157]	0.5996(0.5996)	0.4742(0.4742)	0.143(0.143)	96.88(96.88)
[2023-09-29 13:40:49 10splitTasks](trainer.py 286): INFO [10/157]	0.1028(0.1497)	0.0002(0.0434)	0.236(0.287)	93.75(89.20)
[2023-09-29 13:40:50 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1272)	0.0002(0.0229)	0.265(0.286)	93.75(88.84)
[2023-09-29 13:40:51 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.1196)	0.0003(0.0156)	0.374(0.307)	90.62(88.61)
[2023-09-29 13:40:52 10splitTasks](trainer.py 286): INFO [40/157]	0.1024(0.1153)	0.0003(0.0119)	0.349(0.292)	87.50(89.41)
[2023-09-29 13:40:53 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1130)	0.0002(0.0096)	0.225(0.284)	90.62(89.46)
[2023-09-29 13:40:54 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1116)	0.0002(0.0081)	0.390(0.298)	87.50(88.83)
[2023-09-29 13:40:55 10splitTasks](trainer.py 286): INFO [70/157]	0.1039(0.1104)	0.0002(0.0070)	0.147(0.300)	93.75(88.78)
[2023-09-29 13:40:56 10splitTasks](trainer.py 286): INFO [80/157]	0.1027(0.1095)	0.0002(0.0062)	0.328(0.298)	90.62(88.77)
[2023-09-29 13:40:57 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1089)	0.0002(0.0055)	0.181(0.297)	96.88(89.15)
[2023-09-29 13:40:58 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1085)	0.0001(0.0050)	0.129(0.296)	96.88(89.26)
[2023-09-29 13:40:59 10splitTasks](trainer.py 286): INFO [110/157]	0.1025(0.1079)	0.0003(0.0046)	0.175(0.291)	90.62(89.56)
[2023-09-29 13:41:00 10splitTasks](trainer.py 286): INFO [120/157]	0.1034(0.1075)	0.0002(0.0042)	0.278(0.289)	93.75(89.77)
[2023-09-29 13:41:01 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1072)	0.0003(0.0039)	0.369(0.292)	87.50(89.79)
[2023-09-29 13:41:02 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1070)	0.0002(0.0037)	0.380(0.296)	87.50(89.58)
[2023-09-29 13:41:04 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1068)	0.0002(0.0035)	0.357(0.297)	78.12(89.57)
[2023-09-29 13:41:04 10splitTasks](trainer.py 286): INFO [156/157]	0.0799(0.1065)	0.0001(0.0033)	0.272(0.294)	87.50(89.62)
[2023-09-29 13:41:04 10splitTasks](trainer.py 288): INFO  * Train Acc 89.620
[2023-09-29 13:41:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.400, Total time 1.63
[2023-09-29 13:41:06 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 13:41:06 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 13:41:06 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:41:06 10splitTasks](trainer.py 286): INFO [0/157]	0.5626(0.5626)	0.4448(0.4448)	0.175(0.175)	90.62(90.62)
[2023-09-29 13:41:07 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1459)	0.0001(0.0407)	0.277(0.241)	84.38(90.62)
[2023-09-29 13:41:08 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1262)	0.0003(0.0215)	0.104(0.232)	96.88(91.82)
[2023-09-29 13:41:09 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1182)	0.0002(0.0147)	0.113(0.218)	96.88(92.44)
[2023-09-29 13:41:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1142)	0.0003(0.0112)	0.215(0.231)	90.62(91.84)
[2023-09-29 13:41:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1119)	0.0002(0.0090)	0.432(0.241)	84.38(91.54)
[2023-09-29 13:41:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1169(0.1105)	0.0006(0.0076)	0.273(0.240)	87.50(91.44)
[2023-09-29 13:41:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1095)	0.0003(0.0066)	0.127(0.238)	96.88(91.37)
[2023-09-29 13:41:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1092)	0.0002(0.0058)	0.319(0.237)	84.38(91.36)
[2023-09-29 13:41:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1156(0.1086)	0.0012(0.0052)	0.065(0.236)	100.00(91.55)
[2023-09-29 13:41:17 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1079)	0.0004(0.0047)	0.118(0.239)	96.88(91.55)
[2023-09-29 13:41:18 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1075)	0.0003(0.0043)	0.091(0.238)	96.88(91.58)
[2023-09-29 13:41:19 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1071)	0.0002(0.0040)	0.113(0.239)	93.75(91.66)
[2023-09-29 13:41:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1067)	0.0003(0.0037)	0.441(0.239)	87.50(91.63)
[2023-09-29 13:41:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1064)	0.0003(0.0035)	0.227(0.243)	90.62(91.51)
[2023-09-29 13:41:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1061)	0.0001(0.0033)	0.224(0.242)	90.62(91.39)
[2023-09-29 13:41:22 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1058)	0.0001(0.0032)	1.211(0.242)	50.00(91.36)
[2023-09-29 13:41:23 10splitTasks](trainer.py 288): INFO  * Train Acc 91.360
[2023-09-29 13:41:24 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.600, Total time 1.59
[2023-09-29 13:41:24 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 13:41:24 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 13:41:24 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:41:25 10splitTasks](trainer.py 286): INFO [0/157]	0.6815(0.6815)	0.5759(0.5759)	0.208(0.208)	90.62(90.62)
[2023-09-29 13:41:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1568)	0.0003(0.0527)	0.305(0.209)	87.50(92.61)
[2023-09-29 13:41:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1027(0.1313)	0.0002(0.0277)	0.362(0.238)	84.38(90.77)
[2023-09-29 13:41:28 10splitTasks](trainer.py 286): INFO [30/157]	0.1067(0.1227)	0.0005(0.0189)	0.108(0.266)	96.88(89.92)
[2023-09-29 13:41:29 10splitTasks](trainer.py 286): INFO [40/157]	0.1076(0.1180)	0.0003(0.0144)	0.224(0.266)	87.50(90.02)
[2023-09-29 13:41:30 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1150)	0.0002(0.0116)	0.236(0.253)	90.62(90.87)
[2023-09-29 13:41:31 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1133)	0.0003(0.0098)	0.164(0.263)	96.88(90.42)
[2023-09-29 13:41:32 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1117)	0.0002(0.0084)	0.294(0.263)	90.62(90.36)
[2023-09-29 13:41:33 10splitTasks](trainer.py 286): INFO [80/157]	0.1093(0.1109)	0.0002(0.0074)	0.322(0.264)	90.62(90.39)
[2023-09-29 13:41:34 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1101)	0.0002(0.0067)	0.155(0.263)	87.50(90.45)
[2023-09-29 13:41:35 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1099)	0.0002(0.0060)	0.238(0.257)	90.62(90.72)
[2023-09-29 13:41:36 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1092)	0.0003(0.0055)	0.272(0.256)	87.50(90.77)
[2023-09-29 13:41:37 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1087)	0.0002(0.0051)	0.077(0.253)	96.88(90.88)
[2023-09-29 13:41:38 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1081)	0.0001(0.0047)	0.452(0.252)	84.38(90.89)
[2023-09-29 13:41:39 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1077)	0.0002(0.0044)	0.171(0.254)	90.62(90.87)
[2023-09-29 13:41:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1019(0.1073)	0.0007(0.0041)	0.108(0.254)	93.75(90.81)
[2023-09-29 13:41:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1070)	0.0001(0.0040)	0.434(0.253)	87.50(90.82)
[2023-09-29 13:41:41 10splitTasks](trainer.py 288): INFO  * Train Acc 90.820
[2023-09-29 13:41:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.62
[2023-09-29 13:41:43 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 13:41:43 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 13:41:43 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:41:43 10splitTasks](trainer.py 286): INFO [0/157]	0.6678(0.6678)	0.5637(0.5637)	0.378(0.378)	81.25(81.25)
[2023-09-29 13:41:44 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1542)	0.0003(0.0515)	0.206(0.248)	87.50(88.64)
[2023-09-29 13:41:45 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1297)	0.0003(0.0271)	0.188(0.217)	93.75(91.07)
[2023-09-29 13:41:46 10splitTasks](trainer.py 286): INFO [30/157]	0.1024(0.1212)	0.0003(0.0184)	0.173(0.200)	93.75(91.73)
[2023-09-29 13:41:47 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1167)	0.0001(0.0140)	0.089(0.201)	100.00(92.00)
[2023-09-29 13:41:48 10splitTasks](trainer.py 286): INFO [50/157]	0.1057(0.1142)	0.0002(0.0113)	0.347(0.207)	90.62(92.03)
[2023-09-29 13:41:50 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1121)	0.0002(0.0095)	0.145(0.200)	93.75(92.62)
[2023-09-29 13:41:51 10splitTasks](trainer.py 286): INFO [70/157]	0.1051(0.1107)	0.0002(0.0082)	0.097(0.192)	96.88(93.05)
[2023-09-29 13:41:52 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1097)	0.0001(0.0072)	0.199(0.196)	90.62(92.90)
[2023-09-29 13:41:53 10splitTasks](trainer.py 286): INFO [90/157]	0.1039(0.1092)	0.0002(0.0065)	0.163(0.193)	100.00(93.06)
[2023-09-29 13:41:54 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1086)	0.0002(0.0059)	0.279(0.195)	87.50(92.91)
[2023-09-29 13:41:55 10splitTasks](trainer.py 286): INFO [110/157]	0.1050(0.1081)	0.0001(0.0054)	0.342(0.199)	87.50(92.79)
[2023-09-29 13:41:56 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1077)	0.0003(0.0049)	0.135(0.205)	96.88(92.64)
[2023-09-29 13:41:57 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1072)	0.0003(0.0046)	0.330(0.203)	84.38(92.65)
[2023-09-29 13:41:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1069)	0.0002(0.0043)	0.205(0.206)	93.75(92.55)
[2023-09-29 13:41:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1065)	0.0001(0.0040)	0.117(0.207)	100.00(92.38)
[2023-09-29 13:41:59 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1062)	0.0001(0.0039)	0.224(0.205)	87.50(92.52)
[2023-09-29 13:41:59 10splitTasks](trainer.py 288): INFO  * Train Acc 92.520
[2023-09-29 13:42:01 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.58
[2023-09-29 13:42:01 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 13:42:01 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 13:42:01 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:42:02 10splitTasks](trainer.py 286): INFO [0/157]	0.5967(0.5967)	0.4920(0.4920)	0.147(0.147)	93.75(93.75)
[2023-09-29 13:42:03 10splitTasks](trainer.py 286): INFO [10/157]	0.1047(0.1497)	0.0002(0.0450)	0.262(0.207)	90.62(93.18)
[2023-09-29 13:42:04 10splitTasks](trainer.py 286): INFO [20/157]	0.1009(0.1274)	0.0001(0.0237)	0.117(0.176)	93.75(93.90)
[2023-09-29 13:42:05 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1199)	0.0003(0.0162)	0.160(0.178)	93.75(93.85)
[2023-09-29 13:42:06 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1159)	0.0002(0.0123)	0.250(0.186)	87.50(93.37)
[2023-09-29 13:42:07 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1131)	0.0002(0.0099)	0.139(0.187)	96.88(93.57)
[2023-09-29 13:42:08 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1113)	0.0003(0.0083)	0.155(0.191)	93.75(93.24)
[2023-09-29 13:42:09 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1101)	0.0002(0.0072)	0.227(0.187)	90.62(93.35)
[2023-09-29 13:42:10 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1092)	0.0003(0.0064)	0.172(0.185)	93.75(93.44)
[2023-09-29 13:42:11 10splitTasks](trainer.py 286): INFO [90/157]	0.1103(0.1085)	0.0006(0.0057)	0.283(0.192)	87.50(93.23)
[2023-09-29 13:42:12 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1079)	0.0003(0.0052)	0.102(0.192)	93.75(93.16)
[2023-09-29 13:42:13 10splitTasks](trainer.py 286): INFO [110/157]	0.1060(0.1076)	0.0004(0.0047)	0.223(0.190)	90.62(93.16)
[2023-09-29 13:42:14 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1072)	0.0002(0.0044)	0.176(0.187)	93.75(93.29)
[2023-09-29 13:42:15 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1068)	0.0003(0.0041)	0.119(0.187)	96.88(93.27)
[2023-09-29 13:42:16 10splitTasks](trainer.py 286): INFO [140/157]	0.1059(0.1065)	0.0002(0.0038)	0.167(0.187)	96.88(93.24)
[2023-09-29 13:42:17 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1063)	0.0001(0.0036)	0.158(0.186)	93.75(93.27)
[2023-09-29 13:42:18 10splitTasks](trainer.py 286): INFO [156/157]	0.0821(0.1060)	0.0001(0.0034)	2.307(0.189)	50.00(93.26)
[2023-09-29 13:42:18 10splitTasks](trainer.py 288): INFO  * Train Acc 93.260
[2023-09-29 13:42:19 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.600, Total time 1.59
[2023-09-29 13:42:19 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 13:42:19 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 13:42:19 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:42:20 10splitTasks](trainer.py 286): INFO [0/157]	0.5817(0.5817)	0.4717(0.4717)	0.213(0.213)	87.50(87.50)
[2023-09-29 13:42:21 10splitTasks](trainer.py 286): INFO [10/157]	0.1065(0.1472)	0.0002(0.0431)	0.106(0.141)	93.75(94.32)
[2023-09-29 13:42:22 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1259)	0.0001(0.0227)	0.221(0.177)	93.75(93.15)
[2023-09-29 13:42:23 10splitTasks](trainer.py 286): INFO [30/157]	0.1051(0.1187)	0.0002(0.0154)	0.231(0.175)	87.50(93.55)
[2023-09-29 13:42:24 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1144)	0.0002(0.0117)	0.118(0.169)	96.88(93.83)
[2023-09-29 13:42:25 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1119)	0.0002(0.0095)	0.331(0.177)	93.75(93.75)
[2023-09-29 13:42:26 10splitTasks](trainer.py 286): INFO [60/157]	0.1114(0.1103)	0.0002(0.0080)	0.039(0.177)	100.00(93.85)
[2023-09-29 13:42:27 10splitTasks](trainer.py 286): INFO [70/157]	0.1032(0.1091)	0.0002(0.0069)	0.254(0.174)	90.62(94.10)
[2023-09-29 13:42:28 10splitTasks](trainer.py 286): INFO [80/157]	0.1041(0.1082)	0.0002(0.0060)	0.039(0.170)	100.00(94.17)
[2023-09-29 13:42:29 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1074)	0.0002(0.0054)	0.128(0.178)	96.88(93.85)
[2023-09-29 13:42:30 10splitTasks](trainer.py 286): INFO [100/157]	0.1010(0.1068)	0.0002(0.0049)	0.031(0.176)	100.00(93.94)
[2023-09-29 13:42:31 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1063)	0.0002(0.0045)	0.093(0.179)	96.88(93.81)
[2023-09-29 13:42:32 10splitTasks](trainer.py 286): INFO [120/157]	0.1041(0.1059)	0.0002(0.0041)	0.217(0.182)	87.50(93.62)
[2023-09-29 13:42:33 10splitTasks](trainer.py 286): INFO [130/157]	0.1010(0.1056)	0.0002(0.0038)	0.142(0.179)	93.75(93.73)
[2023-09-29 13:42:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1010(0.1053)	0.0001(0.0036)	0.213(0.182)	90.62(93.62)
[2023-09-29 13:42:35 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1051)	0.0001(0.0033)	0.178(0.179)	96.88(93.83)
[2023-09-29 13:42:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1048)	0.0001(0.0032)	0.125(0.179)	100.00(93.82)
[2023-09-29 13:42:36 10splitTasks](trainer.py 288): INFO  * Train Acc 93.820
[2023-09-29 13:42:38 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.200, Total time 1.60
[2023-09-29 13:42:38 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 13:42:38 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 13:42:38 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:42:38 10splitTasks](trainer.py 286): INFO [0/157]	0.5779(0.5779)	0.4572(0.4572)	0.128(0.128)	96.88(96.88)
[2023-09-29 13:42:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1474)	0.0002(0.0418)	0.122(0.124)	96.88(95.45)
[2023-09-29 13:42:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1259)	0.0002(0.0221)	0.061(0.125)	96.88(96.13)
[2023-09-29 13:42:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1182)	0.0002(0.0150)	0.125(0.146)	93.75(95.67)
[2023-09-29 13:42:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1068(0.1144)	0.0007(0.0115)	0.101(0.153)	96.88(95.27)
[2023-09-29 13:42:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1122)	0.0003(0.0093)	0.058(0.149)	100.00(95.34)
[2023-09-29 13:42:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1110)	0.0003(0.0078)	0.152(0.156)	93.75(94.88)
[2023-09-29 13:42:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1099)	0.0002(0.0068)	0.058(0.155)	100.00(95.03)
[2023-09-29 13:42:46 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1089)	0.0003(0.0060)	0.153(0.157)	93.75(94.95)
[2023-09-29 13:42:47 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1083)	0.0003(0.0054)	0.270(0.156)	93.75(94.85)
[2023-09-29 13:42:48 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1079)	0.0002(0.0048)	0.313(0.162)	87.50(94.62)
[2023-09-29 13:42:49 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1073)	0.0002(0.0044)	0.082(0.158)	93.75(94.71)
[2023-09-29 13:42:50 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1071)	0.0003(0.0041)	0.115(0.159)	96.88(94.68)
[2023-09-29 13:42:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1042(0.1068)	0.0003(0.0038)	0.349(0.161)	90.62(94.63)
[2023-09-29 13:42:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1044(0.1065)	0.0002(0.0036)	0.324(0.164)	90.62(94.55)
[2023-09-29 13:42:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1017(0.1065)	0.0002(0.0034)	0.249(0.167)	90.62(94.50)
[2023-09-29 13:42:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1061)	0.0001(0.0032)	0.328(0.167)	87.50(94.50)
[2023-09-29 13:42:54 10splitTasks](trainer.py 288): INFO  * Train Acc 94.500
[2023-09-29 13:42:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.67
[2023-09-29 13:42:56 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 13:42:56 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 13:42:56 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:42:57 10splitTasks](trainer.py 286): INFO [0/157]	0.5838(0.5838)	0.4756(0.4756)	0.200(0.200)	93.75(93.75)
[2023-09-29 13:42:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1490)	0.0003(0.0435)	0.152(0.190)	96.88(94.32)
[2023-09-29 13:42:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1268)	0.0003(0.0229)	0.040(0.180)	100.00(94.49)
[2023-09-29 13:43:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1187)	0.0002(0.0156)	0.154(0.166)	93.75(94.56)
[2023-09-29 13:43:01 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1145)	0.0002(0.0119)	0.230(0.167)	93.75(94.28)
[2023-09-29 13:43:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1120)	0.0002(0.0096)	0.207(0.169)	90.62(94.18)
[2023-09-29 13:43:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1071(0.1104)	0.0002(0.0081)	0.025(0.164)	100.00(94.42)
[2023-09-29 13:43:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1092)	0.0003(0.0070)	0.140(0.166)	96.88(94.28)
[2023-09-29 13:43:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1025(0.1083)	0.0003(0.0061)	0.037(0.170)	100.00(94.14)
[2023-09-29 13:43:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1009(0.1076)	0.0001(0.0055)	0.121(0.169)	96.88(94.13)
[2023-09-29 13:43:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1070)	0.0002(0.0050)	0.221(0.170)	90.62(94.12)
[2023-09-29 13:43:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1069)	0.0002(0.0046)	0.105(0.172)	100.00(94.03)
[2023-09-29 13:43:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1065)	0.0003(0.0042)	0.167(0.170)	93.75(94.09)
[2023-09-29 13:43:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1062)	0.0003(0.0039)	0.089(0.166)	100.00(94.13)
[2023-09-29 13:43:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1059)	0.0002(0.0037)	0.070(0.164)	93.75(94.24)
[2023-09-29 13:43:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1059)	0.0002(0.0034)	0.047(0.161)	100.00(94.29)
[2023-09-29 13:43:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0803(0.1055)	0.0001(0.0033)	0.018(0.161)	100.00(94.30)
[2023-09-29 13:43:13 10splitTasks](trainer.py 288): INFO  * Train Acc 94.300
[2023-09-29 13:43:14 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.800, Total time 1.68
[2023-09-29 13:43:14 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 13:43:14 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 13:43:14 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:43:15 10splitTasks](trainer.py 286): INFO [0/157]	0.5492(0.5492)	0.4352(0.4352)	0.162(0.162)	93.75(93.75)
[2023-09-29 13:43:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1438)	0.0001(0.0398)	0.212(0.127)	90.62(95.45)
[2023-09-29 13:43:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1026(0.1250)	0.0002(0.0210)	0.081(0.127)	100.00(95.68)
[2023-09-29 13:43:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1025(0.1185)	0.0003(0.0144)	0.223(0.132)	93.75(95.77)
[2023-09-29 13:43:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1042(0.1149)	0.0002(0.0109)	0.147(0.137)	93.75(95.58)
[2023-09-29 13:43:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1126)	0.0003(0.0089)	0.196(0.153)	93.75(94.73)
[2023-09-29 13:43:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1109)	0.0002(0.0074)	0.146(0.149)	93.75(94.83)
[2023-09-29 13:43:22 10splitTasks](trainer.py 286): INFO [70/157]	0.1025(0.1096)	0.0002(0.0064)	0.066(0.142)	100.00(94.98)
[2023-09-29 13:43:23 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1087)	0.0003(0.0057)	0.133(0.140)	93.75(95.06)
[2023-09-29 13:43:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1024(0.1082)	0.0003(0.0051)	0.092(0.136)	96.88(95.23)
[2023-09-29 13:43:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1028(0.1076)	0.0004(0.0046)	0.147(0.133)	93.75(95.27)
[2023-09-29 13:43:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1071)	0.0002(0.0042)	0.126(0.136)	93.75(95.27)
[2023-09-29 13:43:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1068)	0.0003(0.0039)	0.044(0.139)	100.00(95.17)
[2023-09-29 13:43:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1066)	0.0002(0.0036)	0.059(0.137)	100.00(95.23)
[2023-09-29 13:43:29 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1065)	0.0002(0.0034)	0.207(0.137)	93.75(95.26)
[2023-09-29 13:43:30 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1063)	0.0001(0.0032)	0.129(0.136)	96.88(95.30)
[2023-09-29 13:43:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0810(0.1060)	0.0001(0.0031)	0.021(0.136)	100.00(95.30)
[2023-09-29 13:43:31 10splitTasks](trainer.py 288): INFO  * Train Acc 95.300
[2023-09-29 13:43:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.67
[2023-09-29 13:43:33 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 13:43:33 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 13:43:33 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:43:33 10splitTasks](trainer.py 286): INFO [0/157]	0.5990(0.5990)	0.4892(0.4892)	0.171(0.171)	93.75(93.75)
[2023-09-29 13:43:34 10splitTasks](trainer.py 286): INFO [10/157]	0.1231(0.1513)	0.0003(0.0448)	0.172(0.146)	96.88(94.03)
[2023-09-29 13:43:35 10splitTasks](trainer.py 286): INFO [20/157]	0.1009(0.1280)	0.0002(0.0236)	0.127(0.151)	96.88(94.20)
[2023-09-29 13:43:36 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1195)	0.0003(0.0161)	0.371(0.149)	87.50(94.05)
[2023-09-29 13:43:37 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1153)	0.0002(0.0122)	0.038(0.133)	100.00(94.97)
[2023-09-29 13:43:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1068(0.1131)	0.0002(0.0099)	0.097(0.133)	96.88(95.22)
[2023-09-29 13:43:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1114)	0.0002(0.0083)	0.189(0.139)	96.88(95.34)
[2023-09-29 13:43:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1100)	0.0003(0.0072)	0.263(0.139)	87.50(95.20)
[2023-09-29 13:43:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1010(0.1091)	0.0001(0.0063)	0.078(0.150)	96.88(94.87)
[2023-09-29 13:43:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1083)	0.0003(0.0057)	0.110(0.151)	100.00(94.88)
[2023-09-29 13:43:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1025(0.1076)	0.0002(0.0051)	0.112(0.148)	93.75(94.96)
[2023-09-29 13:43:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1072)	0.0004(0.0047)	0.202(0.146)	90.62(95.05)
[2023-09-29 13:43:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1069)	0.0002(0.0043)	0.371(0.146)	90.62(94.94)
[2023-09-29 13:43:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1027(0.1066)	0.0001(0.0040)	0.055(0.150)	100.00(94.78)
[2023-09-29 13:43:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1064)	0.0002(0.0038)	0.184(0.150)	93.75(94.81)
[2023-09-29 13:43:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1061)	0.0001(0.0035)	0.123(0.148)	93.75(94.91)
[2023-09-29 13:43:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1058)	0.0001(0.0034)	0.335(0.148)	87.50(94.94)
[2023-09-29 13:43:49 10splitTasks](trainer.py 288): INFO  * Train Acc 94.940
[2023-09-29 13:43:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.600, Total time 1.61
[2023-09-29 13:43:51 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 13:43:51 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 13:43:51 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:43:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6382(0.6382)	0.5260(0.5260)	0.054(0.054)	100.00(100.00)
[2023-09-29 13:43:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1108(0.1542)	0.0003(0.0481)	0.028(0.133)	100.00(95.74)
[2023-09-29 13:43:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1302)	0.0002(0.0254)	0.105(0.131)	96.88(95.68)
[2023-09-29 13:43:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1093(0.1217)	0.0002(0.0173)	0.174(0.139)	96.88(95.87)
[2023-09-29 13:43:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1169)	0.0002(0.0131)	0.109(0.129)	93.75(96.11)
[2023-09-29 13:43:57 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1141)	0.0002(0.0106)	0.387(0.130)	87.50(96.14)
[2023-09-29 13:43:58 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1121)	0.0004(0.0089)	0.264(0.134)	96.88(95.95)
[2023-09-29 13:43:59 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1108)	0.0002(0.0077)	0.227(0.133)	90.62(95.82)
[2023-09-29 13:44:00 10splitTasks](trainer.py 286): INFO [80/157]	0.1028(0.1098)	0.0002(0.0068)	0.066(0.135)	100.00(95.68)
[2023-09-29 13:44:01 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1089)	0.0002(0.0061)	0.382(0.137)	84.38(95.47)
[2023-09-29 13:44:02 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1084)	0.0002(0.0055)	0.167(0.142)	90.62(95.08)
[2023-09-29 13:44:03 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1078)	0.0002(0.0050)	0.020(0.141)	100.00(95.02)
[2023-09-29 13:44:04 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1074)	0.0002(0.0046)	0.155(0.141)	96.88(95.04)
[2023-09-29 13:44:05 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1070)	0.0003(0.0043)	0.168(0.142)	90.62(94.94)
[2023-09-29 13:44:06 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1066)	0.0002(0.0040)	0.295(0.142)	90.62(95.01)
[2023-09-29 13:44:07 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1064)	0.0001(0.0038)	0.252(0.143)	87.50(94.95)
[2023-09-29 13:44:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0827(0.1061)	0.0001(0.0036)	0.202(0.143)	87.50(94.94)
[2023-09-29 13:44:08 10splitTasks](trainer.py 288): INFO  * Train Acc 94.940
[2023-09-29 13:44:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.62
[2023-09-29 13:44:09 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 13:44:09 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 13:44:09 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:44:10 10splitTasks](trainer.py 286): INFO [0/157]	0.5910(0.5910)	0.4851(0.4851)	0.262(0.262)	93.75(93.75)
[2023-09-29 13:44:11 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1472)	0.0002(0.0443)	0.191(0.145)	90.62(94.03)
[2023-09-29 13:44:12 10splitTasks](trainer.py 286): INFO [20/157]	0.1058(0.1268)	0.0002(0.0234)	0.208(0.135)	87.50(94.05)
[2023-09-29 13:44:13 10splitTasks](trainer.py 286): INFO [30/157]	0.1035(0.1199)	0.0003(0.0160)	0.295(0.131)	90.62(94.56)
[2023-09-29 13:44:14 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1160)	0.0003(0.0122)	0.174(0.128)	90.62(94.82)
[2023-09-29 13:44:15 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1137)	0.0003(0.0099)	0.085(0.136)	96.88(94.73)
[2023-09-29 13:44:16 10splitTasks](trainer.py 286): INFO [60/157]	0.1028(0.1118)	0.0006(0.0083)	0.121(0.141)	96.88(94.83)
[2023-09-29 13:44:17 10splitTasks](trainer.py 286): INFO [70/157]	0.1024(0.1107)	0.0002(0.0072)	0.272(0.139)	93.75(94.94)
[2023-09-29 13:44:18 10splitTasks](trainer.py 286): INFO [80/157]	0.1022(0.1098)	0.0002(0.0063)	0.093(0.141)	96.88(94.79)
[2023-09-29 13:44:19 10splitTasks](trainer.py 286): INFO [90/157]	0.1030(0.1092)	0.0003(0.0057)	0.065(0.139)	100.00(94.88)
[2023-09-29 13:44:20 10splitTasks](trainer.py 286): INFO [100/157]	0.1025(0.1085)	0.0003(0.0051)	0.054(0.141)	100.00(94.93)
[2023-09-29 13:44:21 10splitTasks](trainer.py 286): INFO [110/157]	0.1019(0.1082)	0.0002(0.0047)	0.196(0.142)	90.62(94.88)
[2023-09-29 13:44:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1087(0.1079)	0.0002(0.0043)	0.054(0.139)	96.88(94.96)
[2023-09-29 13:44:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1026(0.1076)	0.0002(0.0040)	0.137(0.137)	96.88(95.06)
[2023-09-29 13:44:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1073)	0.0003(0.0038)	0.094(0.139)	96.88(95.04)
[2023-09-29 13:44:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1070)	0.0001(0.0036)	0.217(0.141)	93.75(95.07)
[2023-09-29 13:44:26 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1066)	0.0001(0.0034)	0.472(0.139)	75.00(95.12)
[2023-09-29 13:44:26 10splitTasks](trainer.py 288): INFO  * Train Acc 95.120
[2023-09-29 13:44:28 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.600, Total time 1.55
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-5.pth
=> Save Done
[2023-09-29 13:44:28 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 13:44:30 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.70
[2023-09-29 13:44:30 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 13:44:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.76
[2023-09-29 13:44:32 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 13:44:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.74
[2023-09-29 13:44:33 10splitTasks](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 13:44:35 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.73
[2023-09-29 13:44:35 10splitTasks](iBatchLearn.py 131): INFO validation split name:4
[2023-09-29 13:44:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.71
[2023-09-29 13:44:37 10splitTasks](iBatchLearn.py 131): INFO validation split name:5
[2023-09-29 13:44:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.600, Total time 1.79
[2023-09-29 13:44:39 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 13:44:39 10splitTasks](trainer.py 341): INFO done
[2023-09-29 13:44:39 10splitTasks](iBatchLearn.py 155): INFO Acc:81.66666666666667; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:44:42 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:44:42 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:44:42 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 5, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-5.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-5.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_5.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:44:43 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-5.pth
[2023-09-29 13:44:43 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:44:45 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:44:46 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:44:46 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 4, 4, 4],
        [3, 3, 2, 4, 6, 4, 4],
        [4, 3, 3, 6, 6, 4, 4],
        [4, 3, 6, 5, 4, 4, 4],
        [4, 6, 6, 5, 4, 5, 5],
        [0, 6, 5, 4, 2, 2, 5],
        [0, 3, 5, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 13:44:46 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:44:46 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:44:46 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 13:44:51 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 13:44:54 10splitTasks](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 13:44:57 10splitTasks](iBatchLearn.py 167): INFO test split name:3
[2023-09-29 13:45:00 10splitTasks](iBatchLearn.py 167): INFO test split name:4
[2023-09-29 13:45:03 10splitTasks](iBatchLearn.py 167): INFO test split name:5
--------------------------------Official Evaluation--------------------------------
5 81.33333333333334
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:45:12 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:45:12 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:45:12 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 6, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-5.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-6.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-5.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-6.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:45:13 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-5.pth
[2023-09-29 13:45:13 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:45:15 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:45:15 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:45:15 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:45:15 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:45:15 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 13:45:15 10splitTasks](iBatchLearn.py 92): INFO ====================== 6 =======================
[2023-09-29 13:45:15 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 13:45:15 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 13:45:15 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 13:45:15 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:45:19 10splitTasks](trainer.py 286): INFO [0/157]	3.6370(3.6370)	0.4977(0.4977)	2.319(2.319)	9.38(9.38)
[2023-09-29 13:45:20 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.4240)	0.0003(0.0455)	2.258(2.266)	6.25(13.35)
[2023-09-29 13:45:21 10splitTasks](trainer.py 286): INFO [20/157]	0.1042(0.2712)	0.0002(0.0240)	1.976(2.200)	31.25(19.20)
[2023-09-29 13:45:22 10splitTasks](trainer.py 286): INFO [30/157]	0.1006(0.2165)	0.0003(0.0164)	1.960(2.099)	34.38(25.30)
[2023-09-29 13:45:23 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1888)	0.0002(0.0124)	1.626(2.014)	34.38(28.58)
[2023-09-29 13:45:24 10splitTasks](trainer.py 286): INFO [50/157]	0.1008(0.1717)	0.0002(0.0101)	1.373(1.910)	50.00(32.60)
[2023-09-29 13:45:25 10splitTasks](trainer.py 286): INFO [60/157]	0.1011(0.1602)	0.0002(0.0085)	1.378(1.858)	56.25(34.43)
[2023-09-29 13:45:26 10splitTasks](trainer.py 286): INFO [70/157]	0.1008(0.1519)	0.0002(0.0073)	1.491(1.798)	56.25(37.06)
[2023-09-29 13:45:27 10splitTasks](trainer.py 286): INFO [80/157]	0.1009(0.1457)	0.0002(0.0064)	1.573(1.748)	37.50(38.93)
[2023-09-29 13:45:28 10splitTasks](trainer.py 286): INFO [90/157]	0.1036(0.1409)	0.0003(0.0058)	1.347(1.705)	56.25(40.45)
[2023-09-29 13:45:29 10splitTasks](trainer.py 286): INFO [100/157]	0.1123(0.1371)	0.0002(0.0052)	1.243(1.658)	50.00(42.26)
[2023-09-29 13:45:30 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1341)	0.0001(0.0048)	1.100(1.604)	62.50(43.58)
[2023-09-29 13:45:31 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1317)	0.0002(0.0044)	1.018(1.564)	59.38(45.09)
[2023-09-29 13:45:32 10splitTasks](trainer.py 286): INFO [130/157]	0.1008(0.1294)	0.0002(0.0041)	1.315(1.525)	65.62(46.59)
[2023-09-29 13:45:33 10splitTasks](trainer.py 286): INFO [140/157]	0.1009(0.1276)	0.0001(0.0038)	0.864(1.492)	65.62(47.70)
[2023-09-29 13:45:34 10splitTasks](trainer.py 286): INFO [150/157]	0.1033(0.1259)	0.0002(0.0036)	1.148(1.459)	59.38(49.01)
[2023-09-29 13:45:35 10splitTasks](trainer.py 286): INFO [156/157]	0.0823(0.1248)	0.0001(0.0035)	1.359(1.449)	37.50(49.46)
[2023-09-29 13:45:35 10splitTasks](trainer.py 288): INFO  * Train Acc 49.460
[2023-09-29 13:45:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.400, Total time 1.76
[2023-09-29 13:45:37 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 13:45:37 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 13:45:37 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:45:38 10splitTasks](trainer.py 286): INFO [0/157]	0.7480(0.7480)	0.6440(0.6440)	0.786(0.786)	68.75(68.75)
[2023-09-29 13:45:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1637)	0.0001(0.0589)	1.179(0.969)	46.88(65.06)
[2023-09-29 13:45:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1350)	0.0002(0.0310)	0.951(0.991)	68.75(65.18)
[2023-09-29 13:45:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1007(0.1243)	0.0002(0.0211)	0.672(0.988)	78.12(65.32)
[2023-09-29 13:45:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1009(0.1191)	0.0002(0.0160)	0.962(0.983)	65.62(64.94)
[2023-09-29 13:45:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1159)	0.0002(0.0129)	0.668(0.977)	78.12(65.44)
[2023-09-29 13:45:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1136)	0.0002(0.0108)	0.935(0.967)	68.75(65.73)
[2023-09-29 13:45:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1119)	0.0002(0.0094)	1.373(0.965)	62.50(65.80)
[2023-09-29 13:45:46 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1108)	0.0002(0.0082)	0.776(0.960)	68.75(66.28)
[2023-09-29 13:45:47 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1099)	0.0002(0.0074)	0.939(0.957)	68.75(66.28)
[2023-09-29 13:45:48 10splitTasks](trainer.py 286): INFO [100/157]	0.1055(0.1092)	0.0002(0.0067)	0.707(0.949)	71.88(66.49)
[2023-09-29 13:45:49 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1086)	0.0002(0.0061)	0.752(0.929)	71.88(67.31)
[2023-09-29 13:45:50 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1081)	0.0003(0.0056)	1.116(0.913)	62.50(68.10)
[2023-09-29 13:45:51 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1077)	0.0002(0.0052)	0.995(0.911)	68.75(68.15)
[2023-09-29 13:45:52 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1074)	0.0002(0.0049)	0.924(0.901)	71.88(68.55)
[2023-09-29 13:45:53 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1071)	0.0001(0.0046)	0.628(0.897)	81.25(68.67)
[2023-09-29 13:45:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0828(0.1067)	0.0001(0.0044)	1.345(0.894)	37.50(68.78)
[2023-09-29 13:45:54 10splitTasks](trainer.py 288): INFO  * Train Acc 68.780
[2023-09-29 13:45:55 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.400, Total time 1.70
[2023-09-29 13:45:55 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 13:45:55 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 13:45:55 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:45:56 10splitTasks](trainer.py 286): INFO [0/157]	0.6682(0.6682)	0.5583(0.5583)	0.942(0.942)	71.88(71.88)
[2023-09-29 13:45:57 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1541)	0.0003(0.0510)	0.767(0.828)	75.00(67.90)
[2023-09-29 13:45:58 10splitTasks](trainer.py 286): INFO [20/157]	0.1026(0.1297)	0.0003(0.0270)	0.760(0.825)	75.00(70.09)
[2023-09-29 13:45:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.1218)	0.0003(0.0184)	0.718(0.819)	68.75(70.67)
[2023-09-29 13:46:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1171)	0.0003(0.0140)	0.698(0.809)	71.88(71.49)
[2023-09-29 13:46:01 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1141)	0.0002(0.0113)	0.375(0.801)	90.62(71.57)
[2023-09-29 13:46:02 10splitTasks](trainer.py 286): INFO [60/157]	0.1048(0.1123)	0.0003(0.0095)	0.531(0.808)	78.12(72.03)
[2023-09-29 13:46:03 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1111)	0.0002(0.0082)	0.789(0.801)	75.00(72.40)
[2023-09-29 13:46:04 10splitTasks](trainer.py 286): INFO [80/157]	0.1033(0.1099)	0.0002(0.0072)	0.864(0.796)	75.00(72.72)
[2023-09-29 13:46:05 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1093)	0.0002(0.0065)	0.575(0.806)	87.50(72.25)
[2023-09-29 13:46:06 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1086)	0.0004(0.0059)	0.715(0.802)	71.88(72.18)
[2023-09-29 13:46:07 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1081)	0.0002(0.0054)	0.731(0.808)	71.88(71.93)
[2023-09-29 13:46:08 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1077)	0.0002(0.0050)	0.729(0.804)	68.75(72.21)
[2023-09-29 13:46:09 10splitTasks](trainer.py 286): INFO [130/157]	0.1027(0.1074)	0.0004(0.0046)	0.725(0.801)	78.12(72.35)
[2023-09-29 13:46:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1070)	0.0001(0.0043)	0.984(0.805)	65.62(72.34)
[2023-09-29 13:46:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1068)	0.0001(0.0040)	0.733(0.804)	84.38(72.48)
[2023-09-29 13:46:12 10splitTasks](trainer.py 286): INFO [156/157]	0.0779(0.1064)	0.0001(0.0039)	0.491(0.809)	75.00(72.40)
[2023-09-29 13:46:12 10splitTasks](trainer.py 288): INFO  * Train Acc 72.400
[2023-09-29 13:46:14 10splitTasks](my_trainer.py 503): INFO  * Val Acc 74.400, Total time 1.67
[2023-09-29 13:46:14 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 13:46:14 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 13:46:14 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:46:15 10splitTasks](trainer.py 286): INFO [0/157]	0.6127(0.6127)	0.4832(0.4832)	0.414(0.414)	90.62(90.62)
[2023-09-29 13:46:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1492)	0.0003(0.0442)	0.583(0.784)	81.25(75.85)
[2023-09-29 13:46:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1124(0.1279)	0.0069(0.0236)	0.414(0.765)	90.62(77.08)
[2023-09-29 13:46:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1007(0.1195)	0.0002(0.0161)	0.932(0.743)	68.75(77.32)
[2023-09-29 13:46:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1155)	0.0003(0.0122)	1.146(0.725)	56.25(77.52)
[2023-09-29 13:46:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1132)	0.0002(0.0099)	0.252(0.703)	90.62(77.82)
[2023-09-29 13:46:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1116)	0.0002(0.0083)	0.560(0.716)	84.38(77.36)
[2023-09-29 13:46:22 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1103)	0.0002(0.0072)	0.552(0.714)	78.12(76.94)
[2023-09-29 13:46:23 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1096)	0.0002(0.0064)	0.755(0.698)	81.25(77.43)
[2023-09-29 13:46:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1082(0.1090)	0.0003(0.0057)	0.678(0.696)	78.12(77.47)
[2023-09-29 13:46:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1084)	0.0001(0.0052)	0.770(0.687)	75.00(77.75)
[2023-09-29 13:46:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1039(0.1079)	0.0002(0.0047)	0.909(0.683)	68.75(77.79)
[2023-09-29 13:46:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1075)	0.0001(0.0043)	0.960(0.686)	71.88(77.61)
[2023-09-29 13:46:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1070)	0.0002(0.0040)	0.514(0.687)	75.00(77.36)
[2023-09-29 13:46:29 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1067)	0.0003(0.0038)	0.560(0.688)	78.12(77.50)
[2023-09-29 13:46:30 10splitTasks](trainer.py 286): INFO [150/157]	0.1042(0.1064)	0.0001(0.0036)	0.304(0.687)	93.75(77.61)
[2023-09-29 13:46:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0776(0.1060)	0.0001(0.0034)	1.398(0.688)	62.50(77.62)
[2023-09-29 13:46:31 10splitTasks](trainer.py 288): INFO  * Train Acc 77.620
[2023-09-29 13:46:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 76.000, Total time 1.68
[2023-09-29 13:46:32 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 13:46:32 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 13:46:32 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:46:33 10splitTasks](trainer.py 286): INFO [0/157]	0.6601(0.6601)	0.5409(0.5409)	0.489(0.489)	81.25(81.25)
[2023-09-29 13:46:34 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1534)	0.0001(0.0494)	0.562(0.580)	75.00(79.26)
[2023-09-29 13:46:35 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1293)	0.0001(0.0260)	0.558(0.565)	81.25(80.06)
[2023-09-29 13:46:36 10splitTasks](trainer.py 286): INFO [30/157]	0.1044(0.1204)	0.0003(0.0177)	0.515(0.536)	78.12(80.95)
[2023-09-29 13:46:37 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1161)	0.0003(0.0135)	0.743(0.543)	71.88(80.79)
[2023-09-29 13:46:38 10splitTasks](trainer.py 286): INFO [50/157]	0.1025(0.1137)	0.0002(0.0109)	0.610(0.540)	78.12(81.25)
[2023-09-29 13:46:39 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1119)	0.0002(0.0092)	0.828(0.561)	75.00(80.69)
[2023-09-29 13:46:40 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1105)	0.0003(0.0080)	0.353(0.556)	90.62(80.90)
[2023-09-29 13:46:41 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1094)	0.0002(0.0070)	0.324(0.558)	87.50(80.63)
[2023-09-29 13:46:42 10splitTasks](trainer.py 286): INFO [90/157]	0.1009(0.1086)	0.0002(0.0063)	0.783(0.556)	75.00(80.67)
[2023-09-29 13:46:43 10splitTasks](trainer.py 286): INFO [100/157]	0.1260(0.1085)	0.0007(0.0057)	0.676(0.565)	78.12(80.54)
[2023-09-29 13:46:44 10splitTasks](trainer.py 286): INFO [110/157]	0.1088(0.1081)	0.0005(0.0052)	0.445(0.572)	87.50(80.38)
[2023-09-29 13:46:45 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1076)	0.0003(0.0048)	0.335(0.572)	93.75(80.48)
[2023-09-29 13:46:46 10splitTasks](trainer.py 286): INFO [130/157]	0.1080(0.1072)	0.0005(0.0045)	0.745(0.572)	71.88(80.58)
[2023-09-29 13:46:47 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1069)	0.0002(0.0042)	0.383(0.568)	90.62(80.70)
[2023-09-29 13:46:48 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1066)	0.0001(0.0039)	0.386(0.564)	81.25(80.79)
[2023-09-29 13:46:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1062)	0.0001(0.0038)	1.437(0.566)	62.50(80.78)
[2023-09-29 13:46:49 10splitTasks](trainer.py 288): INFO  * Train Acc 80.780
[2023-09-29 13:46:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.400, Total time 1.64
[2023-09-29 13:46:51 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 13:46:51 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 13:46:51 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:46:51 10splitTasks](trainer.py 286): INFO [0/157]	0.6191(0.6191)	0.5151(0.5151)	0.339(0.339)	87.50(87.50)
[2023-09-29 13:46:52 10splitTasks](trainer.py 286): INFO [10/157]	0.1010(0.1494)	0.0001(0.0471)	0.425(0.524)	84.38(80.97)
[2023-09-29 13:46:53 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1266)	0.0002(0.0248)	0.573(0.518)	78.12(81.10)
[2023-09-29 13:46:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1009(0.1191)	0.0001(0.0169)	0.473(0.492)	87.50(82.66)
[2023-09-29 13:46:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1158)	0.0002(0.0129)	0.543(0.494)	84.38(82.85)
[2023-09-29 13:46:57 10splitTasks](trainer.py 286): INFO [50/157]	0.1012(0.1131)	0.0002(0.0104)	0.457(0.482)	87.50(83.52)
[2023-09-29 13:46:58 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1114)	0.0003(0.0087)	0.284(0.478)	84.38(83.45)
[2023-09-29 13:46:59 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1101)	0.0002(0.0076)	0.364(0.471)	90.62(83.85)
[2023-09-29 13:47:00 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1092)	0.0003(0.0067)	0.583(0.478)	81.25(83.41)
[2023-09-29 13:47:01 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1084)	0.0002(0.0060)	0.432(0.477)	87.50(83.52)
[2023-09-29 13:47:02 10splitTasks](trainer.py 286): INFO [100/157]	0.1125(0.1079)	0.0006(0.0054)	0.586(0.482)	78.12(83.51)
[2023-09-29 13:47:03 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1075)	0.0002(0.0049)	0.560(0.476)	81.25(83.78)
[2023-09-29 13:47:04 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1072)	0.0002(0.0046)	0.518(0.483)	81.25(83.65)
[2023-09-29 13:47:05 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1068)	0.0002(0.0042)	0.422(0.479)	81.25(83.54)
[2023-09-29 13:47:06 10splitTasks](trainer.py 286): INFO [140/157]	0.1038(0.1064)	0.0002(0.0040)	0.409(0.479)	84.38(83.47)
[2023-09-29 13:47:07 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1061)	0.0001(0.0037)	0.663(0.480)	81.25(83.57)
[2023-09-29 13:47:07 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1058)	0.0001(0.0036)	0.436(0.483)	100.00(83.54)
[2023-09-29 13:47:08 10splitTasks](trainer.py 288): INFO  * Train Acc 83.540
[2023-09-29 13:47:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.70
[2023-09-29 13:47:09 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 13:47:09 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 13:47:09 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:47:10 10splitTasks](trainer.py 286): INFO [0/157]	0.6418(0.6418)	0.5365(0.5365)	0.336(0.336)	87.50(87.50)
[2023-09-29 13:47:11 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1513)	0.0002(0.0494)	0.404(0.398)	87.50(86.93)
[2023-09-29 13:47:12 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1279)	0.0003(0.0260)	0.290(0.425)	93.75(85.71)
[2023-09-29 13:47:13 10splitTasks](trainer.py 286): INFO [30/157]	0.1148(0.1204)	0.0005(0.0178)	0.156(0.415)	96.88(85.99)
[2023-09-29 13:47:14 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1161)	0.0002(0.0135)	0.592(0.423)	78.12(85.67)
[2023-09-29 13:47:15 10splitTasks](trainer.py 286): INFO [50/157]	0.1025(0.1133)	0.0002(0.0109)	0.380(0.410)	87.50(86.27)
[2023-09-29 13:47:16 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1115)	0.0003(0.0092)	0.447(0.418)	87.50(86.22)
[2023-09-29 13:47:17 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1102)	0.0002(0.0079)	0.323(0.409)	90.62(86.49)
[2023-09-29 13:47:18 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1093)	0.0003(0.0070)	0.223(0.408)	93.75(86.46)
[2023-09-29 13:47:19 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1086)	0.0002(0.0063)	0.715(0.397)	71.88(86.68)
[2023-09-29 13:47:20 10splitTasks](trainer.py 286): INFO [100/157]	0.1010(0.1080)	0.0002(0.0057)	0.306(0.391)	90.62(86.94)
[2023-09-29 13:47:21 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1074)	0.0002(0.0052)	0.322(0.385)	90.62(87.19)
[2023-09-29 13:47:22 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1071)	0.0002(0.0048)	0.256(0.382)	87.50(87.14)
[2023-09-29 13:47:23 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1068)	0.0003(0.0044)	0.261(0.395)	96.88(86.74)
[2023-09-29 13:47:24 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1064)	0.0002(0.0041)	0.492(0.392)	87.50(86.75)
[2023-09-29 13:47:25 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1061)	0.0001(0.0039)	0.296(0.392)	90.62(86.82)
[2023-09-29 13:47:26 10splitTasks](trainer.py 286): INFO [156/157]	0.0770(0.1058)	0.0001(0.0037)	0.455(0.392)	87.50(86.86)
[2023-09-29 13:47:26 10splitTasks](trainer.py 288): INFO  * Train Acc 86.860
[2023-09-29 13:47:28 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.200, Total time 1.66
[2023-09-29 13:47:28 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 13:47:28 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 13:47:28 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:47:28 10splitTasks](trainer.py 286): INFO [0/157]	0.6088(0.6088)	0.4955(0.4955)	0.153(0.153)	93.75(93.75)
[2023-09-29 13:47:29 10splitTasks](trainer.py 286): INFO [10/157]	0.1098(0.1500)	0.0002(0.0453)	0.318(0.257)	84.38(92.61)
[2023-09-29 13:47:30 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1279)	0.0001(0.0239)	0.476(0.300)	78.12(90.62)
[2023-09-29 13:47:31 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1196)	0.0003(0.0163)	0.327(0.322)	87.50(89.52)
[2023-09-29 13:47:32 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1158)	0.0003(0.0124)	0.136(0.313)	96.88(89.56)
[2023-09-29 13:47:33 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1131)	0.0003(0.0100)	0.366(0.302)	78.12(90.07)
[2023-09-29 13:47:34 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1112)	0.0002(0.0084)	0.218(0.315)	93.75(89.86)
[2023-09-29 13:47:35 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1100)	0.0002(0.0072)	0.206(0.326)	96.88(89.35)
[2023-09-29 13:47:36 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1092)	0.0003(0.0064)	0.357(0.324)	81.25(89.16)
[2023-09-29 13:47:37 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1084)	0.0003(0.0057)	0.241(0.318)	93.75(89.46)
[2023-09-29 13:47:39 10splitTasks](trainer.py 286): INFO [100/157]	0.1053(0.1079)	0.0004(0.0052)	0.260(0.317)	93.75(89.54)
[2023-09-29 13:47:40 10splitTasks](trainer.py 286): INFO [110/157]	0.1009(0.1073)	0.0001(0.0047)	0.105(0.318)	100.00(89.56)
[2023-09-29 13:47:41 10splitTasks](trainer.py 286): INFO [120/157]	0.1044(0.1071)	0.0003(0.0044)	0.269(0.316)	93.75(89.54)
[2023-09-29 13:47:42 10splitTasks](trainer.py 286): INFO [130/157]	0.1041(0.1070)	0.0002(0.0041)	0.369(0.317)	87.50(89.55)
[2023-09-29 13:47:43 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1066)	0.0002(0.0038)	0.319(0.318)	90.62(89.54)
[2023-09-29 13:47:44 10splitTasks](trainer.py 286): INFO [150/157]	0.1022(0.1063)	0.0001(0.0036)	0.334(0.317)	90.62(89.40)
[2023-09-29 13:47:44 10splitTasks](trainer.py 286): INFO [156/157]	0.0770(0.1060)	0.0001(0.0034)	0.755(0.316)	87.50(89.46)
[2023-09-29 13:47:44 10splitTasks](trainer.py 288): INFO  * Train Acc 89.460
[2023-09-29 13:47:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.68
[2023-09-29 13:47:46 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 13:47:46 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 13:47:46 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:47:47 10splitTasks](trainer.py 286): INFO [0/157]	0.6411(0.6411)	0.5304(0.5304)	0.323(0.323)	93.75(93.75)
[2023-09-29 13:47:48 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1529)	0.0002(0.0485)	0.300(0.326)	87.50(88.92)
[2023-09-29 13:47:49 10splitTasks](trainer.py 286): INFO [20/157]	0.1022(0.1288)	0.0002(0.0255)	0.199(0.331)	96.88(88.69)
[2023-09-29 13:47:50 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1202)	0.0002(0.0174)	0.454(0.319)	90.62(89.42)
[2023-09-29 13:47:51 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1162)	0.0003(0.0132)	0.085(0.313)	100.00(89.48)
[2023-09-29 13:47:52 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1134)	0.0003(0.0107)	0.321(0.313)	87.50(89.58)
[2023-09-29 13:47:53 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1119)	0.0003(0.0090)	0.247(0.295)	93.75(90.27)
[2023-09-29 13:47:54 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1107)	0.0002(0.0078)	0.475(0.286)	84.38(90.58)
[2023-09-29 13:47:55 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1097)	0.0002(0.0069)	0.085(0.284)	100.00(90.74)
[2023-09-29 13:47:56 10splitTasks](trainer.py 286): INFO [90/157]	0.1025(0.1091)	0.0002(0.0061)	0.186(0.277)	90.62(90.90)
[2023-09-29 13:47:57 10splitTasks](trainer.py 286): INFO [100/157]	0.1046(0.1084)	0.0001(0.0056)	0.236(0.275)	90.62(90.87)
[2023-09-29 13:47:58 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1080)	0.0003(0.0051)	0.201(0.279)	90.62(90.71)
[2023-09-29 13:47:59 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1078)	0.0003(0.0047)	0.173(0.281)	93.75(90.57)
[2023-09-29 13:48:00 10splitTasks](trainer.py 286): INFO [130/157]	0.1089(0.1074)	0.0005(0.0044)	0.129(0.278)	100.00(90.89)
[2023-09-29 13:48:01 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1070)	0.0002(0.0041)	0.113(0.278)	96.88(90.91)
[2023-09-29 13:48:02 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1067)	0.0001(0.0038)	0.349(0.277)	90.62(90.87)
[2023-09-29 13:48:03 10splitTasks](trainer.py 286): INFO [156/157]	0.0778(0.1064)	0.0001(0.0037)	0.733(0.278)	75.00(90.82)
[2023-09-29 13:48:03 10splitTasks](trainer.py 288): INFO  * Train Acc 90.820
[2023-09-29 13:48:05 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.600, Total time 1.74
[2023-09-29 13:48:05 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 13:48:05 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 13:48:05 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:48:05 10splitTasks](trainer.py 286): INFO [0/157]	0.6478(0.6478)	0.5187(0.5187)	0.412(0.412)	84.38(84.38)
[2023-09-29 13:48:06 10splitTasks](trainer.py 286): INFO [10/157]	0.1063(0.1532)	0.0002(0.0475)	0.153(0.270)	96.88(91.76)
[2023-09-29 13:48:07 10splitTasks](trainer.py 286): INFO [20/157]	0.1037(0.1302)	0.0004(0.0250)	0.178(0.274)	93.75(90.92)
[2023-09-29 13:48:08 10splitTasks](trainer.py 286): INFO [30/157]	0.1029(0.1217)	0.0002(0.0171)	0.240(0.254)	87.50(91.53)
[2023-09-29 13:48:09 10splitTasks](trainer.py 286): INFO [40/157]	0.1038(0.1171)	0.0004(0.0130)	0.280(0.250)	90.62(91.69)
[2023-09-29 13:48:10 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1143)	0.0003(0.0105)	0.507(0.262)	84.38(91.61)
[2023-09-29 13:48:11 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1126)	0.0002(0.0088)	0.261(0.252)	90.62(92.16)
[2023-09-29 13:48:13 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1112)	0.0002(0.0076)	0.239(0.247)	93.75(92.30)
[2023-09-29 13:48:14 10splitTasks](trainer.py 286): INFO [80/157]	0.1051(0.1104)	0.0003(0.0067)	0.422(0.245)	87.50(92.17)
[2023-09-29 13:48:15 10splitTasks](trainer.py 286): INFO [90/157]	0.1048(0.1096)	0.0003(0.0060)	0.305(0.250)	87.50(91.83)
[2023-09-29 13:48:16 10splitTasks](trainer.py 286): INFO [100/157]	0.1024(0.1090)	0.0003(0.0055)	0.146(0.249)	96.88(91.77)
[2023-09-29 13:48:17 10splitTasks](trainer.py 286): INFO [110/157]	0.1026(0.1086)	0.0003(0.0050)	0.084(0.247)	100.00(91.84)
[2023-09-29 13:48:18 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1082)	0.0002(0.0046)	0.447(0.254)	87.50(91.55)
[2023-09-29 13:48:19 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1078)	0.0002(0.0043)	0.244(0.253)	93.75(91.53)
[2023-09-29 13:48:20 10splitTasks](trainer.py 286): INFO [140/157]	0.1050(0.1077)	0.0002(0.0040)	0.185(0.254)	100.00(91.56)
[2023-09-29 13:48:21 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1074)	0.0001(0.0038)	0.251(0.254)	87.50(91.45)
[2023-09-29 13:48:21 10splitTasks](trainer.py 286): INFO [156/157]	0.0780(0.1070)	0.0001(0.0036)	0.617(0.255)	75.00(91.36)
[2023-09-29 13:48:21 10splitTasks](trainer.py 288): INFO  * Train Acc 91.360
[2023-09-29 13:48:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.69
[2023-09-29 13:48:23 10splitTasks](my_trainer.py 206): INFO Pruning for task6
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 3016/3548 (85.01%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 1313/1545 (84.98%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 11818/13903 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 5252/6179 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 5252/6179 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 5252/6179 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 11818/13903 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 5252/6179 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 5252/6179 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 11818/13903 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 5252/6179 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 10504/12358 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 47271/55613 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 21009/24717 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 42018/49433 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 21009/24717 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 47271/55613 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 21009/24717 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 21009/24717 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 47271/55613 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 21009/24717 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 21009/24717 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 47271/55613 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 21009/24717 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 42018/49433 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 189084/222452 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 168075/197735 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 189084/222452 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 189084/222452 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 189084/222452 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 189084/222452 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 189084/222452 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 84038/98868 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 168075/197735 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 756337/889808 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 336150/395470 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 672298/790939 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 336150/395470 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 756337/889808 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 336150/395470 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 336150/395470 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 756337/889808 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 336150/395470 (85.00%) (Total in layer: 1048576)
[2023-09-29 13:48:23 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 13:48:23 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 13:48:23 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 13:48:23 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:48:24 10splitTasks](trainer.py 286): INFO [0/157]	0.6215(0.6215)	0.4880(0.4880)	0.291(0.291)	87.50(87.50)
[2023-09-29 13:48:25 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1525)	0.0003(0.0446)	0.439(0.403)	84.38(87.22)
[2023-09-29 13:48:26 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1297)	0.0002(0.0235)	0.212(0.387)	90.62(87.20)
[2023-09-29 13:48:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1213)	0.0003(0.0160)	0.382(0.390)	87.50(86.19)
[2023-09-29 13:48:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1042(0.1170)	0.0003(0.0122)	0.218(0.408)	93.75(85.52)
[2023-09-29 13:48:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1141)	0.0002(0.0099)	0.236(0.410)	87.50(85.36)
[2023-09-29 13:48:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1121)	0.0002(0.0083)	0.322(0.415)	84.38(85.14)
[2023-09-29 13:48:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1283(0.1113)	0.0002(0.0072)	0.272(0.423)	93.75(85.12)
[2023-09-29 13:48:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1102)	0.0003(0.0063)	0.582(0.428)	75.00(84.95)
[2023-09-29 13:48:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1095)	0.0002(0.0057)	0.188(0.422)	93.75(85.34)
[2023-09-29 13:48:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1089)	0.0003(0.0051)	0.314(0.423)	87.50(85.27)
[2023-09-29 13:48:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1037(0.1083)	0.0002(0.0047)	0.414(0.416)	81.25(85.42)
[2023-09-29 13:48:36 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1078)	0.0001(0.0043)	0.417(0.423)	84.38(85.15)
[2023-09-29 13:48:38 10splitTasks](trainer.py 286): INFO [130/157]	0.1023(0.1075)	0.0002(0.0040)	0.326(0.423)	81.25(85.16)
[2023-09-29 13:48:39 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1071)	0.0003(0.0038)	0.522(0.425)	71.88(85.15)
[2023-09-29 13:48:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1069)	0.0001(0.0035)	0.383(0.423)	84.38(85.14)
[2023-09-29 13:48:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1065)	0.0001(0.0034)	1.084(0.425)	62.50(85.08)
[2023-09-29 13:48:40 10splitTasks](trainer.py 288): INFO  * Train Acc 85.080
[2023-09-29 13:48:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 78.600, Total time 1.81
[2023-09-29 13:48:42 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 13:48:42 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 13:48:42 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:48:43 10splitTasks](trainer.py 286): INFO [0/157]	0.5989(0.5989)	0.4922(0.4922)	0.222(0.222)	90.62(90.62)
[2023-09-29 13:48:44 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1491)	0.0003(0.0450)	0.247(0.406)	87.50(86.93)
[2023-09-29 13:48:45 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1272)	0.0003(0.0238)	0.331(0.402)	87.50(86.61)
[2023-09-29 13:48:46 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1194)	0.0002(0.0162)	0.383(0.403)	87.50(86.39)
[2023-09-29 13:48:47 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1153)	0.0003(0.0123)	0.239(0.388)	90.62(87.04)
[2023-09-29 13:48:48 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1134)	0.0003(0.0100)	0.364(0.379)	93.75(87.13)
[2023-09-29 13:48:49 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1120)	0.0003(0.0084)	0.549(0.393)	90.62(86.94)
[2023-09-29 13:48:50 10splitTasks](trainer.py 286): INFO [70/157]	0.1043(0.1107)	0.0002(0.0073)	0.430(0.405)	84.38(86.58)
[2023-09-29 13:48:51 10splitTasks](trainer.py 286): INFO [80/157]	0.1024(0.1097)	0.0002(0.0064)	0.298(0.399)	87.50(86.38)
[2023-09-29 13:48:52 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1092)	0.0002(0.0057)	0.275(0.394)	90.62(86.78)
[2023-09-29 13:48:53 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1086)	0.0003(0.0052)	0.600(0.398)	78.12(86.73)
[2023-09-29 13:48:54 10splitTasks](trainer.py 286): INFO [110/157]	0.1050(0.1081)	0.0002(0.0048)	0.440(0.397)	90.62(86.88)
[2023-09-29 13:48:55 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1077)	0.0003(0.0044)	0.463(0.399)	84.38(86.96)
[2023-09-29 13:48:56 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1073)	0.0003(0.0041)	0.812(0.398)	75.00(86.90)
[2023-09-29 13:48:57 10splitTasks](trainer.py 286): INFO [140/157]	0.1097(0.1072)	0.0004(0.0038)	0.634(0.401)	84.38(86.72)
[2023-09-29 13:48:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1069)	0.0001(0.0036)	0.264(0.398)	87.50(86.78)
[2023-09-29 13:48:59 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1065)	0.0001(0.0035)	0.298(0.398)	87.50(86.70)
[2023-09-29 13:48:59 10splitTasks](trainer.py 288): INFO  * Train Acc 86.700
[2023-09-29 13:49:01 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.200, Total time 1.63
[2023-09-29 13:49:01 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 13:49:01 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 13:49:01 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:49:01 10splitTasks](trainer.py 286): INFO [0/157]	0.6223(0.6223)	0.5010(0.5010)	0.316(0.316)	93.75(93.75)
[2023-09-29 13:49:02 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1506)	0.0002(0.0459)	0.839(0.355)	68.75(86.36)
[2023-09-29 13:49:03 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1278)	0.0002(0.0242)	0.367(0.348)	81.25(87.95)
[2023-09-29 13:49:04 10splitTasks](trainer.py 286): INFO [30/157]	0.1047(0.1199)	0.0003(0.0165)	0.302(0.350)	90.62(87.80)
[2023-09-29 13:49:05 10splitTasks](trainer.py 286): INFO [40/157]	0.1038(0.1159)	0.0003(0.0125)	0.311(0.358)	90.62(87.58)
[2023-09-29 13:49:06 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1135)	0.0002(0.0101)	0.273(0.350)	93.75(88.05)
[2023-09-29 13:49:07 10splitTasks](trainer.py 286): INFO [60/157]	0.1056(0.1122)	0.0002(0.0085)	0.335(0.359)	87.50(87.86)
[2023-09-29 13:49:08 10splitTasks](trainer.py 286): INFO [70/157]	0.1045(0.1110)	0.0003(0.0074)	0.466(0.354)	84.38(87.81)
[2023-09-29 13:49:09 10splitTasks](trainer.py 286): INFO [80/157]	0.1041(0.1100)	0.0002(0.0065)	0.255(0.347)	87.50(88.04)
[2023-09-29 13:49:10 10splitTasks](trainer.py 286): INFO [90/157]	0.1027(0.1093)	0.0003(0.0058)	0.544(0.342)	84.38(88.29)
[2023-09-29 13:49:12 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1089)	0.0007(0.0053)	0.338(0.349)	90.62(88.15)
[2023-09-29 13:49:13 10splitTasks](trainer.py 286): INFO [110/157]	0.1022(0.1086)	0.0002(0.0049)	0.326(0.355)	87.50(87.84)
[2023-09-29 13:49:14 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1082)	0.0001(0.0045)	0.384(0.353)	78.12(87.81)
[2023-09-29 13:49:15 10splitTasks](trainer.py 286): INFO [130/157]	0.1155(0.1080)	0.0006(0.0042)	0.330(0.348)	93.75(88.00)
[2023-09-29 13:49:16 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1077)	0.0002(0.0039)	0.227(0.345)	87.50(88.05)
[2023-09-29 13:49:17 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1075)	0.0001(0.0037)	0.527(0.345)	81.25(88.12)
[2023-09-29 13:49:17 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1071)	0.0001(0.0036)	1.499(0.348)	62.50(88.00)
[2023-09-29 13:49:17 10splitTasks](trainer.py 288): INFO  * Train Acc 88.000
[2023-09-29 13:49:19 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.71
[2023-09-29 13:49:19 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 13:49:19 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 13:49:19 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:49:20 10splitTasks](trainer.py 286): INFO [0/157]	0.6499(0.6499)	0.5443(0.5443)	0.173(0.173)	93.75(93.75)
[2023-09-29 13:49:21 10splitTasks](trainer.py 286): INFO [10/157]	0.1024(0.1553)	0.0002(0.0498)	0.217(0.306)	90.62(88.35)
[2023-09-29 13:49:22 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1311)	0.0002(0.0262)	0.238(0.309)	90.62(88.84)
[2023-09-29 13:49:23 10splitTasks](trainer.py 286): INFO [30/157]	0.1068(0.1224)	0.0048(0.0180)	0.284(0.304)	90.62(89.21)
[2023-09-29 13:49:24 10splitTasks](trainer.py 286): INFO [40/157]	0.1023(0.1177)	0.0003(0.0137)	0.161(0.322)	93.75(88.64)
[2023-09-29 13:49:25 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1149)	0.0002(0.0110)	0.229(0.325)	93.75(88.60)
[2023-09-29 13:49:26 10splitTasks](trainer.py 286): INFO [60/157]	0.1081(0.1131)	0.0002(0.0093)	0.428(0.315)	87.50(88.93)
[2023-09-29 13:49:27 10splitTasks](trainer.py 286): INFO [70/157]	0.1024(0.1116)	0.0002(0.0080)	0.414(0.318)	81.25(88.91)
[2023-09-29 13:49:28 10splitTasks](trainer.py 286): INFO [80/157]	0.1085(0.1107)	0.0002(0.0071)	0.669(0.331)	71.88(88.39)
[2023-09-29 13:49:29 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1099)	0.0002(0.0063)	0.129(0.326)	93.75(88.50)
[2023-09-29 13:49:30 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1092)	0.0003(0.0057)	0.282(0.330)	93.75(88.49)
[2023-09-29 13:49:31 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1086)	0.0003(0.0052)	0.549(0.326)	81.25(88.54)
[2023-09-29 13:49:32 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1084)	0.0003(0.0048)	0.346(0.321)	87.50(88.66)
[2023-09-29 13:49:33 10splitTasks](trainer.py 286): INFO [130/157]	0.1024(0.1080)	0.0002(0.0045)	0.417(0.328)	84.38(88.45)
[2023-09-29 13:49:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1076)	0.0002(0.0042)	0.107(0.331)	96.88(88.50)
[2023-09-29 13:49:35 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1074)	0.0001(0.0039)	0.084(0.327)	100.00(88.68)
[2023-09-29 13:49:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1070)	0.0001(0.0038)	1.025(0.328)	62.50(88.60)
[2023-09-29 13:49:36 10splitTasks](trainer.py 288): INFO  * Train Acc 88.600
[2023-09-29 13:49:38 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.400, Total time 1.64
[2023-09-29 13:49:38 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 13:49:38 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 13:49:38 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:49:38 10splitTasks](trainer.py 286): INFO [0/157]	0.6252(0.6252)	0.4982(0.4982)	0.401(0.401)	84.38(84.38)
[2023-09-29 13:49:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1028(0.1514)	0.0001(0.0455)	0.342(0.248)	84.38(90.91)
[2023-09-29 13:49:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1024(0.1277)	0.0002(0.0240)	0.260(0.265)	87.50(90.03)
[2023-09-29 13:49:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1052(0.1197)	0.0003(0.0164)	0.523(0.267)	84.38(90.73)
[2023-09-29 13:49:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1156)	0.0003(0.0125)	0.123(0.259)	96.88(91.31)
[2023-09-29 13:49:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1130)	0.0003(0.0101)	0.227(0.249)	87.50(91.48)
[2023-09-29 13:49:45 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1115)	0.0003(0.0085)	0.414(0.257)	81.25(91.24)
[2023-09-29 13:49:46 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1101)	0.0002(0.0073)	0.313(0.259)	78.12(91.07)
[2023-09-29 13:49:47 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1091)	0.0002(0.0065)	0.216(0.264)	93.75(90.97)
[2023-09-29 13:49:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1083)	0.0002(0.0058)	0.507(0.258)	75.00(91.00)
[2023-09-29 13:49:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1159(0.1080)	0.0003(0.0052)	0.271(0.258)	93.75(91.09)
[2023-09-29 13:49:50 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1075)	0.0002(0.0048)	0.236(0.264)	90.62(90.91)
[2023-09-29 13:49:51 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1071)	0.0003(0.0044)	0.216(0.267)	93.75(90.78)
[2023-09-29 13:49:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1009(0.1068)	0.0002(0.0041)	0.449(0.268)	81.25(90.77)
[2023-09-29 13:49:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1065)	0.0002(0.0038)	0.232(0.269)	90.62(90.69)
[2023-09-29 13:49:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1064)	0.0001(0.0036)	0.431(0.276)	90.62(90.52)
[2023-09-29 13:49:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1060)	0.0001(0.0035)	0.467(0.278)	75.00(90.44)
[2023-09-29 13:49:54 10splitTasks](trainer.py 288): INFO  * Train Acc 90.440
[2023-09-29 13:49:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.68
[2023-09-29 13:49:56 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 13:49:56 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 13:49:56 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:49:57 10splitTasks](trainer.py 286): INFO [0/157]	0.6388(0.6388)	0.5339(0.5339)	0.210(0.210)	93.75(93.75)
[2023-09-29 13:49:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1518)	0.0002(0.0488)	0.208(0.355)	90.62(87.22)
[2023-09-29 13:49:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1034(0.1298)	0.0002(0.0257)	0.121(0.286)	96.88(89.88)
[2023-09-29 13:50:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1026(0.1213)	0.0004(0.0175)	0.314(0.274)	87.50(90.42)
[2023-09-29 13:50:01 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1165)	0.0003(0.0133)	0.217(0.270)	87.50(90.47)
[2023-09-29 13:50:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1136)	0.0002(0.0108)	0.408(0.274)	84.38(90.32)
[2023-09-29 13:50:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1117)	0.0002(0.0091)	0.242(0.274)	93.75(90.37)
[2023-09-29 13:50:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1075(0.1104)	0.0003(0.0078)	0.300(0.281)	90.62(89.96)
[2023-09-29 13:50:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1093)	0.0002(0.0069)	0.306(0.273)	87.50(90.24)
[2023-09-29 13:50:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1088)	0.0001(0.0062)	0.089(0.269)	96.88(90.38)
[2023-09-29 13:50:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1081)	0.0002(0.0056)	0.289(0.268)	90.62(90.59)
[2023-09-29 13:50:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1040(0.1075)	0.0003(0.0051)	0.123(0.264)	96.88(90.62)
[2023-09-29 13:50:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1064(0.1071)	0.0003(0.0047)	0.415(0.263)	84.38(90.65)
[2023-09-29 13:50:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1069)	0.0002(0.0044)	0.331(0.260)	90.62(90.70)
[2023-09-29 13:50:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1031(0.1067)	0.0002(0.0041)	0.316(0.256)	90.62(90.91)
[2023-09-29 13:50:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1065)	0.0001(0.0039)	0.512(0.261)	81.25(90.75)
[2023-09-29 13:50:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1062)	0.0001(0.0038)	0.614(0.259)	75.00(90.74)
[2023-09-29 13:50:13 10splitTasks](trainer.py 288): INFO  * Train Acc 90.740
[2023-09-29 13:50:15 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.68
[2023-09-29 13:50:15 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 13:50:15 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 13:50:15 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:50:15 10splitTasks](trainer.py 286): INFO [0/157]	0.6136(0.6136)	0.5099(0.5099)	0.189(0.189)	90.62(90.62)
[2023-09-29 13:50:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1031(0.1530)	0.0003(0.0468)	0.260(0.198)	90.62(92.90)
[2023-09-29 13:50:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1288)	0.0003(0.0246)	0.136(0.188)	93.75(93.90)
[2023-09-29 13:50:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1204)	0.0002(0.0169)	0.638(0.231)	75.00(91.73)
[2023-09-29 13:50:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1034(0.1159)	0.0004(0.0128)	0.252(0.238)	90.62(91.62)
[2023-09-29 13:50:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1132)	0.0002(0.0104)	0.233(0.239)	90.62(91.36)
[2023-09-29 13:50:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1120)	0.0001(0.0087)	0.144(0.243)	96.88(91.19)
[2023-09-29 13:50:23 10splitTasks](trainer.py 286): INFO [70/157]	0.1051(0.1110)	0.0002(0.0076)	0.400(0.238)	84.38(91.46)
[2023-09-29 13:50:24 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1101)	0.0002(0.0067)	0.142(0.244)	93.75(91.36)
[2023-09-29 13:50:25 10splitTasks](trainer.py 286): INFO [90/157]	0.1122(0.1093)	0.0005(0.0060)	0.142(0.236)	96.88(91.90)
[2023-09-29 13:50:26 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1087)	0.0002(0.0054)	0.304(0.233)	90.62(92.02)
[2023-09-29 13:50:27 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1082)	0.0003(0.0050)	0.134(0.231)	96.88(92.09)
[2023-09-29 13:50:28 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1076)	0.0002(0.0046)	0.428(0.234)	90.62(91.97)
[2023-09-29 13:50:29 10splitTasks](trainer.py 286): INFO [130/157]	0.1131(0.1076)	0.0002(0.0043)	0.122(0.234)	93.75(91.89)
[2023-09-29 13:50:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1183(0.1076)	0.0006(0.0040)	0.073(0.235)	100.00(92.02)
[2023-09-29 13:50:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1072)	0.0001(0.0037)	0.234(0.237)	93.75(91.85)
[2023-09-29 13:50:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0775(0.1068)	0.0001(0.0036)	1.433(0.240)	75.00(91.72)
[2023-09-29 13:50:32 10splitTasks](trainer.py 288): INFO  * Train Acc 91.720
[2023-09-29 13:50:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.600, Total time 1.68
[2023-09-29 13:50:33 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 13:50:33 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 13:50:33 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:50:34 10splitTasks](trainer.py 286): INFO [0/157]	0.6133(0.6133)	0.5087(0.5087)	0.393(0.393)	87.50(87.50)
[2023-09-29 13:50:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1509)	0.0001(0.0465)	0.311(0.288)	90.62(90.34)
[2023-09-29 13:50:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1281)	0.0004(0.0245)	0.227(0.243)	93.75(91.96)
[2023-09-29 13:50:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1197)	0.0003(0.0167)	0.151(0.230)	93.75(92.14)
[2023-09-29 13:50:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1157)	0.0001(0.0127)	0.418(0.245)	81.25(91.08)
[2023-09-29 13:50:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1131)	0.0002(0.0103)	0.213(0.247)	93.75(91.05)
[2023-09-29 13:50:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1064(0.1116)	0.0002(0.0086)	0.212(0.242)	93.75(91.50)
[2023-09-29 13:50:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1025(0.1111)	0.0002(0.0075)	0.186(0.244)	90.62(91.46)
[2023-09-29 13:50:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1099)	0.0001(0.0066)	0.076(0.246)	96.88(91.36)
[2023-09-29 13:50:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1092)	0.0003(0.0059)	0.184(0.240)	96.88(91.45)
[2023-09-29 13:50:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1086)	0.0002(0.0053)	0.361(0.242)	87.50(91.40)
[2023-09-29 13:50:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1081)	0.0003(0.0049)	0.081(0.239)	100.00(91.55)
[2023-09-29 13:50:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1077)	0.0001(0.0045)	0.234(0.234)	90.62(91.68)
[2023-09-29 13:50:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1072)	0.0003(0.0042)	0.343(0.233)	87.50(91.79)
[2023-09-29 13:50:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1068)	0.0002(0.0039)	0.263(0.229)	87.50(92.00)
[2023-09-29 13:50:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1065)	0.0002(0.0037)	0.334(0.233)	84.38(91.93)
[2023-09-29 13:50:50 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1062)	0.0001(0.0035)	0.850(0.234)	62.50(91.86)
[2023-09-29 13:50:50 10splitTasks](trainer.py 288): INFO  * Train Acc 91.860
[2023-09-29 13:50:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.61
[2023-09-29 13:50:52 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 13:50:52 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 13:50:52 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:50:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6483(0.6483)	0.5201(0.5201)	0.126(0.126)	96.88(96.88)
[2023-09-29 13:50:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1128(0.1566)	0.0005(0.0476)	0.306(0.242)	90.62(91.48)
[2023-09-29 13:50:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1307)	0.0002(0.0251)	0.083(0.250)	100.00(91.07)
[2023-09-29 13:50:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1215)	0.0002(0.0171)	0.143(0.235)	93.75(91.63)
[2023-09-29 13:50:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1173)	0.0003(0.0130)	0.104(0.240)	96.88(91.31)
[2023-09-29 13:50:57 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1143)	0.0003(0.0105)	0.242(0.234)	90.62(91.61)
[2023-09-29 13:50:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1026(0.1130)	0.0003(0.0089)	0.240(0.231)	87.50(91.60)
[2023-09-29 13:51:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1115)	0.0003(0.0077)	0.391(0.228)	90.62(91.81)
[2023-09-29 13:51:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1108)	0.0003(0.0067)	0.216(0.223)	87.50(92.01)
[2023-09-29 13:51:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1099)	0.0003(0.0060)	0.119(0.218)	93.75(92.10)
[2023-09-29 13:51:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1093)	0.0002(0.0055)	0.149(0.214)	96.88(92.11)
[2023-09-29 13:51:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1086)	0.0001(0.0050)	0.207(0.214)	93.75(92.17)
[2023-09-29 13:51:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1081)	0.0003(0.0046)	0.043(0.209)	100.00(92.36)
[2023-09-29 13:51:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1010(0.1076)	0.0002(0.0043)	0.169(0.211)	93.75(92.27)
[2023-09-29 13:51:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1072)	0.0002(0.0040)	0.123(0.215)	93.75(92.15)
[2023-09-29 13:51:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1070)	0.0001(0.0038)	0.227(0.218)	90.62(92.03)
[2023-09-29 13:51:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0774(0.1066)	0.0001(0.0036)	0.780(0.215)	87.50(92.22)
[2023-09-29 13:51:08 10splitTasks](trainer.py 288): INFO  * Train Acc 92.220
[2023-09-29 13:51:10 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.400, Total time 1.67
[2023-09-29 13:51:10 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 13:51:10 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 13:51:10 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:51:11 10splitTasks](trainer.py 286): INFO [0/157]	0.7776(0.7776)	0.6613(0.6613)	0.363(0.363)	90.62(90.62)
[2023-09-29 13:51:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1633)	0.0003(0.0604)	0.271(0.166)	93.75(96.02)
[2023-09-29 13:51:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1038(0.1347)	0.0003(0.0318)	0.179(0.168)	96.88(95.09)
[2023-09-29 13:51:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1026(0.1244)	0.0002(0.0216)	0.048(0.157)	100.00(95.36)
[2023-09-29 13:51:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1198)	0.0003(0.0164)	0.472(0.166)	87.50(94.97)
[2023-09-29 13:51:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1045(0.1165)	0.0002(0.0133)	0.143(0.175)	90.62(94.55)
[2023-09-29 13:51:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1028(0.1143)	0.0003(0.0111)	0.089(0.167)	96.88(94.62)
[2023-09-29 13:51:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1034(0.1130)	0.0003(0.0096)	0.112(0.175)	96.88(94.32)
[2023-09-29 13:51:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1024(0.1119)	0.0004(0.0085)	0.219(0.179)	93.75(94.33)
[2023-09-29 13:51:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1109)	0.0001(0.0076)	0.154(0.178)	87.50(94.20)
[2023-09-29 13:51:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1086(0.1104)	0.0003(0.0068)	0.096(0.172)	96.88(94.46)
[2023-09-29 13:51:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1098)	0.0002(0.0063)	0.030(0.181)	100.00(94.17)
[2023-09-29 13:51:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1092)	0.0002(0.0058)	0.276(0.181)	87.50(94.14)
[2023-09-29 13:51:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1087)	0.0001(0.0053)	0.246(0.183)	90.62(94.13)
[2023-09-29 13:51:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1084)	0.0002(0.0050)	0.257(0.183)	90.62(94.02)
[2023-09-29 13:51:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1080)	0.0001(0.0047)	0.208(0.187)	93.75(93.89)
[2023-09-29 13:51:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1076)	0.0001(0.0045)	0.926(0.188)	75.00(93.86)
[2023-09-29 13:51:27 10splitTasks](trainer.py 288): INFO  * Train Acc 93.860
[2023-09-29 13:51:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.600, Total time 1.69
[2023-09-29 13:51:29 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 13:51:29 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 13:51:29 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:51:30 10splitTasks](trainer.py 286): INFO [0/157]	0.6553(0.6553)	0.5489(0.5489)	0.029(0.029)	100.00(100.00)
[2023-09-29 13:51:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1551)	0.0002(0.0502)	0.220(0.160)	93.75(94.60)
[2023-09-29 13:51:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1028(0.1300)	0.0003(0.0264)	0.181(0.146)	93.75(94.94)
[2023-09-29 13:51:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1218)	0.0002(0.0180)	0.300(0.183)	90.62(93.55)
[2023-09-29 13:51:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1170)	0.0002(0.0137)	0.048(0.174)	100.00(93.98)
[2023-09-29 13:51:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1140)	0.0003(0.0110)	0.032(0.174)	100.00(94.00)
[2023-09-29 13:51:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1026(0.1120)	0.0001(0.0093)	0.066(0.178)	96.88(93.90)
[2023-09-29 13:51:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1106)	0.0002(0.0080)	0.154(0.170)	93.75(94.10)
[2023-09-29 13:51:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1010(0.1098)	0.0002(0.0071)	0.064(0.171)	96.88(93.98)
[2023-09-29 13:51:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1089)	0.0002(0.0063)	0.042(0.172)	100.00(94.06)
[2023-09-29 13:51:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1082)	0.0002(0.0057)	0.212(0.175)	90.62(93.90)
[2023-09-29 13:51:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1076)	0.0002(0.0052)	0.218(0.178)	90.62(93.86)
[2023-09-29 13:51:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1062(0.1071)	0.0002(0.0048)	0.144(0.178)	90.62(93.83)
[2023-09-29 13:51:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1067)	0.0002(0.0045)	0.072(0.174)	100.00(93.96)
[2023-09-29 13:51:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1064)	0.0002(0.0042)	0.196(0.177)	93.75(93.88)
[2023-09-29 13:51:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1061)	0.0001(0.0039)	0.223(0.176)	90.62(93.83)
[2023-09-29 13:51:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0772(0.1057)	0.0001(0.0038)	1.342(0.180)	75.00(93.76)
[2023-09-29 13:51:46 10splitTasks](trainer.py 288): INFO  * Train Acc 93.760
[2023-09-29 13:51:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.600, Total time 1.63
[2023-09-29 13:51:47 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 13:51:47 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 13:51:47 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:51:48 10splitTasks](trainer.py 286): INFO [0/157]	0.6083(0.6083)	0.4803(0.4803)	0.066(0.066)	96.88(96.88)
[2023-09-29 13:51:49 10splitTasks](trainer.py 286): INFO [10/157]	0.1008(0.1499)	0.0001(0.0441)	0.069(0.177)	96.88(94.03)
[2023-09-29 13:51:50 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1278)	0.0001(0.0232)	0.139(0.166)	93.75(93.90)
[2023-09-29 13:51:51 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1194)	0.0002(0.0158)	0.355(0.155)	90.62(94.46)
[2023-09-29 13:51:52 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1152)	0.0002(0.0120)	0.140(0.143)	93.75(95.05)
[2023-09-29 13:51:53 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1127)	0.0002(0.0097)	0.122(0.147)	96.88(94.98)
[2023-09-29 13:51:54 10splitTasks](trainer.py 286): INFO [60/157]	0.1059(0.1110)	0.0003(0.0082)	0.220(0.146)	93.75(94.88)
[2023-09-29 13:51:55 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1099)	0.0003(0.0071)	0.051(0.146)	100.00(94.94)
[2023-09-29 13:51:56 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1090)	0.0002(0.0062)	0.092(0.151)	96.88(94.68)
[2023-09-29 13:51:57 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1085)	0.0004(0.0056)	0.653(0.155)	81.25(94.44)
[2023-09-29 13:51:58 10splitTasks](trainer.py 286): INFO [100/157]	0.1037(0.1078)	0.0002(0.0051)	0.284(0.157)	93.75(94.40)
[2023-09-29 13:51:59 10splitTasks](trainer.py 286): INFO [110/157]	0.1028(0.1074)	0.0002(0.0046)	0.119(0.157)	93.75(94.34)
[2023-09-29 13:52:00 10splitTasks](trainer.py 286): INFO [120/157]	0.1295(0.1073)	0.0006(0.0043)	0.072(0.159)	100.00(94.27)
[2023-09-29 13:52:01 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1068)	0.0002(0.0040)	0.237(0.162)	90.62(94.23)
[2023-09-29 13:52:02 10splitTasks](trainer.py 286): INFO [140/157]	0.1081(0.1067)	0.0002(0.0037)	0.150(0.162)	87.50(94.22)
[2023-09-29 13:52:03 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1064)	0.0001(0.0035)	0.175(0.160)	96.88(94.29)
[2023-09-29 13:52:04 10splitTasks](trainer.py 286): INFO [156/157]	0.0771(0.1060)	0.0001(0.0034)	0.109(0.159)	100.00(94.26)
[2023-09-29 13:52:04 10splitTasks](trainer.py 288): INFO  * Train Acc 94.260
[2023-09-29 13:52:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.000, Total time 1.62
[2023-09-29 13:52:06 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 13:52:06 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 13:52:06 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:52:06 10splitTasks](trainer.py 286): INFO [0/157]	0.6273(0.6273)	0.5114(0.5114)	0.064(0.064)	100.00(100.00)
[2023-09-29 13:52:07 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1508)	0.0001(0.0467)	0.182(0.181)	93.75(93.75)
[2023-09-29 13:52:08 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1276)	0.0002(0.0246)	0.095(0.153)	93.75(94.20)
[2023-09-29 13:52:09 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1194)	0.0002(0.0168)	0.149(0.167)	93.75(93.65)
[2023-09-29 13:52:10 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1152)	0.0001(0.0127)	0.126(0.166)	93.75(93.98)
[2023-09-29 13:52:11 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1126)	0.0002(0.0103)	0.053(0.162)	100.00(94.06)
[2023-09-29 13:52:12 10splitTasks](trainer.py 286): INFO [60/157]	0.1037(0.1109)	0.0006(0.0086)	0.037(0.151)	100.00(94.57)
[2023-09-29 13:52:13 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1098)	0.0002(0.0075)	0.180(0.153)	90.62(94.50)
[2023-09-29 13:52:14 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1089)	0.0002(0.0066)	0.108(0.159)	93.75(94.21)
[2023-09-29 13:52:15 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1080)	0.0002(0.0059)	0.177(0.159)	96.88(94.16)
[2023-09-29 13:52:16 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1074)	0.0002(0.0053)	0.099(0.160)	93.75(94.21)
[2023-09-29 13:52:17 10splitTasks](trainer.py 286): INFO [110/157]	0.1156(0.1071)	0.0005(0.0049)	0.381(0.159)	81.25(94.17)
[2023-09-29 13:52:18 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1067)	0.0003(0.0045)	0.102(0.157)	93.75(94.34)
[2023-09-29 13:52:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1064)	0.0002(0.0042)	0.050(0.155)	100.00(94.47)
[2023-09-29 13:52:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1023(0.1061)	0.0003(0.0039)	0.102(0.156)	96.88(94.41)
[2023-09-29 13:52:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1058)	0.0002(0.0037)	0.038(0.154)	100.00(94.43)
[2023-09-29 13:52:22 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1055)	0.0001(0.0035)	0.080(0.153)	100.00(94.48)
[2023-09-29 13:52:22 10splitTasks](trainer.py 288): INFO  * Train Acc 94.480
[2023-09-29 13:52:24 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.75
[2023-09-29 13:52:24 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 13:52:24 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 13:52:24 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:52:25 10splitTasks](trainer.py 286): INFO [0/157]	0.6097(0.6097)	0.4923(0.4923)	0.224(0.224)	93.75(93.75)
[2023-09-29 13:52:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1541)	0.0002(0.0495)	0.117(0.120)	96.88(96.31)
[2023-09-29 13:52:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1031(0.1301)	0.0003(0.0260)	0.120(0.120)	96.88(96.13)
[2023-09-29 13:52:28 10splitTasks](trainer.py 286): INFO [30/157]	0.1073(0.1217)	0.0003(0.0178)	0.150(0.117)	90.62(95.97)
[2023-09-29 13:52:29 10splitTasks](trainer.py 286): INFO [40/157]	0.1008(0.1174)	0.0002(0.0135)	0.087(0.117)	100.00(96.04)
[2023-09-29 13:52:30 10splitTasks](trainer.py 286): INFO [50/157]	0.1040(0.1147)	0.0003(0.0109)	0.073(0.121)	96.88(95.77)
[2023-09-29 13:52:31 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1130)	0.0002(0.0092)	0.061(0.112)	100.00(96.21)
[2023-09-29 13:52:32 10splitTasks](trainer.py 286): INFO [70/157]	0.1043(0.1116)	0.0005(0.0079)	0.140(0.114)	96.88(96.13)
[2023-09-29 13:52:33 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1104)	0.0002(0.0070)	0.099(0.115)	96.88(96.18)
[2023-09-29 13:52:34 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1097)	0.0002(0.0063)	0.035(0.114)	100.00(96.05)
[2023-09-29 13:52:35 10splitTasks](trainer.py 286): INFO [100/157]	0.1010(0.1089)	0.0002(0.0057)	0.092(0.115)	96.88(95.95)
[2023-09-29 13:52:36 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1083)	0.0003(0.0052)	0.084(0.116)	100.00(95.95)
[2023-09-29 13:52:37 10splitTasks](trainer.py 286): INFO [120/157]	0.1012(0.1078)	0.0002(0.0048)	0.117(0.115)	96.88(96.05)
[2023-09-29 13:52:38 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1073)	0.0002(0.0044)	0.141(0.114)	93.75(96.14)
[2023-09-29 13:52:39 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1069)	0.0003(0.0041)	0.145(0.115)	96.88(96.17)
[2023-09-29 13:52:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1068)	0.0001(0.0039)	0.032(0.116)	100.00(96.11)
[2023-09-29 13:52:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0771(0.1064)	0.0001(0.0038)	0.458(0.117)	87.50(96.12)
[2023-09-29 13:52:41 10splitTasks](trainer.py 288): INFO  * Train Acc 96.120
[2023-09-29 13:52:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.000, Total time 1.66
[2023-09-29 13:52:42 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 13:52:42 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 13:52:42 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:52:43 10splitTasks](trainer.py 286): INFO [0/157]	0.7014(0.7014)	0.5927(0.5927)	0.135(0.135)	96.88(96.88)
[2023-09-29 13:52:44 10splitTasks](trainer.py 286): INFO [10/157]	0.1009(0.1581)	0.0002(0.0542)	0.102(0.096)	93.75(96.59)
[2023-09-29 13:52:45 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1315)	0.0002(0.0285)	0.048(0.108)	100.00(96.88)
[2023-09-29 13:52:46 10splitTasks](trainer.py 286): INFO [30/157]	0.1009(0.1222)	0.0001(0.0194)	0.167(0.115)	93.75(96.07)
[2023-09-29 13:52:47 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1173)	0.0002(0.0147)	0.208(0.115)	93.75(96.11)
[2023-09-29 13:52:48 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1145)	0.0002(0.0120)	0.107(0.122)	96.88(95.77)
[2023-09-29 13:52:49 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1124)	0.0003(0.0101)	0.162(0.128)	96.88(95.59)
[2023-09-29 13:52:50 10splitTasks](trainer.py 286): INFO [70/157]	0.1051(0.1109)	0.0002(0.0087)	0.235(0.126)	93.75(95.73)
[2023-09-29 13:52:51 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1098)	0.0002(0.0076)	0.062(0.124)	96.88(95.79)
[2023-09-29 13:52:52 10splitTasks](trainer.py 286): INFO [90/157]	0.1010(0.1089)	0.0003(0.0068)	0.105(0.128)	96.88(95.71)
[2023-09-29 13:52:53 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1082)	0.0003(0.0062)	0.092(0.130)	96.88(95.67)
[2023-09-29 13:52:54 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1078)	0.0002(0.0056)	0.089(0.129)	96.88(95.64)
[2023-09-29 13:52:55 10splitTasks](trainer.py 286): INFO [120/157]	0.1034(0.1073)	0.0002(0.0052)	0.108(0.127)	93.75(95.74)
[2023-09-29 13:52:56 10splitTasks](trainer.py 286): INFO [130/157]	0.1009(0.1069)	0.0002(0.0048)	0.313(0.128)	84.38(95.71)
[2023-09-29 13:52:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1065)	0.0002(0.0045)	0.134(0.131)	96.88(95.70)
[2023-09-29 13:52:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1062)	0.0001(0.0042)	0.108(0.131)	96.88(95.65)
[2023-09-29 13:52:59 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1058)	0.0001(0.0041)	0.779(0.131)	87.50(95.70)
[2023-09-29 13:52:59 10splitTasks](trainer.py 288): INFO  * Train Acc 95.700
[2023-09-29 13:53:01 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.800, Total time 1.63
[2023-09-29 13:53:01 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 13:53:01 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 13:53:01 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:53:01 10splitTasks](trainer.py 286): INFO [0/157]	0.5977(0.5977)	0.4880(0.4880)	0.087(0.087)	96.88(96.88)
[2023-09-29 13:53:03 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1574)	0.0003(0.0534)	0.115(0.114)	93.75(96.02)
[2023-09-29 13:53:04 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1312)	0.0002(0.0281)	0.052(0.119)	96.88(95.83)
[2023-09-29 13:53:05 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1222)	0.0003(0.0191)	0.209(0.121)	93.75(96.07)
[2023-09-29 13:53:06 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1176)	0.0002(0.0145)	0.085(0.124)	96.88(96.04)
[2023-09-29 13:53:07 10splitTasks](trainer.py 286): INFO [50/157]	0.1009(0.1146)	0.0001(0.0117)	0.055(0.119)	100.00(96.20)
[2023-09-29 13:53:08 10splitTasks](trainer.py 286): INFO [60/157]	0.1204(0.1129)	0.0004(0.0099)	0.061(0.120)	96.88(96.11)
[2023-09-29 13:53:09 10splitTasks](trainer.py 286): INFO [70/157]	0.1042(0.1114)	0.0003(0.0085)	0.148(0.120)	93.75(96.17)
[2023-09-29 13:53:10 10splitTasks](trainer.py 286): INFO [80/157]	0.1026(0.1104)	0.0003(0.0075)	0.043(0.120)	100.00(96.14)
[2023-09-29 13:53:11 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1096)	0.0003(0.0067)	0.054(0.120)	100.00(96.19)
[2023-09-29 13:53:12 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1088)	0.0002(0.0061)	0.244(0.127)	90.62(96.01)
[2023-09-29 13:53:13 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1087)	0.0003(0.0056)	0.169(0.127)	90.62(96.03)
[2023-09-29 13:53:14 10splitTasks](trainer.py 286): INFO [120/157]	0.1012(0.1082)	0.0002(0.0051)	0.034(0.129)	100.00(95.95)
[2023-09-29 13:53:15 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1080)	0.0003(0.0048)	0.109(0.129)	96.88(95.97)
[2023-09-29 13:53:16 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1077)	0.0002(0.0045)	0.059(0.128)	93.75(95.99)
[2023-09-29 13:53:17 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1073)	0.0001(0.0042)	0.040(0.128)	100.00(96.01)
[2023-09-29 13:53:18 10splitTasks](trainer.py 286): INFO [156/157]	0.0778(0.1069)	0.0001(0.0041)	0.039(0.126)	100.00(96.06)
[2023-09-29 13:53:18 10splitTasks](trainer.py 288): INFO  * Train Acc 96.060
[2023-09-29 13:53:19 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.800, Total time 1.68
[2023-09-29 13:53:19 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 13:53:19 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 13:53:19 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:53:20 10splitTasks](trainer.py 286): INFO [0/157]	0.5584(0.5584)	0.4532(0.4532)	0.115(0.115)	90.62(90.62)
[2023-09-29 13:53:21 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1476)	0.0001(0.0415)	0.138(0.130)	96.88(95.74)
[2023-09-29 13:53:22 10splitTasks](trainer.py 286): INFO [20/157]	0.1047(0.1264)	0.0003(0.0219)	0.053(0.111)	96.88(96.58)
[2023-09-29 13:53:23 10splitTasks](trainer.py 286): INFO [30/157]	0.1059(0.1186)	0.0001(0.0149)	0.050(0.112)	96.88(96.57)
[2023-09-29 13:53:24 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1144)	0.0003(0.0113)	0.018(0.118)	100.00(96.04)
[2023-09-29 13:53:25 10splitTasks](trainer.py 286): INFO [50/157]	0.1009(0.1120)	0.0002(0.0091)	0.119(0.123)	96.88(95.96)
[2023-09-29 13:53:26 10splitTasks](trainer.py 286): INFO [60/157]	0.1105(0.1105)	0.0001(0.0077)	0.232(0.121)	96.88(96.00)
[2023-09-29 13:53:27 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1095)	0.0002(0.0067)	0.056(0.116)	100.00(96.17)
[2023-09-29 13:53:28 10splitTasks](trainer.py 286): INFO [80/157]	0.1060(0.1089)	0.0002(0.0059)	0.077(0.113)	100.00(96.37)
[2023-09-29 13:53:29 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1082)	0.0002(0.0053)	0.033(0.110)	100.00(96.43)
[2023-09-29 13:53:30 10splitTasks](trainer.py 286): INFO [100/157]	0.1151(0.1079)	0.0005(0.0048)	0.057(0.111)	100.00(96.44)
[2023-09-29 13:53:31 10splitTasks](trainer.py 286): INFO [110/157]	0.1111(0.1076)	0.0002(0.0044)	0.130(0.113)	96.88(96.37)
[2023-09-29 13:53:32 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1071)	0.0002(0.0041)	0.078(0.116)	96.88(96.26)
[2023-09-29 13:53:33 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1068)	0.0001(0.0038)	0.169(0.115)	90.62(96.25)
[2023-09-29 13:53:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1064)	0.0003(0.0035)	0.099(0.112)	96.88(96.37)
[2023-09-29 13:53:36 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1063)	0.0001(0.0033)	0.033(0.114)	100.00(96.36)
[2023-09-29 13:53:36 10splitTasks](trainer.py 286): INFO [156/157]	0.0795(0.1060)	0.0001(0.0032)	0.078(0.113)	100.00(96.38)
[2023-09-29 13:53:36 10splitTasks](trainer.py 288): INFO  * Train Acc 96.380
[2023-09-29 13:53:38 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.600, Total time 1.62
[2023-09-29 13:53:38 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 13:53:38 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 13:53:38 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:53:38 10splitTasks](trainer.py 286): INFO [0/157]	0.6268(0.6268)	0.5180(0.5180)	0.278(0.278)	90.62(90.62)
[2023-09-29 13:53:40 10splitTasks](trainer.py 286): INFO [10/157]	0.1024(0.1530)	0.0002(0.0474)	0.121(0.148)	96.88(94.89)
[2023-09-29 13:53:41 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1286)	0.0003(0.0250)	0.056(0.136)	100.00(95.68)
[2023-09-29 13:53:42 10splitTasks](trainer.py 286): INFO [30/157]	0.1037(0.1199)	0.0003(0.0170)	0.149(0.128)	96.88(95.67)
[2023-09-29 13:53:43 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1157)	0.0003(0.0129)	0.074(0.116)	100.00(96.34)
[2023-09-29 13:53:44 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1131)	0.0002(0.0104)	0.138(0.118)	96.88(96.08)
[2023-09-29 13:53:45 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1113)	0.0003(0.0088)	0.120(0.115)	93.75(96.06)
[2023-09-29 13:53:46 10splitTasks](trainer.py 286): INFO [70/157]	0.1041(0.1104)	0.0002(0.0076)	0.252(0.114)	90.62(96.08)
[2023-09-29 13:53:47 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1094)	0.0003(0.0067)	0.103(0.120)	96.88(96.06)
[2023-09-29 13:53:48 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1086)	0.0002(0.0060)	0.101(0.121)	96.88(96.02)
[2023-09-29 13:53:49 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1081)	0.0002(0.0054)	0.091(0.120)	96.88(96.10)
[2023-09-29 13:53:50 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1075)	0.0001(0.0050)	0.052(0.119)	100.00(96.03)
[2023-09-29 13:53:51 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1070)	0.0003(0.0046)	0.039(0.119)	100.00(96.07)
[2023-09-29 13:53:52 10splitTasks](trainer.py 286): INFO [130/157]	0.1030(0.1066)	0.0003(0.0042)	0.068(0.116)	96.88(96.16)
[2023-09-29 13:53:53 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1064)	0.0003(0.0040)	0.073(0.119)	100.00(96.10)
[2023-09-29 13:53:54 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1060)	0.0001(0.0037)	0.077(0.119)	100.00(96.07)
[2023-09-29 13:53:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0773(0.1057)	0.0001(0.0036)	0.328(0.119)	87.50(96.08)
[2023-09-29 13:53:55 10splitTasks](trainer.py 288): INFO  * Train Acc 96.080
[2023-09-29 13:53:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.600, Total time 1.93
[2023-09-29 13:53:56 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 13:53:56 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 13:53:56 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:53:57 10splitTasks](trainer.py 286): INFO [0/157]	0.6601(0.6601)	0.5561(0.5561)	0.103(0.103)	93.75(93.75)
[2023-09-29 13:53:58 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1523)	0.0001(0.0508)	0.024(0.081)	100.00(97.16)
[2023-09-29 13:53:59 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1289)	0.0002(0.0267)	0.323(0.120)	90.62(95.83)
[2023-09-29 13:54:00 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1201)	0.0003(0.0182)	0.074(0.118)	96.88(95.97)
[2023-09-29 13:54:01 10splitTasks](trainer.py 286): INFO [40/157]	0.1008(0.1157)	0.0001(0.0138)	0.245(0.113)	87.50(96.11)
[2023-09-29 13:54:02 10splitTasks](trainer.py 286): INFO [50/157]	0.1030(0.1129)	0.0001(0.0112)	0.089(0.108)	96.88(96.20)
[2023-09-29 13:54:03 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1114)	0.0002(0.0094)	0.165(0.104)	93.75(96.47)
[2023-09-29 13:54:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1101)	0.0001(0.0081)	0.130(0.106)	96.88(96.39)
[2023-09-29 13:54:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1093)	0.0003(0.0071)	0.063(0.108)	100.00(96.41)
[2023-09-29 13:54:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1087)	0.0002(0.0064)	0.182(0.110)	93.75(96.29)
[2023-09-29 13:54:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1081)	0.0002(0.0058)	0.057(0.112)	96.88(96.38)
[2023-09-29 13:54:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1059(0.1076)	0.0005(0.0053)	0.129(0.114)	96.88(96.23)
[2023-09-29 13:54:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1071)	0.0003(0.0049)	0.214(0.115)	90.62(96.20)
[2023-09-29 13:54:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1067)	0.0002(0.0046)	0.127(0.115)	93.75(96.23)
[2023-09-29 13:54:12 10splitTasks](trainer.py 286): INFO [140/157]	0.1181(0.1067)	0.0006(0.0043)	0.065(0.112)	96.88(96.25)
[2023-09-29 13:54:13 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1066)	0.0001(0.0040)	0.172(0.113)	93.75(96.19)
[2023-09-29 13:54:13 10splitTasks](trainer.py 286): INFO [156/157]	0.0818(0.1063)	0.0001(0.0039)	0.605(0.113)	87.50(96.24)
[2023-09-29 13:54:13 10splitTasks](trainer.py 288): INFO  * Train Acc 96.240
[2023-09-29 13:54:15 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.200, Total time 1.66
[2023-09-29 13:54:15 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 13:54:15 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 13:54:15 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:54:16 10splitTasks](trainer.py 286): INFO [0/157]	0.6136(0.6136)	0.4955(0.4955)	0.075(0.075)	100.00(100.00)
[2023-09-29 13:54:17 10splitTasks](trainer.py 286): INFO [10/157]	0.1046(0.1490)	0.0002(0.0453)	0.046(0.124)	100.00(96.31)
[2023-09-29 13:54:18 10splitTasks](trainer.py 286): INFO [20/157]	0.1032(0.1273)	0.0003(0.0238)	0.105(0.112)	96.88(96.43)
[2023-09-29 13:54:19 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1200)	0.0002(0.0163)	0.117(0.102)	96.88(96.88)
[2023-09-29 13:54:20 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1157)	0.0006(0.0124)	0.230(0.100)	93.75(97.03)
[2023-09-29 13:54:21 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1130)	0.0002(0.0100)	0.081(0.103)	96.88(96.75)
[2023-09-29 13:54:22 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1115)	0.0002(0.0084)	0.298(0.101)	87.50(96.82)
[2023-09-29 13:54:23 10splitTasks](trainer.py 286): INFO [70/157]	0.1019(0.1102)	0.0001(0.0073)	0.221(0.104)	90.62(96.43)
[2023-09-29 13:54:24 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1094)	0.0002(0.0064)	0.091(0.110)	96.88(96.33)
[2023-09-29 13:54:25 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1086)	0.0001(0.0057)	0.175(0.109)	93.75(96.36)
[2023-09-29 13:54:26 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1080)	0.0003(0.0052)	0.060(0.107)	96.88(96.38)
[2023-09-29 13:54:27 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1076)	0.0003(0.0047)	0.072(0.105)	100.00(96.45)
[2023-09-29 13:54:28 10splitTasks](trainer.py 286): INFO [120/157]	0.1037(0.1072)	0.0002(0.0044)	0.124(0.106)	93.75(96.44)
[2023-09-29 13:54:29 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1069)	0.0002(0.0041)	0.039(0.106)	100.00(96.35)
[2023-09-29 13:54:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1028(0.1066)	0.0003(0.0038)	0.038(0.105)	100.00(96.45)
[2023-09-29 13:54:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1064)	0.0001(0.0036)	0.229(0.107)	90.62(96.36)
[2023-09-29 13:54:32 10splitTasks](trainer.py 286): INFO [156/157]	0.0780(0.1061)	0.0001(0.0034)	0.021(0.108)	100.00(96.34)
[2023-09-29 13:54:32 10splitTasks](trainer.py 288): INFO  * Train Acc 96.340
[2023-09-29 13:54:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.70
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-6.pth
=> Save Done
[2023-09-29 13:54:34 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 13:54:35 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.80
[2023-09-29 13:54:35 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 13:54:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.75
[2023-09-29 13:54:37 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 13:54:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.75
[2023-09-29 13:54:39 10splitTasks](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 13:54:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.82
[2023-09-29 13:54:41 10splitTasks](iBatchLearn.py 131): INFO validation split name:4
[2023-09-29 13:54:42 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.74
[2023-09-29 13:54:42 10splitTasks](iBatchLearn.py 131): INFO validation split name:5
[2023-09-29 13:54:44 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.600, Total time 1.80
[2023-09-29 13:54:44 10splitTasks](iBatchLearn.py 131): INFO validation split name:6
[2023-09-29 13:54:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.81
[2023-09-29 13:54:46 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 13:54:46 10splitTasks](trainer.py 341): INFO done
[2023-09-29 13:54:46 10splitTasks](iBatchLearn.py 155): INFO Acc:82.05714285714285; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:54:50 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:54:50 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:54:50 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 6, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-6.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-6.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_6.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:54:50 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-6.pth
[2023-09-29 13:54:51 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:54:53 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:54:53 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:54:53 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 4, 4, 4],
        [3, 3, 2, 4, 6, 4, 4],
        [4, 3, 3, 6, 6, 4, 4],
        [4, 3, 6, 5, 4, 4, 4],
        [4, 6, 6, 5, 4, 5, 5],
        [7, 6, 5, 4, 2, 2, 5],
        [7, 3, 5, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 13:54:53 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:54:53 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:54:53 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 13:54:59 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 13:55:02 10splitTasks](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 13:55:05 10splitTasks](iBatchLearn.py 167): INFO test split name:3
[2023-09-29 13:55:08 10splitTasks](iBatchLearn.py 167): INFO test split name:4
[2023-09-29 13:55:11 10splitTasks](iBatchLearn.py 167): INFO test split name:5
[2023-09-29 13:55:14 10splitTasks](iBatchLearn.py 167): INFO test split name:6
--------------------------------Official Evaluation--------------------------------
6 81.75714285714287
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 13:55:23 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 13:55:23 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 13:55:23 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 7, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-6.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-7.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-6.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-7.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 13:55:23 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-6.pth
[2023-09-29 13:55:23 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 13:55:25 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 13:55:25 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 13:55:25 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 13:55:26 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 13:55:26 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 13:55:26 10splitTasks](iBatchLearn.py 92): INFO ====================== 7 =======================
[2023-09-29 13:55:26 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 13:55:26 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 13:55:26 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 13:55:26 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:55:29 10splitTasks](trainer.py 286): INFO [0/157]	3.4428(3.4428)	0.5132(0.5132)	2.434(2.434)	3.12(3.12)
[2023-09-29 13:55:30 10splitTasks](trainer.py 286): INFO [10/157]	0.1040(0.4061)	0.0005(0.0469)	2.151(2.263)	21.88(15.06)
[2023-09-29 13:55:31 10splitTasks](trainer.py 286): INFO [20/157]	0.1007(0.2614)	0.0002(0.0247)	1.793(2.152)	37.50(22.62)
[2023-09-29 13:55:32 10splitTasks](trainer.py 286): INFO [30/157]	0.1002(0.2099)	0.0001(0.0168)	1.614(2.005)	46.88(29.84)
[2023-09-29 13:55:33 10splitTasks](trainer.py 286): INFO [40/157]	0.1008(0.1835)	0.0002(0.0128)	1.336(1.889)	65.62(34.83)
[2023-09-29 13:55:34 10splitTasks](trainer.py 286): INFO [50/157]	0.1060(0.1681)	0.0005(0.0103)	0.871(1.764)	78.12(39.64)
[2023-09-29 13:55:35 10splitTasks](trainer.py 286): INFO [60/157]	0.1060(0.1575)	0.0004(0.0087)	1.120(1.679)	62.50(41.96)
[2023-09-29 13:55:36 10splitTasks](trainer.py 286): INFO [70/157]	0.1044(0.1497)	0.0002(0.0075)	0.838(1.600)	65.62(44.45)
[2023-09-29 13:55:37 10splitTasks](trainer.py 286): INFO [80/157]	0.1007(0.1437)	0.0001(0.0066)	1.211(1.550)	62.50(46.14)
[2023-09-29 13:55:38 10splitTasks](trainer.py 286): INFO [90/157]	0.1006(0.1391)	0.0001(0.0059)	1.188(1.510)	59.38(47.77)
[2023-09-29 13:55:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1359)	0.0002(0.0054)	1.089(1.467)	68.75(49.13)
[2023-09-29 13:55:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1072(0.1330)	0.0002(0.0049)	1.250(1.420)	62.50(50.73)
[2023-09-29 13:55:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1007(0.1305)	0.0002(0.0045)	0.873(1.382)	75.00(52.20)
[2023-09-29 13:55:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1283)	0.0003(0.0042)	0.980(1.347)	75.00(53.39)
[2023-09-29 13:55:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1006(0.1265)	0.0001(0.0039)	0.803(1.319)	71.88(54.41)
[2023-09-29 13:55:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1005(0.1248)	0.0001(0.0037)	1.027(1.289)	65.62(55.57)
[2023-09-29 13:55:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0805(0.1238)	0.0001(0.0036)	0.808(1.272)	75.00(56.14)
[2023-09-29 13:55:45 10splitTasks](trainer.py 288): INFO  * Train Acc 56.140
[2023-09-29 13:55:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.200, Total time 1.72
[2023-09-29 13:55:47 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 13:55:47 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 13:55:47 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:55:48 10splitTasks](trainer.py 286): INFO [0/157]	0.5634(0.5634)	0.4560(0.4560)	0.592(0.592)	84.38(84.38)
[2023-09-29 13:55:49 10splitTasks](trainer.py 286): INFO [10/157]	0.1007(0.1467)	0.0002(0.0432)	0.767(0.836)	65.62(70.17)
[2023-09-29 13:55:50 10splitTasks](trainer.py 286): INFO [20/157]	0.1051(0.1267)	0.0004(0.0228)	0.698(0.809)	71.88(72.32)
[2023-09-29 13:55:51 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1192)	0.0003(0.0156)	0.967(0.857)	68.75(70.56)
[2023-09-29 13:55:52 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1151)	0.0003(0.0118)	0.819(0.858)	71.88(70.73)
[2023-09-29 13:55:53 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1126)	0.0002(0.0096)	0.840(0.861)	78.12(70.53)
[2023-09-29 13:55:54 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1110)	0.0002(0.0080)	0.554(0.856)	84.38(70.65)
[2023-09-29 13:55:55 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1098)	0.0003(0.0069)	0.806(0.849)	68.75(70.64)
[2023-09-29 13:55:56 10splitTasks](trainer.py 286): INFO [80/157]	0.1026(0.1091)	0.0002(0.0061)	0.453(0.845)	84.38(71.14)
[2023-09-29 13:55:57 10splitTasks](trainer.py 286): INFO [90/157]	0.1011(0.1087)	0.0003(0.0055)	0.744(0.836)	71.88(71.60)
[2023-09-29 13:55:58 10splitTasks](trainer.py 286): INFO [100/157]	0.1009(0.1081)	0.0002(0.0050)	0.689(0.837)	75.00(71.69)
[2023-09-29 13:55:59 10splitTasks](trainer.py 286): INFO [110/157]	0.1153(0.1077)	0.0005(0.0046)	0.516(0.831)	75.00(71.85)
[2023-09-29 13:56:00 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1074)	0.0002(0.0042)	0.894(0.822)	75.00(72.26)
[2023-09-29 13:56:01 10splitTasks](trainer.py 286): INFO [130/157]	0.1065(0.1072)	0.0001(0.0039)	0.815(0.816)	68.75(72.45)
[2023-09-29 13:56:02 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1070)	0.0003(0.0036)	0.702(0.809)	84.38(72.89)
[2023-09-29 13:56:03 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1070)	0.0001(0.0034)	0.850(0.811)	68.75(73.01)
[2023-09-29 13:56:04 10splitTasks](trainer.py 286): INFO [156/157]	0.0848(0.1067)	0.0001(0.0033)	1.292(0.815)	62.50(72.94)
[2023-09-29 13:56:04 10splitTasks](trainer.py 288): INFO  * Train Acc 72.940
[2023-09-29 13:56:06 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.600, Total time 1.84
[2023-09-29 13:56:06 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 13:56:06 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 13:56:06 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:56:06 10splitTasks](trainer.py 286): INFO [0/157]	0.5924(0.5924)	0.4818(0.4818)	0.672(0.672)	78.12(78.12)
[2023-09-29 13:56:07 10splitTasks](trainer.py 286): INFO [10/157]	0.1107(0.1512)	0.0002(0.0459)	0.883(0.834)	75.00(71.31)
[2023-09-29 13:56:08 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1281)	0.0001(0.0242)	0.494(0.798)	81.25(73.21)
[2023-09-29 13:56:10 10splitTasks](trainer.py 286): INFO [30/157]	0.1127(0.1198)	0.0002(0.0165)	0.742(0.761)	71.88(74.40)
[2023-09-29 13:56:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1048(0.1156)	0.0003(0.0125)	0.549(0.750)	81.25(74.47)
[2023-09-29 13:56:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1130)	0.0003(0.0101)	0.488(0.752)	84.38(74.26)
[2023-09-29 13:56:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1114)	0.0002(0.0085)	0.810(0.759)	71.88(74.28)
[2023-09-29 13:56:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1100)	0.0002(0.0073)	0.530(0.746)	81.25(74.47)
[2023-09-29 13:56:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1045(0.1091)	0.0003(0.0065)	0.786(0.742)	59.38(74.31)
[2023-09-29 13:56:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1149(0.1086)	0.0003(0.0058)	1.066(0.744)	68.75(74.59)
[2023-09-29 13:56:17 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1082)	0.0003(0.0052)	0.702(0.741)	75.00(74.75)
[2023-09-29 13:56:18 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1078)	0.0002(0.0048)	1.039(0.745)	71.88(74.77)
[2023-09-29 13:56:19 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1074)	0.0002(0.0044)	0.933(0.738)	68.75(75.21)
[2023-09-29 13:56:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1111(0.1070)	0.0002(0.0041)	0.444(0.734)	90.62(75.43)
[2023-09-29 13:56:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1011(0.1067)	0.0002(0.0038)	0.692(0.726)	71.88(75.73)
[2023-09-29 13:56:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1066)	0.0001(0.0036)	0.633(0.720)	84.38(75.99)
[2023-09-29 13:56:22 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1062)	0.0001(0.0035)	1.124(0.719)	75.00(76.02)
[2023-09-29 13:56:23 10splitTasks](trainer.py 288): INFO  * Train Acc 76.020
[2023-09-29 13:56:24 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.200, Total time 1.81
[2023-09-29 13:56:24 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 13:56:24 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 13:56:24 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:56:25 10splitTasks](trainer.py 286): INFO [0/157]	0.5663(0.5663)	0.4497(0.4497)	0.582(0.582)	78.12(78.12)
[2023-09-29 13:56:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1097(0.1482)	0.0003(0.0415)	0.626(0.599)	78.12(80.97)
[2023-09-29 13:56:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1264)	0.0003(0.0219)	0.381(0.605)	87.50(80.80)
[2023-09-29 13:56:28 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1184)	0.0002(0.0149)	0.453(0.600)	81.25(80.65)
[2023-09-29 13:56:29 10splitTasks](trainer.py 286): INFO [40/157]	0.1052(0.1149)	0.0002(0.0114)	0.414(0.577)	81.25(81.71)
[2023-09-29 13:56:30 10splitTasks](trainer.py 286): INFO [50/157]	0.1118(0.1125)	0.0002(0.0092)	0.250(0.571)	87.50(81.99)
[2023-09-29 13:56:31 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1109)	0.0002(0.0077)	0.572(0.583)	78.12(81.40)
[2023-09-29 13:56:32 10splitTasks](trainer.py 286): INFO [70/157]	0.1081(0.1098)	0.0002(0.0067)	0.696(0.593)	75.00(81.25)
[2023-09-29 13:56:33 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1088)	0.0002(0.0059)	0.732(0.599)	81.25(81.13)
[2023-09-29 13:56:34 10splitTasks](trainer.py 286): INFO [90/157]	0.1032(0.1082)	0.0003(0.0053)	0.488(0.602)	81.25(81.04)
[2023-09-29 13:56:35 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1076)	0.0002(0.0048)	0.594(0.601)	71.88(80.85)
[2023-09-29 13:56:36 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1073)	0.0002(0.0044)	0.493(0.593)	75.00(80.83)
[2023-09-29 13:56:37 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1068)	0.0004(0.0041)	0.469(0.582)	84.38(81.28)
[2023-09-29 13:56:38 10splitTasks](trainer.py 286): INFO [130/157]	0.1060(0.1066)	0.0003(0.0038)	0.671(0.580)	78.12(81.32)
[2023-09-29 13:56:39 10splitTasks](trainer.py 286): INFO [140/157]	0.1078(0.1063)	0.0003(0.0036)	0.387(0.588)	84.38(81.03)
[2023-09-29 13:56:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1061)	0.0002(0.0033)	0.463(0.585)	87.50(81.11)
[2023-09-29 13:56:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1058)	0.0001(0.0032)	1.479(0.587)	75.00(81.14)
[2023-09-29 13:56:41 10splitTasks](trainer.py 288): INFO  * Train Acc 81.140
[2023-09-29 13:56:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.78
[2023-09-29 13:56:43 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 13:56:43 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 13:56:43 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:56:44 10splitTasks](trainer.py 286): INFO [0/157]	0.5965(0.5965)	0.4866(0.4866)	0.464(0.464)	90.62(90.62)
[2023-09-29 13:56:45 10splitTasks](trainer.py 286): INFO [10/157]	0.1061(0.1505)	0.0003(0.0445)	0.393(0.551)	87.50(82.39)
[2023-09-29 13:56:46 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1307)	0.0002(0.0235)	0.603(0.521)	90.62(83.48)
[2023-09-29 13:56:47 10splitTasks](trainer.py 286): INFO [30/157]	0.1138(0.1224)	0.0005(0.0160)	0.236(0.511)	90.62(83.47)
[2023-09-29 13:56:48 10splitTasks](trainer.py 286): INFO [40/157]	0.1054(0.1183)	0.0001(0.0122)	0.501(0.489)	81.25(83.77)
[2023-09-29 13:56:49 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1156)	0.0002(0.0099)	0.460(0.496)	84.38(83.52)
[2023-09-29 13:56:50 10splitTasks](trainer.py 286): INFO [60/157]	0.1042(0.1136)	0.0003(0.0083)	0.338(0.502)	84.38(83.66)
[2023-09-29 13:56:51 10splitTasks](trainer.py 286): INFO [70/157]	0.1175(0.1126)	0.0006(0.0072)	0.415(0.491)	90.62(83.89)
[2023-09-29 13:56:52 10splitTasks](trainer.py 286): INFO [80/157]	0.1039(0.1116)	0.0002(0.0063)	0.609(0.487)	81.25(84.07)
[2023-09-29 13:56:53 10splitTasks](trainer.py 286): INFO [90/157]	0.1149(0.1108)	0.0124(0.0058)	0.598(0.489)	81.25(83.96)
[2023-09-29 13:56:54 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1100)	0.0002(0.0052)	0.298(0.480)	87.50(84.28)
[2023-09-29 13:56:55 10splitTasks](trainer.py 286): INFO [110/157]	0.1083(0.1094)	0.0002(0.0048)	0.377(0.478)	90.62(84.21)
[2023-09-29 13:56:56 10splitTasks](trainer.py 286): INFO [120/157]	0.1043(0.1088)	0.0002(0.0044)	0.563(0.476)	71.88(84.32)
[2023-09-29 13:56:57 10splitTasks](trainer.py 286): INFO [130/157]	0.1044(0.1083)	0.0002(0.0041)	0.468(0.477)	81.25(84.30)
[2023-09-29 13:56:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1079)	0.0003(0.0038)	0.661(0.477)	84.38(84.40)
[2023-09-29 13:56:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1033(0.1080)	0.0001(0.0036)	0.700(0.480)	78.12(84.25)
[2023-09-29 13:57:00 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1076)	0.0001(0.0035)	0.243(0.478)	100.00(84.40)
[2023-09-29 13:57:00 10splitTasks](trainer.py 288): INFO  * Train Acc 84.400
[2023-09-29 13:57:02 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.79
[2023-09-29 13:57:02 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 13:57:02 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 13:57:02 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:57:02 10splitTasks](trainer.py 286): INFO [0/157]	0.6280(0.6280)	0.5068(0.5068)	0.360(0.360)	84.38(84.38)
[2023-09-29 13:57:03 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1506)	0.0002(0.0463)	0.368(0.480)	87.50(84.66)
[2023-09-29 13:57:04 10splitTasks](trainer.py 286): INFO [20/157]	0.1063(0.1277)	0.0006(0.0244)	0.627(0.450)	78.12(85.71)
[2023-09-29 13:57:05 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1195)	0.0002(0.0166)	0.184(0.425)	96.88(86.19)
[2023-09-29 13:57:06 10splitTasks](trainer.py 286): INFO [40/157]	0.1048(0.1154)	0.0003(0.0126)	0.604(0.397)	87.50(87.12)
[2023-09-29 13:57:07 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1128)	0.0002(0.0102)	0.303(0.397)	87.50(86.83)
[2023-09-29 13:57:09 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1112)	0.0002(0.0086)	0.543(0.392)	84.38(86.83)
[2023-09-29 13:57:10 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1100)	0.0002(0.0074)	0.355(0.386)	90.62(86.88)
[2023-09-29 13:57:11 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1091)	0.0002(0.0065)	0.454(0.388)	84.38(86.81)
[2023-09-29 13:57:12 10splitTasks](trainer.py 286): INFO [90/157]	0.1057(0.1088)	0.0002(0.0059)	0.372(0.395)	84.38(86.64)
[2023-09-29 13:57:13 10splitTasks](trainer.py 286): INFO [100/157]	0.1053(0.1082)	0.0002(0.0053)	0.238(0.391)	90.62(86.88)
[2023-09-29 13:57:14 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1077)	0.0002(0.0048)	0.584(0.388)	81.25(86.97)
[2023-09-29 13:57:15 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1072)	0.0002(0.0045)	0.558(0.385)	84.38(87.09)
[2023-09-29 13:57:16 10splitTasks](trainer.py 286): INFO [130/157]	0.1042(0.1068)	0.0002(0.0042)	0.363(0.386)	84.38(87.17)
[2023-09-29 13:57:17 10splitTasks](trainer.py 286): INFO [140/157]	0.1011(0.1069)	0.0002(0.0039)	0.618(0.384)	84.38(87.17)
[2023-09-29 13:57:18 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1066)	0.0001(0.0036)	0.452(0.381)	84.38(87.27)
[2023-09-29 13:57:18 10splitTasks](trainer.py 286): INFO [156/157]	0.0779(0.1062)	0.0001(0.0035)	0.542(0.380)	75.00(87.34)
[2023-09-29 13:57:18 10splitTasks](trainer.py 288): INFO  * Train Acc 87.340
[2023-09-29 13:57:20 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.400, Total time 1.77
[2023-09-29 13:57:20 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 13:57:20 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 13:57:20 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:57:21 10splitTasks](trainer.py 286): INFO [0/157]	0.6351(0.6351)	0.5282(0.5282)	0.283(0.283)	87.50(87.50)
[2023-09-29 13:57:22 10splitTasks](trainer.py 286): INFO [10/157]	0.1039(0.1551)	0.0003(0.0487)	0.619(0.299)	84.38(90.06)
[2023-09-29 13:57:23 10splitTasks](trainer.py 286): INFO [20/157]	0.1041(0.1304)	0.0004(0.0256)	0.311(0.348)	90.62(88.24)
[2023-09-29 13:57:24 10splitTasks](trainer.py 286): INFO [30/157]	0.1042(0.1215)	0.0002(0.0175)	0.274(0.335)	87.50(88.51)
[2023-09-29 13:57:25 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1167)	0.0003(0.0133)	0.492(0.330)	87.50(89.02)
[2023-09-29 13:57:26 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1139)	0.0002(0.0107)	0.202(0.314)	90.62(89.28)
[2023-09-29 13:57:27 10splitTasks](trainer.py 286): INFO [60/157]	0.1024(0.1128)	0.0003(0.0090)	0.078(0.314)	100.00(89.19)
[2023-09-29 13:57:28 10splitTasks](trainer.py 286): INFO [70/157]	0.1009(0.1115)	0.0001(0.0078)	0.098(0.302)	100.00(89.66)
[2023-09-29 13:57:29 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1102)	0.0003(0.0069)	0.350(0.303)	87.50(89.54)
[2023-09-29 13:57:30 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1097)	0.0002(0.0062)	0.352(0.306)	84.38(89.42)
[2023-09-29 13:57:31 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1091)	0.0002(0.0056)	0.228(0.303)	93.75(89.54)
[2023-09-29 13:57:32 10splitTasks](trainer.py 286): INFO [110/157]	0.1115(0.1089)	0.0004(0.0051)	0.291(0.301)	90.62(89.67)
[2023-09-29 13:57:33 10splitTasks](trainer.py 286): INFO [120/157]	0.1038(0.1084)	0.0006(0.0047)	0.158(0.299)	96.88(89.82)
[2023-09-29 13:57:34 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1080)	0.0002(0.0044)	0.274(0.304)	87.50(89.58)
[2023-09-29 13:57:35 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1076)	0.0002(0.0041)	0.135(0.307)	100.00(89.45)
[2023-09-29 13:57:36 10splitTasks](trainer.py 286): INFO [150/157]	0.1017(0.1073)	0.0002(0.0038)	0.320(0.312)	90.62(89.34)
[2023-09-29 13:57:37 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1070)	0.0001(0.0037)	0.658(0.311)	87.50(89.44)
[2023-09-29 13:57:37 10splitTasks](trainer.py 288): INFO  * Train Acc 89.440
[2023-09-29 13:57:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.77
[2023-09-29 13:57:39 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 13:57:39 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 13:57:39 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:57:40 10splitTasks](trainer.py 286): INFO [0/157]	0.5935(0.5935)	0.4762(0.4762)	0.118(0.118)	93.75(93.75)
[2023-09-29 13:57:41 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1500)	0.0002(0.0436)	0.200(0.245)	93.75(92.33)
[2023-09-29 13:57:42 10splitTasks](trainer.py 286): INFO [20/157]	0.1044(0.1281)	0.0001(0.0229)	0.259(0.269)	87.50(90.92)
[2023-09-29 13:57:43 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1210)	0.0002(0.0156)	0.344(0.266)	93.75(91.23)
[2023-09-29 13:57:44 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1165)	0.0003(0.0119)	0.332(0.270)	87.50(90.78)
[2023-09-29 13:57:45 10splitTasks](trainer.py 286): INFO [50/157]	0.1055(0.1138)	0.0003(0.0096)	0.244(0.278)	90.62(90.87)
[2023-09-29 13:57:46 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1121)	0.0002(0.0081)	0.412(0.276)	87.50(90.68)
[2023-09-29 13:57:47 10splitTasks](trainer.py 286): INFO [70/157]	0.1055(0.1109)	0.0003(0.0070)	0.266(0.271)	90.62(90.80)
[2023-09-29 13:57:48 10splitTasks](trainer.py 286): INFO [80/157]	0.1064(0.1100)	0.0002(0.0062)	0.438(0.268)	87.50(90.97)
[2023-09-29 13:57:49 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1093)	0.0002(0.0055)	0.178(0.266)	96.88(91.14)
[2023-09-29 13:57:50 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1089)	0.0002(0.0050)	0.570(0.265)	78.12(91.12)
[2023-09-29 13:57:51 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1083)	0.0002(0.0046)	0.375(0.260)	81.25(91.27)
[2023-09-29 13:57:52 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1078)	0.0002(0.0042)	0.103(0.259)	96.88(91.40)
[2023-09-29 13:57:53 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1075)	0.0003(0.0039)	0.156(0.257)	96.88(91.44)
[2023-09-29 13:57:54 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1072)	0.0002(0.0037)	0.333(0.255)	90.62(91.53)
[2023-09-29 13:57:55 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1070)	0.0001(0.0035)	0.377(0.250)	84.38(91.72)
[2023-09-29 13:57:56 10splitTasks](trainer.py 286): INFO [156/157]	0.0795(0.1066)	0.0001(0.0033)	0.279(0.251)	87.50(91.78)
[2023-09-29 13:57:56 10splitTasks](trainer.py 288): INFO  * Train Acc 91.780
[2023-09-29 13:57:58 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.600, Total time 1.77
[2023-09-29 13:57:58 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 13:57:58 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 13:57:58 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:57:58 10splitTasks](trainer.py 286): INFO [0/157]	0.6313(0.6313)	0.5256(0.5256)	0.324(0.324)	87.50(87.50)
[2023-09-29 13:57:59 10splitTasks](trainer.py 286): INFO [10/157]	0.1043(0.1514)	0.0002(0.0480)	0.165(0.232)	93.75(91.48)
[2023-09-29 13:58:00 10splitTasks](trainer.py 286): INFO [20/157]	0.1171(0.1290)	0.0042(0.0255)	0.183(0.211)	96.88(93.30)
[2023-09-29 13:58:01 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1216)	0.0002(0.0174)	0.188(0.198)	93.75(93.95)
[2023-09-29 13:58:02 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1178)	0.0002(0.0132)	0.112(0.195)	96.88(93.75)
[2023-09-29 13:58:03 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1151)	0.0002(0.0107)	0.396(0.202)	84.38(93.38)
[2023-09-29 13:58:04 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1132)	0.0002(0.0090)	0.205(0.216)	93.75(92.93)
[2023-09-29 13:58:06 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1119)	0.0002(0.0078)	0.116(0.208)	100.00(93.44)
[2023-09-29 13:58:07 10splitTasks](trainer.py 286): INFO [80/157]	0.1092(0.1109)	0.0003(0.0068)	0.238(0.213)	93.75(93.33)
[2023-09-29 13:58:08 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1103)	0.0002(0.0061)	0.410(0.216)	87.50(93.10)
[2023-09-29 13:58:09 10splitTasks](trainer.py 286): INFO [100/157]	0.1042(0.1096)	0.0002(0.0055)	0.143(0.218)	96.88(93.10)
[2023-09-29 13:58:10 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1089)	0.0001(0.0051)	0.439(0.224)	87.50(92.96)
[2023-09-29 13:58:11 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1084)	0.0002(0.0047)	0.249(0.231)	96.88(92.69)
[2023-09-29 13:58:12 10splitTasks](trainer.py 286): INFO [130/157]	0.1050(0.1081)	0.0002(0.0044)	0.286(0.227)	90.62(92.84)
[2023-09-29 13:58:13 10splitTasks](trainer.py 286): INFO [140/157]	0.1076(0.1078)	0.0005(0.0041)	0.332(0.226)	84.38(92.89)
[2023-09-29 13:58:14 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1079)	0.0001(0.0038)	0.174(0.226)	93.75(92.90)
[2023-09-29 13:58:14 10splitTasks](trainer.py 286): INFO [156/157]	0.0791(0.1075)	0.0001(0.0037)	1.424(0.227)	75.00(92.84)
[2023-09-29 13:58:15 10splitTasks](trainer.py 288): INFO  * Train Acc 92.840
[2023-09-29 13:58:16 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.600, Total time 1.74
[2023-09-29 13:58:16 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 13:58:16 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 13:58:16 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:58:17 10splitTasks](trainer.py 286): INFO [0/157]	0.6427(0.6427)	0.5254(0.5254)	0.139(0.139)	93.75(93.75)
[2023-09-29 13:58:18 10splitTasks](trainer.py 286): INFO [10/157]	0.1136(0.1529)	0.0002(0.0480)	0.269(0.177)	93.75(94.03)
[2023-09-29 13:58:19 10splitTasks](trainer.py 286): INFO [20/157]	0.1031(0.1296)	0.0003(0.0253)	0.160(0.214)	93.75(91.96)
[2023-09-29 13:58:20 10splitTasks](trainer.py 286): INFO [30/157]	0.1099(0.1213)	0.0002(0.0172)	0.259(0.206)	93.75(92.94)
[2023-09-29 13:58:21 10splitTasks](trainer.py 286): INFO [40/157]	0.1072(0.1170)	0.0003(0.0131)	0.155(0.220)	96.88(92.53)
[2023-09-29 13:58:22 10splitTasks](trainer.py 286): INFO [50/157]	0.1067(0.1142)	0.0007(0.0106)	0.130(0.208)	96.88(93.08)
[2023-09-29 13:58:23 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1123)	0.0001(0.0089)	0.246(0.207)	87.50(93.19)
[2023-09-29 13:58:24 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1113)	0.0002(0.0077)	0.372(0.213)	87.50(93.00)
[2023-09-29 13:58:25 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1102)	0.0003(0.0068)	0.106(0.211)	100.00(93.25)
[2023-09-29 13:58:26 10splitTasks](trainer.py 286): INFO [90/157]	0.1025(0.1095)	0.0005(0.0061)	0.197(0.209)	93.75(93.17)
[2023-09-29 13:58:27 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1088)	0.0005(0.0055)	0.091(0.207)	100.00(93.13)
[2023-09-29 13:58:28 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1090)	0.0001(0.0050)	0.084(0.203)	100.00(93.50)
[2023-09-29 13:58:29 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1087)	0.0003(0.0047)	0.172(0.204)	93.75(93.44)
[2023-09-29 13:58:30 10splitTasks](trainer.py 286): INFO [130/157]	0.1036(0.1082)	0.0003(0.0043)	0.511(0.203)	81.25(93.46)
[2023-09-29 13:58:32 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1079)	0.0002(0.0040)	0.181(0.203)	93.75(93.37)
[2023-09-29 13:58:33 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1077)	0.0001(0.0038)	0.219(0.203)	93.75(93.48)
[2023-09-29 13:58:33 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1073)	0.0001(0.0037)	0.839(0.203)	75.00(93.50)
[2023-09-29 13:58:33 10splitTasks](trainer.py 288): INFO  * Train Acc 93.500
[2023-09-29 13:58:35 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.78
[2023-09-29 13:58:35 10splitTasks](my_trainer.py 206): INFO Pruning for task7
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 2564/3016 (85.01%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 1116/1313 (85.00%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 10045/11818 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 4464/5252 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 4464/5252 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 4464/5252 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 10045/11818 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 4464/5252 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 4464/5252 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 10045/11818 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 4464/5252 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 8928/10504 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 40180/47271 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 17858/21009 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 35715/42018 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 17858/21009 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 40180/47271 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 17858/21009 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 17858/21009 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 40180/47271 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 17858/21009 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 17858/21009 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 40180/47271 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 17858/21009 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 35715/42018 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 160721/189084 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 142864/168075 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 160721/189084 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 160721/189084 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 160721/189084 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 160721/189084 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 160721/189084 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 71432/84038 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 142864/168075 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 642886/756337 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 285728/336150 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 571453/672298 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 285728/336150 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 642886/756337 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 285728/336150 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 285728/336150 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 642886/756337 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 285728/336150 (85.00%) (Total in layer: 1048576)
[2023-09-29 13:58:35 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 13:58:35 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 13:58:35 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 13:58:35 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:58:36 10splitTasks](trainer.py 286): INFO [0/157]	0.6323(0.6323)	0.5104(0.5104)	0.239(0.239)	93.75(93.75)
[2023-09-29 13:58:37 10splitTasks](trainer.py 286): INFO [10/157]	0.1031(0.1519)	0.0002(0.0467)	0.340(0.305)	84.38(89.20)
[2023-09-29 13:58:38 10splitTasks](trainer.py 286): INFO [20/157]	0.1024(0.1301)	0.0003(0.0246)	0.213(0.335)	90.62(88.84)
[2023-09-29 13:58:39 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1217)	0.0002(0.0168)	0.150(0.342)	93.75(89.31)
[2023-09-29 13:58:40 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1170)	0.0002(0.0128)	0.476(0.374)	81.25(87.96)
[2023-09-29 13:58:41 10splitTasks](trainer.py 286): INFO [50/157]	0.1051(0.1141)	0.0002(0.0103)	0.439(0.389)	87.50(87.38)
[2023-09-29 13:58:42 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1124)	0.0002(0.0087)	0.295(0.387)	90.62(87.40)
[2023-09-29 13:58:43 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1110)	0.0002(0.0075)	0.393(0.384)	87.50(87.59)
[2023-09-29 13:58:44 10splitTasks](trainer.py 286): INFO [80/157]	0.1224(0.1102)	0.0002(0.0066)	0.364(0.367)	78.12(88.12)
[2023-09-29 13:58:45 10splitTasks](trainer.py 286): INFO [90/157]	0.1030(0.1093)	0.0002(0.0059)	0.212(0.353)	96.88(88.63)
[2023-09-29 13:58:46 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1086)	0.0003(0.0053)	0.171(0.345)	93.75(88.86)
[2023-09-29 13:58:47 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1082)	0.0003(0.0049)	0.217(0.339)	93.75(88.85)
[2023-09-29 13:58:48 10splitTasks](trainer.py 286): INFO [120/157]	0.1032(0.1077)	0.0002(0.0045)	0.249(0.339)	87.50(88.82)
[2023-09-29 13:58:49 10splitTasks](trainer.py 286): INFO [130/157]	0.1023(0.1075)	0.0003(0.0042)	0.150(0.335)	93.75(88.81)
[2023-09-29 13:58:50 10splitTasks](trainer.py 286): INFO [140/157]	0.1075(0.1072)	0.0002(0.0039)	0.650(0.333)	71.88(88.79)
[2023-09-29 13:58:51 10splitTasks](trainer.py 286): INFO [150/157]	0.1020(0.1069)	0.0002(0.0037)	0.543(0.341)	87.50(88.56)
[2023-09-29 13:58:52 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1066)	0.0001(0.0035)	2.319(0.351)	37.50(88.26)
[2023-09-29 13:58:52 10splitTasks](trainer.py 288): INFO  * Train Acc 88.260
[2023-09-29 13:58:54 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.600, Total time 1.79
[2023-09-29 13:58:54 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 13:58:54 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 13:58:54 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:58:54 10splitTasks](trainer.py 286): INFO [0/157]	0.6048(0.6048)	0.4824(0.4824)	0.420(0.420)	87.50(87.50)
[2023-09-29 13:58:56 10splitTasks](trainer.py 286): INFO [10/157]	0.1176(0.1566)	0.0001(0.0488)	0.203(0.501)	90.62(82.95)
[2023-09-29 13:58:57 10splitTasks](trainer.py 286): INFO [20/157]	0.1048(0.1317)	0.0002(0.0257)	0.425(0.418)	90.62(85.86)
[2023-09-29 13:58:58 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1223)	0.0002(0.0175)	0.534(0.403)	90.62(86.39)
[2023-09-29 13:58:59 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1178)	0.0003(0.0133)	0.225(0.376)	96.88(87.04)
[2023-09-29 13:59:00 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1148)	0.0002(0.0107)	0.372(0.363)	81.25(87.32)
[2023-09-29 13:59:01 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1127)	0.0002(0.0090)	0.316(0.371)	87.50(86.99)
[2023-09-29 13:59:02 10splitTasks](trainer.py 286): INFO [70/157]	0.1022(0.1113)	0.0003(0.0078)	0.119(0.374)	93.75(86.80)
[2023-09-29 13:59:03 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1104)	0.0002(0.0069)	0.153(0.371)	93.75(86.84)
[2023-09-29 13:59:04 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1096)	0.0001(0.0061)	0.216(0.374)	87.50(86.98)
[2023-09-29 13:59:05 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1092)	0.0002(0.0056)	0.432(0.369)	87.50(87.16)
[2023-09-29 13:59:06 10splitTasks](trainer.py 286): INFO [110/157]	0.1047(0.1086)	0.0003(0.0051)	0.148(0.357)	93.75(87.56)
[2023-09-29 13:59:07 10splitTasks](trainer.py 286): INFO [120/157]	0.1055(0.1081)	0.0002(0.0047)	0.708(0.360)	75.00(87.40)
[2023-09-29 13:59:08 10splitTasks](trainer.py 286): INFO [130/157]	0.1075(0.1077)	0.0005(0.0044)	0.329(0.355)	90.62(87.71)
[2023-09-29 13:59:09 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1073)	0.0002(0.0041)	0.287(0.355)	87.50(87.72)
[2023-09-29 13:59:10 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1071)	0.0001(0.0038)	0.226(0.347)	90.62(87.98)
[2023-09-29 13:59:11 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1067)	0.0001(0.0037)	1.508(0.350)	50.00(87.90)
[2023-09-29 13:59:11 10splitTasks](trainer.py 288): INFO  * Train Acc 87.900
[2023-09-29 13:59:13 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.89
[2023-09-29 13:59:13 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 13:59:13 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 13:59:13 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:59:13 10splitTasks](trainer.py 286): INFO [0/157]	0.6159(0.6159)	0.5110(0.5110)	0.462(0.462)	87.50(87.50)
[2023-09-29 13:59:14 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1492)	0.0001(0.0467)	0.274(0.319)	90.62(87.50)
[2023-09-29 13:59:15 10splitTasks](trainer.py 286): INFO [20/157]	0.1047(0.1292)	0.0002(0.0246)	0.200(0.358)	93.75(86.31)
[2023-09-29 13:59:16 10splitTasks](trainer.py 286): INFO [30/157]	0.1058(0.1215)	0.0003(0.0168)	0.321(0.343)	87.50(86.39)
[2023-09-29 13:59:17 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1172)	0.0002(0.0128)	0.403(0.347)	87.50(86.74)
[2023-09-29 13:59:18 10splitTasks](trainer.py 286): INFO [50/157]	0.1052(0.1145)	0.0003(0.0103)	0.414(0.330)	84.38(87.81)
[2023-09-29 13:59:19 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1124)	0.0002(0.0087)	0.328(0.331)	81.25(87.81)
[2023-09-29 13:59:21 10splitTasks](trainer.py 286): INFO [70/157]	0.1062(0.1113)	0.0005(0.0075)	0.643(0.339)	78.12(87.63)
[2023-09-29 13:59:22 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1103)	0.0002(0.0066)	0.531(0.328)	81.25(88.19)
[2023-09-29 13:59:23 10splitTasks](trainer.py 286): INFO [90/157]	0.1074(0.1095)	0.0005(0.0059)	0.383(0.337)	90.62(88.02)
[2023-09-29 13:59:24 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1091)	0.0003(0.0054)	0.187(0.338)	93.75(88.03)
[2023-09-29 13:59:25 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1087)	0.0002(0.0049)	0.216(0.334)	96.88(88.09)
[2023-09-29 13:59:26 10splitTasks](trainer.py 286): INFO [120/157]	0.1057(0.1082)	0.0003(0.0045)	0.323(0.324)	87.50(88.38)
[2023-09-29 13:59:27 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1079)	0.0004(0.0042)	0.307(0.321)	87.50(88.50)
[2023-09-29 13:59:28 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1075)	0.0002(0.0039)	0.187(0.318)	93.75(88.54)
[2023-09-29 13:59:29 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1072)	0.0001(0.0037)	0.276(0.316)	90.62(88.70)
[2023-09-29 13:59:29 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1068)	0.0001(0.0036)	0.676(0.312)	87.50(88.90)
[2023-09-29 13:59:29 10splitTasks](trainer.py 288): INFO  * Train Acc 88.900
[2023-09-29 13:59:31 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.000, Total time 1.81
[2023-09-29 13:59:31 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 13:59:31 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 13:59:31 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:59:32 10splitTasks](trainer.py 286): INFO [0/157]	0.6072(0.6072)	0.5025(0.5025)	0.537(0.537)	78.12(78.12)
[2023-09-29 13:59:33 10splitTasks](trainer.py 286): INFO [10/157]	0.1026(0.1503)	0.0003(0.0476)	0.359(0.299)	87.50(90.06)
[2023-09-29 13:59:34 10splitTasks](trainer.py 286): INFO [20/157]	0.1010(0.1273)	0.0002(0.0250)	0.102(0.284)	100.00(90.33)
[2023-09-29 13:59:35 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1191)	0.0002(0.0170)	0.240(0.260)	93.75(91.53)
[2023-09-29 13:59:36 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1154)	0.0003(0.0130)	0.104(0.268)	96.88(91.39)
[2023-09-29 13:59:37 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1127)	0.0003(0.0105)	0.104(0.282)	93.75(90.93)
[2023-09-29 13:59:38 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1110)	0.0002(0.0088)	0.434(0.281)	84.38(90.83)
[2023-09-29 13:59:39 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1098)	0.0002(0.0076)	0.276(0.285)	87.50(90.67)
[2023-09-29 13:59:40 10splitTasks](trainer.py 286): INFO [80/157]	0.1011(0.1088)	0.0002(0.0067)	0.179(0.285)	96.88(90.82)
[2023-09-29 13:59:41 10splitTasks](trainer.py 286): INFO [90/157]	0.1038(0.1083)	0.0002(0.0060)	0.108(0.277)	96.88(91.11)
[2023-09-29 13:59:42 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1077)	0.0002(0.0054)	0.287(0.286)	90.62(90.78)
[2023-09-29 13:59:43 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1072)	0.0002(0.0050)	0.308(0.291)	87.50(90.51)
[2023-09-29 13:59:44 10splitTasks](trainer.py 286): INFO [120/157]	0.1010(0.1068)	0.0002(0.0046)	0.182(0.294)	96.88(90.24)
[2023-09-29 13:59:45 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1065)	0.0002(0.0042)	0.310(0.295)	93.75(90.15)
[2023-09-29 13:59:46 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1062)	0.0001(0.0040)	0.126(0.288)	100.00(90.40)
[2023-09-29 13:59:47 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1060)	0.0001(0.0037)	0.359(0.285)	87.50(90.58)
[2023-09-29 13:59:48 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1057)	0.0001(0.0036)	0.101(0.286)	100.00(90.62)
[2023-09-29 13:59:48 10splitTasks](trainer.py 288): INFO  * Train Acc 90.620
[2023-09-29 13:59:50 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.600, Total time 1.82
[2023-09-29 13:59:50 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 13:59:50 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 13:59:50 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 13:59:50 10splitTasks](trainer.py 286): INFO [0/157]	0.5989(0.5989)	0.4683(0.4683)	0.565(0.565)	75.00(75.00)
[2023-09-29 13:59:51 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1480)	0.0002(0.0428)	0.466(0.226)	90.62(92.61)
[2023-09-29 13:59:52 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1267)	0.0002(0.0226)	0.059(0.195)	100.00(94.20)
[2023-09-29 13:59:54 10splitTasks](trainer.py 286): INFO [30/157]	0.1057(0.1189)	0.0002(0.0154)	0.246(0.214)	90.62(93.04)
[2023-09-29 13:59:55 10splitTasks](trainer.py 286): INFO [40/157]	0.1051(0.1149)	0.0002(0.0117)	0.346(0.221)	87.50(92.99)
[2023-09-29 13:59:56 10splitTasks](trainer.py 286): INFO [50/157]	0.1056(0.1124)	0.0003(0.0095)	0.312(0.228)	90.62(92.95)
[2023-09-29 13:59:57 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1113)	0.0003(0.0080)	0.295(0.229)	90.62(92.98)
[2023-09-29 13:59:58 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1103)	0.0002(0.0069)	0.378(0.238)	84.38(92.47)
[2023-09-29 13:59:59 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1095)	0.0001(0.0061)	0.138(0.246)	96.88(92.25)
[2023-09-29 14:00:00 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1089)	0.0002(0.0055)	0.128(0.248)	96.88(92.24)
[2023-09-29 14:00:01 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1083)	0.0004(0.0050)	0.172(0.245)	96.88(92.26)
[2023-09-29 14:00:02 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1078)	0.0002(0.0045)	0.430(0.245)	84.38(92.29)
[2023-09-29 14:00:03 10splitTasks](trainer.py 286): INFO [120/157]	0.1050(0.1078)	0.0005(0.0042)	0.277(0.248)	90.62(92.02)
[2023-09-29 14:00:04 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1074)	0.0003(0.0039)	0.131(0.242)	100.00(92.22)
[2023-09-29 14:00:05 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1070)	0.0002(0.0036)	0.095(0.244)	96.88(92.13)
[2023-09-29 14:00:06 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1068)	0.0001(0.0034)	0.172(0.244)	93.75(92.16)
[2023-09-29 14:00:07 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1064)	0.0001(0.0033)	0.778(0.244)	87.50(92.18)
[2023-09-29 14:00:07 10splitTasks](trainer.py 288): INFO  * Train Acc 92.180
[2023-09-29 14:00:08 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.82
[2023-09-29 14:00:08 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 14:00:08 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 14:00:08 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:00:09 10splitTasks](trainer.py 286): INFO [0/157]	0.5669(0.5669)	0.4574(0.4574)	0.277(0.277)	90.62(90.62)
[2023-09-29 14:00:10 10splitTasks](trainer.py 286): INFO [10/157]	0.1007(0.1496)	0.0001(0.0458)	0.225(0.230)	96.88(91.19)
[2023-09-29 14:00:11 10splitTasks](trainer.py 286): INFO [20/157]	0.1037(0.1279)	0.0002(0.0241)	0.108(0.208)	96.88(92.11)
[2023-09-29 14:00:12 10splitTasks](trainer.py 286): INFO [30/157]	0.1037(0.1202)	0.0004(0.0164)	0.110(0.202)	96.88(92.34)
[2023-09-29 14:00:13 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1158)	0.0001(0.0125)	0.301(0.205)	84.38(92.68)
[2023-09-29 14:00:14 10splitTasks](trainer.py 286): INFO [50/157]	0.1028(0.1133)	0.0002(0.0101)	0.357(0.206)	84.38(92.71)
[2023-09-29 14:00:15 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1114)	0.0002(0.0085)	0.064(0.206)	100.00(92.67)
[2023-09-29 14:00:16 10splitTasks](trainer.py 286): INFO [70/157]	0.1029(0.1101)	0.0002(0.0073)	0.256(0.213)	90.62(92.39)
[2023-09-29 14:00:17 10splitTasks](trainer.py 286): INFO [80/157]	0.1039(0.1094)	0.0002(0.0065)	0.204(0.224)	90.62(92.01)
[2023-09-29 14:00:18 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1086)	0.0002(0.0058)	0.239(0.230)	93.75(92.00)
[2023-09-29 14:00:19 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1080)	0.0003(0.0053)	0.181(0.222)	93.75(92.26)
[2023-09-29 14:00:20 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1076)	0.0003(0.0048)	0.342(0.226)	87.50(92.12)
[2023-09-29 14:00:21 10splitTasks](trainer.py 286): INFO [120/157]	0.1062(0.1072)	0.0002(0.0044)	0.317(0.226)	93.75(92.17)
[2023-09-29 14:00:22 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1069)	0.0002(0.0041)	0.153(0.230)	96.88(92.08)
[2023-09-29 14:00:23 10splitTasks](trainer.py 286): INFO [140/157]	0.1059(0.1066)	0.0002(0.0038)	0.283(0.226)	93.75(92.18)
[2023-09-29 14:00:25 10splitTasks](trainer.py 286): INFO [150/157]	0.1043(0.1065)	0.0001(0.0036)	0.426(0.230)	93.75(92.12)
[2023-09-29 14:00:25 10splitTasks](trainer.py 286): INFO [156/157]	0.0794(0.1062)	0.0001(0.0035)	0.395(0.230)	75.00(92.08)
[2023-09-29 14:00:25 10splitTasks](trainer.py 288): INFO  * Train Acc 92.080
[2023-09-29 14:00:27 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.75
[2023-09-29 14:00:27 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 14:00:27 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 14:00:27 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:00:28 10splitTasks](trainer.py 286): INFO [0/157]	0.6286(0.6286)	0.5241(0.5241)	0.081(0.081)	96.88(96.88)
[2023-09-29 14:00:29 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1503)	0.0003(0.0479)	0.168(0.162)	96.88(94.60)
[2023-09-29 14:00:30 10splitTasks](trainer.py 286): INFO [20/157]	0.1047(0.1276)	0.0002(0.0252)	0.414(0.207)	81.25(92.86)
[2023-09-29 14:00:31 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1196)	0.0002(0.0172)	0.389(0.211)	87.50(92.64)
[2023-09-29 14:00:32 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1153)	0.0002(0.0131)	0.115(0.224)	96.88(92.76)
[2023-09-29 14:00:33 10splitTasks](trainer.py 286): INFO [50/157]	0.1120(0.1132)	0.0003(0.0105)	0.269(0.217)	87.50(92.83)
[2023-09-29 14:00:34 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1120)	0.0002(0.0089)	0.186(0.214)	90.62(93.03)
[2023-09-29 14:00:35 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1111)	0.0001(0.0077)	0.213(0.211)	93.75(93.05)
[2023-09-29 14:00:36 10splitTasks](trainer.py 286): INFO [80/157]	0.1098(0.1107)	0.0008(0.0068)	0.172(0.212)	90.62(93.02)
[2023-09-29 14:00:37 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1098)	0.0003(0.0061)	0.191(0.211)	87.50(93.06)
[2023-09-29 14:00:38 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1092)	0.0003(0.0055)	0.073(0.209)	100.00(93.04)
[2023-09-29 14:00:39 10splitTasks](trainer.py 286): INFO [110/157]	0.1030(0.1087)	0.0003(0.0050)	0.067(0.204)	96.88(93.19)
[2023-09-29 14:00:40 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1082)	0.0002(0.0046)	0.227(0.204)	90.62(93.13)
[2023-09-29 14:00:41 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1078)	0.0003(0.0043)	0.042(0.206)	96.88(93.15)
[2023-09-29 14:00:42 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1076)	0.0003(0.0040)	0.259(0.206)	93.75(93.31)
[2023-09-29 14:00:43 10splitTasks](trainer.py 286): INFO [150/157]	0.1017(0.1074)	0.0001(0.0038)	0.053(0.204)	100.00(93.23)
[2023-09-29 14:00:44 10splitTasks](trainer.py 286): INFO [156/157]	0.0801(0.1070)	0.0001(0.0036)	1.137(0.206)	87.50(93.22)
[2023-09-29 14:00:44 10splitTasks](trainer.py 288): INFO  * Train Acc 93.220
[2023-09-29 14:00:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.75
[2023-09-29 14:00:46 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 14:00:46 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 14:00:46 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:00:46 10splitTasks](trainer.py 286): INFO [0/157]	0.5537(0.5537)	0.4450(0.4450)	0.432(0.432)	87.50(87.50)
[2023-09-29 14:00:47 10splitTasks](trainer.py 286): INFO [10/157]	0.1028(0.1438)	0.0002(0.0407)	0.277(0.259)	90.62(90.91)
[2023-09-29 14:00:48 10splitTasks](trainer.py 286): INFO [20/157]	0.1126(0.1251)	0.0008(0.0215)	0.133(0.224)	93.75(92.11)
[2023-09-29 14:00:49 10splitTasks](trainer.py 286): INFO [30/157]	0.1042(0.1183)	0.0003(0.0147)	0.186(0.228)	96.88(92.14)
[2023-09-29 14:00:50 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1144)	0.0002(0.0111)	0.258(0.208)	87.50(92.68)
[2023-09-29 14:00:51 10splitTasks](trainer.py 286): INFO [50/157]	0.1022(0.1125)	0.0004(0.0090)	0.217(0.209)	93.75(92.77)
[2023-09-29 14:00:52 10splitTasks](trainer.py 286): INFO [60/157]	0.1064(0.1109)	0.0007(0.0076)	0.157(0.205)	93.75(92.73)
[2023-09-29 14:00:53 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1099)	0.0002(0.0066)	0.097(0.203)	96.88(92.83)
[2023-09-29 14:00:55 10splitTasks](trainer.py 286): INFO [80/157]	0.1079(0.1092)	0.0019(0.0058)	0.043(0.197)	100.00(93.06)
[2023-09-29 14:00:56 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1088)	0.0003(0.0052)	0.603(0.204)	78.12(92.86)
[2023-09-29 14:00:57 10splitTasks](trainer.py 286): INFO [100/157]	0.1043(0.1082)	0.0002(0.0047)	0.027(0.197)	100.00(93.10)
[2023-09-29 14:00:58 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1077)	0.0003(0.0043)	0.154(0.197)	96.88(93.02)
[2023-09-29 14:00:59 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1072)	0.0003(0.0040)	0.241(0.205)	93.75(92.82)
[2023-09-29 14:01:00 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1069)	0.0002(0.0037)	0.101(0.205)	96.88(92.92)
[2023-09-29 14:01:01 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1066)	0.0002(0.0035)	0.264(0.206)	90.62(92.89)
[2023-09-29 14:01:02 10splitTasks](trainer.py 286): INFO [150/157]	0.1020(0.1065)	0.0001(0.0033)	0.115(0.202)	93.75(92.98)
[2023-09-29 14:01:02 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1062)	0.0001(0.0031)	0.101(0.202)	100.00(92.98)
[2023-09-29 14:01:02 10splitTasks](trainer.py 288): INFO  * Train Acc 92.980
[2023-09-29 14:01:04 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.83
[2023-09-29 14:01:04 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 14:01:04 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 14:01:04 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:01:05 10splitTasks](trainer.py 286): INFO [0/157]	0.5929(0.5929)	0.4665(0.4665)	0.117(0.117)	96.88(96.88)
[2023-09-29 14:01:06 10splitTasks](trainer.py 286): INFO [10/157]	0.1009(0.1512)	0.0002(0.0426)	0.105(0.200)	93.75(94.03)
[2023-09-29 14:01:07 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1283)	0.0002(0.0225)	0.149(0.168)	93.75(94.64)
[2023-09-29 14:01:08 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1200)	0.0003(0.0153)	0.232(0.167)	93.75(94.86)
[2023-09-29 14:01:09 10splitTasks](trainer.py 286): INFO [40/157]	0.1065(0.1165)	0.0003(0.0116)	0.209(0.164)	90.62(94.59)
[2023-09-29 14:01:10 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1137)	0.0003(0.0094)	0.290(0.169)	90.62(94.30)
[2023-09-29 14:01:11 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1119)	0.0002(0.0079)	0.103(0.169)	96.88(94.06)
[2023-09-29 14:01:12 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1106)	0.0003(0.0068)	0.141(0.164)	96.88(94.23)
[2023-09-29 14:01:13 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1096)	0.0003(0.0060)	0.047(0.162)	100.00(94.29)
[2023-09-29 14:01:14 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1088)	0.0002(0.0054)	0.125(0.167)	93.75(94.02)
[2023-09-29 14:01:15 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1082)	0.0002(0.0049)	0.217(0.170)	90.62(94.03)
[2023-09-29 14:01:16 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1077)	0.0002(0.0045)	0.114(0.171)	93.75(94.12)
[2023-09-29 14:01:17 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1072)	0.0002(0.0041)	0.099(0.171)	96.88(94.06)
[2023-09-29 14:01:18 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1070)	0.0001(0.0038)	0.366(0.175)	87.50(93.99)
[2023-09-29 14:01:19 10splitTasks](trainer.py 286): INFO [140/157]	0.1044(0.1068)	0.0002(0.0036)	0.139(0.176)	96.88(93.95)
[2023-09-29 14:01:20 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1065)	0.0001(0.0034)	0.081(0.175)	96.88(94.04)
[2023-09-29 14:01:21 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1062)	0.0001(0.0033)	0.049(0.173)	100.00(94.10)
[2023-09-29 14:01:21 10splitTasks](trainer.py 288): INFO  * Train Acc 94.100
[2023-09-29 14:01:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.000, Total time 1.76
[2023-09-29 14:01:23 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 14:01:23 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 14:01:23 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:01:23 10splitTasks](trainer.py 286): INFO [0/157]	0.5897(0.5897)	0.4747(0.4747)	0.071(0.071)	96.88(96.88)
[2023-09-29 14:01:24 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1489)	0.0002(0.0434)	0.095(0.163)	96.88(94.32)
[2023-09-29 14:01:25 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1268)	0.0003(0.0229)	0.047(0.133)	100.00(95.68)
[2023-09-29 14:01:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1188)	0.0003(0.0156)	0.113(0.121)	93.75(96.17)
[2023-09-29 14:01:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1150)	0.0002(0.0119)	0.210(0.133)	90.62(95.81)
[2023-09-29 14:01:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1124)	0.0002(0.0096)	0.174(0.139)	90.62(95.65)
[2023-09-29 14:01:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1108)	0.0003(0.0081)	0.189(0.147)	93.75(95.24)
[2023-09-29 14:01:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1103)	0.0003(0.0070)	0.349(0.149)	93.75(95.16)
[2023-09-29 14:01:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1096)	0.0002(0.0062)	0.112(0.143)	93.75(95.33)
[2023-09-29 14:01:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1088)	0.0003(0.0055)	0.079(0.145)	96.88(95.12)
[2023-09-29 14:01:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1048(0.1082)	0.0002(0.0050)	0.164(0.143)	93.75(95.30)
[2023-09-29 14:01:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1078)	0.0003(0.0046)	0.101(0.144)	100.00(95.30)
[2023-09-29 14:01:36 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1075)	0.0003(0.0042)	0.237(0.147)	93.75(95.27)
[2023-09-29 14:01:37 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1074)	0.0002(0.0039)	0.039(0.145)	100.00(95.23)
[2023-09-29 14:01:38 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1071)	0.0002(0.0037)	0.110(0.144)	96.88(95.30)
[2023-09-29 14:01:39 10splitTasks](trainer.py 286): INFO [150/157]	0.1042(0.1068)	0.0001(0.0035)	0.253(0.145)	90.62(95.14)
[2023-09-29 14:01:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0779(0.1064)	0.0001(0.0033)	0.106(0.146)	100.00(95.14)
[2023-09-29 14:01:40 10splitTasks](trainer.py 288): INFO  * Train Acc 95.140
[2023-09-29 14:01:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.000, Total time 1.85
[2023-09-29 14:01:41 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 14:01:41 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 14:01:41 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:01:42 10splitTasks](trainer.py 286): INFO [0/157]	0.5914(0.5914)	0.4830(0.4830)	0.325(0.325)	90.62(90.62)
[2023-09-29 14:01:43 10splitTasks](trainer.py 286): INFO [10/157]	0.1052(0.1477)	0.0002(0.0442)	0.228(0.135)	87.50(95.17)
[2023-09-29 14:01:44 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1259)	0.0003(0.0233)	0.275(0.141)	93.75(95.24)
[2023-09-29 14:01:45 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1183)	0.0002(0.0158)	0.313(0.144)	93.75(95.16)
[2023-09-29 14:01:46 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1144)	0.0002(0.0120)	0.218(0.150)	93.75(95.12)
[2023-09-29 14:01:47 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1122)	0.0004(0.0097)	0.035(0.145)	100.00(95.34)
[2023-09-29 14:01:48 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1105)	0.0003(0.0082)	0.085(0.154)	96.88(94.77)
[2023-09-29 14:01:49 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1095)	0.0002(0.0071)	0.210(0.158)	84.38(94.54)
[2023-09-29 14:01:50 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1090)	0.0002(0.0062)	0.078(0.157)	96.88(94.60)
[2023-09-29 14:01:51 10splitTasks](trainer.py 286): INFO [90/157]	0.1027(0.1083)	0.0002(0.0056)	0.051(0.160)	96.88(94.37)
[2023-09-29 14:01:52 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1077)	0.0003(0.0051)	0.113(0.166)	96.88(94.21)
[2023-09-29 14:01:53 10splitTasks](trainer.py 286): INFO [110/157]	0.1055(0.1075)	0.0047(0.0047)	0.033(0.168)	100.00(94.03)
[2023-09-29 14:01:54 10splitTasks](trainer.py 286): INFO [120/157]	0.1013(0.1070)	0.0003(0.0043)	0.224(0.167)	90.62(94.06)
[2023-09-29 14:01:55 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1066)	0.0003(0.0040)	0.249(0.164)	87.50(94.11)
[2023-09-29 14:01:56 10splitTasks](trainer.py 286): INFO [140/157]	0.1023(0.1063)	0.0003(0.0037)	0.076(0.162)	100.00(94.24)
[2023-09-29 14:01:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1050(0.1061)	0.0002(0.0035)	0.170(0.160)	93.75(94.37)
[2023-09-29 14:01:58 10splitTasks](trainer.py 286): INFO [156/157]	0.0800(0.1058)	0.0001(0.0034)	0.564(0.158)	75.00(94.40)
[2023-09-29 14:01:58 10splitTasks](trainer.py 288): INFO  * Train Acc 94.400
[2023-09-29 14:02:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.77
[2023-09-29 14:02:00 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 14:02:00 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 14:02:00 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:02:01 10splitTasks](trainer.py 286): INFO [0/157]	0.5947(0.5947)	0.4897(0.4897)	0.333(0.333)	90.62(90.62)
[2023-09-29 14:02:02 10splitTasks](trainer.py 286): INFO [10/157]	0.1030(0.1479)	0.0002(0.0447)	0.125(0.127)	93.75(95.74)
[2023-09-29 14:02:03 10splitTasks](trainer.py 286): INFO [20/157]	0.1065(0.1266)	0.0003(0.0236)	0.137(0.133)	96.88(95.83)
[2023-09-29 14:02:04 10splitTasks](trainer.py 286): INFO [30/157]	0.1062(0.1196)	0.0002(0.0161)	0.213(0.135)	93.75(95.97)
[2023-09-29 14:02:05 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1154)	0.0002(0.0122)	0.139(0.140)	93.75(95.43)
[2023-09-29 14:02:06 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1129)	0.0002(0.0099)	0.315(0.146)	84.38(94.98)
[2023-09-29 14:02:07 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1111)	0.0002(0.0083)	0.193(0.138)	93.75(95.13)
[2023-09-29 14:02:08 10splitTasks](trainer.py 286): INFO [70/157]	0.1009(0.1099)	0.0001(0.0072)	0.216(0.132)	90.62(95.33)
[2023-09-29 14:02:09 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1090)	0.0002(0.0063)	0.070(0.131)	100.00(95.49)
[2023-09-29 14:02:10 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1082)	0.0002(0.0056)	0.053(0.126)	100.00(95.67)
[2023-09-29 14:02:11 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1076)	0.0002(0.0051)	0.055(0.125)	100.00(95.73)
[2023-09-29 14:02:12 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1072)	0.0001(0.0047)	0.134(0.127)	93.75(95.61)
[2023-09-29 14:02:13 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1067)	0.0002(0.0043)	0.030(0.126)	100.00(95.66)
[2023-09-29 14:02:14 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1064)	0.0003(0.0040)	0.326(0.129)	87.50(95.61)
[2023-09-29 14:02:15 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1060)	0.0002(0.0037)	0.017(0.127)	100.00(95.70)
[2023-09-29 14:02:16 10splitTasks](trainer.py 286): INFO [150/157]	0.1051(0.1058)	0.0001(0.0035)	0.342(0.130)	90.62(95.67)
[2023-09-29 14:02:17 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1055)	0.0001(0.0034)	0.023(0.132)	100.00(95.58)
[2023-09-29 14:02:17 10splitTasks](trainer.py 288): INFO  * Train Acc 95.580
[2023-09-29 14:02:18 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.75
[2023-09-29 14:02:18 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 14:02:18 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 14:02:18 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:02:19 10splitTasks](trainer.py 286): INFO [0/157]	0.5752(0.5752)	0.4706(0.4706)	0.063(0.063)	100.00(100.00)
[2023-09-29 14:02:20 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1460)	0.0002(0.0430)	0.031(0.068)	100.00(98.58)
[2023-09-29 14:02:21 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1257)	0.0003(0.0227)	0.010(0.073)	100.00(97.92)
[2023-09-29 14:02:22 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1189)	0.0001(0.0156)	0.230(0.098)	90.62(97.28)
[2023-09-29 14:02:23 10splitTasks](trainer.py 286): INFO [40/157]	0.1043(0.1149)	0.0003(0.0119)	0.129(0.112)	96.88(96.72)
[2023-09-29 14:02:24 10splitTasks](trainer.py 286): INFO [50/157]	0.1091(0.1129)	0.0002(0.0096)	0.052(0.109)	96.88(96.51)
[2023-09-29 14:02:25 10splitTasks](trainer.py 286): INFO [60/157]	0.1036(0.1115)	0.0008(0.0081)	0.086(0.109)	96.88(96.47)
[2023-09-29 14:02:26 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1106)	0.0002(0.0070)	0.106(0.106)	96.88(96.65)
[2023-09-29 14:02:27 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1096)	0.0003(0.0062)	0.055(0.106)	100.00(96.64)
[2023-09-29 14:02:28 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1091)	0.0002(0.0055)	0.096(0.109)	96.88(96.53)
[2023-09-29 14:02:29 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1085)	0.0002(0.0050)	0.095(0.112)	93.75(96.29)
[2023-09-29 14:02:30 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1080)	0.0002(0.0046)	0.037(0.112)	100.00(96.31)
[2023-09-29 14:02:31 10splitTasks](trainer.py 286): INFO [120/157]	0.1024(0.1076)	0.0002(0.0042)	0.088(0.112)	93.75(96.18)
[2023-09-29 14:02:32 10splitTasks](trainer.py 286): INFO [130/157]	0.1068(0.1073)	0.0003(0.0039)	0.059(0.115)	100.00(96.06)
[2023-09-29 14:02:34 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1071)	0.0002(0.0037)	0.056(0.116)	96.88(96.01)
[2023-09-29 14:02:35 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1068)	0.0001(0.0035)	0.210(0.114)	96.88(96.17)
[2023-09-29 14:02:35 10splitTasks](trainer.py 286): INFO [156/157]	0.0780(0.1064)	0.0001(0.0033)	0.182(0.116)	100.00(96.16)
[2023-09-29 14:02:35 10splitTasks](trainer.py 288): INFO  * Train Acc 96.160
[2023-09-29 14:02:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.800, Total time 1.75
[2023-09-29 14:02:37 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 14:02:37 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 14:02:37 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:02:38 10splitTasks](trainer.py 286): INFO [0/157]	0.7732(0.7732)	0.6663(0.6663)	0.091(0.091)	96.88(96.88)
[2023-09-29 14:02:39 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1642)	0.0001(0.0608)	0.123(0.102)	96.88(96.59)
[2023-09-29 14:02:40 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1349)	0.0003(0.0320)	0.114(0.098)	96.88(96.58)
[2023-09-29 14:02:41 10splitTasks](trainer.py 286): INFO [30/157]	0.1052(0.1248)	0.0004(0.0218)	0.153(0.133)	90.62(94.96)
[2023-09-29 14:02:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1195)	0.0002(0.0166)	0.043(0.125)	100.00(95.35)
[2023-09-29 14:02:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1162)	0.0003(0.0134)	0.021(0.130)	100.00(94.73)
[2023-09-29 14:02:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1140)	0.0002(0.0112)	0.090(0.135)	96.88(94.93)
[2023-09-29 14:02:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1048(0.1124)	0.0003(0.0097)	0.020(0.130)	100.00(95.20)
[2023-09-29 14:02:46 10splitTasks](trainer.py 286): INFO [80/157]	0.1012(0.1114)	0.0001(0.0085)	0.194(0.131)	93.75(95.18)
[2023-09-29 14:02:47 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1104)	0.0003(0.0076)	0.028(0.129)	100.00(95.19)
[2023-09-29 14:02:48 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1097)	0.0002(0.0069)	0.100(0.127)	96.88(95.30)
[2023-09-29 14:02:49 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1091)	0.0002(0.0063)	0.057(0.124)	100.00(95.38)
[2023-09-29 14:02:50 10splitTasks](trainer.py 286): INFO [120/157]	0.1153(0.1086)	0.0005(0.0058)	0.049(0.124)	100.00(95.38)
[2023-09-29 14:02:51 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1081)	0.0003(0.0054)	0.051(0.121)	100.00(95.52)
[2023-09-29 14:02:52 10splitTasks](trainer.py 286): INFO [140/157]	0.1033(0.1080)	0.0002(0.0050)	0.166(0.121)	93.75(95.46)
[2023-09-29 14:02:53 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1076)	0.0001(0.0047)	0.043(0.119)	100.00(95.61)
[2023-09-29 14:02:54 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1072)	0.0001(0.0045)	0.096(0.119)	100.00(95.64)
[2023-09-29 14:02:54 10splitTasks](trainer.py 288): INFO  * Train Acc 95.640
[2023-09-29 14:02:56 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.75
[2023-09-29 14:02:56 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 14:02:56 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 14:02:56 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:02:56 10splitTasks](trainer.py 286): INFO [0/157]	0.5906(0.5906)	0.4830(0.4830)	0.046(0.046)	100.00(100.00)
[2023-09-29 14:02:57 10splitTasks](trainer.py 286): INFO [10/157]	0.1039(0.1486)	0.0002(0.0441)	0.104(0.099)	93.75(96.59)
[2023-09-29 14:02:58 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1262)	0.0002(0.0232)	0.078(0.103)	93.75(95.98)
[2023-09-29 14:02:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1058(0.1191)	0.0002(0.0158)	0.219(0.117)	96.88(95.56)
[2023-09-29 14:03:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1080(0.1150)	0.0001(0.0120)	0.121(0.110)	93.75(95.66)
[2023-09-29 14:03:01 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1127)	0.0002(0.0097)	0.182(0.104)	93.75(95.89)
[2023-09-29 14:03:02 10splitTasks](trainer.py 286): INFO [60/157]	0.1030(0.1117)	0.0003(0.0082)	0.145(0.111)	96.88(95.65)
[2023-09-29 14:03:04 10splitTasks](trainer.py 286): INFO [70/157]	0.1027(0.1104)	0.0002(0.0071)	0.184(0.110)	90.62(95.69)
[2023-09-29 14:03:05 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1093)	0.0002(0.0062)	0.098(0.108)	96.88(95.87)
[2023-09-29 14:03:06 10splitTasks](trainer.py 286): INFO [90/157]	0.1060(0.1088)	0.0002(0.0056)	0.093(0.110)	96.88(95.74)
[2023-09-29 14:03:07 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1085)	0.0003(0.0051)	0.028(0.108)	100.00(95.85)
[2023-09-29 14:03:08 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1080)	0.0002(0.0046)	0.123(0.107)	96.88(95.83)
[2023-09-29 14:03:09 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1076)	0.0002(0.0043)	0.129(0.108)	96.88(95.84)
[2023-09-29 14:03:10 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1072)	0.0002(0.0040)	0.015(0.106)	100.00(95.92)
[2023-09-29 14:03:11 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1069)	0.0002(0.0037)	0.048(0.105)	100.00(96.08)
[2023-09-29 14:03:12 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1067)	0.0001(0.0035)	0.205(0.105)	96.88(96.13)
[2023-09-29 14:03:12 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1063)	0.0001(0.0034)	0.107(0.106)	100.00(96.12)
[2023-09-29 14:03:12 10splitTasks](trainer.py 288): INFO  * Train Acc 96.120
[2023-09-29 14:03:14 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.600, Total time 1.90
[2023-09-29 14:03:14 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 14:03:14 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 14:03:14 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:03:15 10splitTasks](trainer.py 286): INFO [0/157]	0.5656(0.5656)	0.4569(0.4569)	0.060(0.060)	96.88(96.88)
[2023-09-29 14:03:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1084(0.1453)	0.0005(0.0418)	0.103(0.110)	93.75(95.74)
[2023-09-29 14:03:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1260)	0.0003(0.0221)	0.034(0.126)	100.00(95.83)
[2023-09-29 14:03:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1103(0.1190)	0.0006(0.0151)	0.122(0.115)	93.75(95.97)
[2023-09-29 14:03:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1072(0.1154)	0.0003(0.0115)	0.207(0.115)	96.88(96.19)
[2023-09-29 14:03:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1129)	0.0002(0.0093)	0.237(0.109)	93.75(96.38)
[2023-09-29 14:03:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1117)	0.0002(0.0079)	0.025(0.103)	100.00(96.57)
[2023-09-29 14:03:22 10splitTasks](trainer.py 286): INFO [70/157]	0.1292(0.1108)	0.0006(0.0068)	0.171(0.110)	90.62(96.21)
[2023-09-29 14:03:23 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1100)	0.0002(0.0060)	0.110(0.108)	96.88(96.33)
[2023-09-29 14:03:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1093)	0.0002(0.0054)	0.085(0.103)	96.88(96.53)
[2023-09-29 14:03:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1086)	0.0003(0.0049)	0.085(0.101)	100.00(96.66)
[2023-09-29 14:03:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1123(0.1082)	0.0006(0.0045)	0.176(0.105)	96.88(96.62)
[2023-09-29 14:03:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1079)	0.0003(0.0041)	0.061(0.103)	100.00(96.67)
[2023-09-29 14:03:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1075)	0.0003(0.0038)	0.165(0.101)	96.88(96.78)
[2023-09-29 14:03:30 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1072)	0.0002(0.0036)	0.012(0.103)	100.00(96.65)
[2023-09-29 14:03:31 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1071)	0.0001(0.0034)	0.025(0.101)	100.00(96.77)
[2023-09-29 14:03:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0796(0.1067)	0.0001(0.0032)	0.020(0.101)	100.00(96.78)
[2023-09-29 14:03:31 10splitTasks](trainer.py 288): INFO  * Train Acc 96.780
[2023-09-29 14:03:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.600, Total time 1.84
[2023-09-29 14:03:33 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 14:03:33 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 14:03:33 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:03:34 10splitTasks](trainer.py 286): INFO [0/157]	0.5842(0.5842)	0.4710(0.4710)	0.024(0.024)	100.00(100.00)
[2023-09-29 14:03:35 10splitTasks](trainer.py 286): INFO [10/157]	0.1020(0.1473)	0.0003(0.0431)	0.315(0.088)	93.75(97.16)
[2023-09-29 14:03:36 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1256)	0.0003(0.0227)	0.056(0.075)	100.00(97.92)
[2023-09-29 14:03:37 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1196)	0.0002(0.0155)	0.119(0.080)	93.75(97.48)
[2023-09-29 14:03:38 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1154)	0.0002(0.0118)	0.032(0.077)	100.00(97.48)
[2023-09-29 14:03:39 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1129)	0.0002(0.0095)	0.174(0.090)	96.88(97.18)
[2023-09-29 14:03:40 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1113)	0.0002(0.0080)	0.128(0.087)	96.88(97.28)
[2023-09-29 14:03:41 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1102)	0.0002(0.0069)	0.160(0.085)	93.75(97.32)
[2023-09-29 14:03:42 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1094)	0.0003(0.0061)	0.051(0.085)	100.00(97.26)
[2023-09-29 14:03:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1088)	0.0003(0.0054)	0.026(0.086)	100.00(97.22)
[2023-09-29 14:03:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1082)	0.0002(0.0049)	0.097(0.090)	96.88(97.06)
[2023-09-29 14:03:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1029(0.1079)	0.0004(0.0045)	0.279(0.093)	93.75(97.04)
[2023-09-29 14:03:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1076)	0.0002(0.0042)	0.148(0.094)	96.88(96.93)
[2023-09-29 14:03:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1073)	0.0002(0.0039)	0.027(0.093)	100.00(97.04)
[2023-09-29 14:03:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1086(0.1070)	0.0002(0.0036)	0.074(0.094)	96.88(96.92)
[2023-09-29 14:03:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1067)	0.0001(0.0034)	0.089(0.092)	96.88(96.98)
[2023-09-29 14:03:50 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1064)	0.0001(0.0033)	0.230(0.092)	87.50(96.96)
[2023-09-29 14:03:50 10splitTasks](trainer.py 288): INFO  * Train Acc 96.960
[2023-09-29 14:03:52 10splitTasks](my_trainer.py 503): INFO  * Val Acc 86.200, Total time 1.81
[2023-09-29 14:03:52 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 14:03:52 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 14:03:52 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:03:52 10splitTasks](trainer.py 286): INFO [0/157]	0.5812(0.5812)	0.4589(0.4589)	0.216(0.216)	87.50(87.50)
[2023-09-29 14:03:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1218(0.1492)	0.0002(0.0420)	0.240(0.140)	93.75(95.74)
[2023-09-29 14:03:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1030(0.1274)	0.0002(0.0221)	0.103(0.127)	93.75(95.54)
[2023-09-29 14:03:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1193)	0.0002(0.0150)	0.043(0.122)	100.00(95.67)
[2023-09-29 14:03:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1011(0.1150)	0.0002(0.0114)	0.116(0.105)	93.75(96.42)
[2023-09-29 14:03:58 10splitTasks](trainer.py 286): INFO [50/157]	0.1184(0.1132)	0.0003(0.0092)	0.024(0.101)	100.00(96.69)
[2023-09-29 14:03:59 10splitTasks](trainer.py 286): INFO [60/157]	0.1012(0.1117)	0.0002(0.0078)	0.096(0.100)	96.88(96.62)
[2023-09-29 14:04:00 10splitTasks](trainer.py 286): INFO [70/157]	0.1215(0.1108)	0.0006(0.0067)	0.094(0.096)	96.88(96.88)
[2023-09-29 14:04:01 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1097)	0.0001(0.0059)	0.293(0.101)	87.50(96.64)
[2023-09-29 14:04:02 10splitTasks](trainer.py 286): INFO [90/157]	0.1027(0.1089)	0.0003(0.0053)	0.092(0.099)	93.75(96.70)
[2023-09-29 14:04:03 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1082)	0.0007(0.0048)	0.074(0.099)	96.88(96.66)
[2023-09-29 14:04:04 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1078)	0.0001(0.0044)	0.077(0.100)	96.88(96.59)
[2023-09-29 14:04:05 10splitTasks](trainer.py 286): INFO [120/157]	0.1115(0.1075)	0.0006(0.0041)	0.365(0.104)	93.75(96.57)
[2023-09-29 14:04:06 10splitTasks](trainer.py 286): INFO [130/157]	0.1174(0.1073)	0.0005(0.0038)	0.133(0.106)	96.88(96.40)
[2023-09-29 14:04:07 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1070)	0.0002(0.0035)	0.078(0.107)	96.88(96.39)
[2023-09-29 14:04:08 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1067)	0.0001(0.0033)	0.024(0.105)	100.00(96.46)
[2023-09-29 14:04:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1064)	0.0001(0.0032)	0.414(0.106)	87.50(96.42)
[2023-09-29 14:04:09 10splitTasks](trainer.py 288): INFO  * Train Acc 96.420
[2023-09-29 14:04:10 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.77
[2023-09-29 14:04:10 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 14:04:10 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 14:04:10 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:04:11 10splitTasks](trainer.py 286): INFO [0/157]	0.6450(0.6450)	0.5396(0.5396)	0.105(0.105)	96.88(96.88)
[2023-09-29 14:04:12 10splitTasks](trainer.py 286): INFO [10/157]	0.1106(0.1541)	0.0002(0.0493)	0.097(0.090)	96.88(96.59)
[2023-09-29 14:04:13 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1302)	0.0003(0.0260)	0.025(0.084)	100.00(97.02)
[2023-09-29 14:04:14 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1217)	0.0002(0.0177)	0.329(0.099)	84.38(96.17)
[2023-09-29 14:04:15 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1169)	0.0003(0.0135)	0.075(0.092)	96.88(96.49)
[2023-09-29 14:04:16 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1140)	0.0002(0.0109)	0.067(0.095)	96.88(96.45)
[2023-09-29 14:04:17 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1120)	0.0004(0.0091)	0.203(0.101)	93.75(96.26)
[2023-09-29 14:04:18 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1107)	0.0002(0.0079)	0.020(0.108)	100.00(96.17)
[2023-09-29 14:04:19 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1099)	0.0003(0.0070)	0.047(0.112)	96.88(96.10)
[2023-09-29 14:04:20 10splitTasks](trainer.py 286): INFO [90/157]	0.1041(0.1094)	0.0002(0.0062)	0.124(0.112)	96.88(96.12)
[2023-09-29 14:04:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1089)	0.0003(0.0056)	0.051(0.108)	96.88(96.29)
[2023-09-29 14:04:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1083)	0.0001(0.0052)	0.024(0.109)	100.00(96.26)
[2023-09-29 14:04:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1046(0.1078)	0.0003(0.0048)	0.034(0.106)	100.00(96.36)
[2023-09-29 14:04:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1074(0.1074)	0.0006(0.0044)	0.083(0.103)	96.88(96.52)
[2023-09-29 14:04:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1080(0.1073)	0.0003(0.0041)	0.092(0.103)	96.88(96.61)
[2023-09-29 14:04:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1019(0.1070)	0.0002(0.0039)	0.100(0.105)	96.88(96.56)
[2023-09-29 14:04:27 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1066)	0.0001(0.0037)	0.528(0.107)	87.50(96.50)
[2023-09-29 14:04:27 10splitTasks](trainer.py 288): INFO  * Train Acc 96.500
[2023-09-29 14:04:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.600, Total time 1.81
[2023-09-29 14:04:29 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 14:04:29 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 14:04:29 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:04:30 10splitTasks](trainer.py 286): INFO [0/157]	0.5544(0.5544)	0.4423(0.4423)	0.040(0.040)	100.00(100.00)
[2023-09-29 14:04:31 10splitTasks](trainer.py 286): INFO [10/157]	0.1134(0.1570)	0.0001(0.0496)	0.152(0.072)	93.75(98.30)
[2023-09-29 14:04:32 10splitTasks](trainer.py 286): INFO [20/157]	0.1055(0.1322)	0.0003(0.0261)	0.235(0.100)	93.75(97.17)
[2023-09-29 14:04:33 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1226)	0.0002(0.0179)	0.176(0.113)	90.62(96.37)
[2023-09-29 14:04:34 10splitTasks](trainer.py 286): INFO [40/157]	0.1024(0.1180)	0.0002(0.0136)	0.152(0.109)	93.75(96.42)
[2023-09-29 14:04:35 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1149)	0.0003(0.0110)	0.009(0.107)	100.00(96.51)
[2023-09-29 14:04:36 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1129)	0.0002(0.0092)	0.207(0.105)	90.62(96.41)
[2023-09-29 14:04:37 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1114)	0.0003(0.0080)	0.241(0.104)	96.88(96.61)
[2023-09-29 14:04:38 10splitTasks](trainer.py 286): INFO [80/157]	0.1014(0.1102)	0.0003(0.0070)	0.021(0.104)	100.00(96.60)
[2023-09-29 14:04:39 10splitTasks](trainer.py 286): INFO [90/157]	0.1054(0.1096)	0.0003(0.0063)	0.071(0.105)	100.00(96.53)
[2023-09-29 14:04:40 10splitTasks](trainer.py 286): INFO [100/157]	0.1049(0.1089)	0.0002(0.0057)	0.048(0.102)	100.00(96.63)
[2023-09-29 14:04:41 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1082)	0.0003(0.0052)	0.255(0.104)	93.75(96.65)
[2023-09-29 14:04:42 10splitTasks](trainer.py 286): INFO [120/157]	0.1027(0.1078)	0.0003(0.0048)	0.021(0.100)	100.00(96.80)
[2023-09-29 14:04:43 10splitTasks](trainer.py 286): INFO [130/157]	0.1062(0.1076)	0.0006(0.0045)	0.037(0.100)	100.00(96.83)
[2023-09-29 14:04:44 10splitTasks](trainer.py 286): INFO [140/157]	0.1043(0.1073)	0.0003(0.0042)	0.069(0.099)	96.88(96.83)
[2023-09-29 14:04:45 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1072)	0.0001(0.0039)	0.120(0.097)	96.88(96.90)
[2023-09-29 14:04:46 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1068)	0.0001(0.0038)	0.127(0.098)	87.50(96.80)
[2023-09-29 14:04:46 10splitTasks](trainer.py 288): INFO  * Train Acc 96.800
[2023-09-29 14:04:48 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.81
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-7.pth
=> Save Done
[2023-09-29 14:04:48 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 14:04:50 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.73
[2023-09-29 14:04:50 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 14:04:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.80
[2023-09-29 14:04:51 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 14:04:53 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.76
[2023-09-29 14:04:53 10splitTasks](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 14:04:55 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.77
[2023-09-29 14:04:55 10splitTasks](iBatchLearn.py 131): INFO validation split name:4
[2023-09-29 14:04:57 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.72
[2023-09-29 14:04:57 10splitTasks](iBatchLearn.py 131): INFO validation split name:5
[2023-09-29 14:04:59 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.600, Total time 1.81
[2023-09-29 14:04:59 10splitTasks](iBatchLearn.py 131): INFO validation split name:6
[2023-09-29 14:05:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.75
[2023-09-29 14:05:00 10splitTasks](iBatchLearn.py 131): INFO validation split name:7
[2023-09-29 14:05:02 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.93
[2023-09-29 14:05:02 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 14:05:02 10splitTasks](trainer.py 341): INFO done
[2023-09-29 14:05:02 10splitTasks](iBatchLearn.py 155): INFO Acc:82.475; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 14:05:06 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 14:05:06 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 14:05:06 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 7, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-7.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-7.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_7.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 14:05:07 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-7.pth
[2023-09-29 14:05:07 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 14:05:09 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 14:05:09 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 14:05:09 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 4, 4, 4],
        [3, 3, 2, 4, 6, 4, 4],
        [4, 3, 3, 6, 6, 4, 4],
        [4, 3, 6, 5, 4, 4, 4],
        [4, 6, 6, 5, 4, 5, 5],
        [7, 6, 5, 4, 2, 2, 5],
        [7, 3, 5, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 14:05:09 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 14:05:10 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 14:05:10 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 14:05:15 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 14:05:18 10splitTasks](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 14:05:21 10splitTasks](iBatchLearn.py 167): INFO test split name:3
[2023-09-29 14:05:24 10splitTasks](iBatchLearn.py 167): INFO test split name:4
[2023-09-29 14:05:27 10splitTasks](iBatchLearn.py 167): INFO test split name:5
[2023-09-29 14:05:30 10splitTasks](iBatchLearn.py 167): INFO test split name:6
[2023-09-29 14:05:34 10splitTasks](iBatchLearn.py 167): INFO test split name:7
--------------------------------Official Evaluation--------------------------------
7 82.05000000000001
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 14:05:42 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 14:05:42 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 14:05:42 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 8, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-7.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-8.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-7.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-8.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 14:05:43 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-7.pth
[2023-09-29 14:05:43 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 14:05:45 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 14:05:45 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 14:05:45 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 14:05:45 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 14:05:45 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 14:05:45 10splitTasks](iBatchLearn.py 92): INFO ====================== 8 =======================
[2023-09-29 14:05:45 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 14:05:45 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 14:05:45 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 14:05:45 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:05:49 10splitTasks](trainer.py 286): INFO [0/157]	3.5996(3.5996)	0.5632(0.5632)	2.319(2.319)	12.50(12.50)
[2023-09-29 14:05:50 10splitTasks](trainer.py 286): INFO [10/157]	0.1038(0.4216)	0.0003(0.0515)	2.074(2.232)	28.12(16.19)
[2023-09-29 14:05:51 10splitTasks](trainer.py 286): INFO [20/157]	0.1008(0.2697)	0.0002(0.0271)	2.038(2.112)	28.12(22.17)
[2023-09-29 14:05:52 10splitTasks](trainer.py 286): INFO [30/157]	0.1092(0.2159)	0.0002(0.0185)	1.575(1.972)	53.12(29.23)
[2023-09-29 14:05:53 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1881)	0.0002(0.0140)	1.500(1.852)	46.88(34.15)
[2023-09-29 14:05:54 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1713)	0.0003(0.0114)	1.234(1.753)	53.12(37.99)
[2023-09-29 14:05:55 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1602)	0.0003(0.0095)	1.117(1.685)	62.50(40.37)
[2023-09-29 14:05:56 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1520)	0.0003(0.0082)	1.164(1.614)	62.50(42.83)
[2023-09-29 14:05:57 10splitTasks](trainer.py 286): INFO [80/157]	0.1087(0.1463)	0.0006(0.0073)	0.874(1.561)	75.00(44.91)
[2023-09-29 14:05:58 10splitTasks](trainer.py 286): INFO [90/157]	0.1040(0.1415)	0.0002(0.0065)	1.383(1.519)	50.00(46.09)
[2023-09-29 14:05:59 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1378)	0.0002(0.0059)	1.275(1.490)	50.00(46.94)
[2023-09-29 14:06:00 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1345)	0.0002(0.0054)	1.387(1.458)	43.75(48.00)
[2023-09-29 14:06:01 10splitTasks](trainer.py 286): INFO [120/157]	0.1011(0.1319)	0.0003(0.0050)	1.030(1.423)	62.50(49.33)
[2023-09-29 14:06:02 10splitTasks](trainer.py 286): INFO [130/157]	0.1053(0.1296)	0.0002(0.0046)	0.698(1.396)	71.88(50.19)
[2023-09-29 14:06:03 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1277)	0.0003(0.0043)	0.979(1.365)	68.75(51.42)
[2023-09-29 14:06:05 10splitTasks](trainer.py 286): INFO [150/157]	0.1006(0.1260)	0.0001(0.0040)	1.200(1.338)	50.00(52.36)
[2023-09-29 14:06:05 10splitTasks](trainer.py 286): INFO [156/157]	0.0799(0.1249)	0.0001(0.0039)	1.023(1.326)	62.50(52.76)
[2023-09-29 14:06:05 10splitTasks](trainer.py 288): INFO  * Train Acc 52.760
[2023-09-29 14:06:07 10splitTasks](my_trainer.py 503): INFO  * Val Acc 64.600, Total time 1.72
[2023-09-29 14:06:07 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 14:06:07 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 14:06:07 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:06:08 10splitTasks](trainer.py 286): INFO [0/157]	0.6350(0.6350)	0.5290(0.5290)	0.982(0.982)	62.50(62.50)
[2023-09-29 14:06:09 10splitTasks](trainer.py 286): INFO [10/157]	0.1056(0.1556)	0.0003(0.0524)	1.052(0.943)	68.75(66.76)
[2023-09-29 14:06:10 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1309)	0.0002(0.0276)	0.721(0.939)	81.25(66.82)
[2023-09-29 14:06:11 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1218)	0.0002(0.0188)	0.786(0.942)	71.88(67.24)
[2023-09-29 14:06:12 10splitTasks](trainer.py 286): INFO [40/157]	0.1026(0.1174)	0.0010(0.0143)	1.240(0.960)	62.50(66.92)
[2023-09-29 14:06:13 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1146)	0.0002(0.0116)	0.628(0.949)	84.38(66.91)
[2023-09-29 14:06:14 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1125)	0.0002(0.0097)	0.846(0.930)	68.75(67.52)
[2023-09-29 14:06:15 10splitTasks](trainer.py 286): INFO [70/157]	0.1026(0.1113)	0.0003(0.0084)	0.610(0.922)	81.25(68.13)
[2023-09-29 14:06:16 10splitTasks](trainer.py 286): INFO [80/157]	0.1066(0.1102)	0.0003(0.0074)	1.353(0.928)	62.50(68.09)
[2023-09-29 14:06:17 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1095)	0.0002(0.0066)	0.564(0.929)	75.00(68.20)
[2023-09-29 14:06:18 10splitTasks](trainer.py 286): INFO [100/157]	0.1045(0.1090)	0.0002(0.0060)	0.855(0.923)	71.88(68.32)
[2023-09-29 14:06:19 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1084)	0.0002(0.0055)	0.680(0.919)	75.00(68.47)
[2023-09-29 14:06:20 10splitTasks](trainer.py 286): INFO [120/157]	0.1025(0.1079)	0.0004(0.0051)	1.155(0.913)	62.50(68.78)
[2023-09-29 14:06:21 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1076)	0.0002(0.0047)	1.250(0.907)	71.88(68.96)
[2023-09-29 14:06:22 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1073)	0.0002(0.0044)	0.804(0.899)	75.00(69.30)
[2023-09-29 14:06:23 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1070)	0.0002(0.0041)	0.854(0.895)	68.75(69.50)
[2023-09-29 14:06:24 10splitTasks](trainer.py 286): INFO [156/157]	0.0824(0.1066)	0.0001(0.0040)	0.841(0.891)	87.50(69.76)
[2023-09-29 14:06:24 10splitTasks](trainer.py 288): INFO  * Train Acc 69.760
[2023-09-29 14:06:26 10splitTasks](my_trainer.py 503): INFO  * Val Acc 69.400, Total time 1.80
[2023-09-29 14:06:26 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 14:06:26 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 14:06:26 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:06:26 10splitTasks](trainer.py 286): INFO [0/157]	0.6156(0.6156)	0.4990(0.4990)	0.972(0.972)	65.62(65.62)
[2023-09-29 14:06:27 10splitTasks](trainer.py 286): INFO [10/157]	0.1068(0.1524)	0.0002(0.0457)	0.665(0.818)	78.12(71.31)
[2023-09-29 14:06:28 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1300)	0.0004(0.0241)	0.422(0.739)	90.62(73.81)
[2023-09-29 14:06:29 10splitTasks](trainer.py 286): INFO [30/157]	0.1037(0.1212)	0.0004(0.0165)	0.744(0.740)	75.00(74.50)
[2023-09-29 14:06:30 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1167)	0.0002(0.0125)	0.640(0.728)	71.88(74.16)
[2023-09-29 14:06:31 10splitTasks](trainer.py 286): INFO [50/157]	0.1055(0.1141)	0.0002(0.0101)	0.499(0.720)	90.62(74.39)
[2023-09-29 14:06:32 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1123)	0.0002(0.0085)	0.743(0.717)	78.12(74.69)
[2023-09-29 14:06:33 10splitTasks](trainer.py 286): INFO [70/157]	0.1024(0.1109)	0.0004(0.0074)	0.836(0.721)	75.00(74.96)
[2023-09-29 14:06:34 10splitTasks](trainer.py 286): INFO [80/157]	0.1073(0.1100)	0.0006(0.0065)	0.867(0.721)	68.75(75.04)
[2023-09-29 14:06:36 10splitTasks](trainer.py 286): INFO [90/157]	0.1025(0.1094)	0.0004(0.0058)	0.767(0.727)	68.75(75.07)
[2023-09-29 14:06:37 10splitTasks](trainer.py 286): INFO [100/157]	0.1044(0.1088)	0.0003(0.0053)	0.749(0.727)	68.75(74.97)
[2023-09-29 14:06:38 10splitTasks](trainer.py 286): INFO [110/157]	0.1022(0.1084)	0.0002(0.0049)	1.152(0.732)	59.38(74.58)
[2023-09-29 14:06:39 10splitTasks](trainer.py 286): INFO [120/157]	0.1064(0.1082)	0.0002(0.0045)	0.865(0.733)	75.00(74.66)
[2023-09-29 14:06:40 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1078)	0.0003(0.0042)	0.665(0.734)	71.88(74.62)
[2023-09-29 14:06:41 10splitTasks](trainer.py 286): INFO [140/157]	0.1024(0.1075)	0.0002(0.0039)	1.024(0.738)	65.62(74.58)
[2023-09-29 14:06:42 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1071)	0.0001(0.0037)	0.765(0.750)	75.00(74.28)
[2023-09-29 14:06:42 10splitTasks](trainer.py 286): INFO [156/157]	0.0807(0.1068)	0.0001(0.0035)	0.905(0.752)	62.50(74.18)
[2023-09-29 14:06:42 10splitTasks](trainer.py 288): INFO  * Train Acc 74.180
[2023-09-29 14:06:44 10splitTasks](my_trainer.py 503): INFO  * Val Acc 71.400, Total time 1.69
[2023-09-29 14:06:44 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 14:06:44 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 14:06:44 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:06:45 10splitTasks](trainer.py 286): INFO [0/157]	0.6228(0.6228)	0.5130(0.5130)	0.837(0.837)	78.12(78.12)
[2023-09-29 14:06:46 10splitTasks](trainer.py 286): INFO [10/157]	0.1064(0.1507)	0.0002(0.0469)	0.794(0.823)	78.12(71.59)
[2023-09-29 14:06:47 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1283)	0.0002(0.0247)	0.693(0.764)	68.75(73.36)
[2023-09-29 14:06:48 10splitTasks](trainer.py 286): INFO [30/157]	0.1040(0.1201)	0.0003(0.0168)	0.613(0.708)	78.12(75.71)
[2023-09-29 14:06:49 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1166)	0.0003(0.0128)	0.592(0.698)	81.25(76.22)
[2023-09-29 14:06:50 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1138)	0.0002(0.0104)	0.223(0.686)	96.88(76.90)
[2023-09-29 14:06:51 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1120)	0.0002(0.0087)	0.787(0.693)	81.25(76.74)
[2023-09-29 14:06:52 10splitTasks](trainer.py 286): INFO [70/157]	0.1013(0.1106)	0.0002(0.0075)	0.420(0.682)	81.25(77.11)
[2023-09-29 14:06:53 10splitTasks](trainer.py 286): INFO [80/157]	0.1039(0.1096)	0.0011(0.0066)	0.726(0.667)	65.62(77.62)
[2023-09-29 14:06:54 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1089)	0.0004(0.0059)	0.690(0.653)	71.88(77.92)
[2023-09-29 14:06:55 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1084)	0.0002(0.0054)	0.388(0.643)	87.50(78.22)
[2023-09-29 14:06:56 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1080)	0.0003(0.0049)	0.577(0.645)	78.12(78.12)
[2023-09-29 14:06:57 10splitTasks](trainer.py 286): INFO [120/157]	0.1055(0.1076)	0.0003(0.0046)	0.554(0.640)	75.00(78.28)
[2023-09-29 14:06:58 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1073)	0.0003(0.0042)	0.592(0.638)	78.12(78.44)
[2023-09-29 14:06:59 10splitTasks](trainer.py 286): INFO [140/157]	0.1033(0.1071)	0.0003(0.0040)	0.861(0.637)	59.38(78.41)
[2023-09-29 14:07:00 10splitTasks](trainer.py 286): INFO [150/157]	0.1024(0.1070)	0.0002(0.0037)	0.385(0.640)	84.38(78.33)
[2023-09-29 14:07:01 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1066)	0.0001(0.0036)	1.271(0.640)	50.00(78.34)
[2023-09-29 14:07:01 10splitTasks](trainer.py 288): INFO  * Train Acc 78.340
[2023-09-29 14:07:03 10splitTasks](my_trainer.py 503): INFO  * Val Acc 72.200, Total time 1.74
[2023-09-29 14:07:03 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 14:07:03 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 14:07:03 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:07:03 10splitTasks](trainer.py 286): INFO [0/157]	0.5971(0.5971)	0.4809(0.4809)	0.391(0.391)	81.25(81.25)
[2023-09-29 14:07:04 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1483)	0.0002(0.0440)	0.647(0.559)	71.88(77.27)
[2023-09-29 14:07:05 10splitTasks](trainer.py 286): INFO [20/157]	0.1046(0.1283)	0.0009(0.0233)	0.309(0.592)	90.62(78.12)
[2023-09-29 14:07:06 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1205)	0.0002(0.0159)	0.312(0.566)	93.75(79.44)
[2023-09-29 14:07:07 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1161)	0.0003(0.0121)	0.705(0.565)	78.12(80.03)
[2023-09-29 14:07:09 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1136)	0.0003(0.0098)	0.459(0.547)	78.12(80.82)
[2023-09-29 14:07:10 10splitTasks](trainer.py 286): INFO [60/157]	0.1079(0.1123)	0.0008(0.0082)	0.471(0.546)	81.25(81.05)
[2023-09-29 14:07:11 10splitTasks](trainer.py 286): INFO [70/157]	0.1073(0.1110)	0.0007(0.0071)	0.568(0.544)	87.50(81.47)
[2023-09-29 14:07:12 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1101)	0.0002(0.0063)	0.399(0.542)	84.38(81.52)
[2023-09-29 14:07:13 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1092)	0.0003(0.0056)	0.742(0.550)	62.50(81.15)
[2023-09-29 14:07:14 10splitTasks](trainer.py 286): INFO [100/157]	0.1032(0.1086)	0.0002(0.0051)	0.460(0.547)	78.12(81.16)
[2023-09-29 14:07:15 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1081)	0.0002(0.0047)	0.384(0.547)	90.62(81.02)
[2023-09-29 14:07:16 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1076)	0.0002(0.0043)	0.482(0.552)	87.50(81.04)
[2023-09-29 14:07:17 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1073)	0.0003(0.0040)	0.656(0.546)	78.12(81.30)
[2023-09-29 14:07:18 10splitTasks](trainer.py 286): INFO [140/157]	0.1101(0.1070)	0.0003(0.0037)	0.287(0.541)	90.62(81.45)
[2023-09-29 14:07:19 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1067)	0.0001(0.0035)	0.329(0.542)	93.75(81.44)
[2023-09-29 14:07:19 10splitTasks](trainer.py 286): INFO [156/157]	0.0807(0.1064)	0.0001(0.0034)	1.272(0.541)	75.00(81.46)
[2023-09-29 14:07:19 10splitTasks](trainer.py 288): INFO  * Train Acc 81.460
[2023-09-29 14:07:21 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.200, Total time 1.66
[2023-09-29 14:07:21 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 14:07:21 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 14:07:21 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:07:22 10splitTasks](trainer.py 286): INFO [0/157]	0.5902(0.5902)	0.4715(0.4715)	0.150(0.150)	96.88(96.88)
[2023-09-29 14:07:23 10splitTasks](trainer.py 286): INFO [10/157]	0.1042(0.1484)	0.0004(0.0431)	0.345(0.449)	90.62(85.51)
[2023-09-29 14:07:24 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1271)	0.0002(0.0227)	0.551(0.462)	84.38(84.82)
[2023-09-29 14:07:25 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1191)	0.0002(0.0155)	0.321(0.431)	84.38(85.38)
[2023-09-29 14:07:26 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1151)	0.0002(0.0118)	0.097(0.395)	100.00(86.97)
[2023-09-29 14:07:27 10splitTasks](trainer.py 286): INFO [50/157]	0.1038(0.1130)	0.0002(0.0095)	0.542(0.409)	78.12(86.46)
[2023-09-29 14:07:28 10splitTasks](trainer.py 286): INFO [60/157]	0.1058(0.1113)	0.0002(0.0080)	0.295(0.411)	90.62(86.32)
[2023-09-29 14:07:29 10splitTasks](trainer.py 286): INFO [70/157]	0.1033(0.1102)	0.0002(0.0069)	0.464(0.409)	81.25(86.31)
[2023-09-29 14:07:30 10splitTasks](trainer.py 286): INFO [80/157]	0.1052(0.1095)	0.0002(0.0061)	0.354(0.423)	87.50(85.69)
[2023-09-29 14:07:31 10splitTasks](trainer.py 286): INFO [90/157]	0.1051(0.1087)	0.0002(0.0055)	0.292(0.420)	87.50(85.71)
[2023-09-29 14:07:32 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1082)	0.0003(0.0049)	0.491(0.420)	93.75(85.74)
[2023-09-29 14:07:33 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1078)	0.0002(0.0045)	0.552(0.423)	75.00(85.47)
[2023-09-29 14:07:34 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1075)	0.0003(0.0042)	0.649(0.422)	81.25(85.41)
[2023-09-29 14:07:35 10splitTasks](trainer.py 286): INFO [130/157]	0.1083(0.1072)	0.0002(0.0039)	0.330(0.422)	90.62(85.23)
[2023-09-29 14:07:36 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1070)	0.0002(0.0036)	0.570(0.421)	81.25(85.35)
[2023-09-29 14:07:37 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1067)	0.0002(0.0034)	0.692(0.422)	78.12(85.35)
[2023-09-29 14:07:38 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1064)	0.0001(0.0033)	0.441(0.420)	75.00(85.42)
[2023-09-29 14:07:38 10splitTasks](trainer.py 288): INFO  * Train Acc 85.420
[2023-09-29 14:07:40 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.88
[2023-09-29 14:07:40 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 14:07:40 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 14:07:40 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:07:41 10splitTasks](trainer.py 286): INFO [0/157]	0.6422(0.6422)	0.5311(0.5311)	0.240(0.240)	90.62(90.62)
[2023-09-29 14:07:42 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1525)	0.0001(0.0486)	0.321(0.293)	90.62(90.34)
[2023-09-29 14:07:43 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1284)	0.0003(0.0256)	0.225(0.369)	90.62(88.10)
[2023-09-29 14:07:44 10splitTasks](trainer.py 286): INFO [30/157]	0.1022(0.1201)	0.0002(0.0174)	0.139(0.348)	100.00(88.31)
[2023-09-29 14:07:45 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1159)	0.0004(0.0133)	0.505(0.347)	84.38(88.49)
[2023-09-29 14:07:46 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1136)	0.0002(0.0107)	0.194(0.337)	93.75(88.97)
[2023-09-29 14:07:47 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1120)	0.0003(0.0090)	0.306(0.335)	90.62(89.29)
[2023-09-29 14:07:48 10splitTasks](trainer.py 286): INFO [70/157]	0.1039(0.1110)	0.0003(0.0078)	0.405(0.332)	84.38(89.13)
[2023-09-29 14:07:49 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1101)	0.0002(0.0069)	0.431(0.333)	81.25(88.85)
[2023-09-29 14:07:50 10splitTasks](trainer.py 286): INFO [90/157]	0.1044(0.1095)	0.0002(0.0062)	0.223(0.337)	93.75(88.53)
[2023-09-29 14:07:51 10splitTasks](trainer.py 286): INFO [100/157]	0.1058(0.1089)	0.0004(0.0056)	0.392(0.339)	87.50(88.58)
[2023-09-29 14:07:52 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1085)	0.0002(0.0051)	0.534(0.344)	78.12(88.49)
[2023-09-29 14:07:53 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1082)	0.0002(0.0047)	0.305(0.340)	84.38(88.56)
[2023-09-29 14:07:54 10splitTasks](trainer.py 286): INFO [130/157]	0.1038(0.1078)	0.0002(0.0044)	0.273(0.341)	87.50(88.45)
[2023-09-29 14:07:55 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1076)	0.0002(0.0041)	0.194(0.345)	96.88(88.39)
[2023-09-29 14:07:56 10splitTasks](trainer.py 286): INFO [150/157]	0.1082(0.1074)	0.0001(0.0038)	0.578(0.349)	78.12(88.20)
[2023-09-29 14:07:57 10splitTasks](trainer.py 286): INFO [156/157]	0.0802(0.1070)	0.0001(0.0037)	0.372(0.348)	87.50(88.24)
[2023-09-29 14:07:57 10splitTasks](trainer.py 288): INFO  * Train Acc 88.240
[2023-09-29 14:07:59 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.200, Total time 1.81
[2023-09-29 14:07:59 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 14:07:59 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 14:07:59 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:07:59 10splitTasks](trainer.py 286): INFO [0/157]	0.6234(0.6234)	0.5104(0.5104)	0.328(0.328)	87.50(87.50)
[2023-09-29 14:08:00 10splitTasks](trainer.py 286): INFO [10/157]	0.1025(0.1574)	0.0002(0.0467)	0.450(0.239)	84.38(92.33)
[2023-09-29 14:08:01 10splitTasks](trainer.py 286): INFO [20/157]	0.1022(0.1331)	0.0002(0.0246)	0.242(0.258)	90.62(91.52)
[2023-09-29 14:08:02 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1237)	0.0003(0.0168)	0.321(0.255)	87.50(91.63)
[2023-09-29 14:08:03 10splitTasks](trainer.py 286): INFO [40/157]	0.1022(0.1186)	0.0003(0.0128)	0.286(0.256)	93.75(91.77)
[2023-09-29 14:08:05 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1156)	0.0003(0.0104)	0.333(0.262)	90.62(91.61)
[2023-09-29 14:08:06 10splitTasks](trainer.py 286): INFO [60/157]	0.1031(0.1135)	0.0002(0.0087)	0.213(0.267)	90.62(91.44)
[2023-09-29 14:08:07 10splitTasks](trainer.py 286): INFO [70/157]	0.1082(0.1122)	0.0003(0.0075)	0.154(0.275)	96.88(91.24)
[2023-09-29 14:08:08 10splitTasks](trainer.py 286): INFO [80/157]	0.1059(0.1114)	0.0002(0.0067)	0.211(0.276)	90.62(91.28)
[2023-09-29 14:08:09 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1104)	0.0003(0.0060)	0.562(0.274)	87.50(91.41)
[2023-09-29 14:08:10 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1098)	0.0002(0.0054)	0.403(0.274)	81.25(91.15)
[2023-09-29 14:08:11 10splitTasks](trainer.py 286): INFO [110/157]	0.1130(0.1094)	0.0003(0.0049)	0.341(0.276)	90.62(90.99)
[2023-09-29 14:08:12 10splitTasks](trainer.py 286): INFO [120/157]	0.1023(0.1089)	0.0003(0.0046)	0.276(0.277)	90.62(90.88)
[2023-09-29 14:08:13 10splitTasks](trainer.py 286): INFO [130/157]	0.1060(0.1088)	0.0003(0.0042)	0.380(0.277)	84.38(90.84)
[2023-09-29 14:08:14 10splitTasks](trainer.py 286): INFO [140/157]	0.1079(0.1085)	0.0003(0.0040)	0.145(0.276)	93.75(90.87)
[2023-09-29 14:08:15 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1082)	0.0001(0.0037)	0.351(0.277)	87.50(90.71)
[2023-09-29 14:08:16 10splitTasks](trainer.py 286): INFO [156/157]	0.0797(0.1078)	0.0001(0.0036)	0.521(0.281)	87.50(90.56)
[2023-09-29 14:08:16 10splitTasks](trainer.py 288): INFO  * Train Acc 90.560
[2023-09-29 14:08:17 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.66
[2023-09-29 14:08:17 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 14:08:17 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 14:08:17 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:08:18 10splitTasks](trainer.py 286): INFO [0/157]	0.6079(0.6079)	0.4972(0.4972)	0.058(0.058)	100.00(100.00)
[2023-09-29 14:08:19 10splitTasks](trainer.py 286): INFO [10/157]	0.1027(0.1519)	0.0003(0.0456)	0.273(0.266)	87.50(90.62)
[2023-09-29 14:08:20 10splitTasks](trainer.py 286): INFO [20/157]	0.1017(0.1292)	0.0003(0.0241)	0.090(0.258)	96.88(91.22)
[2023-09-29 14:08:21 10splitTasks](trainer.py 286): INFO [30/157]	0.1046(0.1208)	0.0003(0.0164)	0.287(0.254)	93.75(91.23)
[2023-09-29 14:08:22 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1167)	0.0003(0.0125)	0.207(0.262)	93.75(91.23)
[2023-09-29 14:08:23 10splitTasks](trainer.py 286): INFO [50/157]	0.1042(0.1142)	0.0003(0.0101)	0.590(0.261)	71.88(90.99)
[2023-09-29 14:08:24 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1125)	0.0003(0.0085)	0.333(0.264)	87.50(91.14)
[2023-09-29 14:08:25 10splitTasks](trainer.py 286): INFO [70/157]	0.1029(0.1111)	0.0002(0.0073)	0.191(0.263)	96.88(91.24)
[2023-09-29 14:08:26 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1102)	0.0003(0.0065)	0.161(0.252)	96.88(91.67)
[2023-09-29 14:08:27 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1093)	0.0002(0.0058)	0.233(0.253)	87.50(91.41)
[2023-09-29 14:08:28 10splitTasks](trainer.py 286): INFO [100/157]	0.1028(0.1086)	0.0002(0.0052)	0.379(0.253)	90.62(91.43)
[2023-09-29 14:08:29 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1081)	0.0003(0.0048)	0.218(0.250)	90.62(91.50)
[2023-09-29 14:08:30 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1076)	0.0002(0.0044)	0.129(0.246)	96.88(91.71)
[2023-09-29 14:08:31 10splitTasks](trainer.py 286): INFO [130/157]	0.1050(0.1074)	0.0002(0.0041)	0.156(0.247)	96.88(91.67)
[2023-09-29 14:08:32 10splitTasks](trainer.py 286): INFO [140/157]	0.1054(0.1071)	0.0002(0.0038)	0.410(0.250)	87.50(91.69)
[2023-09-29 14:08:33 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1068)	0.0002(0.0036)	0.217(0.256)	87.50(91.31)
[2023-09-29 14:08:34 10splitTasks](trainer.py 286): INFO [156/157]	0.0810(0.1065)	0.0001(0.0035)	0.314(0.257)	87.50(91.32)
[2023-09-29 14:08:34 10splitTasks](trainer.py 288): INFO  * Train Acc 91.320
[2023-09-29 14:08:36 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.82
[2023-09-29 14:08:36 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 14:08:36 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 14:08:36 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:08:37 10splitTasks](trainer.py 286): INFO [0/157]	0.5829(0.5829)	0.4628(0.4628)	0.190(0.190)	93.75(93.75)
[2023-09-29 14:08:38 10splitTasks](trainer.py 286): INFO [10/157]	0.1076(0.1490)	0.0003(0.0424)	0.126(0.236)	96.88(91.76)
[2023-09-29 14:08:39 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1269)	0.0002(0.0223)	0.231(0.245)	93.75(91.82)
[2023-09-29 14:08:40 10splitTasks](trainer.py 286): INFO [30/157]	0.1052(0.1195)	0.0002(0.0152)	0.154(0.240)	93.75(91.83)
[2023-09-29 14:08:41 10splitTasks](trainer.py 286): INFO [40/157]	0.1039(0.1156)	0.0003(0.0116)	0.181(0.238)	93.75(92.53)
[2023-09-29 14:08:42 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1136)	0.0003(0.0094)	0.187(0.237)	96.88(92.65)
[2023-09-29 14:08:43 10splitTasks](trainer.py 286): INFO [60/157]	0.1042(0.1121)	0.0003(0.0079)	0.210(0.237)	90.62(92.26)
[2023-09-29 14:08:44 10splitTasks](trainer.py 286): INFO [70/157]	0.1020(0.1109)	0.0002(0.0068)	0.139(0.235)	96.88(92.52)
[2023-09-29 14:08:45 10splitTasks](trainer.py 286): INFO [80/157]	0.1056(0.1102)	0.0002(0.0060)	0.168(0.235)	93.75(92.59)
[2023-09-29 14:08:46 10splitTasks](trainer.py 286): INFO [90/157]	0.1029(0.1094)	0.0002(0.0054)	0.255(0.234)	87.50(92.41)
[2023-09-29 14:08:47 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1089)	0.0002(0.0049)	0.176(0.234)	90.62(92.39)
[2023-09-29 14:08:48 10splitTasks](trainer.py 286): INFO [110/157]	0.1027(0.1084)	0.0003(0.0045)	0.257(0.235)	87.50(92.31)
[2023-09-29 14:08:49 10splitTasks](trainer.py 286): INFO [120/157]	0.1077(0.1080)	0.0003(0.0041)	0.248(0.238)	90.62(92.23)
[2023-09-29 14:08:50 10splitTasks](trainer.py 286): INFO [130/157]	0.1051(0.1077)	0.0005(0.0039)	0.109(0.237)	100.00(92.20)
[2023-09-29 14:08:51 10splitTasks](trainer.py 286): INFO [140/157]	0.1189(0.1077)	0.0137(0.0037)	0.282(0.239)	90.62(92.15)
[2023-09-29 14:08:52 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1074)	0.0001(0.0035)	0.219(0.238)	90.62(92.18)
[2023-09-29 14:08:53 10splitTasks](trainer.py 286): INFO [156/157]	0.0811(0.1071)	0.0001(0.0033)	0.551(0.237)	75.00(92.16)
[2023-09-29 14:08:53 10splitTasks](trainer.py 288): INFO  * Train Acc 92.160
[2023-09-29 14:08:55 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.72
[2023-09-29 14:08:55 10splitTasks](my_trainer.py 206): INFO Pruning for task8
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 2179/2564 (84.98%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 949/1116 (85.04%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 8538/10045 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 3794/4464 (84.99%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 3794/4464 (84.99%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 3794/4464 (84.99%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 8538/10045 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 3794/4464 (84.99%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 3794/4464 (84.99%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 8538/10045 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 3794/4464 (84.99%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 7589/8928 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 34153/40180 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 15179/17858 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 30358/35715 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 15179/17858 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 34153/40180 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 15179/17858 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 15179/17858 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 34153/40180 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 15179/17858 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 15179/17858 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 34153/40180 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 15179/17858 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 30358/35715 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 136613/160721 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 121434/142864 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 136613/160721 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 136613/160721 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 136613/160721 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 136613/160721 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 136613/160721 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 60717/71432 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 121434/142864 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 546453/642886 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 242869/285728 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 485735/571453 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 242869/285728 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 546453/642886 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 242869/285728 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 242869/285728 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 546453/642886 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 242869/285728 (85.00%) (Total in layer: 1048576)
[2023-09-29 14:08:55 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 14:08:55 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 14:08:55 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 14:08:55 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:08:55 10splitTasks](trainer.py 286): INFO [0/157]	0.6131(0.6131)	0.4933(0.4933)	0.332(0.332)	90.62(90.62)
[2023-09-29 14:08:56 10splitTasks](trainer.py 286): INFO [10/157]	0.1038(0.1510)	0.0003(0.0452)	0.528(0.383)	81.25(86.93)
[2023-09-29 14:08:58 10splitTasks](trainer.py 286): INFO [20/157]	0.1025(0.1281)	0.0004(0.0238)	0.469(0.427)	84.38(85.42)
[2023-09-29 14:08:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1025(0.1204)	0.0002(0.0163)	0.143(0.402)	93.75(86.79)
[2023-09-29 14:09:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1162)	0.0003(0.0124)	0.482(0.417)	75.00(85.75)
[2023-09-29 14:09:01 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1138)	0.0002(0.0100)	0.246(0.421)	87.50(85.66)
[2023-09-29 14:09:02 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1122)	0.0003(0.0084)	0.380(0.424)	90.62(85.09)
[2023-09-29 14:09:03 10splitTasks](trainer.py 286): INFO [70/157]	0.1029(0.1111)	0.0003(0.0073)	0.161(0.412)	93.75(85.74)
[2023-09-29 14:09:04 10splitTasks](trainer.py 286): INFO [80/157]	0.1023(0.1102)	0.0002(0.0064)	0.411(0.407)	84.38(85.73)
[2023-09-29 14:09:05 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1094)	0.0003(0.0058)	0.517(0.404)	81.25(85.92)
[2023-09-29 14:09:06 10splitTasks](trainer.py 286): INFO [100/157]	0.1082(0.1090)	0.0003(0.0052)	0.436(0.412)	84.38(85.95)
[2023-09-29 14:09:07 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1084)	0.0003(0.0048)	0.479(0.416)	78.12(85.84)
[2023-09-29 14:09:08 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1079)	0.0003(0.0044)	0.460(0.414)	81.25(85.95)
[2023-09-29 14:09:09 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1075)	0.0002(0.0041)	0.218(0.410)	90.62(86.19)
[2023-09-29 14:09:10 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1071)	0.0002(0.0038)	0.309(0.407)	87.50(86.21)
[2023-09-29 14:09:11 10splitTasks](trainer.py 286): INFO [150/157]	0.1018(0.1069)	0.0001(0.0036)	0.334(0.407)	90.62(86.15)
[2023-09-29 14:09:12 10splitTasks](trainer.py 286): INFO [156/157]	0.0792(0.1065)	0.0001(0.0035)	2.395(0.410)	62.50(86.16)
[2023-09-29 14:09:12 10splitTasks](trainer.py 288): INFO  * Train Acc 86.160
[2023-09-29 14:09:13 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.77
[2023-09-29 14:09:13 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 14:09:13 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 14:09:13 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:09:14 10splitTasks](trainer.py 286): INFO [0/157]	0.6085(0.6085)	0.4983(0.4983)	0.323(0.323)	93.75(93.75)
[2023-09-29 14:09:15 10splitTasks](trainer.py 286): INFO [10/157]	0.1045(0.1500)	0.0004(0.0456)	0.419(0.335)	84.38(89.49)
[2023-09-29 14:09:16 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1279)	0.0002(0.0242)	0.286(0.328)	87.50(89.14)
[2023-09-29 14:09:17 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1205)	0.0002(0.0165)	0.499(0.340)	84.38(88.21)
[2023-09-29 14:09:18 10splitTasks](trainer.py 286): INFO [40/157]	0.1023(0.1164)	0.0003(0.0126)	0.158(0.326)	93.75(88.72)
[2023-09-29 14:09:19 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1137)	0.0002(0.0102)	0.404(0.343)	90.62(88.30)
[2023-09-29 14:09:20 10splitTasks](trainer.py 286): INFO [60/157]	0.1061(0.1121)	0.0002(0.0085)	0.737(0.350)	78.12(88.01)
[2023-09-29 14:09:21 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1109)	0.0002(0.0074)	0.213(0.351)	93.75(88.03)
[2023-09-29 14:09:22 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1101)	0.0002(0.0065)	0.378(0.354)	87.50(88.04)
[2023-09-29 14:09:23 10splitTasks](trainer.py 286): INFO [90/157]	0.1047(0.1096)	0.0004(0.0059)	0.308(0.363)	93.75(87.43)
[2023-09-29 14:09:24 10splitTasks](trainer.py 286): INFO [100/157]	0.1035(0.1090)	0.0003(0.0053)	0.358(0.360)	87.50(87.69)
[2023-09-29 14:09:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1041(0.1088)	0.0002(0.0049)	0.434(0.353)	84.38(87.78)
[2023-09-29 14:09:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1120(0.1085)	0.0003(0.0045)	0.588(0.360)	81.25(87.55)
[2023-09-29 14:09:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1031(0.1081)	0.0002(0.0042)	0.568(0.363)	78.12(87.40)
[2023-09-29 14:09:29 10splitTasks](trainer.py 286): INFO [140/157]	0.1051(0.1077)	0.0003(0.0039)	0.359(0.363)	90.62(87.41)
[2023-09-29 14:09:30 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1074)	0.0001(0.0037)	0.276(0.359)	90.62(87.46)
[2023-09-29 14:09:30 10splitTasks](trainer.py 286): INFO [156/157]	0.0803(0.1070)	0.0001(0.0035)	1.206(0.360)	62.50(87.42)
[2023-09-29 14:09:30 10splitTasks](trainer.py 288): INFO  * Train Acc 87.420
[2023-09-29 14:09:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.79
[2023-09-29 14:09:32 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 14:09:32 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 14:09:32 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:09:33 10splitTasks](trainer.py 286): INFO [0/157]	0.6569(0.6569)	0.5439(0.5439)	0.304(0.304)	87.50(87.50)
[2023-09-29 14:09:34 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1539)	0.0002(0.0498)	0.895(0.350)	81.25(88.07)
[2023-09-29 14:09:35 10splitTasks](trainer.py 286): INFO [20/157]	0.1049(0.1303)	0.0002(0.0262)	0.245(0.313)	90.62(89.43)
[2023-09-29 14:09:36 10splitTasks](trainer.py 286): INFO [30/157]	0.1060(0.1216)	0.0003(0.0179)	0.376(0.301)	90.62(89.11)
[2023-09-29 14:09:37 10splitTasks](trainer.py 286): INFO [40/157]	0.1044(0.1171)	0.0002(0.0136)	0.337(0.297)	93.75(89.48)
[2023-09-29 14:09:38 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1141)	0.0002(0.0110)	0.553(0.293)	81.25(89.52)
[2023-09-29 14:09:39 10splitTasks](trainer.py 286): INFO [60/157]	0.1027(0.1123)	0.0004(0.0092)	0.439(0.291)	78.12(89.40)
[2023-09-29 14:09:40 10splitTasks](trainer.py 286): INFO [70/157]	0.1025(0.1111)	0.0003(0.0080)	0.266(0.293)	90.62(89.44)
[2023-09-29 14:09:41 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1102)	0.0002(0.0070)	0.350(0.295)	84.38(89.39)
[2023-09-29 14:09:42 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1095)	0.0002(0.0063)	0.282(0.299)	90.62(89.11)
[2023-09-29 14:09:43 10splitTasks](trainer.py 286): INFO [100/157]	0.1043(0.1093)	0.0002(0.0057)	0.271(0.308)	87.50(88.92)
[2023-09-29 14:09:44 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1087)	0.0003(0.0052)	0.409(0.312)	84.38(88.96)
[2023-09-29 14:09:45 10splitTasks](trainer.py 286): INFO [120/157]	0.1025(0.1083)	0.0014(0.0048)	0.476(0.317)	81.25(88.87)
[2023-09-29 14:09:46 10splitTasks](trainer.py 286): INFO [130/157]	0.1018(0.1080)	0.0002(0.0045)	0.811(0.326)	65.62(88.41)
[2023-09-29 14:09:47 10splitTasks](trainer.py 286): INFO [140/157]	0.1022(0.1077)	0.0002(0.0042)	0.149(0.333)	93.75(88.14)
[2023-09-29 14:09:48 10splitTasks](trainer.py 286): INFO [150/157]	0.1018(0.1075)	0.0002(0.0039)	0.347(0.337)	90.62(88.08)
[2023-09-29 14:09:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1072)	0.0001(0.0038)	0.400(0.333)	75.00(88.22)
[2023-09-29 14:09:49 10splitTasks](trainer.py 288): INFO  * Train Acc 88.220
[2023-09-29 14:09:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.76
[2023-09-29 14:09:51 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 14:09:51 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 14:09:51 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:09:51 10splitTasks](trainer.py 286): INFO [0/157]	0.6640(0.6640)	0.5580(0.5580)	0.103(0.103)	96.88(96.88)
[2023-09-29 14:09:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1537)	0.0002(0.0510)	0.405(0.277)	87.50(88.35)
[2023-09-29 14:09:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1297)	0.0002(0.0268)	0.224(0.251)	90.62(89.88)
[2023-09-29 14:09:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1207)	0.0002(0.0183)	0.161(0.274)	93.75(89.82)
[2023-09-29 14:09:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1166)	0.0002(0.0139)	0.300(0.282)	90.62(89.63)
[2023-09-29 14:09:57 10splitTasks](trainer.py 286): INFO [50/157]	0.1051(0.1139)	0.0002(0.0112)	0.187(0.306)	93.75(88.85)
[2023-09-29 14:09:58 10splitTasks](trainer.py 286): INFO [60/157]	0.1077(0.1123)	0.0005(0.0095)	0.527(0.304)	87.50(89.09)
[2023-09-29 14:09:59 10splitTasks](trainer.py 286): INFO [70/157]	0.1055(0.1112)	0.0004(0.0082)	0.307(0.303)	87.50(89.04)
[2023-09-29 14:10:00 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1102)	0.0002(0.0072)	0.051(0.304)	100.00(89.12)
[2023-09-29 14:10:01 10splitTasks](trainer.py 286): INFO [90/157]	0.1049(0.1095)	0.0002(0.0064)	0.399(0.300)	81.25(89.29)
[2023-09-29 14:10:02 10splitTasks](trainer.py 286): INFO [100/157]	0.1068(0.1092)	0.0003(0.0058)	0.143(0.307)	96.88(89.11)
[2023-09-29 14:10:03 10splitTasks](trainer.py 286): INFO [110/157]	0.1047(0.1086)	0.0002(0.0053)	0.250(0.304)	90.62(89.10)
[2023-09-29 14:10:04 10splitTasks](trainer.py 286): INFO [120/157]	0.1047(0.1083)	0.0003(0.0049)	0.382(0.303)	87.50(89.18)
[2023-09-29 14:10:05 10splitTasks](trainer.py 286): INFO [130/157]	0.1048(0.1080)	0.0002(0.0046)	0.087(0.304)	96.88(89.27)
[2023-09-29 14:10:06 10splitTasks](trainer.py 286): INFO [140/157]	0.1162(0.1078)	0.0002(0.0043)	0.323(0.301)	90.62(89.45)
[2023-09-29 14:10:07 10splitTasks](trainer.py 286): INFO [150/157]	0.1038(0.1076)	0.0002(0.0040)	0.202(0.298)	90.62(89.51)
[2023-09-29 14:10:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0811(0.1073)	0.0001(0.0039)	0.052(0.301)	100.00(89.42)
[2023-09-29 14:10:08 10splitTasks](trainer.py 288): INFO  * Train Acc 89.420
[2023-09-29 14:10:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.200, Total time 1.68
[2023-09-29 14:10:09 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 14:10:09 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 14:10:09 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:10:10 10splitTasks](trainer.py 286): INFO [0/157]	0.6870(0.6870)	0.5777(0.5777)	0.398(0.398)	90.62(90.62)
[2023-09-29 14:10:11 10splitTasks](trainer.py 286): INFO [10/157]	0.1046(0.1572)	0.0003(0.0528)	0.198(0.266)	93.75(92.33)
[2023-09-29 14:10:12 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1323)	0.0002(0.0278)	0.200(0.270)	87.50(90.77)
[2023-09-29 14:10:13 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1230)	0.0002(0.0189)	0.441(0.267)	87.50(91.23)
[2023-09-29 14:10:14 10splitTasks](trainer.py 286): INFO [40/157]	0.1036(0.1181)	0.0002(0.0144)	0.287(0.266)	90.62(90.93)
[2023-09-29 14:10:15 10splitTasks](trainer.py 286): INFO [50/157]	0.1018(0.1153)	0.0002(0.0116)	0.299(0.250)	87.50(91.54)
[2023-09-29 14:10:16 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1133)	0.0003(0.0098)	0.184(0.245)	93.75(91.65)
[2023-09-29 14:10:17 10splitTasks](trainer.py 286): INFO [70/157]	0.1048(0.1119)	0.0003(0.0084)	0.186(0.241)	90.62(91.77)
[2023-09-29 14:10:18 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1108)	0.0003(0.0074)	0.149(0.251)	90.62(91.28)
[2023-09-29 14:10:19 10splitTasks](trainer.py 286): INFO [90/157]	0.1027(0.1100)	0.0002(0.0067)	0.297(0.253)	90.62(91.24)
[2023-09-29 14:10:21 10splitTasks](trainer.py 286): INFO [100/157]	0.1036(0.1094)	0.0002(0.0060)	0.403(0.254)	78.12(91.15)
[2023-09-29 14:10:22 10splitTasks](trainer.py 286): INFO [110/157]	0.1095(0.1089)	0.0005(0.0055)	0.055(0.252)	100.00(91.19)
[2023-09-29 14:10:23 10splitTasks](trainer.py 286): INFO [120/157]	0.1036(0.1084)	0.0003(0.0051)	0.165(0.254)	96.88(91.30)
[2023-09-29 14:10:24 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1080)	0.0003(0.0047)	0.387(0.253)	90.62(91.46)
[2023-09-29 14:10:25 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1078)	0.0003(0.0044)	0.392(0.257)	87.50(91.33)
[2023-09-29 14:10:26 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1074)	0.0001(0.0042)	0.254(0.256)	87.50(91.23)
[2023-09-29 14:10:26 10splitTasks](trainer.py 286): INFO [156/157]	0.0774(0.1070)	0.0001(0.0040)	1.129(0.260)	75.00(91.08)
[2023-09-29 14:10:26 10splitTasks](trainer.py 288): INFO  * Train Acc 91.080
[2023-09-29 14:10:28 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.71
[2023-09-29 14:10:28 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 14:10:28 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 14:10:28 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:10:29 10splitTasks](trainer.py 286): INFO [0/157]	0.6371(0.6371)	0.5185(0.5185)	0.033(0.033)	100.00(100.00)
[2023-09-29 14:10:30 10splitTasks](trainer.py 286): INFO [10/157]	0.1046(0.1519)	0.0001(0.0475)	0.160(0.221)	96.88(92.05)
[2023-09-29 14:10:31 10splitTasks](trainer.py 286): INFO [20/157]	0.1019(0.1291)	0.0002(0.0250)	0.376(0.256)	84.38(90.48)
[2023-09-29 14:10:32 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1208)	0.0003(0.0170)	0.101(0.237)	93.75(91.43)
[2023-09-29 14:10:33 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1166)	0.0002(0.0130)	0.255(0.229)	90.62(91.69)
[2023-09-29 14:10:34 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1142)	0.0003(0.0105)	0.122(0.226)	96.88(91.54)
[2023-09-29 14:10:35 10splitTasks](trainer.py 286): INFO [60/157]	0.1022(0.1124)	0.0003(0.0088)	0.212(0.220)	87.50(91.60)
[2023-09-29 14:10:36 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1110)	0.0002(0.0076)	0.133(0.229)	93.75(91.37)
[2023-09-29 14:10:37 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1099)	0.0003(0.0067)	0.266(0.222)	90.62(91.59)
[2023-09-29 14:10:38 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1092)	0.0002(0.0060)	0.463(0.233)	87.50(91.41)
[2023-09-29 14:10:39 10splitTasks](trainer.py 286): INFO [100/157]	0.1012(0.1086)	0.0002(0.0055)	0.273(0.235)	90.62(91.43)
[2023-09-29 14:10:40 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1080)	0.0002(0.0050)	0.065(0.234)	100.00(91.41)
[2023-09-29 14:10:41 10splitTasks](trainer.py 286): INFO [120/157]	0.1021(0.1076)	0.0002(0.0046)	0.428(0.244)	90.62(91.14)
[2023-09-29 14:10:42 10splitTasks](trainer.py 286): INFO [130/157]	0.1042(0.1074)	0.0002(0.0043)	0.178(0.242)	93.75(91.27)
[2023-09-29 14:10:43 10splitTasks](trainer.py 286): INFO [140/157]	0.1028(0.1071)	0.0005(0.0040)	0.684(0.243)	75.00(91.25)
[2023-09-29 14:10:44 10splitTasks](trainer.py 286): INFO [150/157]	0.1017(0.1070)	0.0001(0.0038)	0.219(0.245)	90.62(91.12)
[2023-09-29 14:10:45 10splitTasks](trainer.py 286): INFO [156/157]	0.0775(0.1066)	0.0001(0.0036)	0.252(0.246)	87.50(91.02)
[2023-09-29 14:10:45 10splitTasks](trainer.py 288): INFO  * Train Acc 91.020
[2023-09-29 14:10:47 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.400, Total time 1.66
[2023-09-29 14:10:47 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 14:10:47 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 14:10:47 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:10:47 10splitTasks](trainer.py 286): INFO [0/157]	0.6513(0.6513)	0.5460(0.5460)	0.247(0.247)	87.50(87.50)
[2023-09-29 14:10:48 10splitTasks](trainer.py 286): INFO [10/157]	0.1196(0.1578)	0.0002(0.0499)	0.237(0.199)	90.62(92.05)
[2023-09-29 14:10:49 10splitTasks](trainer.py 286): INFO [20/157]	0.1024(0.1323)	0.0005(0.0263)	0.109(0.241)	96.88(91.22)
[2023-09-29 14:10:50 10splitTasks](trainer.py 286): INFO [30/157]	0.1022(0.1234)	0.0002(0.0179)	0.076(0.239)	96.88(91.53)
[2023-09-29 14:10:51 10splitTasks](trainer.py 286): INFO [40/157]	0.1057(0.1188)	0.0002(0.0136)	0.271(0.226)	87.50(91.69)
[2023-09-29 14:10:53 10splitTasks](trainer.py 286): INFO [50/157]	0.1026(0.1158)	0.0002(0.0110)	0.256(0.219)	90.62(91.91)
[2023-09-29 14:10:54 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1138)	0.0002(0.0093)	0.354(0.216)	87.50(92.06)
[2023-09-29 14:10:55 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1126)	0.0002(0.0080)	0.192(0.226)	90.62(91.90)
[2023-09-29 14:10:56 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1117)	0.0002(0.0071)	0.162(0.219)	90.62(92.13)
[2023-09-29 14:10:57 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1107)	0.0003(0.0063)	0.391(0.219)	84.38(92.17)
[2023-09-29 14:10:58 10splitTasks](trainer.py 286): INFO [100/157]	0.1040(0.1100)	0.0002(0.0057)	0.198(0.218)	90.62(92.20)
[2023-09-29 14:10:59 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1095)	0.0003(0.0053)	0.175(0.221)	96.88(92.15)
[2023-09-29 14:11:00 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1090)	0.0002(0.0049)	0.127(0.221)	93.75(92.12)
[2023-09-29 14:11:01 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1086)	0.0003(0.0045)	0.294(0.222)	87.50(92.06)
[2023-09-29 14:11:02 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1082)	0.0003(0.0042)	0.071(0.222)	96.88(92.09)
[2023-09-29 14:11:03 10splitTasks](trainer.py 286): INFO [150/157]	0.1042(0.1079)	0.0002(0.0040)	0.193(0.221)	90.62(92.12)
[2023-09-29 14:11:03 10splitTasks](trainer.py 286): INFO [156/157]	0.0799(0.1075)	0.0001(0.0038)	1.182(0.222)	37.50(92.00)
[2023-09-29 14:11:04 10splitTasks](trainer.py 288): INFO  * Train Acc 92.000
[2023-09-29 14:11:05 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.200, Total time 1.79
[2023-09-29 14:11:05 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 14:11:05 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 14:11:05 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:11:06 10splitTasks](trainer.py 286): INFO [0/157]	0.6640(0.6640)	0.5521(0.5521)	0.329(0.329)	81.25(81.25)
[2023-09-29 14:11:07 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1541)	0.0004(0.0505)	0.118(0.250)	96.88(90.62)
[2023-09-29 14:11:08 10splitTasks](trainer.py 286): INFO [20/157]	0.1020(0.1297)	0.0002(0.0266)	0.392(0.229)	84.38(91.37)
[2023-09-29 14:11:09 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1211)	0.0003(0.0181)	0.294(0.244)	96.88(90.83)
[2023-09-29 14:11:10 10splitTasks](trainer.py 286): INFO [40/157]	0.1021(0.1168)	0.0003(0.0138)	0.476(0.242)	84.38(91.23)
[2023-09-29 14:11:11 10splitTasks](trainer.py 286): INFO [50/157]	0.1040(0.1140)	0.0003(0.0111)	0.256(0.247)	90.62(90.99)
[2023-09-29 14:11:12 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1122)	0.0003(0.0094)	0.226(0.241)	93.75(91.34)
[2023-09-29 14:11:13 10splitTasks](trainer.py 286): INFO [70/157]	0.1021(0.1112)	0.0003(0.0081)	0.333(0.239)	81.25(91.33)
[2023-09-29 14:11:14 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1101)	0.0003(0.0071)	0.273(0.236)	90.62(91.44)
[2023-09-29 14:11:15 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1096)	0.0003(0.0064)	0.181(0.234)	96.88(91.41)
[2023-09-29 14:11:16 10splitTasks](trainer.py 286): INFO [100/157]	0.1024(0.1092)	0.0003(0.0058)	0.116(0.240)	93.75(91.18)
[2023-09-29 14:11:17 10splitTasks](trainer.py 286): INFO [110/157]	0.1022(0.1086)	0.0003(0.0053)	0.097(0.237)	96.88(91.36)
[2023-09-29 14:11:18 10splitTasks](trainer.py 286): INFO [120/157]	0.1055(0.1082)	0.0003(0.0049)	0.102(0.232)	93.75(91.53)
[2023-09-29 14:11:20 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1078)	0.0003(0.0045)	0.215(0.230)	93.75(91.65)
[2023-09-29 14:11:21 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1076)	0.0002(0.0042)	0.823(0.230)	81.25(91.73)
[2023-09-29 14:11:22 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1073)	0.0001(0.0040)	0.141(0.228)	93.75(91.83)
[2023-09-29 14:11:22 10splitTasks](trainer.py 286): INFO [156/157]	0.0777(0.1069)	0.0001(0.0038)	1.302(0.230)	62.50(91.80)
[2023-09-29 14:11:22 10splitTasks](trainer.py 288): INFO  * Train Acc 91.800
[2023-09-29 14:11:24 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.75
[2023-09-29 14:11:24 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 14:11:24 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 14:11:24 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:11:25 10splitTasks](trainer.py 286): INFO [0/157]	0.6322(0.6322)	0.5102(0.5102)	0.331(0.331)	87.50(87.50)
[2023-09-29 14:11:26 10splitTasks](trainer.py 286): INFO [10/157]	0.1088(0.1532)	0.0010(0.0467)	0.263(0.206)	96.88(94.32)
[2023-09-29 14:11:27 10splitTasks](trainer.py 286): INFO [20/157]	0.1033(0.1291)	0.0002(0.0246)	0.102(0.193)	100.00(93.75)
[2023-09-29 14:11:28 10splitTasks](trainer.py 286): INFO [30/157]	0.1093(0.1212)	0.0002(0.0168)	0.067(0.191)	96.88(93.85)
[2023-09-29 14:11:29 10splitTasks](trainer.py 286): INFO [40/157]	0.1054(0.1170)	0.0003(0.0128)	0.281(0.195)	90.62(93.60)
[2023-09-29 14:11:30 10splitTasks](trainer.py 286): INFO [50/157]	0.1083(0.1146)	0.0006(0.0103)	0.372(0.203)	87.50(93.08)
[2023-09-29 14:11:31 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1129)	0.0002(0.0087)	0.076(0.197)	96.88(93.24)
[2023-09-29 14:11:32 10splitTasks](trainer.py 286): INFO [70/157]	0.1146(0.1118)	0.0003(0.0075)	0.300(0.197)	87.50(93.13)
[2023-09-29 14:11:33 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1110)	0.0003(0.0067)	0.243(0.199)	87.50(92.90)
[2023-09-29 14:11:34 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1103)	0.0003(0.0060)	0.197(0.205)	96.88(92.75)
[2023-09-29 14:11:35 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1097)	0.0002(0.0054)	0.307(0.205)	90.62(92.76)
[2023-09-29 14:11:36 10splitTasks](trainer.py 286): INFO [110/157]	0.1085(0.1092)	0.0006(0.0050)	0.119(0.205)	96.88(92.82)
[2023-09-29 14:11:37 10splitTasks](trainer.py 286): INFO [120/157]	0.1040(0.1087)	0.0002(0.0046)	0.023(0.200)	100.00(93.03)
[2023-09-29 14:11:38 10splitTasks](trainer.py 286): INFO [130/157]	0.1029(0.1083)	0.0002(0.0043)	0.074(0.196)	96.88(93.08)
[2023-09-29 14:11:39 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1080)	0.0002(0.0040)	0.115(0.205)	96.88(92.77)
[2023-09-29 14:11:40 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1077)	0.0002(0.0038)	0.138(0.204)	93.75(92.80)
[2023-09-29 14:11:41 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1074)	0.0001(0.0037)	0.327(0.205)	87.50(92.72)
[2023-09-29 14:11:41 10splitTasks](trainer.py 288): INFO  * Train Acc 92.720
[2023-09-29 14:11:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.89
[2023-09-29 14:11:43 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 14:11:43 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 14:11:43 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:11:44 10splitTasks](trainer.py 286): INFO [0/157]	0.7522(0.7522)	0.6434(0.6434)	0.238(0.238)	93.75(93.75)
[2023-09-29 14:11:45 10splitTasks](trainer.py 286): INFO [10/157]	0.1031(0.1623)	0.0002(0.0588)	0.282(0.173)	84.38(93.18)
[2023-09-29 14:11:46 10splitTasks](trainer.py 286): INFO [20/157]	0.1100(0.1344)	0.0002(0.0309)	0.125(0.171)	96.88(93.45)
[2023-09-29 14:11:47 10splitTasks](trainer.py 286): INFO [30/157]	0.1028(0.1253)	0.0003(0.0211)	0.424(0.177)	84.38(93.04)
[2023-09-29 14:11:48 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1200)	0.0003(0.0160)	0.080(0.176)	100.00(93.06)
[2023-09-29 14:11:49 10splitTasks](trainer.py 286): INFO [50/157]	0.1023(0.1169)	0.0004(0.0129)	0.110(0.169)	93.75(93.57)
[2023-09-29 14:11:50 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1146)	0.0002(0.0109)	0.110(0.167)	93.75(93.65)
[2023-09-29 14:11:51 10splitTasks](trainer.py 286): INFO [70/157]	0.1039(0.1130)	0.0003(0.0094)	0.156(0.164)	93.75(93.79)
[2023-09-29 14:11:52 10splitTasks](trainer.py 286): INFO [80/157]	0.1020(0.1119)	0.0003(0.0083)	0.202(0.170)	93.75(93.60)
[2023-09-29 14:11:53 10splitTasks](trainer.py 286): INFO [90/157]	0.1023(0.1110)	0.0002(0.0074)	0.151(0.168)	93.75(93.72)
[2023-09-29 14:11:54 10splitTasks](trainer.py 286): INFO [100/157]	0.1023(0.1102)	0.0002(0.0067)	0.280(0.164)	90.62(93.94)
[2023-09-29 14:11:55 10splitTasks](trainer.py 286): INFO [110/157]	0.1023(0.1096)	0.0002(0.0061)	0.195(0.164)	87.50(93.98)
[2023-09-29 14:11:56 10splitTasks](trainer.py 286): INFO [120/157]	0.1031(0.1092)	0.0002(0.0056)	0.047(0.166)	100.00(93.96)
[2023-09-29 14:11:57 10splitTasks](trainer.py 286): INFO [130/157]	0.1017(0.1088)	0.0003(0.0052)	0.122(0.166)	96.88(94.01)
[2023-09-29 14:11:58 10splitTasks](trainer.py 286): INFO [140/157]	0.1047(0.1084)	0.0002(0.0049)	0.232(0.166)	93.75(94.02)
[2023-09-29 14:11:59 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1080)	0.0002(0.0046)	0.358(0.169)	90.62(93.96)
[2023-09-29 14:12:00 10splitTasks](trainer.py 286): INFO [156/157]	0.0784(0.1076)	0.0001(0.0044)	0.565(0.168)	62.50(94.00)
[2023-09-29 14:12:00 10splitTasks](trainer.py 288): INFO  * Train Acc 94.000
[2023-09-29 14:12:02 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.77
[2023-09-29 14:12:02 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 14:12:02 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 14:12:02 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:12:02 10splitTasks](trainer.py 286): INFO [0/157]	0.7792(0.7792)	0.6682(0.6682)	0.083(0.083)	96.88(96.88)
[2023-09-29 14:12:04 10splitTasks](trainer.py 286): INFO [10/157]	0.1123(0.1713)	0.0006(0.0654)	0.534(0.197)	84.38(93.47)
[2023-09-29 14:12:05 10splitTasks](trainer.py 286): INFO [20/157]	0.1036(0.1401)	0.0002(0.0345)	0.188(0.158)	90.62(94.35)
[2023-09-29 14:12:06 10splitTasks](trainer.py 286): INFO [30/157]	0.1054(0.1285)	0.0002(0.0235)	0.494(0.189)	84.38(93.04)
[2023-09-29 14:12:07 10splitTasks](trainer.py 286): INFO [40/157]	0.1175(0.1234)	0.0007(0.0179)	0.070(0.175)	96.88(93.52)
[2023-09-29 14:12:08 10splitTasks](trainer.py 286): INFO [50/157]	0.1034(0.1202)	0.0002(0.0144)	0.202(0.178)	93.75(93.26)
[2023-09-29 14:12:09 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1177)	0.0002(0.0121)	0.126(0.191)	93.75(92.67)
[2023-09-29 14:12:10 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1158)	0.0003(0.0104)	0.157(0.189)	90.62(92.74)
[2023-09-29 14:12:11 10splitTasks](trainer.py 286): INFO [80/157]	0.1033(0.1142)	0.0003(0.0092)	0.121(0.185)	96.88(92.98)
[2023-09-29 14:12:12 10splitTasks](trainer.py 286): INFO [90/157]	0.1044(0.1130)	0.0003(0.0082)	0.090(0.185)	96.88(93.03)
[2023-09-29 14:12:13 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1120)	0.0002(0.0074)	0.227(0.190)	90.62(92.82)
[2023-09-29 14:12:14 10splitTasks](trainer.py 286): INFO [110/157]	0.1092(0.1114)	0.0006(0.0068)	0.024(0.185)	100.00(93.05)
[2023-09-29 14:12:15 10splitTasks](trainer.py 286): INFO [120/157]	0.1020(0.1108)	0.0002(0.0063)	0.170(0.185)	90.62(93.05)
[2023-09-29 14:12:16 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1102)	0.0002(0.0058)	0.458(0.182)	84.38(93.15)
[2023-09-29 14:12:17 10splitTasks](trainer.py 286): INFO [140/157]	0.1134(0.1097)	0.0002(0.0054)	0.089(0.181)	96.88(93.15)
[2023-09-29 14:12:18 10splitTasks](trainer.py 286): INFO [150/157]	0.1026(0.1093)	0.0002(0.0051)	0.262(0.180)	90.62(93.23)
[2023-09-29 14:12:19 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1089)	0.0001(0.0049)	0.186(0.177)	87.50(93.34)
[2023-09-29 14:12:19 10splitTasks](trainer.py 288): INFO  * Train Acc 93.340
[2023-09-29 14:12:21 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.400, Total time 1.74
[2023-09-29 14:12:21 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 14:12:21 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 14:12:21 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:12:21 10splitTasks](trainer.py 286): INFO [0/157]	0.6222(0.6222)	0.5090(0.5090)	0.093(0.093)	96.88(96.88)
[2023-09-29 14:12:22 10splitTasks](trainer.py 286): INFO [10/157]	0.1058(0.1526)	0.0002(0.0467)	0.087(0.145)	100.00(94.89)
[2023-09-29 14:12:23 10splitTasks](trainer.py 286): INFO [20/157]	0.1055(0.1302)	0.0002(0.0248)	0.180(0.131)	93.75(95.39)
[2023-09-29 14:12:24 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1219)	0.0002(0.0169)	0.243(0.126)	90.62(95.56)
[2023-09-29 14:12:26 10splitTasks](trainer.py 286): INFO [40/157]	0.1100(0.1177)	0.0002(0.0129)	0.203(0.136)	96.88(95.50)
[2023-09-29 14:12:27 10splitTasks](trainer.py 286): INFO [50/157]	0.1024(0.1152)	0.0003(0.0104)	0.113(0.133)	96.88(95.65)
[2023-09-29 14:12:28 10splitTasks](trainer.py 286): INFO [60/157]	0.1073(0.1136)	0.0002(0.0088)	0.442(0.134)	90.62(95.65)
[2023-09-29 14:12:29 10splitTasks](trainer.py 286): INFO [70/157]	0.1051(0.1122)	0.0003(0.0076)	0.082(0.148)	96.88(94.98)
[2023-09-29 14:12:30 10splitTasks](trainer.py 286): INFO [80/157]	0.1021(0.1115)	0.0003(0.0067)	0.241(0.149)	93.75(94.98)
[2023-09-29 14:12:31 10splitTasks](trainer.py 286): INFO [90/157]	0.1017(0.1106)	0.0003(0.0060)	0.155(0.147)	96.88(95.09)
[2023-09-29 14:12:32 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1097)	0.0003(0.0055)	0.085(0.144)	96.88(95.17)
[2023-09-29 14:12:33 10splitTasks](trainer.py 286): INFO [110/157]	0.1051(0.1092)	0.0002(0.0050)	0.036(0.146)	100.00(95.02)
[2023-09-29 14:12:34 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1087)	0.0002(0.0046)	0.033(0.150)	100.00(94.94)
[2023-09-29 14:12:35 10splitTasks](trainer.py 286): INFO [130/157]	0.1162(0.1084)	0.0003(0.0043)	0.206(0.151)	90.62(94.94)
[2023-09-29 14:12:36 10splitTasks](trainer.py 286): INFO [140/157]	0.1025(0.1080)	0.0003(0.0040)	0.119(0.156)	93.75(94.75)
[2023-09-29 14:12:37 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1077)	0.0001(0.0038)	0.385(0.158)	84.38(94.64)
[2023-09-29 14:12:38 10splitTasks](trainer.py 286): INFO [156/157]	0.0777(0.1073)	0.0001(0.0036)	0.043(0.160)	100.00(94.58)
[2023-09-29 14:12:38 10splitTasks](trainer.py 288): INFO  * Train Acc 94.580
[2023-09-29 14:12:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.400, Total time 1.72
[2023-09-29 14:12:39 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 14:12:39 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 14:12:39 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:12:40 10splitTasks](trainer.py 286): INFO [0/157]	0.5868(0.5868)	0.4713(0.4713)	0.144(0.144)	93.75(93.75)
[2023-09-29 14:12:41 10splitTasks](trainer.py 286): INFO [10/157]	0.1015(0.1480)	0.0002(0.0432)	0.076(0.107)	100.00(97.16)
[2023-09-29 14:12:42 10splitTasks](trainer.py 286): INFO [20/157]	0.1117(0.1273)	0.0005(0.0228)	0.039(0.132)	100.00(95.24)
[2023-09-29 14:12:43 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.1195)	0.0002(0.0155)	0.261(0.158)	87.50(94.66)
[2023-09-29 14:12:44 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1158)	0.0002(0.0118)	0.021(0.142)	100.00(95.12)
[2023-09-29 14:12:45 10splitTasks](trainer.py 286): INFO [50/157]	0.1017(0.1132)	0.0002(0.0095)	0.062(0.146)	100.00(94.91)
[2023-09-29 14:12:46 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1114)	0.0003(0.0080)	0.066(0.137)	96.88(95.34)
[2023-09-29 14:12:47 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1104)	0.0003(0.0069)	0.273(0.137)	90.62(95.33)
[2023-09-29 14:12:48 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1097)	0.0002(0.0061)	0.213(0.142)	90.62(95.25)
[2023-09-29 14:12:49 10splitTasks](trainer.py 286): INFO [90/157]	0.1112(0.1091)	0.0005(0.0055)	0.105(0.142)	93.75(95.19)
[2023-09-29 14:12:50 10splitTasks](trainer.py 286): INFO [100/157]	0.1017(0.1085)	0.0003(0.0050)	0.017(0.138)	100.00(95.30)
[2023-09-29 14:12:51 10splitTasks](trainer.py 286): INFO [110/157]	0.1055(0.1081)	0.0003(0.0046)	0.065(0.135)	96.88(95.35)
[2023-09-29 14:12:52 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1076)	0.0003(0.0042)	0.191(0.136)	93.75(95.33)
[2023-09-29 14:12:53 10splitTasks](trainer.py 286): INFO [130/157]	0.1061(0.1073)	0.0002(0.0039)	0.107(0.137)	93.75(95.28)
[2023-09-29 14:12:54 10splitTasks](trainer.py 286): INFO [140/157]	0.1042(0.1070)	0.0003(0.0037)	0.108(0.138)	96.88(95.39)
[2023-09-29 14:12:55 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1067)	0.0002(0.0035)	0.058(0.137)	100.00(95.45)
[2023-09-29 14:12:56 10splitTasks](trainer.py 286): INFO [156/157]	0.0779(0.1064)	0.0001(0.0033)	0.752(0.139)	87.50(95.38)
[2023-09-29 14:12:56 10splitTasks](trainer.py 288): INFO  * Train Acc 95.380
[2023-09-29 14:12:58 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.82
[2023-09-29 14:12:58 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 14:12:58 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 14:12:58 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:12:59 10splitTasks](trainer.py 286): INFO [0/157]	0.6377(0.6377)	0.5260(0.5260)	0.024(0.024)	100.00(100.00)
[2023-09-29 14:13:00 10splitTasks](trainer.py 286): INFO [10/157]	0.1120(0.1529)	0.0002(0.0481)	0.108(0.125)	96.88(95.74)
[2023-09-29 14:13:01 10splitTasks](trainer.py 286): INFO [20/157]	0.1022(0.1300)	0.0002(0.0253)	0.353(0.130)	87.50(95.68)
[2023-09-29 14:13:02 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1217)	0.0003(0.0173)	0.098(0.127)	100.00(95.77)
[2023-09-29 14:13:03 10splitTasks](trainer.py 286): INFO [40/157]	0.1019(0.1174)	0.0002(0.0131)	0.169(0.146)	90.62(94.74)
[2023-09-29 14:13:04 10splitTasks](trainer.py 286): INFO [50/157]	0.1116(0.1158)	0.0003(0.0106)	0.100(0.145)	96.88(95.04)
[2023-09-29 14:13:05 10splitTasks](trainer.py 286): INFO [60/157]	0.1085(0.1141)	0.0005(0.0090)	0.107(0.138)	96.88(95.44)
[2023-09-29 14:13:06 10splitTasks](trainer.py 286): INFO [70/157]	0.1026(0.1125)	0.0002(0.0077)	0.297(0.136)	87.50(95.55)
[2023-09-29 14:13:07 10splitTasks](trainer.py 286): INFO [80/157]	0.1053(0.1116)	0.0002(0.0068)	0.073(0.133)	100.00(95.60)
[2023-09-29 14:13:08 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1109)	0.0003(0.0061)	0.182(0.133)	93.75(95.57)
[2023-09-29 14:13:09 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1104)	0.0002(0.0055)	0.167(0.129)	93.75(95.73)
[2023-09-29 14:13:10 10splitTasks](trainer.py 286): INFO [110/157]	0.1059(0.1098)	0.0002(0.0051)	0.139(0.129)	96.88(95.69)
[2023-09-29 14:13:11 10splitTasks](trainer.py 286): INFO [120/157]	0.1030(0.1093)	0.0002(0.0047)	0.053(0.128)	100.00(95.79)
[2023-09-29 14:13:12 10splitTasks](trainer.py 286): INFO [130/157]	0.1022(0.1089)	0.0002(0.0043)	0.257(0.129)	87.50(95.75)
[2023-09-29 14:13:13 10splitTasks](trainer.py 286): INFO [140/157]	0.1023(0.1085)	0.0003(0.0041)	0.219(0.132)	93.75(95.77)
[2023-09-29 14:13:14 10splitTasks](trainer.py 286): INFO [150/157]	0.1028(0.1083)	0.0002(0.0038)	0.128(0.130)	93.75(95.74)
[2023-09-29 14:13:15 10splitTasks](trainer.py 286): INFO [156/157]	0.0806(0.1079)	0.0001(0.0037)	0.792(0.132)	87.50(95.68)
[2023-09-29 14:13:15 10splitTasks](trainer.py 288): INFO  * Train Acc 95.680
[2023-09-29 14:13:17 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.400, Total time 1.73
[2023-09-29 14:13:17 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 14:13:17 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 14:13:17 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:13:17 10splitTasks](trainer.py 286): INFO [0/157]	0.6331(0.6331)	0.5183(0.5183)	0.058(0.058)	100.00(100.00)
[2023-09-29 14:13:19 10splitTasks](trainer.py 286): INFO [10/157]	0.1039(0.1566)	0.0002(0.0508)	0.033(0.107)	100.00(96.59)
[2023-09-29 14:13:20 10splitTasks](trainer.py 286): INFO [20/157]	0.1062(0.1316)	0.0002(0.0268)	0.085(0.102)	96.88(96.58)
[2023-09-29 14:13:21 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1227)	0.0002(0.0183)	0.070(0.112)	100.00(96.07)
[2023-09-29 14:13:22 10splitTasks](trainer.py 286): INFO [40/157]	0.1035(0.1181)	0.0002(0.0139)	0.185(0.114)	93.75(96.04)
[2023-09-29 14:13:23 10splitTasks](trainer.py 286): INFO [50/157]	0.1021(0.1152)	0.0002(0.0112)	0.207(0.110)	90.62(96.08)
[2023-09-29 14:13:24 10splitTasks](trainer.py 286): INFO [60/157]	0.1097(0.1134)	0.0006(0.0094)	0.039(0.116)	100.00(95.90)
[2023-09-29 14:13:25 10splitTasks](trainer.py 286): INFO [70/157]	0.1018(0.1123)	0.0002(0.0082)	0.137(0.111)	93.75(96.17)
[2023-09-29 14:13:26 10splitTasks](trainer.py 286): INFO [80/157]	0.1105(0.1113)	0.0003(0.0072)	0.114(0.120)	96.88(96.03)
[2023-09-29 14:13:27 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1104)	0.0002(0.0065)	0.044(0.120)	100.00(95.91)
[2023-09-29 14:13:28 10splitTasks](trainer.py 286): INFO [100/157]	0.1019(0.1096)	0.0003(0.0059)	0.112(0.122)	96.88(95.95)
[2023-09-29 14:13:29 10splitTasks](trainer.py 286): INFO [110/157]	0.1161(0.1092)	0.0098(0.0055)	0.050(0.121)	96.88(95.89)
[2023-09-29 14:13:30 10splitTasks](trainer.py 286): INFO [120/157]	0.1019(0.1087)	0.0002(0.0050)	0.094(0.120)	96.88(95.89)
[2023-09-29 14:13:31 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1083)	0.0002(0.0047)	0.088(0.118)	96.88(95.97)
[2023-09-29 14:13:32 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1079)	0.0003(0.0044)	0.099(0.117)	96.88(95.97)
[2023-09-29 14:13:33 10splitTasks](trainer.py 286): INFO [150/157]	0.1022(0.1077)	0.0002(0.0041)	0.060(0.122)	100.00(95.82)
[2023-09-29 14:13:34 10splitTasks](trainer.py 286): INFO [156/157]	0.0810(0.1073)	0.0001(0.0040)	0.378(0.123)	75.00(95.76)
[2023-09-29 14:13:34 10splitTasks](trainer.py 288): INFO  * Train Acc 95.760
[2023-09-29 14:13:36 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.75
[2023-09-29 14:13:36 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 14:13:36 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 14:13:36 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:13:36 10splitTasks](trainer.py 286): INFO [0/157]	0.6298(0.6298)	0.5117(0.5117)	0.035(0.035)	100.00(100.00)
[2023-09-29 14:13:37 10splitTasks](trainer.py 286): INFO [10/157]	0.1021(0.1527)	0.0001(0.0468)	0.113(0.120)	93.75(95.74)
[2023-09-29 14:13:38 10splitTasks](trainer.py 286): INFO [20/157]	0.1018(0.1293)	0.0002(0.0247)	0.080(0.120)	100.00(96.28)
[2023-09-29 14:13:39 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1222)	0.0002(0.0168)	0.158(0.120)	96.88(96.17)
[2023-09-29 14:13:40 10splitTasks](trainer.py 286): INFO [40/157]	0.1036(0.1177)	0.0002(0.0128)	0.323(0.148)	87.50(95.27)
[2023-09-29 14:13:41 10splitTasks](trainer.py 286): INFO [50/157]	0.1024(0.1152)	0.0003(0.0104)	0.148(0.142)	93.75(95.40)
[2023-09-29 14:13:42 10splitTasks](trainer.py 286): INFO [60/157]	0.1059(0.1134)	0.0002(0.0087)	0.201(0.140)	90.62(95.49)
[2023-09-29 14:13:44 10splitTasks](trainer.py 286): INFO [70/157]	0.1058(0.1124)	0.0002(0.0075)	0.118(0.135)	100.00(95.69)
[2023-09-29 14:13:45 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1113)	0.0002(0.0066)	0.044(0.131)	100.00(95.79)
[2023-09-29 14:13:46 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1104)	0.0002(0.0060)	0.169(0.130)	90.62(95.78)
[2023-09-29 14:13:47 10splitTasks](trainer.py 286): INFO [100/157]	0.1209(0.1101)	0.0005(0.0054)	0.079(0.130)	96.88(95.79)
[2023-09-29 14:13:48 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1095)	0.0003(0.0049)	0.283(0.132)	93.75(95.66)
[2023-09-29 14:13:49 10splitTasks](trainer.py 286): INFO [120/157]	0.1113(0.1090)	0.0006(0.0046)	0.057(0.131)	96.88(95.74)
[2023-09-29 14:13:50 10splitTasks](trainer.py 286): INFO [130/157]	0.1039(0.1087)	0.0005(0.0043)	0.125(0.130)	96.88(95.73)
[2023-09-29 14:13:51 10splitTasks](trainer.py 286): INFO [140/157]	0.1148(0.1084)	0.0009(0.0040)	0.042(0.132)	96.88(95.63)
[2023-09-29 14:13:52 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1081)	0.0001(0.0038)	0.019(0.131)	100.00(95.59)
[2023-09-29 14:13:52 10splitTasks](trainer.py 286): INFO [156/157]	0.0781(0.1077)	0.0001(0.0036)	0.806(0.131)	75.00(95.54)
[2023-09-29 14:13:53 10splitTasks](trainer.py 288): INFO  * Train Acc 95.540
[2023-09-29 14:13:54 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.69
[2023-09-29 14:13:54 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 14:13:54 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 14:13:54 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:13:55 10splitTasks](trainer.py 286): INFO [0/157]	0.6379(0.6379)	0.5144(0.5144)	0.101(0.101)	96.88(96.88)
[2023-09-29 14:13:56 10splitTasks](trainer.py 286): INFO [10/157]	0.1017(0.1540)	0.0003(0.0471)	0.122(0.136)	93.75(94.89)
[2023-09-29 14:13:57 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1299)	0.0003(0.0248)	0.271(0.128)	93.75(95.54)
[2023-09-29 14:13:58 10splitTasks](trainer.py 286): INFO [30/157]	0.1047(0.1211)	0.0002(0.0169)	0.186(0.136)	93.75(95.46)
[2023-09-29 14:13:59 10splitTasks](trainer.py 286): INFO [40/157]	0.1018(0.1168)	0.0003(0.0129)	0.017(0.131)	100.00(95.58)
[2023-09-29 14:14:00 10splitTasks](trainer.py 286): INFO [50/157]	0.1084(0.1147)	0.0003(0.0104)	0.063(0.136)	100.00(95.40)
[2023-09-29 14:14:01 10splitTasks](trainer.py 286): INFO [60/157]	0.1075(0.1131)	0.0003(0.0088)	0.132(0.129)	93.75(95.70)
[2023-09-29 14:14:02 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1118)	0.0002(0.0076)	0.066(0.124)	100.00(95.77)
[2023-09-29 14:14:03 10splitTasks](trainer.py 286): INFO [80/157]	0.1022(0.1108)	0.0002(0.0067)	0.125(0.127)	93.75(95.68)
[2023-09-29 14:14:04 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1103)	0.0002(0.0060)	0.171(0.127)	90.62(95.67)
[2023-09-29 14:14:05 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1096)	0.0002(0.0054)	0.093(0.128)	100.00(95.67)
[2023-09-29 14:14:06 10splitTasks](trainer.py 286): INFO [110/157]	0.1025(0.1091)	0.0002(0.0050)	0.050(0.123)	100.00(95.83)
[2023-09-29 14:14:07 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1087)	0.0003(0.0046)	0.230(0.123)	93.75(95.89)
[2023-09-29 14:14:08 10splitTasks](trainer.py 286): INFO [130/157]	0.1029(0.1084)	0.0003(0.0043)	0.126(0.123)	93.75(95.83)
[2023-09-29 14:14:09 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1080)	0.0003(0.0040)	0.052(0.121)	96.88(95.88)
[2023-09-29 14:14:11 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1077)	0.0001(0.0037)	0.087(0.120)	96.88(95.86)
[2023-09-29 14:14:11 10splitTasks](trainer.py 286): INFO [156/157]	0.0773(0.1073)	0.0001(0.0036)	0.178(0.120)	100.00(95.84)
[2023-09-29 14:14:11 10splitTasks](trainer.py 288): INFO  * Train Acc 95.840
[2023-09-29 14:14:13 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.69
[2023-09-29 14:14:13 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 14:14:13 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 14:14:13 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:14:14 10splitTasks](trainer.py 286): INFO [0/157]	0.6806(0.6806)	0.5731(0.5731)	0.093(0.093)	100.00(100.00)
[2023-09-29 14:14:15 10splitTasks](trainer.py 286): INFO [10/157]	0.1047(0.1590)	0.0003(0.0524)	0.099(0.095)	100.00(97.44)
[2023-09-29 14:14:16 10splitTasks](trainer.py 286): INFO [20/157]	0.1078(0.1336)	0.0002(0.0276)	0.171(0.097)	96.88(97.32)
[2023-09-29 14:14:17 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1234)	0.0002(0.0188)	0.237(0.106)	90.62(96.98)
[2023-09-29 14:14:18 10splitTasks](trainer.py 286): INFO [40/157]	0.1123(0.1187)	0.0002(0.0143)	0.052(0.104)	100.00(96.57)
[2023-09-29 14:14:19 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1158)	0.0002(0.0115)	0.211(0.110)	93.75(96.51)
[2023-09-29 14:14:20 10splitTasks](trainer.py 286): INFO [60/157]	0.1045(0.1140)	0.0005(0.0097)	0.208(0.106)	90.62(96.62)
[2023-09-29 14:14:21 10splitTasks](trainer.py 286): INFO [70/157]	0.1023(0.1125)	0.0003(0.0084)	0.044(0.102)	100.00(96.65)
[2023-09-29 14:14:22 10splitTasks](trainer.py 286): INFO [80/157]	0.1025(0.1114)	0.0004(0.0074)	0.248(0.107)	90.62(96.45)
[2023-09-29 14:14:23 10splitTasks](trainer.py 286): INFO [90/157]	0.1021(0.1104)	0.0002(0.0066)	0.092(0.105)	96.88(96.39)
[2023-09-29 14:14:24 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1098)	0.0002(0.0060)	0.047(0.105)	100.00(96.53)
[2023-09-29 14:14:25 10splitTasks](trainer.py 286): INFO [110/157]	0.1093(0.1093)	0.0003(0.0055)	0.307(0.107)	93.75(96.48)
[2023-09-29 14:14:26 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1089)	0.0002(0.0051)	0.122(0.109)	96.88(96.49)
[2023-09-29 14:14:27 10splitTasks](trainer.py 286): INFO [130/157]	0.1058(0.1085)	0.0003(0.0047)	0.149(0.110)	93.75(96.35)
[2023-09-29 14:14:28 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1081)	0.0002(0.0044)	0.034(0.112)	100.00(96.30)
[2023-09-29 14:14:29 10splitTasks](trainer.py 286): INFO [150/157]	0.1027(0.1079)	0.0001(0.0041)	0.104(0.113)	93.75(96.23)
[2023-09-29 14:14:30 10splitTasks](trainer.py 286): INFO [156/157]	0.0790(0.1075)	0.0001(0.0040)	0.275(0.112)	87.50(96.24)
[2023-09-29 14:14:30 10splitTasks](trainer.py 288): INFO  * Train Acc 96.240
[2023-09-29 14:14:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.75
[2023-09-29 14:14:32 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 14:14:32 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 14:14:32 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:14:32 10splitTasks](trainer.py 286): INFO [0/157]	0.6568(0.6568)	0.5512(0.5512)	0.081(0.081)	96.88(96.88)
[2023-09-29 14:14:33 10splitTasks](trainer.py 286): INFO [10/157]	0.1016(0.1545)	0.0002(0.0506)	0.059(0.106)	100.00(97.16)
[2023-09-29 14:14:34 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1298)	0.0002(0.0266)	0.159(0.117)	93.75(95.83)
[2023-09-29 14:14:35 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1212)	0.0002(0.0182)	0.031(0.111)	100.00(96.07)
[2023-09-29 14:14:36 10splitTasks](trainer.py 286): INFO [40/157]	0.1098(0.1170)	0.0003(0.0138)	0.136(0.106)	93.75(96.11)
[2023-09-29 14:14:37 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1143)	0.0003(0.0112)	0.109(0.114)	93.75(95.77)
[2023-09-29 14:14:39 10splitTasks](trainer.py 286): INFO [60/157]	0.1026(0.1126)	0.0002(0.0094)	0.144(0.120)	96.88(95.80)
[2023-09-29 14:14:40 10splitTasks](trainer.py 286): INFO [70/157]	0.1044(0.1112)	0.0003(0.0081)	0.125(0.118)	96.88(95.91)
[2023-09-29 14:14:41 10splitTasks](trainer.py 286): INFO [80/157]	0.1029(0.1104)	0.0003(0.0072)	0.033(0.119)	100.00(95.87)
[2023-09-29 14:14:42 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1102)	0.0003(0.0065)	0.279(0.118)	90.62(95.88)
[2023-09-29 14:14:43 10splitTasks](trainer.py 286): INFO [100/157]	0.1072(0.1097)	0.0002(0.0059)	0.179(0.118)	93.75(95.98)
[2023-09-29 14:14:44 10splitTasks](trainer.py 286): INFO [110/157]	0.1020(0.1091)	0.0002(0.0054)	0.060(0.116)	96.88(96.09)
[2023-09-29 14:14:45 10splitTasks](trainer.py 286): INFO [120/157]	0.1025(0.1087)	0.0003(0.0050)	0.173(0.116)	90.62(96.13)
[2023-09-29 14:14:46 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1083)	0.0002(0.0046)	0.035(0.116)	100.00(96.14)
[2023-09-29 14:14:47 10splitTasks](trainer.py 286): INFO [140/157]	0.1019(0.1079)	0.0002(0.0043)	0.032(0.114)	100.00(96.23)
[2023-09-29 14:14:48 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1078)	0.0003(0.0041)	0.055(0.112)	100.00(96.27)
[2023-09-29 14:14:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0775(0.1074)	0.0001(0.0039)	0.201(0.113)	87.50(96.24)
[2023-09-29 14:14:49 10splitTasks](trainer.py 288): INFO  * Train Acc 96.240
[2023-09-29 14:14:50 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.600, Total time 1.67
[2023-09-29 14:14:50 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 14:14:50 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 14:14:50 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:14:51 10splitTasks](trainer.py 286): INFO [0/157]	0.6178(0.6178)	0.5058(0.5058)	0.074(0.074)	100.00(100.00)
[2023-09-29 14:14:52 10splitTasks](trainer.py 286): INFO [10/157]	0.1053(0.1517)	0.0003(0.0463)	0.057(0.106)	100.00(96.31)
[2023-09-29 14:14:53 10splitTasks](trainer.py 286): INFO [20/157]	0.1021(0.1287)	0.0003(0.0244)	0.140(0.111)	96.88(95.83)
[2023-09-29 14:14:54 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1205)	0.0003(0.0166)	0.232(0.107)	87.50(95.97)
[2023-09-29 14:14:55 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1161)	0.0002(0.0127)	0.099(0.122)	96.88(95.43)
[2023-09-29 14:14:56 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1134)	0.0002(0.0103)	0.091(0.118)	96.88(95.89)
[2023-09-29 14:14:57 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1117)	0.0002(0.0086)	0.199(0.116)	90.62(95.95)
[2023-09-29 14:14:58 10splitTasks](trainer.py 286): INFO [70/157]	0.1045(0.1106)	0.0003(0.0075)	0.178(0.115)	93.75(96.08)
[2023-09-29 14:14:59 10splitTasks](trainer.py 286): INFO [80/157]	0.1049(0.1099)	0.0003(0.0066)	0.052(0.116)	100.00(96.03)
[2023-09-29 14:15:00 10splitTasks](trainer.py 286): INFO [90/157]	0.1067(0.1093)	0.0002(0.0059)	0.067(0.114)	96.88(96.12)
[2023-09-29 14:15:01 10splitTasks](trainer.py 286): INFO [100/157]	0.1031(0.1087)	0.0002(0.0053)	0.048(0.114)	100.00(96.13)
[2023-09-29 14:15:02 10splitTasks](trainer.py 286): INFO [110/157]	0.1017(0.1083)	0.0003(0.0049)	0.098(0.115)	96.88(96.03)
[2023-09-29 14:15:03 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1079)	0.0002(0.0045)	0.284(0.114)	93.75(96.13)
[2023-09-29 14:15:04 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1075)	0.0003(0.0042)	0.097(0.111)	96.88(96.25)
[2023-09-29 14:15:05 10splitTasks](trainer.py 286): INFO [140/157]	0.1055(0.1073)	0.0002(0.0039)	0.058(0.109)	96.88(96.37)
[2023-09-29 14:15:06 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1071)	0.0002(0.0037)	0.055(0.107)	100.00(96.46)
[2023-09-29 14:15:07 10splitTasks](trainer.py 286): INFO [156/157]	0.0786(0.1067)	0.0001(0.0035)	0.069(0.108)	100.00(96.46)
[2023-09-29 14:15:07 10splitTasks](trainer.py 288): INFO  * Train Acc 96.460
[2023-09-29 14:15:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.75
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-8.pth
=> Save Done
[2023-09-29 14:15:09 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 14:15:11 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.71
[2023-09-29 14:15:11 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 14:15:13 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.77
[2023-09-29 14:15:13 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 14:15:14 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.75
[2023-09-29 14:15:14 10splitTasks](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 14:15:16 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.78
[2023-09-29 14:15:16 10splitTasks](iBatchLearn.py 131): INFO validation split name:4
[2023-09-29 14:15:18 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.67
[2023-09-29 14:15:18 10splitTasks](iBatchLearn.py 131): INFO validation split name:5
[2023-09-29 14:15:20 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.600, Total time 1.80
[2023-09-29 14:15:20 10splitTasks](iBatchLearn.py 131): INFO validation split name:6
[2023-09-29 14:15:21 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.76
[2023-09-29 14:15:21 10splitTasks](iBatchLearn.py 131): INFO validation split name:7
[2023-09-29 14:15:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.93
[2023-09-29 14:15:23 10splitTasks](iBatchLearn.py 131): INFO validation split name:8
[2023-09-29 14:15:25 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.82
[2023-09-29 14:15:25 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 14:15:26 10splitTasks](trainer.py 341): INFO done
[2023-09-29 14:15:26 10splitTasks](iBatchLearn.py 155): INFO Acc:82.48888888888888; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 14:15:29 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 14:15:29 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 14:15:29 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 8, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-8.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-8.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_8.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 14:15:30 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-8.pth
[2023-09-29 14:15:30 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 14:15:32 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 14:15:32 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 14:15:32 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 4, 4, 4],
        [3, 3, 2, 4, 6, 4, 4],
        [4, 3, 3, 6, 6, 4, 4],
        [4, 3, 6, 5, 4, 4, 4],
        [4, 6, 6, 5, 4, 5, 5],
        [7, 6, 5, 4, 2, 2, 5],
        [7, 3, 5, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 14:15:32 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 14:15:32 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 14:15:32 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 14:15:38 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 14:15:41 10splitTasks](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 14:15:44 10splitTasks](iBatchLearn.py 167): INFO test split name:3
[2023-09-29 14:15:47 10splitTasks](iBatchLearn.py 167): INFO test split name:4
[2023-09-29 14:15:50 10splitTasks](iBatchLearn.py 167): INFO test split name:5
[2023-09-29 14:15:53 10splitTasks](iBatchLearn.py 167): INFO test split name:6
[2023-09-29 14:15:57 10splitTasks](iBatchLearn.py 167): INFO test split name:7
[2023-09-29 14:16:00 10splitTasks](iBatchLearn.py 167): INFO test split name:8
--------------------------------Official Evaluation--------------------------------
8 82.33333333333334
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 14:16:08 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 14:16:08 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 14:16:08 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": false, "task_count": 9, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-8.pth", "save_ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-9.pth", "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-8.pth", "save_storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-9.pth", "dest_path": null, "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 14:16:09 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-8.pth
[2023-09-29 14:16:09 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 14:16:11 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 14:16:11 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 14:16:11 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 14:16:12 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 14:16:12 10splitTasks](iBatchLearn.py 84): INFO memory score: 0.0
[2023-09-29 14:16:12 10splitTasks](iBatchLearn.py 92): INFO ====================== 9 =======================
[2023-09-29 14:16:12 10splitTasks](trainer.py 207): INFO Optimizer is reset!
[2023-09-29 14:16:12 10splitTasks](my_trainer.py 328): INFO Epoch:0
[2023-09-29 14:16:12 10splitTasks](my_trainer.py 335): INFO LR:0.0033340000000000006
[2023-09-29 14:16:12 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:16:15 10splitTasks](trainer.py 286): INFO [0/157]	3.4660(3.4660)	0.5399(0.5399)	2.339(2.339)	9.38(9.38)
[2023-09-29 14:16:16 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.4081)	0.0001(0.0493)	2.119(2.233)	18.75(15.06)
[2023-09-29 14:16:17 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.2625)	0.0001(0.0259)	1.732(2.088)	50.00(25.30)
[2023-09-29 14:16:18 10splitTasks](trainer.py 286): INFO [30/157]	0.1008(0.2105)	0.0002(0.0176)	1.558(1.954)	46.88(31.65)
[2023-09-29 14:16:19 10splitTasks](trainer.py 286): INFO [40/157]	0.1070(0.1840)	0.0003(0.0134)	1.395(1.858)	43.75(35.82)
[2023-09-29 14:16:20 10splitTasks](trainer.py 286): INFO [50/157]	0.1010(0.1678)	0.0002(0.0108)	1.119(1.751)	53.12(40.01)
[2023-09-29 14:16:21 10splitTasks](trainer.py 286): INFO [60/157]	0.1006(0.1569)	0.0001(0.0091)	1.396(1.692)	50.00(42.01)
[2023-09-29 14:16:22 10splitTasks](trainer.py 286): INFO [70/157]	0.1008(0.1491)	0.0003(0.0078)	0.835(1.620)	71.88(44.85)
[2023-09-29 14:16:23 10splitTasks](trainer.py 286): INFO [80/157]	0.1009(0.1432)	0.0003(0.0069)	1.207(1.581)	56.25(46.33)
[2023-09-29 14:16:24 10splitTasks](trainer.py 286): INFO [90/157]	0.1039(0.1387)	0.0002(0.0062)	1.278(1.528)	50.00(48.11)
[2023-09-29 14:16:25 10splitTasks](trainer.py 286): INFO [100/157]	0.1155(0.1353)	0.0006(0.0056)	0.961(1.491)	68.75(49.29)
[2023-09-29 14:16:26 10splitTasks](trainer.py 286): INFO [110/157]	0.1011(0.1324)	0.0001(0.0051)	0.974(1.453)	65.62(50.70)
[2023-09-29 14:16:27 10splitTasks](trainer.py 286): INFO [120/157]	0.1008(0.1299)	0.0001(0.0047)	0.802(1.412)	68.75(51.96)
[2023-09-29 14:16:28 10splitTasks](trainer.py 286): INFO [130/157]	0.1007(0.1277)	0.0002(0.0044)	0.770(1.376)	68.75(52.96)
[2023-09-29 14:16:29 10splitTasks](trainer.py 286): INFO [140/157]	0.1062(0.1259)	0.0006(0.0041)	1.079(1.342)	59.38(54.06)
[2023-09-29 14:16:30 10splitTasks](trainer.py 286): INFO [150/157]	0.1007(0.1243)	0.0001(0.0038)	1.062(1.309)	53.12(55.19)
[2023-09-29 14:16:31 10splitTasks](trainer.py 286): INFO [156/157]	0.0793(0.1232)	0.0001(0.0037)	0.737(1.295)	62.50(55.76)
[2023-09-29 14:16:31 10splitTasks](trainer.py 288): INFO  * Train Acc 55.760
[2023-09-29 14:16:33 10splitTasks](my_trainer.py 503): INFO  * Val Acc 73.400, Total time 1.65
[2023-09-29 14:16:33 10splitTasks](my_trainer.py 328): INFO Epoch:1
[2023-09-29 14:16:33 10splitTasks](my_trainer.py 335): INFO LR:0.006667000000000001
[2023-09-29 14:16:33 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:16:33 10splitTasks](trainer.py 286): INFO [0/157]	0.5746(0.5746)	0.4655(0.4655)	0.752(0.752)	68.75(68.75)
[2023-09-29 14:16:34 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1449)	0.0002(0.0426)	0.955(0.727)	71.88(73.01)
[2023-09-29 14:16:35 10splitTasks](trainer.py 286): INFO [20/157]	0.1006(0.1250)	0.0003(0.0224)	1.007(0.823)	65.62(71.43)
[2023-09-29 14:16:36 10splitTasks](trainer.py 286): INFO [30/157]	0.1043(0.1176)	0.0002(0.0153)	1.184(0.840)	56.25(71.27)
[2023-09-29 14:16:37 10splitTasks](trainer.py 286): INFO [40/157]	0.1005(0.1137)	0.0003(0.0116)	0.547(0.846)	81.25(71.11)
[2023-09-29 14:16:38 10splitTasks](trainer.py 286): INFO [50/157]	0.1026(0.1112)	0.0002(0.0094)	0.687(0.828)	78.12(72.18)
[2023-09-29 14:16:39 10splitTasks](trainer.py 286): INFO [60/157]	0.1032(0.1100)	0.0002(0.0079)	0.721(0.825)	75.00(72.23)
[2023-09-29 14:16:40 10splitTasks](trainer.py 286): INFO [70/157]	0.1007(0.1088)	0.0001(0.0068)	0.881(0.810)	68.75(72.71)
[2023-09-29 14:16:41 10splitTasks](trainer.py 286): INFO [80/157]	0.1007(0.1079)	0.0002(0.0060)	0.841(0.806)	75.00(72.84)
[2023-09-29 14:16:43 10splitTasks](trainer.py 286): INFO [90/157]	0.1083(0.1075)	0.0006(0.0054)	0.789(0.796)	68.75(73.25)
[2023-09-29 14:16:44 10splitTasks](trainer.py 286): INFO [100/157]	0.1021(0.1071)	0.0002(0.0049)	0.858(0.799)	68.75(73.27)
[2023-09-29 14:16:45 10splitTasks](trainer.py 286): INFO [110/157]	0.1008(0.1066)	0.0001(0.0045)	1.061(0.795)	59.38(73.25)
[2023-09-29 14:16:46 10splitTasks](trainer.py 286): INFO [120/157]	0.1190(0.1064)	0.0002(0.0042)	0.690(0.792)	78.12(73.50)
[2023-09-29 14:16:47 10splitTasks](trainer.py 286): INFO [130/157]	0.1020(0.1060)	0.0006(0.0039)	0.647(0.785)	75.00(73.64)
[2023-09-29 14:16:48 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1057)	0.0002(0.0036)	0.720(0.783)	71.88(73.80)
[2023-09-29 14:16:49 10splitTasks](trainer.py 286): INFO [150/157]	0.1006(0.1055)	0.0001(0.0034)	0.988(0.776)	75.00(74.07)
[2023-09-29 14:16:49 10splitTasks](trainer.py 286): INFO [156/157]	0.0803(0.1052)	0.0001(0.0033)	0.730(0.774)	87.50(74.08)
[2023-09-29 14:16:49 10splitTasks](trainer.py 288): INFO  * Train Acc 74.080
[2023-09-29 14:16:51 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.200, Total time 1.62
[2023-09-29 14:16:51 10splitTasks](my_trainer.py 328): INFO Epoch:2
[2023-09-29 14:16:51 10splitTasks](my_trainer.py 335): INFO LR:0.01
[2023-09-29 14:16:51 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:16:52 10splitTasks](trainer.py 286): INFO [0/157]	0.6077(0.6077)	0.4923(0.4923)	1.035(1.035)	65.62(65.62)
[2023-09-29 14:16:53 10splitTasks](trainer.py 286): INFO [10/157]	0.1076(0.1506)	0.0002(0.0450)	0.835(0.795)	68.75(71.31)
[2023-09-29 14:16:54 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1276)	0.0003(0.0238)	0.560(0.789)	81.25(72.47)
[2023-09-29 14:16:55 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1197)	0.0002(0.0162)	0.730(0.782)	65.62(72.38)
[2023-09-29 14:16:56 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1161)	0.0002(0.0124)	0.870(0.763)	71.88(73.25)
[2023-09-29 14:16:57 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1134)	0.0003(0.0100)	0.615(0.735)	75.00(74.20)
[2023-09-29 14:16:58 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1114)	0.0003(0.0084)	0.464(0.726)	81.25(74.49)
[2023-09-29 14:16:59 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1100)	0.0002(0.0073)	1.011(0.727)	68.75(74.52)
[2023-09-29 14:17:00 10splitTasks](trainer.py 286): INFO [80/157]	0.1010(0.1090)	0.0002(0.0064)	0.892(0.729)	62.50(74.07)
[2023-09-29 14:17:01 10splitTasks](trainer.py 286): INFO [90/157]	0.1016(0.1081)	0.0003(0.0057)	0.739(0.725)	75.00(74.21)
[2023-09-29 14:17:02 10splitTasks](trainer.py 286): INFO [100/157]	0.1022(0.1077)	0.0003(0.0052)	0.690(0.719)	75.00(74.69)
[2023-09-29 14:17:03 10splitTasks](trainer.py 286): INFO [110/157]	0.1062(0.1072)	0.0005(0.0048)	0.659(0.720)	84.38(75.03)
[2023-09-29 14:17:04 10splitTasks](trainer.py 286): INFO [120/157]	0.1041(0.1068)	0.0005(0.0044)	0.680(0.711)	78.12(75.44)
[2023-09-29 14:17:05 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1064)	0.0002(0.0041)	0.876(0.709)	62.50(75.38)
[2023-09-29 14:17:06 10splitTasks](trainer.py 286): INFO [140/157]	0.1122(0.1063)	0.0006(0.0038)	0.394(0.705)	84.38(75.55)
[2023-09-29 14:17:07 10splitTasks](trainer.py 286): INFO [150/157]	0.1005(0.1060)	0.0001(0.0036)	0.517(0.706)	81.25(75.64)
[2023-09-29 14:17:08 10splitTasks](trainer.py 286): INFO [156/157]	0.0769(0.1057)	0.0001(0.0035)	0.256(0.707)	100.00(75.74)
[2023-09-29 14:17:08 10splitTasks](trainer.py 288): INFO  * Train Acc 75.740
[2023-09-29 14:17:09 10splitTasks](my_trainer.py 503): INFO  * Val Acc 77.600, Total time 1.62
[2023-09-29 14:17:09 10splitTasks](my_trainer.py 328): INFO Epoch:3
[2023-09-29 14:17:09 10splitTasks](my_trainer.py 335): INFO LR:0.009504893855078144
[2023-09-29 14:17:09 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:17:10 10splitTasks](trainer.py 286): INFO [0/157]	0.5709(0.5709)	0.4535(0.4535)	0.579(0.579)	81.25(81.25)
[2023-09-29 14:17:11 10splitTasks](trainer.py 286): INFO [10/157]	0.1019(0.1443)	0.0003(0.0415)	0.604(0.624)	81.25(78.98)
[2023-09-29 14:17:12 10splitTasks](trainer.py 286): INFO [20/157]	0.1012(0.1239)	0.0003(0.0219)	0.757(0.620)	75.00(78.72)
[2023-09-29 14:17:13 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1168)	0.0003(0.0149)	0.299(0.577)	93.75(79.84)
[2023-09-29 14:17:14 10splitTasks](trainer.py 286): INFO [40/157]	0.1010(0.1132)	0.0003(0.0113)	0.508(0.570)	81.25(80.41)
[2023-09-29 14:17:15 10splitTasks](trainer.py 286): INFO [50/157]	0.1009(0.1109)	0.0002(0.0092)	0.726(0.581)	84.38(80.51)
[2023-09-29 14:17:16 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1104)	0.0002(0.0078)	0.345(0.568)	87.50(80.89)
[2023-09-29 14:17:17 10splitTasks](trainer.py 286): INFO [70/157]	0.1109(0.1094)	0.0003(0.0068)	0.427(0.571)	84.38(80.81)
[2023-09-29 14:17:18 10splitTasks](trainer.py 286): INFO [80/157]	0.1064(0.1086)	0.0005(0.0060)	0.560(0.573)	81.25(80.79)
[2023-09-29 14:17:19 10splitTasks](trainer.py 286): INFO [90/157]	0.1005(0.1078)	0.0002(0.0054)	0.539(0.569)	84.38(80.94)
[2023-09-29 14:17:20 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1072)	0.0003(0.0049)	0.303(0.563)	87.50(81.22)
[2023-09-29 14:17:21 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1067)	0.0003(0.0045)	0.629(0.564)	84.38(81.05)
[2023-09-29 14:17:22 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1064)	0.0003(0.0041)	0.688(0.561)	78.12(81.02)
[2023-09-29 14:17:23 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1060)	0.0002(0.0038)	0.776(0.568)	78.12(80.99)
[2023-09-29 14:17:24 10splitTasks](trainer.py 286): INFO [140/157]	0.1010(0.1057)	0.0003(0.0036)	0.431(0.573)	81.25(80.85)
[2023-09-29 14:17:25 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1055)	0.0001(0.0034)	0.552(0.575)	75.00(80.75)
[2023-09-29 14:17:26 10splitTasks](trainer.py 286): INFO [156/157]	0.0770(0.1052)	0.0001(0.0033)	0.969(0.576)	62.50(80.60)
[2023-09-29 14:17:26 10splitTasks](trainer.py 288): INFO  * Train Acc 80.600
[2023-09-29 14:17:28 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.62
[2023-09-29 14:17:28 10splitTasks](my_trainer.py 328): INFO Epoch:4
[2023-09-29 14:17:28 10splitTasks](my_trainer.py 335): INFO LR:0.008117637264392739
[2023-09-29 14:17:28 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:17:28 10splitTasks](trainer.py 286): INFO [0/157]	0.6259(0.6259)	0.5006(0.5006)	0.531(0.531)	75.00(75.00)
[2023-09-29 14:17:29 10splitTasks](trainer.py 286): INFO [10/157]	0.1123(0.1518)	0.0002(0.0458)	0.872(0.473)	75.00(83.52)
[2023-09-29 14:17:30 10splitTasks](trainer.py 286): INFO [20/157]	0.1010(0.1282)	0.0002(0.0241)	0.381(0.457)	90.62(84.08)
[2023-09-29 14:17:31 10splitTasks](trainer.py 286): INFO [30/157]	0.1011(0.1198)	0.0002(0.0164)	0.395(0.475)	84.38(84.07)
[2023-09-29 14:17:32 10splitTasks](trainer.py 286): INFO [40/157]	0.1020(0.1157)	0.0002(0.0125)	0.196(0.480)	96.88(83.38)
[2023-09-29 14:17:33 10splitTasks](trainer.py 286): INFO [50/157]	0.1008(0.1131)	0.0001(0.0101)	0.314(0.478)	87.50(83.39)
[2023-09-29 14:17:34 10splitTasks](trainer.py 286): INFO [60/157]	0.1011(0.1115)	0.0002(0.0085)	0.403(0.486)	90.62(83.40)
[2023-09-29 14:17:35 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1101)	0.0002(0.0074)	0.362(0.480)	90.62(83.41)
[2023-09-29 14:17:36 10splitTasks](trainer.py 286): INFO [80/157]	0.1030(0.1092)	0.0003(0.0065)	0.191(0.476)	90.62(83.68)
[2023-09-29 14:17:37 10splitTasks](trainer.py 286): INFO [90/157]	0.1067(0.1086)	0.0003(0.0058)	0.469(0.476)	84.38(83.52)
[2023-09-29 14:17:38 10splitTasks](trainer.py 286): INFO [100/157]	0.1027(0.1079)	0.0003(0.0053)	0.401(0.461)	81.25(83.97)
[2023-09-29 14:17:39 10splitTasks](trainer.py 286): INFO [110/157]	0.1010(0.1073)	0.0002(0.0048)	0.773(0.456)	84.38(84.32)
[2023-09-29 14:17:41 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1070)	0.0002(0.0044)	0.264(0.457)	90.62(84.25)
[2023-09-29 14:17:42 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1067)	0.0002(0.0041)	0.433(0.460)	84.38(84.23)
[2023-09-29 14:17:43 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1063)	0.0003(0.0039)	0.388(0.460)	81.25(84.26)
[2023-09-29 14:17:44 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1060)	0.0001(0.0036)	0.894(0.465)	75.00(84.15)
[2023-09-29 14:17:44 10splitTasks](trainer.py 286): INFO [156/157]	0.0787(0.1057)	0.0001(0.0035)	1.272(0.463)	62.50(84.28)
[2023-09-29 14:17:44 10splitTasks](trainer.py 288): INFO  * Train Acc 84.280
[2023-09-29 14:17:46 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.800, Total time 1.67
[2023-09-29 14:17:46 10splitTasks](my_trainer.py 328): INFO Epoch:5
[2023-09-29 14:17:46 10splitTasks](my_trainer.py 335): INFO LR:0.006112993409314594
[2023-09-29 14:17:46 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:17:47 10splitTasks](trainer.py 286): INFO [0/157]	0.6367(0.6367)	0.5155(0.5155)	0.203(0.203)	90.62(90.62)
[2023-09-29 14:17:48 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1528)	0.0003(0.0472)	0.341(0.494)	93.75(84.38)
[2023-09-29 14:17:49 10splitTasks](trainer.py 286): INFO [20/157]	0.1014(0.1287)	0.0003(0.0249)	0.400(0.468)	84.38(84.38)
[2023-09-29 14:17:50 10splitTasks](trainer.py 286): INFO [30/157]	0.1012(0.1202)	0.0003(0.0169)	0.636(0.436)	81.25(85.79)
[2023-09-29 14:17:51 10splitTasks](trainer.py 286): INFO [40/157]	0.1047(0.1160)	0.0003(0.0129)	0.436(0.420)	78.12(86.13)
[2023-09-29 14:17:52 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1136)	0.0003(0.0106)	0.241(0.410)	90.62(86.76)
[2023-09-29 14:17:53 10splitTasks](trainer.py 286): INFO [60/157]	0.1015(0.1117)	0.0002(0.0089)	0.356(0.407)	84.38(86.58)
[2023-09-29 14:17:54 10splitTasks](trainer.py 286): INFO [70/157]	0.1071(0.1104)	0.0005(0.0077)	0.570(0.403)	84.38(86.53)
[2023-09-29 14:17:55 10splitTasks](trainer.py 286): INFO [80/157]	0.1009(0.1095)	0.0002(0.0068)	0.241(0.393)	93.75(86.84)
[2023-09-29 14:17:56 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1089)	0.0003(0.0061)	0.154(0.391)	96.88(86.98)
[2023-09-29 14:17:57 10splitTasks](trainer.py 286): INFO [100/157]	0.1026(0.1083)	0.0003(0.0055)	0.270(0.389)	90.62(87.22)
[2023-09-29 14:17:58 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1078)	0.0003(0.0050)	0.373(0.386)	93.75(87.30)
[2023-09-29 14:17:59 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1075)	0.0002(0.0046)	0.730(0.390)	71.88(87.01)
[2023-09-29 14:18:00 10splitTasks](trainer.py 286): INFO [130/157]	0.1019(0.1071)	0.0002(0.0043)	0.434(0.387)	90.62(87.19)
[2023-09-29 14:18:01 10splitTasks](trainer.py 286): INFO [140/157]	0.1075(0.1069)	0.0003(0.0040)	0.444(0.389)	87.50(87.08)
[2023-09-29 14:18:02 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1066)	0.0001(0.0038)	0.552(0.399)	81.25(86.84)
[2023-09-29 14:18:03 10splitTasks](trainer.py 286): INFO [156/157]	0.0785(0.1063)	0.0001(0.0036)	0.143(0.398)	100.00(86.82)
[2023-09-29 14:18:03 10splitTasks](trainer.py 288): INFO  * Train Acc 86.820
[2023-09-29 14:18:04 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.61
[2023-09-29 14:18:04 10splitTasks](my_trainer.py 328): INFO Epoch:6
[2023-09-29 14:18:04 10splitTasks](my_trainer.py 335): INFO LR:0.003888006590685407
[2023-09-29 14:18:04 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:18:05 10splitTasks](trainer.py 286): INFO [0/157]	0.8454(0.8454)	0.7339(0.7339)	0.146(0.146)	93.75(93.75)
[2023-09-29 14:18:06 10splitTasks](trainer.py 286): INFO [10/157]	0.1044(0.1703)	0.0001(0.0669)	0.821(0.264)	81.25(92.61)
[2023-09-29 14:18:07 10splitTasks](trainer.py 286): INFO [20/157]	0.1011(0.1382)	0.0003(0.0352)	0.423(0.304)	81.25(91.07)
[2023-09-29 14:18:08 10splitTasks](trainer.py 286): INFO [30/157]	0.1034(0.1266)	0.0002(0.0239)	0.247(0.314)	90.62(90.12)
[2023-09-29 14:18:09 10splitTasks](trainer.py 286): INFO [40/157]	0.1147(0.1208)	0.0003(0.0182)	0.541(0.346)	81.25(89.02)
[2023-09-29 14:18:10 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1171)	0.0002(0.0147)	0.280(0.331)	93.75(89.83)
[2023-09-29 14:18:11 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1147)	0.0001(0.0123)	0.423(0.317)	84.38(90.16)
[2023-09-29 14:18:12 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1129)	0.0002(0.0106)	0.495(0.323)	90.62(89.79)
[2023-09-29 14:18:13 10splitTasks](trainer.py 286): INFO [80/157]	0.1017(0.1116)	0.0002(0.0093)	0.308(0.328)	90.62(89.93)
[2023-09-29 14:18:14 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1106)	0.0002(0.0083)	0.331(0.322)	87.50(90.01)
[2023-09-29 14:18:15 10splitTasks](trainer.py 286): INFO [100/157]	0.1085(0.1099)	0.0007(0.0075)	0.202(0.330)	93.75(89.76)
[2023-09-29 14:18:16 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1092)	0.0002(0.0069)	0.106(0.326)	96.88(89.78)
[2023-09-29 14:18:18 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1086)	0.0002(0.0063)	0.272(0.328)	93.75(89.82)
[2023-09-29 14:18:19 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1081)	0.0002(0.0059)	0.556(0.331)	84.38(89.81)
[2023-09-29 14:18:20 10splitTasks](trainer.py 286): INFO [140/157]	0.1012(0.1076)	0.0002(0.0055)	0.621(0.335)	78.12(89.61)
[2023-09-29 14:18:21 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1072)	0.0001(0.0051)	0.241(0.334)	90.62(89.67)
[2023-09-29 14:18:21 10splitTasks](trainer.py 286): INFO [156/157]	0.0773(0.1069)	0.0001(0.0049)	1.018(0.334)	62.50(89.68)
[2023-09-29 14:18:21 10splitTasks](trainer.py 288): INFO  * Train Acc 89.680
[2023-09-29 14:18:23 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.000, Total time 1.66
[2023-09-29 14:18:23 10splitTasks](my_trainer.py 328): INFO Epoch:7
[2023-09-29 14:18:23 10splitTasks](my_trainer.py 335): INFO LR:0.0018833627356072621
[2023-09-29 14:18:23 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:18:24 10splitTasks](trainer.py 286): INFO [0/157]	0.5990(0.5990)	0.4883(0.4883)	0.284(0.284)	93.75(93.75)
[2023-09-29 14:18:25 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1484)	0.0003(0.0447)	0.304(0.239)	90.62(93.47)
[2023-09-29 14:18:26 10splitTasks](trainer.py 286): INFO [20/157]	0.1188(0.1277)	0.0005(0.0236)	0.267(0.266)	93.75(92.71)
[2023-09-29 14:18:27 10splitTasks](trainer.py 286): INFO [30/157]	0.1019(0.1200)	0.0002(0.0161)	0.164(0.284)	90.62(91.43)
[2023-09-29 14:18:28 10splitTasks](trainer.py 286): INFO [40/157]	0.1146(0.1160)	0.0013(0.0123)	0.208(0.276)	93.75(91.39)
[2023-09-29 14:18:29 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1133)	0.0002(0.0100)	0.193(0.280)	90.62(91.36)
[2023-09-29 14:18:30 10splitTasks](trainer.py 286): INFO [60/157]	0.1031(0.1116)	0.0003(0.0084)	0.171(0.268)	93.75(91.75)
[2023-09-29 14:18:31 10splitTasks](trainer.py 286): INFO [70/157]	0.1045(0.1104)	0.0003(0.0072)	0.378(0.274)	84.38(91.55)
[2023-09-29 14:18:32 10splitTasks](trainer.py 286): INFO [80/157]	0.1045(0.1094)	0.0003(0.0064)	0.143(0.279)	96.88(91.47)
[2023-09-29 14:18:33 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1085)	0.0002(0.0057)	0.397(0.274)	84.38(91.59)
[2023-09-29 14:18:34 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1079)	0.0002(0.0052)	0.341(0.274)	90.62(91.55)
[2023-09-29 14:18:35 10splitTasks](trainer.py 286): INFO [110/157]	0.1048(0.1075)	0.0001(0.0047)	0.382(0.275)	84.38(91.50)
[2023-09-29 14:18:36 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1071)	0.0003(0.0044)	0.216(0.276)	93.75(91.37)
[2023-09-29 14:18:37 10splitTasks](trainer.py 286): INFO [130/157]	0.1140(0.1068)	0.0002(0.0040)	0.346(0.273)	87.50(91.48)
[2023-09-29 14:18:38 10splitTasks](trainer.py 286): INFO [140/157]	0.1066(0.1067)	0.0002(0.0038)	0.368(0.277)	93.75(91.38)
[2023-09-29 14:18:39 10splitTasks](trainer.py 286): INFO [150/157]	0.1016(0.1064)	0.0002(0.0036)	0.352(0.279)	87.50(91.35)
[2023-09-29 14:18:40 10splitTasks](trainer.py 286): INFO [156/157]	0.0789(0.1061)	0.0001(0.0034)	0.433(0.278)	87.50(91.32)
[2023-09-29 14:18:40 10splitTasks](trainer.py 288): INFO  * Train Acc 91.320
[2023-09-29 14:18:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.67
[2023-09-29 14:18:41 10splitTasks](my_trainer.py 328): INFO Epoch:8
[2023-09-29 14:18:41 10splitTasks](my_trainer.py 335): INFO LR:0.0004961061449218562
[2023-09-29 14:18:41 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:18:42 10splitTasks](trainer.py 286): INFO [0/157]	0.6113(0.6113)	0.4927(0.4927)	0.352(0.352)	90.62(90.62)
[2023-09-29 14:18:43 10splitTasks](trainer.py 286): INFO [10/157]	0.1014(0.1526)	0.0003(0.0453)	0.245(0.263)	90.62(91.19)
[2023-09-29 14:18:44 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1293)	0.0002(0.0239)	0.068(0.278)	100.00(90.77)
[2023-09-29 14:18:45 10splitTasks](trainer.py 286): INFO [30/157]	0.1026(0.1210)	0.0002(0.0164)	0.149(0.275)	93.75(90.83)
[2023-09-29 14:18:46 10splitTasks](trainer.py 286): INFO [40/157]	0.1036(0.1166)	0.0003(0.0125)	0.122(0.269)	96.88(91.01)
[2023-09-29 14:18:47 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1143)	0.0002(0.0101)	0.413(0.256)	84.38(91.48)
[2023-09-29 14:18:48 10splitTasks](trainer.py 286): INFO [60/157]	0.1018(0.1124)	0.0004(0.0085)	0.283(0.251)	81.25(91.60)
[2023-09-29 14:18:49 10splitTasks](trainer.py 286): INFO [70/157]	0.1052(0.1113)	0.0002(0.0073)	0.161(0.249)	96.88(91.86)
[2023-09-29 14:18:50 10splitTasks](trainer.py 286): INFO [80/157]	0.1028(0.1105)	0.0003(0.0065)	0.357(0.249)	87.50(91.90)
[2023-09-29 14:18:51 10splitTasks](trainer.py 286): INFO [90/157]	0.1019(0.1098)	0.0002(0.0058)	0.342(0.253)	90.62(91.66)
[2023-09-29 14:18:52 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1090)	0.0002(0.0053)	0.398(0.253)	84.38(91.65)
[2023-09-29 14:18:53 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1085)	0.0002(0.0048)	0.184(0.256)	93.75(91.50)
[2023-09-29 14:18:54 10splitTasks](trainer.py 286): INFO [120/157]	0.1018(0.1081)	0.0002(0.0044)	0.334(0.256)	87.50(91.50)
[2023-09-29 14:18:55 10splitTasks](trainer.py 286): INFO [130/157]	0.1055(0.1077)	0.0002(0.0041)	0.223(0.263)	90.62(91.22)
[2023-09-29 14:18:56 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1073)	0.0003(0.0038)	0.239(0.262)	93.75(91.29)
[2023-09-29 14:18:58 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1070)	0.0001(0.0036)	0.211(0.261)	90.62(91.18)
[2023-09-29 14:18:58 10splitTasks](trainer.py 286): INFO [156/157]	0.0779(0.1066)	0.0001(0.0035)	0.236(0.258)	87.50(91.26)
[2023-09-29 14:18:58 10splitTasks](trainer.py 288): INFO  * Train Acc 91.260
[2023-09-29 14:19:00 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.000, Total time 1.62
[2023-09-29 14:19:00 10splitTasks](my_trainer.py 328): INFO Epoch:9
[2023-09-29 14:19:00 10splitTasks](my_trainer.py 335): INFO LR:1e-06
[2023-09-29 14:19:00 10splitTasks](my_trainer.py 341): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:19:01 10splitTasks](trainer.py 286): INFO [0/157]	0.6930(0.6930)	0.5845(0.5845)	0.380(0.380)	84.38(84.38)
[2023-09-29 14:19:02 10splitTasks](trainer.py 286): INFO [10/157]	0.1054(0.1564)	0.0002(0.0535)	0.135(0.276)	96.88(91.76)
[2023-09-29 14:19:03 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1305)	0.0002(0.0282)	0.183(0.260)	96.88(91.96)
[2023-09-29 14:19:04 10splitTasks](trainer.py 286): INFO [30/157]	0.1025(0.1213)	0.0003(0.0192)	0.178(0.264)	96.88(91.83)
[2023-09-29 14:19:05 10splitTasks](trainer.py 286): INFO [40/157]	0.1014(0.1170)	0.0002(0.0146)	0.312(0.258)	93.75(92.23)
[2023-09-29 14:19:06 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1142)	0.0002(0.0118)	0.364(0.245)	90.62(92.46)
[2023-09-29 14:19:07 10splitTasks](trainer.py 286): INFO [60/157]	0.1021(0.1125)	0.0003(0.0099)	0.131(0.246)	93.75(92.37)
[2023-09-29 14:19:08 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1111)	0.0003(0.0086)	0.209(0.240)	93.75(92.69)
[2023-09-29 14:19:09 10splitTasks](trainer.py 286): INFO [80/157]	0.1024(0.1100)	0.0002(0.0075)	0.210(0.235)	96.88(92.98)
[2023-09-29 14:19:10 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1091)	0.0002(0.0067)	0.234(0.236)	93.75(92.79)
[2023-09-29 14:19:11 10splitTasks](trainer.py 286): INFO [100/157]	0.1033(0.1084)	0.0001(0.0061)	0.098(0.237)	96.88(92.76)
[2023-09-29 14:19:12 10splitTasks](trainer.py 286): INFO [110/157]	0.1054(0.1079)	0.0003(0.0056)	0.031(0.233)	100.00(92.96)
[2023-09-29 14:19:13 10splitTasks](trainer.py 286): INFO [120/157]	0.1068(0.1075)	0.0002(0.0051)	0.453(0.237)	84.38(92.82)
[2023-09-29 14:19:14 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1070)	0.0003(0.0048)	0.093(0.233)	100.00(92.82)
[2023-09-29 14:19:15 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1067)	0.0002(0.0044)	0.269(0.234)	90.62(92.64)
[2023-09-29 14:19:16 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1063)	0.0002(0.0042)	0.458(0.239)	84.38(92.45)
[2023-09-29 14:19:16 10splitTasks](trainer.py 286): INFO [156/157]	0.0772(0.1060)	0.0001(0.0040)	0.239(0.238)	87.50(92.44)
[2023-09-29 14:19:17 10splitTasks](trainer.py 288): INFO  * Train Acc 92.440
[2023-09-29 14:19:18 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.200, Total time 1.60
[2023-09-29 14:19:18 10splitTasks](my_trainer.py 206): INFO Pruning for task9
Pruning each layer by removing 85.00% of values
Layer #conv1, pruned 1852/2179 (84.99%) (Total in layer: 9408)
Layer #layer1.0.conv1, pruned 807/949 (85.04%) (Total in layer: 4096)
Layer #layer1.0.conv2, pruned 7257/8538 (85.00%) (Total in layer: 36864)
Layer #layer1.0.conv3, pruned 3225/3794 (85.00%) (Total in layer: 16384)
Layer #layer1.0.downsample.0, pruned 3225/3794 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv1, pruned 3225/3794 (85.00%) (Total in layer: 16384)
Layer #layer1.1.conv2, pruned 7257/8538 (85.00%) (Total in layer: 36864)
Layer #layer1.1.conv3, pruned 3225/3794 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv1, pruned 3225/3794 (85.00%) (Total in layer: 16384)
Layer #layer1.2.conv2, pruned 7257/8538 (85.00%) (Total in layer: 36864)
Layer #layer1.2.conv3, pruned 3225/3794 (85.00%) (Total in layer: 16384)
Layer #layer2.0.conv1, pruned 6451/7589 (85.00%) (Total in layer: 32768)
Layer #layer2.0.conv2, pruned 29030/34153 (85.00%) (Total in layer: 147456)
Layer #layer2.0.conv3, pruned 12902/15179 (85.00%) (Total in layer: 65536)
Layer #layer2.0.downsample.0, pruned 25804/30358 (85.00%) (Total in layer: 131072)
Layer #layer2.1.conv1, pruned 12902/15179 (85.00%) (Total in layer: 65536)
Layer #layer2.1.conv2, pruned 29030/34153 (85.00%) (Total in layer: 147456)
Layer #layer2.1.conv3, pruned 12902/15179 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv1, pruned 12902/15179 (85.00%) (Total in layer: 65536)
Layer #layer2.2.conv2, pruned 29030/34153 (85.00%) (Total in layer: 147456)
Layer #layer2.2.conv3, pruned 12902/15179 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv1, pruned 12902/15179 (85.00%) (Total in layer: 65536)
Layer #layer2.3.conv2, pruned 29030/34153 (85.00%) (Total in layer: 147456)
Layer #layer2.3.conv3, pruned 12902/15179 (85.00%) (Total in layer: 65536)
Layer #layer3.0.conv1, pruned 25804/30358 (85.00%) (Total in layer: 131072)
Layer #layer3.0.conv2, pruned 116121/136613 (85.00%) (Total in layer: 589824)
Layer #layer3.0.conv3, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.0.downsample.0, pruned 103219/121434 (85.00%) (Total in layer: 524288)
Layer #layer3.1.conv1, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.1.conv2, pruned 116121/136613 (85.00%) (Total in layer: 589824)
Layer #layer3.1.conv3, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv1, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.2.conv2, pruned 116121/136613 (85.00%) (Total in layer: 589824)
Layer #layer3.2.conv3, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv1, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.3.conv2, pruned 116121/136613 (85.00%) (Total in layer: 589824)
Layer #layer3.3.conv3, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv1, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.4.conv2, pruned 116121/136613 (85.00%) (Total in layer: 589824)
Layer #layer3.4.conv3, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv1, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer3.5.conv2, pruned 116121/136613 (85.00%) (Total in layer: 589824)
Layer #layer3.5.conv3, pruned 51609/60717 (85.00%) (Total in layer: 262144)
Layer #layer4.0.conv1, pruned 103219/121434 (85.00%) (Total in layer: 524288)
Layer #layer4.0.conv2, pruned 464485/546453 (85.00%) (Total in layer: 2359296)
Layer #layer4.0.conv3, pruned 206439/242869 (85.00%) (Total in layer: 1048576)
Layer #layer4.0.downsample.0, pruned 412875/485735 (85.00%) (Total in layer: 2097152)
Layer #layer4.1.conv1, pruned 206439/242869 (85.00%) (Total in layer: 1048576)
Layer #layer4.1.conv2, pruned 464485/546453 (85.00%) (Total in layer: 2359296)
Layer #layer4.1.conv3, pruned 206439/242869 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv1, pruned 206439/242869 (85.00%) (Total in layer: 1048576)
Layer #layer4.2.conv2, pruned 464485/546453 (85.00%) (Total in layer: 2359296)
Layer #layer4.2.conv3, pruned 206439/242869 (85.00%) (Total in layer: 1048576)
[2023-09-29 14:19:18 10splitTasks](my_trainer.py 298): INFO start retrain model
[2023-09-29 14:19:18 10splitTasks](my_trainer.py 302): INFO Epoch:0
[2023-09-29 14:19:18 10splitTasks](my_trainer.py 308): INFO LR:0.01
[2023-09-29 14:19:18 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:19:19 10splitTasks](trainer.py 286): INFO [0/157]	0.5899(0.5899)	0.4808(0.4808)	0.279(0.279)	87.50(87.50)
[2023-09-29 14:19:20 10splitTasks](trainer.py 286): INFO [10/157]	0.1125(0.1528)	0.0002(0.0440)	0.274(0.395)	93.75(86.36)
[2023-09-29 14:19:21 10splitTasks](trainer.py 286): INFO [20/157]	0.1023(0.1298)	0.0001(0.0232)	0.160(0.385)	93.75(86.61)
[2023-09-29 14:19:22 10splitTasks](trainer.py 286): INFO [30/157]	0.1072(0.1210)	0.0005(0.0158)	0.130(0.367)	96.88(87.50)
[2023-09-29 14:19:23 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1167)	0.0003(0.0120)	0.339(0.388)	87.50(86.59)
[2023-09-29 14:19:24 10splitTasks](trainer.py 286): INFO [50/157]	0.1129(0.1144)	0.0005(0.0097)	0.214(0.384)	96.88(86.76)
[2023-09-29 14:19:25 10splitTasks](trainer.py 286): INFO [60/157]	0.1020(0.1126)	0.0003(0.0082)	0.209(0.378)	93.75(87.14)
[2023-09-29 14:19:26 10splitTasks](trainer.py 286): INFO [70/157]	0.1009(0.1112)	0.0001(0.0071)	0.421(0.385)	84.38(86.62)
[2023-09-29 14:19:27 10splitTasks](trainer.py 286): INFO [80/157]	0.1038(0.1100)	0.0002(0.0063)	0.321(0.391)	93.75(86.54)
[2023-09-29 14:19:28 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1093)	0.0003(0.0056)	0.187(0.377)	96.88(87.12)
[2023-09-29 14:19:29 10splitTasks](trainer.py 286): INFO [100/157]	0.1034(0.1087)	0.0003(0.0051)	0.486(0.374)	84.38(87.22)
[2023-09-29 14:19:30 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1085)	0.0001(0.0046)	0.376(0.369)	87.50(87.42)
[2023-09-29 14:19:31 10splitTasks](trainer.py 286): INFO [120/157]	0.1049(0.1079)	0.0002(0.0043)	0.454(0.371)	81.25(87.35)
[2023-09-29 14:19:32 10splitTasks](trainer.py 286): INFO [130/157]	0.1009(0.1075)	0.0002(0.0040)	0.538(0.370)	81.25(87.33)
[2023-09-29 14:19:33 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1072)	0.0003(0.0037)	0.586(0.372)	84.38(87.32)
[2023-09-29 14:19:34 10splitTasks](trainer.py 286): INFO [150/157]	0.1013(0.1068)	0.0001(0.0035)	0.413(0.373)	81.25(87.36)
[2023-09-29 14:19:35 10splitTasks](trainer.py 286): INFO [156/157]	0.0774(0.1064)	0.0001(0.0034)	1.712(0.380)	62.50(87.14)
[2023-09-29 14:19:35 10splitTasks](trainer.py 288): INFO  * Train Acc 87.140
[2023-09-29 14:19:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.64
[2023-09-29 14:19:37 10splitTasks](my_trainer.py 302): INFO Epoch:1
[2023-09-29 14:19:37 10splitTasks](my_trainer.py 308): INFO LR:0.00993181333636191
[2023-09-29 14:19:37 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:19:37 10splitTasks](trainer.py 286): INFO [0/157]	0.5729(0.5729)	0.4542(0.4542)	0.268(0.268)	87.50(87.50)
[2023-09-29 14:19:38 10splitTasks](trainer.py 286): INFO [10/157]	0.1010(0.1476)	0.0001(0.0416)	0.627(0.389)	81.25(87.50)
[2023-09-29 14:19:39 10splitTasks](trainer.py 286): INFO [20/157]	0.1033(0.1265)	0.0003(0.0219)	0.378(0.389)	87.50(87.35)
[2023-09-29 14:19:40 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1192)	0.0002(0.0149)	0.292(0.375)	90.62(87.80)
[2023-09-29 14:19:42 10splitTasks](trainer.py 286): INFO [40/157]	0.1125(0.1154)	0.0006(0.0114)	0.210(0.352)	100.00(88.26)
[2023-09-29 14:19:43 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1127)	0.0002(0.0092)	0.353(0.362)	87.50(87.62)
[2023-09-29 14:19:44 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1110)	0.0002(0.0077)	0.561(0.355)	81.25(87.76)
[2023-09-29 14:19:45 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1097)	0.0002(0.0067)	0.222(0.353)	96.88(88.16)
[2023-09-29 14:19:46 10splitTasks](trainer.py 286): INFO [80/157]	0.1015(0.1087)	0.0001(0.0059)	0.098(0.354)	100.00(87.96)
[2023-09-29 14:19:47 10splitTasks](trainer.py 286): INFO [90/157]	0.1061(0.1082)	0.0005(0.0053)	0.416(0.353)	87.50(87.95)
[2023-09-29 14:19:48 10splitTasks](trainer.py 286): INFO [100/157]	0.1054(0.1077)	0.0003(0.0048)	0.435(0.349)	87.50(88.12)
[2023-09-29 14:19:49 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1072)	0.0002(0.0044)	0.383(0.353)	93.75(88.15)
[2023-09-29 14:19:50 10splitTasks](trainer.py 286): INFO [120/157]	0.1010(0.1068)	0.0002(0.0040)	0.663(0.356)	81.25(88.07)
[2023-09-29 14:19:51 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1065)	0.0002(0.0038)	0.694(0.355)	75.00(88.05)
[2023-09-29 14:19:52 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1062)	0.0002(0.0035)	0.471(0.359)	81.25(87.85)
[2023-09-29 14:19:53 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1059)	0.0001(0.0033)	0.355(0.364)	87.50(87.58)
[2023-09-29 14:19:53 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1056)	0.0001(0.0032)	0.246(0.365)	87.50(87.60)
[2023-09-29 14:19:53 10splitTasks](trainer.py 288): INFO  * Train Acc 87.600
[2023-09-29 14:19:55 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.200, Total time 1.57
[2023-09-29 14:19:55 10splitTasks](my_trainer.py 302): INFO Epoch:2
[2023-09-29 14:19:55 10splitTasks](my_trainer.py 308): INFO LR:0.009729113299882323
[2023-09-29 14:19:55 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:19:56 10splitTasks](trainer.py 286): INFO [0/157]	0.6436(0.6436)	0.5267(0.5267)	0.376(0.376)	87.50(87.50)
[2023-09-29 14:19:57 10splitTasks](trainer.py 286): INFO [10/157]	0.1053(0.1529)	0.0003(0.0488)	0.322(0.272)	87.50(90.34)
[2023-09-29 14:19:58 10splitTasks](trainer.py 286): INFO [20/157]	0.1016(0.1288)	0.0003(0.0257)	0.384(0.261)	87.50(91.07)
[2023-09-29 14:19:59 10splitTasks](trainer.py 286): INFO [30/157]	0.1016(0.1201)	0.0003(0.0175)	0.125(0.273)	93.75(90.62)
[2023-09-29 14:20:00 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1159)	0.0002(0.0133)	0.377(0.299)	84.38(89.63)
[2023-09-29 14:20:01 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1136)	0.0002(0.0108)	0.569(0.295)	87.50(89.95)
[2023-09-29 14:20:02 10splitTasks](trainer.py 286): INFO [60/157]	0.1041(0.1122)	0.0005(0.0091)	0.231(0.293)	87.50(89.96)
[2023-09-29 14:20:03 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1108)	0.0002(0.0078)	0.414(0.303)	84.38(89.61)
[2023-09-29 14:20:04 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1097)	0.0003(0.0069)	0.312(0.315)	90.62(89.12)
[2023-09-29 14:20:05 10splitTasks](trainer.py 286): INFO [90/157]	0.1046(0.1092)	0.0002(0.0062)	0.378(0.314)	90.62(89.01)
[2023-09-29 14:20:06 10splitTasks](trainer.py 286): INFO [100/157]	0.1013(0.1086)	0.0001(0.0056)	0.319(0.319)	90.62(88.83)
[2023-09-29 14:20:07 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1082)	0.0003(0.0051)	0.135(0.323)	96.88(88.68)
[2023-09-29 14:20:08 10splitTasks](trainer.py 286): INFO [120/157]	0.1015(0.1077)	0.0003(0.0047)	0.287(0.318)	96.88(88.89)
[2023-09-29 14:20:09 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1072)	0.0001(0.0044)	0.357(0.315)	90.62(89.00)
[2023-09-29 14:20:10 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1068)	0.0003(0.0041)	0.193(0.314)	96.88(89.10)
[2023-09-29 14:20:11 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1066)	0.0002(0.0038)	0.273(0.315)	87.50(89.03)
[2023-09-29 14:20:12 10splitTasks](trainer.py 286): INFO [156/157]	0.0771(0.1063)	0.0001(0.0037)	0.178(0.312)	100.00(89.22)
[2023-09-29 14:20:12 10splitTasks](trainer.py 288): INFO  * Train Acc 89.220
[2023-09-29 14:20:13 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.55
[2023-09-29 14:20:13 10splitTasks](my_trainer.py 302): INFO Epoch:3
[2023-09-29 14:20:13 10splitTasks](my_trainer.py 308): INFO LR:0.009397429019156842
[2023-09-29 14:20:13 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:20:14 10splitTasks](trainer.py 286): INFO [0/157]	0.6550(0.6550)	0.5512(0.5512)	0.145(0.145)	96.88(96.88)
[2023-09-29 14:20:15 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1538)	0.0002(0.0504)	0.441(0.319)	84.38(90.62)
[2023-09-29 14:20:16 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1301)	0.0003(0.0267)	0.323(0.288)	87.50(91.07)
[2023-09-29 14:20:17 10splitTasks](trainer.py 286): INFO [30/157]	0.1038(0.1213)	0.0002(0.0182)	0.271(0.300)	93.75(90.12)
[2023-09-29 14:20:18 10splitTasks](trainer.py 286): INFO [40/157]	0.1013(0.1166)	0.0002(0.0139)	0.195(0.323)	93.75(89.02)
[2023-09-29 14:20:19 10splitTasks](trainer.py 286): INFO [50/157]	0.1008(0.1136)	0.0002(0.0112)	0.153(0.327)	93.75(88.73)
[2023-09-29 14:20:20 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1117)	0.0002(0.0094)	0.437(0.333)	81.25(88.63)
[2023-09-29 14:20:21 10splitTasks](trainer.py 286): INFO [70/157]	0.1037(0.1103)	0.0003(0.0082)	0.350(0.331)	87.50(88.69)
[2023-09-29 14:20:22 10splitTasks](trainer.py 286): INFO [80/157]	0.1041(0.1094)	0.0004(0.0072)	0.088(0.323)	96.88(88.93)
[2023-09-29 14:20:23 10splitTasks](trainer.py 286): INFO [90/157]	0.1011(0.1087)	0.0002(0.0064)	0.205(0.327)	93.75(88.74)
[2023-09-29 14:20:24 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1080)	0.0003(0.0058)	0.279(0.326)	87.50(88.64)
[2023-09-29 14:20:25 10splitTasks](trainer.py 286): INFO [110/157]	0.1037(0.1075)	0.0003(0.0053)	0.271(0.319)	90.62(89.02)
[2023-09-29 14:20:26 10splitTasks](trainer.py 286): INFO [120/157]	0.1039(0.1071)	0.0002(0.0049)	0.092(0.318)	100.00(89.20)
[2023-09-29 14:20:27 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1067)	0.0003(0.0046)	0.242(0.317)	87.50(89.10)
[2023-09-29 14:20:28 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1063)	0.0003(0.0043)	0.185(0.312)	93.75(89.30)
[2023-09-29 14:20:29 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1060)	0.0001(0.0040)	0.193(0.308)	93.75(89.40)
[2023-09-29 14:20:30 10splitTasks](trainer.py 286): INFO [156/157]	0.0776(0.1057)	0.0001(0.0038)	0.384(0.305)	87.50(89.44)
[2023-09-29 14:20:30 10splitTasks](trainer.py 288): INFO  * Train Acc 89.440
[2023-09-29 14:20:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.800, Total time 1.71
[2023-09-29 14:20:32 10splitTasks](my_trainer.py 302): INFO Epoch:4
[2023-09-29 14:20:32 10splitTasks](my_trainer.py 308): INFO LR:0.00894580797672727
[2023-09-29 14:20:32 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:20:32 10splitTasks](trainer.py 286): INFO [0/157]	0.6102(0.6102)	0.5058(0.5058)	0.635(0.635)	84.38(84.38)
[2023-09-29 14:20:33 10splitTasks](trainer.py 286): INFO [10/157]	0.1051(0.1510)	0.0044(0.0466)	0.180(0.307)	96.88(90.06)
[2023-09-29 14:20:34 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1275)	0.0002(0.0245)	0.439(0.288)	93.75(91.07)
[2023-09-29 14:20:36 10splitTasks](trainer.py 286): INFO [30/157]	0.1021(0.1200)	0.0002(0.0167)	0.320(0.279)	84.38(91.13)
[2023-09-29 14:20:37 10splitTasks](trainer.py 286): INFO [40/157]	0.1012(0.1156)	0.0002(0.0127)	0.258(0.280)	93.75(91.23)
[2023-09-29 14:20:38 10splitTasks](trainer.py 286): INFO [50/157]	0.1013(0.1128)	0.0002(0.0103)	0.260(0.279)	90.62(91.12)
[2023-09-29 14:20:39 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1112)	0.0002(0.0086)	0.207(0.276)	96.88(91.19)
[2023-09-29 14:20:40 10splitTasks](trainer.py 286): INFO [70/157]	0.1025(0.1100)	0.0002(0.0075)	0.225(0.267)	87.50(91.29)
[2023-09-29 14:20:41 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1089)	0.0001(0.0066)	0.210(0.267)	93.75(91.44)
[2023-09-29 14:20:42 10splitTasks](trainer.py 286): INFO [90/157]	0.1020(0.1083)	0.0002(0.0059)	0.080(0.266)	96.88(91.41)
[2023-09-29 14:20:43 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1078)	0.0002(0.0053)	0.602(0.273)	84.38(91.27)
[2023-09-29 14:20:44 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1074)	0.0002(0.0049)	0.216(0.271)	93.75(91.39)
[2023-09-29 14:20:45 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1069)	0.0002(0.0045)	0.131(0.273)	96.88(91.24)
[2023-09-29 14:20:46 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1065)	0.0001(0.0042)	0.762(0.276)	75.00(91.22)
[2023-09-29 14:20:47 10splitTasks](trainer.py 286): INFO [140/157]	0.1035(0.1062)	0.0002(0.0039)	0.245(0.277)	90.62(91.07)
[2023-09-29 14:20:48 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1059)	0.0001(0.0037)	0.217(0.277)	87.50(91.00)
[2023-09-29 14:20:48 10splitTasks](trainer.py 286): INFO [156/157]	0.0775(0.1055)	0.0001(0.0035)	0.768(0.279)	75.00(91.00)
[2023-09-29 14:20:48 10splitTasks](trainer.py 288): INFO  * Train Acc 91.000
[2023-09-29 14:20:50 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.55
[2023-09-29 14:20:50 10splitTasks](my_trainer.py 302): INFO Epoch:5
[2023-09-29 14:20:50 10splitTasks](my_trainer.py 308): INFO LR:0.008386569217342894
[2023-09-29 14:20:50 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:20:51 10splitTasks](trainer.py 286): INFO [0/157]	0.5756(0.5756)	0.4615(0.4615)	0.115(0.115)	96.88(96.88)
[2023-09-29 14:20:52 10splitTasks](trainer.py 286): INFO [10/157]	0.1036(0.1488)	0.0001(0.0422)	0.225(0.314)	90.62(88.64)
[2023-09-29 14:20:53 10splitTasks](trainer.py 286): INFO [20/157]	0.1009(0.1278)	0.0001(0.0223)	0.133(0.286)	100.00(90.62)
[2023-09-29 14:20:54 10splitTasks](trainer.py 286): INFO [30/157]	0.1058(0.1200)	0.0003(0.0152)	0.179(0.274)	93.75(90.73)
[2023-09-29 14:20:55 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1156)	0.0002(0.0115)	0.146(0.270)	96.88(90.55)
[2023-09-29 14:20:56 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1130)	0.0002(0.0093)	0.288(0.280)	90.62(90.50)
[2023-09-29 14:20:57 10splitTasks](trainer.py 286): INFO [60/157]	0.1052(0.1112)	0.0002(0.0078)	0.137(0.269)	96.88(91.03)
[2023-09-29 14:20:58 10splitTasks](trainer.py 286): INFO [70/157]	0.1012(0.1099)	0.0001(0.0068)	0.373(0.266)	90.62(90.93)
[2023-09-29 14:20:59 10splitTasks](trainer.py 286): INFO [80/157]	0.1046(0.1090)	0.0002(0.0060)	0.262(0.260)	93.75(91.17)
[2023-09-29 14:21:00 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1083)	0.0001(0.0053)	0.133(0.259)	93.75(91.24)
[2023-09-29 14:21:01 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1076)	0.0002(0.0048)	0.223(0.257)	93.75(91.27)
[2023-09-29 14:21:02 10splitTasks](trainer.py 286): INFO [110/157]	0.1060(0.1071)	0.0003(0.0044)	0.235(0.257)	90.62(91.22)
[2023-09-29 14:21:03 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1067)	0.0003(0.0041)	0.294(0.260)	81.25(91.04)
[2023-09-29 14:21:04 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1064)	0.0003(0.0038)	0.049(0.258)	100.00(91.13)
[2023-09-29 14:21:05 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1061)	0.0002(0.0035)	0.315(0.257)	96.88(91.25)
[2023-09-29 14:21:06 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1059)	0.0002(0.0033)	0.235(0.259)	93.75(91.23)
[2023-09-29 14:21:07 10splitTasks](trainer.py 286): INFO [156/157]	0.0773(0.1055)	0.0001(0.0032)	0.559(0.261)	75.00(91.06)
[2023-09-29 14:21:07 10splitTasks](trainer.py 288): INFO  * Train Acc 91.060
[2023-09-29 14:21:08 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.56
[2023-09-29 14:21:08 10splitTasks](my_trainer.py 302): INFO Epoch:6
[2023-09-29 14:21:08 10splitTasks](my_trainer.py 308): INFO LR:0.0077349673165330755
[2023-09-29 14:21:08 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:21:09 10splitTasks](trainer.py 286): INFO [0/157]	0.6033(0.6033)	0.4949(0.4949)	0.061(0.061)	100.00(100.00)
[2023-09-29 14:21:10 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1501)	0.0002(0.0458)	0.117(0.232)	96.88(92.90)
[2023-09-29 14:21:11 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1274)	0.0002(0.0242)	0.218(0.217)	90.62(92.71)
[2023-09-29 14:21:12 10splitTasks](trainer.py 286): INFO [30/157]	0.1086(0.1195)	0.0002(0.0165)	0.219(0.228)	93.75(92.44)
[2023-09-29 14:21:13 10splitTasks](trainer.py 286): INFO [40/157]	0.1189(0.1159)	0.0003(0.0125)	0.501(0.247)	81.25(92.00)
[2023-09-29 14:21:14 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1133)	0.0002(0.0101)	0.562(0.246)	84.38(92.16)
[2023-09-29 14:21:15 10splitTasks](trainer.py 286): INFO [60/157]	0.1013(0.1116)	0.0001(0.0085)	0.538(0.245)	84.38(92.11)
[2023-09-29 14:21:16 10splitTasks](trainer.py 286): INFO [70/157]	0.1052(0.1105)	0.0002(0.0073)	0.213(0.242)	93.75(92.21)
[2023-09-29 14:21:17 10splitTasks](trainer.py 286): INFO [80/157]	0.1114(0.1095)	0.0005(0.0065)	0.212(0.234)	93.75(92.44)
[2023-09-29 14:21:18 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1087)	0.0002(0.0058)	0.202(0.231)	87.50(92.34)
[2023-09-29 14:21:19 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1080)	0.0002(0.0053)	0.122(0.231)	93.75(92.33)
[2023-09-29 14:21:20 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1075)	0.0001(0.0048)	0.170(0.229)	93.75(92.43)
[2023-09-29 14:21:21 10splitTasks](trainer.py 286): INFO [120/157]	0.1016(0.1072)	0.0003(0.0044)	0.078(0.227)	93.75(92.41)
[2023-09-29 14:21:22 10splitTasks](trainer.py 286): INFO [130/157]	0.1034(0.1069)	0.0003(0.0041)	0.073(0.225)	96.88(92.44)
[2023-09-29 14:21:23 10splitTasks](trainer.py 286): INFO [140/157]	0.1018(0.1066)	0.0002(0.0038)	0.513(0.235)	84.38(92.07)
[2023-09-29 14:21:24 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1063)	0.0001(0.0036)	0.236(0.233)	90.62(92.07)
[2023-09-29 14:21:25 10splitTasks](trainer.py 286): INFO [156/157]	0.0772(0.1059)	0.0001(0.0035)	1.092(0.238)	75.00(91.98)
[2023-09-29 14:21:25 10splitTasks](trainer.py 288): INFO  * Train Acc 91.980
[2023-09-29 14:21:27 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.200, Total time 1.64
[2023-09-29 14:21:27 10splitTasks](my_trainer.py 302): INFO Epoch:7
[2023-09-29 14:21:27 10splitTasks](my_trainer.py 308): INFO LR:0.007008776275552522
[2023-09-29 14:21:27 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:21:27 10splitTasks](trainer.py 286): INFO [0/157]	0.5917(0.5917)	0.4736(0.4736)	0.848(0.848)	75.00(75.00)
[2023-09-29 14:21:28 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1484)	0.0002(0.0433)	0.219(0.333)	90.62(87.50)
[2023-09-29 14:21:29 10splitTasks](trainer.py 286): INFO [20/157]	0.1009(0.1263)	0.0001(0.0228)	0.144(0.297)	93.75(89.29)
[2023-09-29 14:21:30 10splitTasks](trainer.py 286): INFO [30/157]	0.1023(0.1185)	0.0003(0.0155)	0.120(0.279)	96.88(89.92)
[2023-09-29 14:21:31 10splitTasks](trainer.py 286): INFO [40/157]	0.1040(0.1148)	0.0003(0.0118)	0.152(0.270)	96.88(90.17)
[2023-09-29 14:21:32 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1124)	0.0003(0.0096)	0.059(0.266)	100.00(90.69)
[2023-09-29 14:21:33 10splitTasks](trainer.py 286): INFO [60/157]	0.1017(0.1110)	0.0003(0.0080)	0.357(0.253)	87.50(91.29)
[2023-09-29 14:21:34 10splitTasks](trainer.py 286): INFO [70/157]	0.1010(0.1097)	0.0002(0.0069)	0.096(0.247)	96.88(91.37)
[2023-09-29 14:21:35 10splitTasks](trainer.py 286): INFO [80/157]	0.1018(0.1087)	0.0002(0.0061)	0.486(0.250)	90.62(91.44)
[2023-09-29 14:21:36 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1080)	0.0002(0.0055)	0.114(0.245)	93.75(91.72)
[2023-09-29 14:21:38 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1077)	0.0003(0.0050)	0.282(0.244)	90.62(91.68)
[2023-09-29 14:21:39 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1071)	0.0002(0.0045)	0.072(0.236)	100.00(92.06)
[2023-09-29 14:21:40 10splitTasks](trainer.py 286): INFO [120/157]	0.1022(0.1068)	0.0003(0.0042)	0.423(0.236)	81.25(92.07)
[2023-09-29 14:21:41 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1064)	0.0003(0.0039)	0.040(0.233)	100.00(92.20)
[2023-09-29 14:21:42 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1061)	0.0003(0.0037)	0.273(0.228)	90.62(92.38)
[2023-09-29 14:21:43 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1059)	0.0001(0.0034)	0.255(0.229)	87.50(92.28)
[2023-09-29 14:21:43 10splitTasks](trainer.py 286): INFO [156/157]	0.0782(0.1056)	0.0001(0.0033)	0.601(0.232)	87.50(92.18)
[2023-09-29 14:21:43 10splitTasks](trainer.py 288): INFO  * Train Acc 92.180
[2023-09-29 14:21:45 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.400, Total time 1.61
[2023-09-29 14:21:45 10splitTasks](my_trainer.py 302): INFO Epoch:8
[2023-09-29 14:21:45 10splitTasks](my_trainer.py 308): INFO LR:0.006227804692960426
[2023-09-29 14:21:45 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:21:46 10splitTasks](trainer.py 286): INFO [0/157]	0.6000(0.6000)	0.4716(0.4716)	0.052(0.052)	100.00(100.00)
[2023-09-29 14:21:47 10splitTasks](trainer.py 286): INFO [10/157]	0.1057(0.1510)	0.0005(0.0432)	0.153(0.184)	93.75(95.17)
[2023-09-29 14:21:48 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1280)	0.0002(0.0228)	0.052(0.194)	100.00(94.20)
[2023-09-29 14:21:49 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1197)	0.0002(0.0156)	0.247(0.193)	87.50(94.15)
[2023-09-29 14:21:50 10splitTasks](trainer.py 286): INFO [40/157]	0.1054(0.1163)	0.0003(0.0119)	0.242(0.193)	96.88(93.98)
[2023-09-29 14:21:51 10splitTasks](trainer.py 286): INFO [50/157]	0.1048(0.1137)	0.0005(0.0096)	0.471(0.194)	84.38(93.93)
[2023-09-29 14:21:52 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1118)	0.0003(0.0081)	0.129(0.200)	93.75(93.49)
[2023-09-29 14:21:53 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1105)	0.0003(0.0070)	0.093(0.200)	96.88(93.66)
[2023-09-29 14:21:54 10splitTasks](trainer.py 286): INFO [80/157]	0.1042(0.1095)	0.0003(0.0062)	0.285(0.198)	87.50(93.60)
[2023-09-29 14:21:55 10splitTasks](trainer.py 286): INFO [90/157]	0.1158(0.1089)	0.0006(0.0055)	0.100(0.197)	96.88(93.72)
[2023-09-29 14:21:56 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1084)	0.0005(0.0050)	0.047(0.194)	100.00(93.81)
[2023-09-29 14:21:57 10splitTasks](trainer.py 286): INFO [110/157]	0.1018(0.1079)	0.0003(0.0046)	0.204(0.199)	90.62(93.47)
[2023-09-29 14:21:58 10splitTasks](trainer.py 286): INFO [120/157]	0.1027(0.1077)	0.0004(0.0043)	0.210(0.202)	90.62(93.21)
[2023-09-29 14:21:59 10splitTasks](trainer.py 286): INFO [130/157]	0.1021(0.1076)	0.0003(0.0040)	0.180(0.200)	93.75(93.30)
[2023-09-29 14:22:00 10splitTasks](trainer.py 286): INFO [140/157]	0.1009(0.1073)	0.0001(0.0037)	0.224(0.201)	93.75(93.20)
[2023-09-29 14:22:01 10splitTasks](trainer.py 286): INFO [150/157]	0.1012(0.1069)	0.0001(0.0035)	0.531(0.206)	81.25(92.94)
[2023-09-29 14:22:02 10splitTasks](trainer.py 286): INFO [156/157]	0.0774(0.1066)	0.0001(0.0034)	0.062(0.204)	100.00(93.04)
[2023-09-29 14:22:02 10splitTasks](trainer.py 288): INFO  * Train Acc 93.040
[2023-09-29 14:22:03 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.000, Total time 1.65
[2023-09-29 14:22:03 10splitTasks](my_trainer.py 302): INFO Epoch:9
[2023-09-29 14:22:03 10splitTasks](my_trainer.py 308): INFO LR:0.005413355437688927
[2023-09-29 14:22:03 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:22:04 10splitTasks](trainer.py 286): INFO [0/157]	0.5871(0.5871)	0.4733(0.4733)	0.258(0.258)	90.62(90.62)
[2023-09-29 14:22:05 10splitTasks](trainer.py 286): INFO [10/157]	0.1022(0.1475)	0.0003(0.0433)	0.100(0.158)	96.88(94.32)
[2023-09-29 14:22:06 10splitTasks](trainer.py 286): INFO [20/157]	0.1026(0.1266)	0.0001(0.0228)	0.182(0.170)	93.75(93.60)
[2023-09-29 14:22:07 10splitTasks](trainer.py 286): INFO [30/157]	0.1037(0.1191)	0.0002(0.0155)	0.245(0.174)	90.62(93.55)
[2023-09-29 14:22:08 10splitTasks](trainer.py 286): INFO [40/157]	0.1041(0.1155)	0.0002(0.0118)	0.506(0.188)	81.25(92.91)
[2023-09-29 14:22:09 10splitTasks](trainer.py 286): INFO [50/157]	0.1370(0.1141)	0.0009(0.0096)	0.580(0.191)	84.38(93.08)
[2023-09-29 14:22:10 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1125)	0.0003(0.0081)	0.237(0.185)	90.62(93.24)
[2023-09-29 14:22:11 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1111)	0.0001(0.0070)	0.111(0.182)	96.88(93.49)
[2023-09-29 14:22:12 10splitTasks](trainer.py 286): INFO [80/157]	0.1019(0.1101)	0.0002(0.0061)	0.242(0.180)	87.50(93.36)
[2023-09-29 14:22:13 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1092)	0.0003(0.0055)	0.258(0.178)	90.62(93.37)
[2023-09-29 14:22:14 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1086)	0.0002(0.0050)	0.236(0.181)	93.75(93.41)
[2023-09-29 14:22:15 10splitTasks](trainer.py 286): INFO [110/157]	0.1024(0.1082)	0.0003(0.0046)	0.143(0.181)	93.75(93.38)
[2023-09-29 14:22:16 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1076)	0.0003(0.0042)	0.140(0.182)	93.75(93.26)
[2023-09-29 14:22:18 10splitTasks](trainer.py 286): INFO [130/157]	0.1013(0.1072)	0.0001(0.0039)	0.214(0.181)	96.88(93.39)
[2023-09-29 14:22:19 10splitTasks](trainer.py 286): INFO [140/157]	0.1014(0.1070)	0.0003(0.0036)	0.178(0.181)	90.62(93.42)
[2023-09-29 14:22:20 10splitTasks](trainer.py 286): INFO [150/157]	0.1015(0.1069)	0.0002(0.0034)	0.299(0.182)	87.50(93.44)
[2023-09-29 14:22:20 10splitTasks](trainer.py 286): INFO [156/157]	0.0809(0.1066)	0.0001(0.0033)	0.955(0.183)	87.50(93.48)
[2023-09-29 14:22:20 10splitTasks](trainer.py 288): INFO  * Train Acc 93.480
[2023-09-29 14:22:22 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.56
[2023-09-29 14:22:22 10splitTasks](my_trainer.py 302): INFO Epoch:10
[2023-09-29 14:22:22 10splitTasks](my_trainer.py 308): INFO LR:0.004587644562311075
[2023-09-29 14:22:22 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:22:22 10splitTasks](trainer.py 286): INFO [0/157]	0.5999(0.5999)	0.4875(0.4875)	0.381(0.381)	78.12(78.12)
[2023-09-29 14:22:24 10splitTasks](trainer.py 286): INFO [10/157]	0.1005(0.1495)	0.0001(0.0446)	0.178(0.235)	93.75(90.91)
[2023-09-29 14:22:25 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1274)	0.0002(0.0237)	0.111(0.185)	93.75(93.45)
[2023-09-29 14:22:26 10splitTasks](trainer.py 286): INFO [30/157]	0.1045(0.1195)	0.0002(0.0161)	0.056(0.191)	100.00(93.35)
[2023-09-29 14:22:27 10splitTasks](trainer.py 286): INFO [40/157]	0.1016(0.1151)	0.0002(0.0122)	0.269(0.188)	90.62(93.75)
[2023-09-29 14:22:28 10splitTasks](trainer.py 286): INFO [50/157]	0.1041(0.1125)	0.0002(0.0099)	0.124(0.174)	93.75(94.06)
[2023-09-29 14:22:29 10splitTasks](trainer.py 286): INFO [60/157]	0.1112(0.1110)	0.0002(0.0083)	0.123(0.177)	96.88(93.95)
[2023-09-29 14:22:30 10splitTasks](trainer.py 286): INFO [70/157]	0.1040(0.1100)	0.0002(0.0072)	0.089(0.174)	93.75(93.88)
[2023-09-29 14:22:31 10splitTasks](trainer.py 286): INFO [80/157]	0.1036(0.1091)	0.0001(0.0063)	0.383(0.179)	87.50(93.71)
[2023-09-29 14:22:32 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1084)	0.0001(0.0057)	0.260(0.183)	87.50(93.58)
[2023-09-29 14:22:33 10splitTasks](trainer.py 286): INFO [100/157]	0.1018(0.1077)	0.0003(0.0051)	0.148(0.190)	96.88(93.53)
[2023-09-29 14:22:34 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1072)	0.0002(0.0047)	0.382(0.196)	93.75(93.41)
[2023-09-29 14:22:35 10splitTasks](trainer.py 286): INFO [120/157]	0.1079(0.1068)	0.0003(0.0043)	0.047(0.193)	100.00(93.49)
[2023-09-29 14:22:36 10splitTasks](trainer.py 286): INFO [130/157]	0.1015(0.1065)	0.0002(0.0040)	0.158(0.190)	96.88(93.65)
[2023-09-29 14:22:37 10splitTasks](trainer.py 286): INFO [140/157]	0.1021(0.1063)	0.0003(0.0037)	0.352(0.190)	87.50(93.59)
[2023-09-29 14:22:38 10splitTasks](trainer.py 286): INFO [150/157]	0.1011(0.1060)	0.0001(0.0035)	0.178(0.188)	90.62(93.52)
[2023-09-29 14:22:38 10splitTasks](trainer.py 286): INFO [156/157]	0.0774(0.1057)	0.0001(0.0034)	0.383(0.188)	75.00(93.50)
[2023-09-29 14:22:39 10splitTasks](trainer.py 288): INFO  * Train Acc 93.500
[2023-09-29 14:22:40 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.800, Total time 1.66
[2023-09-29 14:22:40 10splitTasks](my_trainer.py 302): INFO Epoch:11
[2023-09-29 14:22:40 10splitTasks](my_trainer.py 308): INFO LR:0.003773195307039575
[2023-09-29 14:22:40 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:22:41 10splitTasks](trainer.py 286): INFO [0/157]	0.5484(0.5484)	0.4379(0.4379)	0.141(0.141)	90.62(90.62)
[2023-09-29 14:22:42 10splitTasks](trainer.py 286): INFO [10/157]	0.1040(0.1487)	0.0003(0.0457)	0.260(0.172)	90.62(93.47)
[2023-09-29 14:22:43 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1272)	0.0003(0.0241)	0.131(0.141)	93.75(94.94)
[2023-09-29 14:22:44 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1192)	0.0002(0.0164)	0.080(0.134)	100.00(95.26)
[2023-09-29 14:22:45 10splitTasks](trainer.py 286): INFO [40/157]	0.1038(0.1151)	0.0002(0.0125)	0.316(0.141)	87.50(95.20)
[2023-09-29 14:22:46 10splitTasks](trainer.py 286): INFO [50/157]	0.1019(0.1128)	0.0002(0.0101)	0.211(0.154)	93.75(95.10)
[2023-09-29 14:22:47 10splitTasks](trainer.py 286): INFO [60/157]	0.1008(0.1113)	0.0002(0.0085)	0.477(0.166)	87.50(94.67)
[2023-09-29 14:22:48 10splitTasks](trainer.py 286): INFO [70/157]	0.1017(0.1100)	0.0003(0.0073)	0.075(0.162)	96.88(94.81)
[2023-09-29 14:22:49 10splitTasks](trainer.py 286): INFO [80/157]	0.1010(0.1091)	0.0003(0.0065)	0.059(0.158)	96.88(94.83)
[2023-09-29 14:22:50 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1083)	0.0002(0.0058)	0.158(0.162)	90.62(94.64)
[2023-09-29 14:22:51 10splitTasks](trainer.py 286): INFO [100/157]	0.1015(0.1077)	0.0003(0.0052)	0.200(0.159)	93.75(94.65)
[2023-09-29 14:22:52 10splitTasks](trainer.py 286): INFO [110/157]	0.1015(0.1071)	0.0003(0.0048)	0.260(0.168)	93.75(94.34)
[2023-09-29 14:22:53 10splitTasks](trainer.py 286): INFO [120/157]	0.1030(0.1068)	0.0003(0.0044)	0.059(0.170)	100.00(94.27)
[2023-09-29 14:22:54 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1064)	0.0003(0.0041)	0.072(0.169)	96.88(94.39)
[2023-09-29 14:22:55 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1061)	0.0003(0.0039)	0.131(0.166)	96.88(94.44)
[2023-09-29 14:22:56 10splitTasks](trainer.py 286): INFO [150/157]	0.1050(0.1059)	0.0001(0.0036)	0.220(0.169)	90.62(94.37)
[2023-09-29 14:22:57 10splitTasks](trainer.py 286): INFO [156/157]	0.0773(0.1055)	0.0001(0.0035)	0.226(0.169)	87.50(94.26)
[2023-09-29 14:22:57 10splitTasks](trainer.py 288): INFO  * Train Acc 94.260
[2023-09-29 14:22:58 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.61
[2023-09-29 14:22:58 10splitTasks](my_trainer.py 302): INFO Epoch:12
[2023-09-29 14:22:58 10splitTasks](my_trainer.py 308): INFO LR:0.0029922237244474808
[2023-09-29 14:22:58 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:22:59 10splitTasks](trainer.py 286): INFO [0/157]	0.6094(0.6094)	0.5052(0.5052)	0.088(0.088)	100.00(100.00)
[2023-09-29 14:23:00 10splitTasks](trainer.py 286): INFO [10/157]	0.1049(0.1491)	0.0002(0.0461)	0.083(0.108)	96.88(96.59)
[2023-09-29 14:23:01 10splitTasks](trainer.py 286): INFO [20/157]	0.1010(0.1264)	0.0001(0.0243)	0.182(0.139)	93.75(94.94)
[2023-09-29 14:23:02 10splitTasks](trainer.py 286): INFO [30/157]	0.1043(0.1185)	0.0002(0.0165)	0.212(0.145)	90.62(95.16)
[2023-09-29 14:23:03 10splitTasks](trainer.py 286): INFO [40/157]	0.1082(0.1145)	0.0002(0.0125)	0.116(0.161)	96.88(94.74)
[2023-09-29 14:23:04 10splitTasks](trainer.py 286): INFO [50/157]	0.1016(0.1119)	0.0003(0.0101)	0.175(0.169)	93.75(94.30)
[2023-09-29 14:23:05 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1102)	0.0002(0.0085)	0.040(0.166)	100.00(94.31)
[2023-09-29 14:23:06 10splitTasks](trainer.py 286): INFO [70/157]	0.1011(0.1089)	0.0001(0.0073)	0.281(0.161)	93.75(94.50)
[2023-09-29 14:23:07 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1081)	0.0002(0.0064)	0.211(0.158)	93.75(94.79)
[2023-09-29 14:23:08 10splitTasks](trainer.py 286): INFO [90/157]	0.1012(0.1075)	0.0001(0.0058)	0.173(0.159)	93.75(94.61)
[2023-09-29 14:23:09 10splitTasks](trainer.py 286): INFO [100/157]	0.1020(0.1068)	0.0002(0.0052)	0.212(0.157)	87.50(94.65)
[2023-09-29 14:23:10 10splitTasks](trainer.py 286): INFO [110/157]	0.1038(0.1065)	0.0005(0.0048)	0.353(0.158)	87.50(94.54)
[2023-09-29 14:23:11 10splitTasks](trainer.py 286): INFO [120/157]	0.1059(0.1061)	0.0002(0.0044)	0.045(0.155)	100.00(94.63)
[2023-09-29 14:23:12 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1058)	0.0002(0.0041)	0.071(0.158)	96.88(94.51)
[2023-09-29 14:23:13 10splitTasks](trainer.py 286): INFO [140/157]	0.1013(0.1055)	0.0002(0.0038)	0.137(0.158)	93.75(94.59)
[2023-09-29 14:23:14 10splitTasks](trainer.py 286): INFO [150/157]	0.1043(0.1052)	0.0001(0.0036)	0.053(0.156)	100.00(94.64)
[2023-09-29 14:23:15 10splitTasks](trainer.py 286): INFO [156/157]	0.0772(0.1049)	0.0001(0.0035)	0.143(0.156)	100.00(94.66)
[2023-09-29 14:23:15 10splitTasks](trainer.py 288): INFO  * Train Acc 94.660
[2023-09-29 14:23:17 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.58
[2023-09-29 14:23:17 10splitTasks](my_trainer.py 302): INFO Epoch:13
[2023-09-29 14:23:17 10splitTasks](my_trainer.py 308): INFO LR:0.002266032683466928
[2023-09-29 14:23:17 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:23:17 10splitTasks](trainer.py 286): INFO [0/157]	0.5865(0.5865)	0.4625(0.4625)	0.201(0.201)	90.62(90.62)
[2023-09-29 14:23:18 10splitTasks](trainer.py 286): INFO [10/157]	0.1012(0.1472)	0.0002(0.0423)	0.134(0.203)	93.75(92.33)
[2023-09-29 14:23:19 10splitTasks](trainer.py 286): INFO [20/157]	0.1054(0.1262)	0.0003(0.0223)	0.121(0.186)	96.88(93.01)
[2023-09-29 14:23:20 10splitTasks](trainer.py 286): INFO [30/157]	0.1020(0.1187)	0.0002(0.0152)	0.146(0.168)	96.88(93.95)
[2023-09-29 14:23:21 10splitTasks](trainer.py 286): INFO [40/157]	0.1025(0.1151)	0.0003(0.0116)	0.516(0.167)	93.75(94.28)
[2023-09-29 14:23:22 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1126)	0.0002(0.0094)	0.171(0.169)	93.75(94.12)
[2023-09-29 14:23:23 10splitTasks](trainer.py 286): INFO [60/157]	0.1014(0.1110)	0.0002(0.0079)	0.040(0.160)	100.00(94.36)
[2023-09-29 14:23:24 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1099)	0.0002(0.0068)	0.074(0.155)	96.88(94.54)
[2023-09-29 14:23:26 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1091)	0.0002(0.0060)	0.041(0.151)	100.00(94.79)
[2023-09-29 14:23:27 10splitTasks](trainer.py 286): INFO [90/157]	0.1018(0.1084)	0.0003(0.0054)	0.153(0.149)	96.88(94.85)
[2023-09-29 14:23:28 10splitTasks](trainer.py 286): INFO [100/157]	0.1011(0.1077)	0.0002(0.0049)	0.059(0.147)	96.88(94.86)
[2023-09-29 14:23:29 10splitTasks](trainer.py 286): INFO [110/157]	0.1012(0.1072)	0.0002(0.0045)	0.227(0.149)	90.62(94.76)
[2023-09-29 14:23:30 10splitTasks](trainer.py 286): INFO [120/157]	0.1035(0.1067)	0.0002(0.0041)	0.113(0.150)	96.88(94.68)
[2023-09-29 14:23:31 10splitTasks](trainer.py 286): INFO [130/157]	0.1010(0.1064)	0.0001(0.0038)	0.161(0.152)	90.62(94.63)
[2023-09-29 14:23:32 10splitTasks](trainer.py 286): INFO [140/157]	0.1016(0.1061)	0.0002(0.0036)	0.097(0.155)	93.75(94.48)
[2023-09-29 14:23:33 10splitTasks](trainer.py 286): INFO [150/157]	0.1014(0.1059)	0.0002(0.0034)	0.061(0.154)	96.88(94.54)
[2023-09-29 14:23:33 10splitTasks](trainer.py 286): INFO [156/157]	0.0774(0.1056)	0.0001(0.0032)	0.641(0.154)	75.00(94.54)
[2023-09-29 14:23:33 10splitTasks](trainer.py 288): INFO  * Train Acc 94.540
[2023-09-29 14:23:35 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.68
[2023-09-29 14:23:35 10splitTasks](my_trainer.py 302): INFO Epoch:14
[2023-09-29 14:23:35 10splitTasks](my_trainer.py 308): INFO LR:0.0016144307826571086
[2023-09-29 14:23:35 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:23:36 10splitTasks](trainer.py 286): INFO [0/157]	0.6687(0.6687)	0.5517(0.5517)	0.041(0.041)	100.00(100.00)
[2023-09-29 14:23:37 10splitTasks](trainer.py 286): INFO [10/157]	0.1011(0.1544)	0.0002(0.0504)	0.111(0.057)	93.75(98.01)
[2023-09-29 14:23:38 10splitTasks](trainer.py 286): INFO [20/157]	0.1015(0.1294)	0.0003(0.0265)	0.085(0.139)	93.75(94.79)
[2023-09-29 14:23:39 10splitTasks](trainer.py 286): INFO [30/157]	0.1028(0.1206)	0.0002(0.0181)	0.209(0.136)	96.88(95.16)
[2023-09-29 14:23:40 10splitTasks](trainer.py 286): INFO [40/157]	0.1045(0.1164)	0.0002(0.0138)	0.189(0.137)	90.62(95.12)
[2023-09-29 14:23:41 10splitTasks](trainer.py 286): INFO [50/157]	0.1011(0.1137)	0.0002(0.0111)	0.151(0.134)	93.75(95.40)
[2023-09-29 14:23:42 10splitTasks](trainer.py 286): INFO [60/157]	0.1016(0.1119)	0.0002(0.0093)	0.156(0.133)	93.75(95.34)
[2023-09-29 14:23:43 10splitTasks](trainer.py 286): INFO [70/157]	0.1014(0.1105)	0.0002(0.0081)	0.293(0.138)	87.50(95.07)
[2023-09-29 14:23:44 10splitTasks](trainer.py 286): INFO [80/157]	0.1035(0.1094)	0.0005(0.0071)	0.079(0.137)	96.88(95.06)
[2023-09-29 14:23:45 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1087)	0.0003(0.0064)	0.176(0.141)	96.88(94.99)
[2023-09-29 14:23:46 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1080)	0.0002(0.0057)	0.137(0.138)	93.75(95.11)
[2023-09-29 14:23:47 10splitTasks](trainer.py 286): INFO [110/157]	0.1014(0.1077)	0.0002(0.0053)	0.086(0.137)	100.00(95.21)
[2023-09-29 14:23:48 10splitTasks](trainer.py 286): INFO [120/157]	0.1101(0.1074)	0.0005(0.0049)	0.046(0.135)	100.00(95.30)
[2023-09-29 14:23:49 10splitTasks](trainer.py 286): INFO [130/157]	0.1014(0.1070)	0.0003(0.0045)	0.101(0.135)	96.88(95.40)
[2023-09-29 14:23:50 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1066)	0.0002(0.0042)	0.069(0.135)	100.00(95.32)
[2023-09-29 14:23:51 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1063)	0.0001(0.0039)	0.155(0.141)	93.75(95.14)
[2023-09-29 14:23:52 10splitTasks](trainer.py 286): INFO [156/157]	0.0772(0.1059)	0.0001(0.0038)	0.409(0.140)	87.50(95.14)
[2023-09-29 14:23:52 10splitTasks](trainer.py 288): INFO  * Train Acc 95.140
[2023-09-29 14:23:53 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.58
[2023-09-29 14:23:53 10splitTasks](my_trainer.py 302): INFO Epoch:15
[2023-09-29 14:23:53 10splitTasks](my_trainer.py 308): INFO LR:0.001055192023272731
[2023-09-29 14:23:53 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:23:54 10splitTasks](trainer.py 286): INFO [0/157]	0.6083(0.6083)	0.4950(0.4950)	0.084(0.084)	96.88(96.88)
[2023-09-29 14:23:55 10splitTasks](trainer.py 286): INFO [10/157]	0.1026(0.1500)	0.0002(0.0453)	0.270(0.135)	93.75(95.17)
[2023-09-29 14:23:56 10splitTasks](trainer.py 286): INFO [20/157]	0.1229(0.1283)	0.0005(0.0239)	0.055(0.126)	96.88(95.39)
[2023-09-29 14:23:57 10splitTasks](trainer.py 286): INFO [30/157]	0.1015(0.1198)	0.0003(0.0163)	0.263(0.138)	93.75(94.96)
[2023-09-29 14:23:58 10splitTasks](trainer.py 286): INFO [40/157]	0.1057(0.1157)	0.0003(0.0124)	0.460(0.152)	90.62(94.82)
[2023-09-29 14:23:59 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1132)	0.0003(0.0100)	0.230(0.149)	93.75(94.79)
[2023-09-29 14:24:00 10splitTasks](trainer.py 286): INFO [60/157]	0.1029(0.1114)	0.0003(0.0084)	0.262(0.152)	93.75(94.83)
[2023-09-29 14:24:01 10splitTasks](trainer.py 286): INFO [70/157]	0.1015(0.1103)	0.0001(0.0073)	0.185(0.149)	96.88(94.94)
[2023-09-29 14:24:02 10splitTasks](trainer.py 286): INFO [80/157]	0.1118(0.1097)	0.0006(0.0064)	0.059(0.146)	96.88(95.10)
[2023-09-29 14:24:03 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1091)	0.0003(0.0058)	0.053(0.141)	100.00(95.40)
[2023-09-29 14:24:04 10splitTasks](trainer.py 286): INFO [100/157]	0.1032(0.1085)	0.0002(0.0052)	0.220(0.141)	90.62(95.36)
[2023-09-29 14:24:05 10splitTasks](trainer.py 286): INFO [110/157]	0.1013(0.1079)	0.0002(0.0048)	0.029(0.137)	100.00(95.58)
[2023-09-29 14:24:06 10splitTasks](trainer.py 286): INFO [120/157]	0.1026(0.1074)	0.0003(0.0044)	0.113(0.136)	93.75(95.61)
[2023-09-29 14:24:07 10splitTasks](trainer.py 286): INFO [130/157]	0.1011(0.1069)	0.0001(0.0041)	0.074(0.134)	96.88(95.59)
[2023-09-29 14:24:08 10splitTasks](trainer.py 286): INFO [140/157]	0.1015(0.1065)	0.0003(0.0038)	0.143(0.134)	96.88(95.52)
[2023-09-29 14:24:09 10splitTasks](trainer.py 286): INFO [150/157]	0.1009(0.1062)	0.0001(0.0036)	0.169(0.134)	93.75(95.55)
[2023-09-29 14:24:10 10splitTasks](trainer.py 286): INFO [156/157]	0.0773(0.1059)	0.0001(0.0035)	0.039(0.134)	100.00(95.60)
[2023-09-29 14:24:10 10splitTasks](trainer.py 288): INFO  * Train Acc 95.600
[2023-09-29 14:24:12 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.200, Total time 1.59
[2023-09-29 14:24:12 10splitTasks](my_trainer.py 302): INFO Epoch:16
[2023-09-29 14:24:12 10splitTasks](my_trainer.py 308): INFO LR:0.0006035709808431585
[2023-09-29 14:24:12 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:24:12 10splitTasks](trainer.py 286): INFO [0/157]	0.6369(0.6369)	0.5217(0.5217)	0.121(0.121)	96.88(96.88)
[2023-09-29 14:24:13 10splitTasks](trainer.py 286): INFO [10/157]	0.1013(0.1542)	0.0002(0.0477)	0.049(0.101)	96.88(96.31)
[2023-09-29 14:24:14 10splitTasks](trainer.py 286): INFO [20/157]	0.1010(0.1293)	0.0002(0.0251)	0.064(0.117)	100.00(96.43)
[2023-09-29 14:24:15 10splitTasks](trainer.py 286): INFO [30/157]	0.1013(0.1208)	0.0001(0.0171)	0.077(0.129)	100.00(96.37)
[2023-09-29 14:24:16 10splitTasks](trainer.py 286): INFO [40/157]	0.1009(0.1165)	0.0001(0.0130)	0.075(0.137)	96.88(95.96)
[2023-09-29 14:24:17 10splitTasks](trainer.py 286): INFO [50/157]	0.1020(0.1137)	0.0003(0.0105)	0.045(0.139)	100.00(95.77)
[2023-09-29 14:24:19 10splitTasks](trainer.py 286): INFO [60/157]	0.1019(0.1117)	0.0003(0.0088)	0.078(0.131)	96.88(96.00)
[2023-09-29 14:24:20 10splitTasks](trainer.py 286): INFO [70/157]	0.1044(0.1104)	0.0002(0.0076)	0.110(0.136)	100.00(95.77)
[2023-09-29 14:24:21 10splitTasks](trainer.py 286): INFO [80/157]	0.1016(0.1094)	0.0002(0.0067)	0.045(0.133)	100.00(95.87)
[2023-09-29 14:24:22 10splitTasks](trainer.py 286): INFO [90/157]	0.1013(0.1086)	0.0002(0.0060)	0.228(0.133)	87.50(95.67)
[2023-09-29 14:24:23 10splitTasks](trainer.py 286): INFO [100/157]	0.1051(0.1080)	0.0002(0.0054)	0.063(0.133)	96.88(95.64)
[2023-09-29 14:24:24 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1075)	0.0002(0.0050)	0.283(0.135)	87.50(95.52)
[2023-09-29 14:24:25 10splitTasks](trainer.py 286): INFO [120/157]	0.1014(0.1070)	0.0003(0.0046)	0.147(0.141)	93.75(95.22)
[2023-09-29 14:24:26 10splitTasks](trainer.py 286): INFO [130/157]	0.1035(0.1066)	0.0005(0.0043)	0.038(0.138)	100.00(95.40)
[2023-09-29 14:24:27 10splitTasks](trainer.py 286): INFO [140/157]	0.1009(0.1065)	0.0001(0.0040)	0.275(0.143)	93.75(95.19)
[2023-09-29 14:24:28 10splitTasks](trainer.py 286): INFO [150/157]	0.1010(0.1062)	0.0001(0.0038)	0.029(0.140)	100.00(95.26)
[2023-09-29 14:24:28 10splitTasks](trainer.py 286): INFO [156/157]	0.0770(0.1059)	0.0001(0.0036)	0.248(0.139)	87.50(95.26)
[2023-09-29 14:24:28 10splitTasks](trainer.py 288): INFO  * Train Acc 95.260
[2023-09-29 14:24:30 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.600, Total time 1.62
[2023-09-29 14:24:30 10splitTasks](my_trainer.py 302): INFO Epoch:17
[2023-09-29 14:24:30 10splitTasks](my_trainer.py 308): INFO LR:0.0002718867001176772
[2023-09-29 14:24:30 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:24:31 10splitTasks](trainer.py 286): INFO [0/157]	0.6131(0.6131)	0.4885(0.4885)	0.182(0.182)	93.75(93.75)
[2023-09-29 14:24:32 10splitTasks](trainer.py 286): INFO [10/157]	0.1009(0.1502)	0.0002(0.0447)	0.275(0.152)	93.75(94.89)
[2023-09-29 14:24:33 10splitTasks](trainer.py 286): INFO [20/157]	0.1042(0.1289)	0.0002(0.0236)	0.063(0.152)	100.00(94.49)
[2023-09-29 14:24:34 10splitTasks](trainer.py 286): INFO [30/157]	0.1017(0.1202)	0.0002(0.0161)	0.076(0.131)	96.88(95.36)
[2023-09-29 14:24:35 10splitTasks](trainer.py 286): INFO [40/157]	0.1015(0.1156)	0.0003(0.0122)	0.299(0.128)	90.62(95.88)
[2023-09-29 14:24:36 10splitTasks](trainer.py 286): INFO [50/157]	0.1014(0.1132)	0.0003(0.0099)	0.221(0.131)	90.62(95.89)
[2023-09-29 14:24:37 10splitTasks](trainer.py 286): INFO [60/157]	0.1031(0.1115)	0.0002(0.0083)	0.106(0.133)	96.88(96.00)
[2023-09-29 14:24:38 10splitTasks](trainer.py 286): INFO [70/157]	0.1031(0.1105)	0.0002(0.0072)	0.120(0.128)	96.88(96.17)
[2023-09-29 14:24:39 10splitTasks](trainer.py 286): INFO [80/157]	0.1046(0.1096)	0.0002(0.0064)	0.151(0.136)	93.75(95.79)
[2023-09-29 14:24:40 10splitTasks](trainer.py 286): INFO [90/157]	0.1014(0.1089)	0.0001(0.0057)	0.099(0.134)	100.00(95.98)
[2023-09-29 14:24:41 10splitTasks](trainer.py 286): INFO [100/157]	0.1016(0.1082)	0.0002(0.0051)	0.090(0.133)	96.88(95.92)
[2023-09-29 14:24:42 10splitTasks](trainer.py 286): INFO [110/157]	0.1016(0.1077)	0.0002(0.0047)	0.046(0.131)	100.00(95.97)
[2023-09-29 14:24:43 10splitTasks](trainer.py 286): INFO [120/157]	0.1025(0.1074)	0.0003(0.0044)	0.118(0.132)	96.88(95.87)
[2023-09-29 14:24:44 10splitTasks](trainer.py 286): INFO [130/157]	0.1010(0.1070)	0.0002(0.0040)	0.222(0.135)	87.50(95.71)
[2023-09-29 14:24:45 10splitTasks](trainer.py 286): INFO [140/157]	0.1020(0.1066)	0.0002(0.0038)	0.078(0.132)	100.00(95.83)
[2023-09-29 14:24:46 10splitTasks](trainer.py 286): INFO [150/157]	0.1027(0.1063)	0.0002(0.0036)	0.169(0.131)	90.62(95.84)
[2023-09-29 14:24:47 10splitTasks](trainer.py 286): INFO [156/157]	0.0773(0.1060)	0.0001(0.0034)	0.029(0.131)	100.00(95.80)
[2023-09-29 14:24:47 10splitTasks](trainer.py 288): INFO  * Train Acc 95.800
[2023-09-29 14:24:48 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.57
[2023-09-29 14:24:48 10splitTasks](my_trainer.py 302): INFO Epoch:18
[2023-09-29 14:24:48 10splitTasks](my_trainer.py 308): INFO LR:6.918666363808975e-05
[2023-09-29 14:24:48 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:24:49 10splitTasks](trainer.py 286): INFO [0/157]	0.5782(0.5782)	0.4736(0.4736)	0.033(0.033)	100.00(100.00)
[2023-09-29 14:24:50 10splitTasks](trainer.py 286): INFO [10/157]	0.1042(0.1460)	0.0003(0.0433)	0.033(0.148)	100.00(95.45)
[2023-09-29 14:24:51 10splitTasks](trainer.py 286): INFO [20/157]	0.1033(0.1248)	0.0001(0.0228)	0.103(0.126)	93.75(95.39)
[2023-09-29 14:24:52 10splitTasks](trainer.py 286): INFO [30/157]	0.1018(0.1177)	0.0002(0.0155)	0.114(0.129)	93.75(95.16)
[2023-09-29 14:24:53 10splitTasks](trainer.py 286): INFO [40/157]	0.1046(0.1141)	0.0003(0.0118)	0.042(0.126)	96.88(95.12)
[2023-09-29 14:24:54 10splitTasks](trainer.py 286): INFO [50/157]	0.1015(0.1118)	0.0002(0.0095)	0.054(0.123)	100.00(95.34)
[2023-09-29 14:24:55 10splitTasks](trainer.py 286): INFO [60/157]	0.1010(0.1103)	0.0002(0.0080)	0.110(0.128)	96.88(95.24)
[2023-09-29 14:24:56 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1091)	0.0002(0.0069)	0.076(0.122)	100.00(95.47)
[2023-09-29 14:24:57 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1082)	0.0003(0.0061)	0.235(0.127)	90.62(95.33)
[2023-09-29 14:24:58 10splitTasks](trainer.py 286): INFO [90/157]	0.1022(0.1075)	0.0003(0.0055)	0.123(0.127)	93.75(95.33)
[2023-09-29 14:24:59 10splitTasks](trainer.py 286): INFO [100/157]	0.1014(0.1071)	0.0002(0.0049)	0.179(0.128)	93.75(95.36)
[2023-09-29 14:25:00 10splitTasks](trainer.py 286): INFO [110/157]	0.1078(0.1068)	0.0002(0.0045)	0.035(0.127)	100.00(95.50)
[2023-09-29 14:25:01 10splitTasks](trainer.py 286): INFO [120/157]	0.1017(0.1065)	0.0003(0.0042)	0.077(0.128)	96.88(95.45)
[2023-09-29 14:25:02 10splitTasks](trainer.py 286): INFO [130/157]	0.1016(0.1061)	0.0002(0.0039)	0.256(0.126)	90.62(95.56)
[2023-09-29 14:25:03 10splitTasks](trainer.py 286): INFO [140/157]	0.1009(0.1058)	0.0002(0.0036)	0.076(0.127)	96.88(95.61)
[2023-09-29 14:25:04 10splitTasks](trainer.py 286): INFO [150/157]	0.1008(0.1058)	0.0001(0.0034)	0.146(0.127)	96.88(95.61)
[2023-09-29 14:25:05 10splitTasks](trainer.py 286): INFO [156/157]	0.0783(0.1055)	0.0001(0.0033)	0.212(0.127)	87.50(95.62)
[2023-09-29 14:25:05 10splitTasks](trainer.py 288): INFO  * Train Acc 95.620
[2023-09-29 14:25:07 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.400, Total time 1.57
[2023-09-29 14:25:07 10splitTasks](my_trainer.py 302): INFO Epoch:19
[2023-09-29 14:25:07 10splitTasks](my_trainer.py 308): INFO LR:1e-06
[2023-09-29 14:25:07 10splitTasks](my_trainer.py 314): INFO  Itr	    Time  	    Data  	  Loss  	  Acc
[2023-09-29 14:25:07 10splitTasks](trainer.py 286): INFO [0/157]	0.6035(0.6035)	0.4881(0.4881)	0.297(0.297)	84.38(84.38)
[2023-09-29 14:25:08 10splitTasks](trainer.py 286): INFO [10/157]	0.1006(0.1480)	0.0001(0.0446)	0.215(0.129)	90.62(95.17)
[2023-09-29 14:25:09 10splitTasks](trainer.py 286): INFO [20/157]	0.1013(0.1265)	0.0002(0.0235)	0.297(0.113)	87.50(96.13)
[2023-09-29 14:25:10 10splitTasks](trainer.py 286): INFO [30/157]	0.1014(0.1186)	0.0002(0.0160)	0.184(0.124)	90.62(95.67)
[2023-09-29 14:25:11 10splitTasks](trainer.py 286): INFO [40/157]	0.1017(0.1147)	0.0003(0.0122)	0.308(0.124)	84.38(95.58)
[2023-09-29 14:25:12 10splitTasks](trainer.py 286): INFO [50/157]	0.1051(0.1123)	0.0002(0.0099)	0.050(0.119)	100.00(95.77)
[2023-09-29 14:25:13 10splitTasks](trainer.py 286): INFO [60/157]	0.1023(0.1110)	0.0003(0.0083)	0.143(0.119)	93.75(95.85)
[2023-09-29 14:25:14 10splitTasks](trainer.py 286): INFO [70/157]	0.1016(0.1098)	0.0003(0.0072)	0.165(0.123)	87.50(95.51)
[2023-09-29 14:25:15 10splitTasks](trainer.py 286): INFO [80/157]	0.1013(0.1089)	0.0002(0.0063)	0.092(0.132)	96.88(95.37)
[2023-09-29 14:25:16 10splitTasks](trainer.py 286): INFO [90/157]	0.1015(0.1083)	0.0002(0.0057)	0.109(0.133)	96.88(95.43)
[2023-09-29 14:25:18 10splitTasks](trainer.py 286): INFO [100/157]	0.1055(0.1078)	0.0003(0.0051)	0.062(0.134)	100.00(95.36)
[2023-09-29 14:25:19 10splitTasks](trainer.py 286): INFO [110/157]	0.1021(0.1073)	0.0001(0.0047)	0.089(0.131)	96.88(95.44)
[2023-09-29 14:25:20 10splitTasks](trainer.py 286): INFO [120/157]	0.1161(0.1070)	0.0006(0.0043)	0.025(0.132)	100.00(95.40)
[2023-09-29 14:25:21 10splitTasks](trainer.py 286): INFO [130/157]	0.1012(0.1067)	0.0002(0.0040)	0.224(0.132)	90.62(95.44)
[2023-09-29 14:25:22 10splitTasks](trainer.py 286): INFO [140/157]	0.1017(0.1063)	0.0003(0.0038)	0.024(0.132)	100.00(95.50)
[2023-09-29 14:25:23 10splitTasks](trainer.py 286): INFO [150/157]	0.1037(0.1060)	0.0002(0.0035)	0.105(0.130)	96.88(95.57)
[2023-09-29 14:25:23 10splitTasks](trainer.py 286): INFO [156/157]	0.0777(0.1057)	0.0001(0.0034)	0.388(0.130)	87.50(95.56)
[2023-09-29 14:25:23 10splitTasks](trainer.py 288): INFO  * Train Acc 95.560
[2023-09-29 14:25:25 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.800, Total time 1.63
=> Saving model to: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-9.pth
=> Save Done
[2023-09-29 14:25:25 10splitTasks](iBatchLearn.py 131): INFO validation split name:0
[2023-09-29 14:25:27 10splitTasks](my_trainer.py 503): INFO  * Val Acc 79.400, Total time 1.77
[2023-09-29 14:25:27 10splitTasks](iBatchLearn.py 131): INFO validation split name:1
[2023-09-29 14:25:29 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.000, Total time 1.70
[2023-09-29 14:25:29 10splitTasks](iBatchLearn.py 131): INFO validation split name:2
[2023-09-29 14:25:30 10splitTasks](my_trainer.py 503): INFO  * Val Acc 80.000, Total time 1.74
[2023-09-29 14:25:30 10splitTasks](iBatchLearn.py 131): INFO validation split name:3
[2023-09-29 14:25:32 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.200, Total time 1.78
[2023-09-29 14:25:32 10splitTasks](iBatchLearn.py 131): INFO validation split name:4
[2023-09-29 14:25:34 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.800, Total time 1.69
[2023-09-29 14:25:34 10splitTasks](iBatchLearn.py 131): INFO validation split name:5
[2023-09-29 14:25:36 10splitTasks](my_trainer.py 503): INFO  * Val Acc 81.600, Total time 1.77
[2023-09-29 14:25:36 10splitTasks](iBatchLearn.py 131): INFO validation split name:6
[2023-09-29 14:25:37 10splitTasks](my_trainer.py 503): INFO  * Val Acc 84.400, Total time 1.73
[2023-09-29 14:25:37 10splitTasks](iBatchLearn.py 131): INFO validation split name:7
[2023-09-29 14:25:39 10splitTasks](my_trainer.py 503): INFO  * Val Acc 85.400, Total time 1.85
[2023-09-29 14:25:39 10splitTasks](iBatchLearn.py 131): INFO validation split name:8
[2023-09-29 14:25:41 10splitTasks](my_trainer.py 503): INFO  * Val Acc 82.600, Total time 1.82
[2023-09-29 14:25:41 10splitTasks](iBatchLearn.py 131): INFO validation split name:9
[2023-09-29 14:25:43 10splitTasks](my_trainer.py 503): INFO  * Val Acc 83.800, Total time 1.69
[2023-09-29 14:25:43 10splitTasks](trainer.py 335): INFO saving storage...
[2023-09-29 14:25:43 10splitTasks](trainer.py 341): INFO done
[2023-09-29 14:25:43 10splitTasks](iBatchLearn.py 155): INFO Acc:82.61999999999999; BWT:0.0;
=> merge config from utils/user_10splitTasks.yaml
=> merge config from ../official_eva/configs/10splitTasks.yaml
[2023-09-29 14:25:47 10splitTasks](iBatchLearn.py 230): INFO Full config saved to outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/config.json
[2023-09-29 14:25:47 10splitTasks](iBatchLearn.py 232): INFO AGENT:
  FIX_BN: false
  FIX_HEAD: true
  MODEL_NAME: resnet50
  MODEL_TYPE: resnet
  NAME: packnet
  REG_COEF: 0.1
  TYPE: my_trainer
DATASET:
  BATCHSIZE: 32
  NAME: 10splitTasks
  NUM_CLASSES: 100
  NUM_TASKS: 10
  NUM_WORKERS: 4
  ROOT: input/contest_data/10splitTasks
DOMAIN_INCR: false
GPUID:
- 0
LOGGER_PATH: outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59
OPT:
  GAMMA: 0.1
  LR: 0.01
  MOMENTUM: 0.9
  NAME: SGD
  SCHEDULE:
  - 10
  - 20
  WEIGHT_DECAY: 0.0
PRINT_FREQ: 10
SEED: 0
TIL: true

[2023-09-29 14:25:47 10splitTasks](iBatchLearn.py 233): INFO {"cfg": "../official_eva/configs/10splitTasks.yaml", "user_cfg": "utils/user_10splitTasks.yaml", "test": true, "task_count": 9, "init_path": "input/init_models/10splitTasks.pth", "ckpt_path": "outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-9.pth", "save_ckpt_path": null, "storage_path": "outputs/2023-09-29-12:01:59/10splitTasks/storage-9.pth", "save_storage_path": null, "dest_path": "outputs/2023-09-29-12:01:59/prediction_9.pkl", "suffix": "2023-09-29-12:01:59", "distributed": false, "is_main_process": true}
[2023-09-29 14:25:47 10splitTasks](my_trainer.py 108): INFO => Load model weights: outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-9.pth
[2023-09-29 14:25:47 10splitTasks](my_trainer.py 113): INFO => Load Done
[2023-09-29 14:25:50 10splitTasks](my_trainer.py 67): INFO load storage...
[2023-09-29 14:25:50 10splitTasks](my_trainer.py 71): INFO done
[2023-09-29 14:25:50 10splitTasks](my_trainer.py 64): INFO tensor([[3, 3, 2, 2, 4, 4, 4],
        [3, 3, 2, 4, 6, 4, 4],
        [4, 3, 3, 6, 6, 4, 4],
        [4, 3, 6, 5, 4, 4, 4],
        [4, 6, 6, 5, 4, 5, 5],
        [7, 6, 5, 4, 2, 2, 5],
        [7, 3, 5, 2, 2, 2, 3]], device='cuda:0', dtype=torch.uint8)
[2023-09-29 14:25:50 10splitTasks](iBatchLearn.py 57): INFO my_IncreResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0, inplace=False)
  (last): ModuleDict(
    (0): Linear(in_features=2048, out_features=10, bias=False)
    (1): Linear(in_features=2048, out_features=10, bias=False)
    (2): Linear(in_features=2048, out_features=10, bias=False)
    (3): Linear(in_features=2048, out_features=10, bias=False)
    (4): Linear(in_features=2048, out_features=10, bias=False)
    (5): Linear(in_features=2048, out_features=10, bias=False)
    (6): Linear(in_features=2048, out_features=10, bias=False)
    (7): Linear(in_features=2048, out_features=10, bias=False)
    (8): Linear(in_features=2048, out_features=10, bias=False)
    (9): Linear(in_features=2048, out_features=10, bias=False)
  )
)
[2023-09-29 14:25:50 10splitTasks](iBatchLearn.py 58): INFO #parameter of model:23712832
[2023-09-29 14:25:50 10splitTasks](iBatchLearn.py 167): INFO test split name:0
[2023-09-29 14:25:56 10splitTasks](iBatchLearn.py 167): INFO test split name:1
[2023-09-29 14:25:59 10splitTasks](iBatchLearn.py 167): INFO test split name:2
[2023-09-29 14:26:02 10splitTasks](iBatchLearn.py 167): INFO test split name:3
[2023-09-29 14:26:05 10splitTasks](iBatchLearn.py 167): INFO test split name:4
[2023-09-29 14:26:08 10splitTasks](iBatchLearn.py 167): INFO test split name:5
[2023-09-29 14:26:11 10splitTasks](iBatchLearn.py 167): INFO test split name:6
[2023-09-29 14:26:14 10splitTasks](iBatchLearn.py 167): INFO test split name:7
[2023-09-29 14:26:17 10splitTasks](iBatchLearn.py 167): INFO test split name:8
[2023-09-29 14:26:20 10splitTasks](iBatchLearn.py 167): INFO test split name:9
--------------------------------Official Evaluation--------------------------------
9 82.46000000000001
4splitDomains_0, 90.44M, 2023-09-29 12:09:22, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-0.pth
4splitDomains_0, 22.98M, 2023-09-29 12:09:24, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/storage-0.pth
4splitDomains_1, 90.44M, 2023-09-29 12:15:25, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-1.pth
4splitDomains_1, 23.47M, 2023-09-29 12:15:30, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/storage-1.pth
4splitDomains_2, 90.44M, 2023-09-29 12:30:30, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-2.pth
4splitDomains_2, 23.95M, 2023-09-29 12:30:41, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/storage-2.pth
4splitDomains_3, 90.44M, 2023-09-29 12:35:48, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/checkpoint-3.pth
4splitDomains_3, 24.43M, 2023-09-29 12:36:04, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/4splitDomains/storage-3.pth
10splitTasks_0, 90.76M, 2023-09-29 12:55:17, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-0.pth
10splitTasks_0, 22.87M, 2023-09-29 12:55:18, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-0.pth
10splitTasks_1, 90.76M, 2023-09-29 13:05:00, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-1.pth
10splitTasks_1, 23.35M, 2023-09-29 13:05:04, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-1.pth
10splitTasks_2, 90.76M, 2023-09-29 13:14:47, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-2.pth
10splitTasks_2, 23.83M, 2023-09-29 13:14:53, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-2.pth
10splitTasks_3, 90.76M, 2023-09-29 13:24:40, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-3.pth
10splitTasks_3, 24.32M, 2023-09-29 13:24:48, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-3.pth
10splitTasks_4, 90.76M, 2023-09-29 13:34:32, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-4.pth
10splitTasks_4, 24.8M, 2023-09-29 13:34:41, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-4.pth
10splitTasks_5, 90.76M, 2023-09-29 13:44:28, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-5.pth
10splitTasks_5, 25.28M, 2023-09-29 13:44:39, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-5.pth
10splitTasks_6, 90.76M, 2023-09-29 13:54:34, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-6.pth
10splitTasks_6, 25.77M, 2023-09-29 13:54:46, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-6.pth
10splitTasks_6, 1.09M, 2023-09-29 13:54:46, /work/home/pzds_a002/contest/code_/log.txt
10splitTasks_7, 90.76M, 2023-09-29 14:04:48, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-7.pth
10splitTasks_7, 26.25M, 2023-09-29 14:05:02, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-7.pth
10splitTasks_7, 1.2M, 2023-09-29 14:05:02, /work/home/pzds_a002/contest/code_/log.txt
10splitTasks_8, 90.76M, 2023-09-29 14:15:09, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-8.pth
10splitTasks_8, 26.74M, 2023-09-29 14:15:26, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-8.pth
10splitTasks_8, 1.3M, 2023-09-29 14:15:26, /work/home/pzds_a002/contest/code_/log.txt
10splitTasks_9, 1.06M, 2023-09-29 14:25:43, /work/home/pzds_a002/contest/code_/outputs/10splitTasks/my_trainer-packnet-2023-09-29-12:01:59/10splitTasks.txt
10splitTasks_9, 90.76M, 2023-09-29 14:25:25, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/checkpoint-9.pth
10splitTasks_9, 27.22M, 2023-09-29 14:25:43, /work/home/pzds_a002/contest/code_/outputs/2023-09-29-12:01:59/10splitTasks/storage-9.pth
10splitTasks_9, 1.41M, 2023-09-29 14:25:43, /work/home/pzds_a002/contest/code_/log.txt
--------------------------------Final Official Evaluation--------------------------------
81.44040890611831
